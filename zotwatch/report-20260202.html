<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-02 11:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖3篇关于场景图生成的论文、1篇关于电路图解析的论文和1篇关于工业知识抽取的论文。</p>
            
            <p><strong class="text-accent">场景图生成</strong>：《Lifelong Scene Graph Generation》提出终身学习范式，使模型在新数据到来时无需重训即可持续更新场景图；《SimGraph》构建统一框架，将场景图同时用于图像生成与编辑，实现双向任务协同；两者共同推进了动态、可控的视觉语义建模。</p>
            
            <p><strong class="text-accent">电路图解析</strong>：《SINA》利用深度学习把电路原理图图像直接转换成可综合的网表，解决元件识别与连线推断难题，为电子设计自动化提供开源端到端工具。</p>
            
            <p><strong class="text-accent">工业知识抽取</strong>：《Procedural Knowledge Extraction from Industrial Troubleshooting Guides》借助视觉-语言模型解析维修手册中的流程图与文本，自动提取可执行诊断步骤，实现工业运维知识的数字化迁移。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于场景图与多模态生成的论文、6篇关于视觉-语言模型推理与评测的论文、5篇关于遥感与事件视觉理解的论文、4篇关于视频检索与异常检测的论文、3篇关于高效检索与重排序的论文、2篇关于相机位姿与3D感知的论文、2篇关于多模态提示与异常定位的论文。</p>
            
            <p><strong class="text-text-secondary">场景图生成</strong>：《Lifelong Scene Graph Generation》提出终身式场景图生成框架，避免重复训练；《SimGraph》统一了基于场景图的图像生成与编辑；另有研究将场景图用于跨模态生成与可控编辑，推动图-文双向合成。</p>
            
            <p><strong class="text-text-secondary">多模态生成</strong>：《SimGraph》利用场景图驱动生成与编辑同框架；《CoVA》实现文本引导的组合视频检索与再生成；其余工作探索文本-图像-视频联合生成与多模态扩散模型优化。</p>
            
            <p><strong class="text-text-secondary">VLM推理增强</strong>：《FRISM》通过子空间级模型融合把大推理模型能力注入视觉-语言模型；《Lost in Space?》揭示VLM在相对相机位姿估计上的3D空间推理短板；相关研究继续提出几何-语义联合训练策略。</p>
            
            <p><strong class="text-text-secondary">VLM评测</strong>：《A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions》系统评测VLM在自动驾驶SOTIF场景下的检测安全性；其余论文进一步建立VLMs在鲁棒性、跨域与长尾设定下的基准指标。</p>
            
            <p><strong class="text-text-secondary">遥感分割</strong>：《Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery》提出双向交叉感知实现开放词表遥感语义分割；同主题工作聚焦高分辨率影像的细粒度边界定位与跨域泛化。</p>
            
            <p><strong class="text-text-secondary">事件视觉</strong>：《Segment Any Events with Language》首次用自由语言提示分割事件相机数据，建立事件-语言对齐基准；后续研究提升事件流的时序一致性与跨模态检索精度。</p>
            
            <p><strong class="text-text-secondary">视频检索</strong>：《CoVA》引入文本引导的组合视频检索；《Generative Recall, Dense Reranking》提出多视角语义ID生成-重排序两阶段框架，实现高效文本到视频召回。</p>
            
            <p><strong class="text-text-secondary">异常检测</strong>：《PromptMAD》通过跨模态提示完成多类视觉异常定位；另一篇工作结合场景图与语言描述提升 camouflaged 缺陷检测的语义可解释性。</p>
            
            <p><strong class="text-text-secondary">高效检索</strong>：《Generative Recall, Dense Reranking》用生成式召回与稠密重排序缓解大规模文本-视频检索的算力瓶颈；同组论文探索语义ID、哈希与量化技术降低存储与延迟。</p>
            
            <p><strong class="text-text-secondary">相机位姿</strong>：《Lost in Space?》系统评估VLM在相对相机位姿估计上的不足；补充工作提出结合语言描述的几何验证与神经辐射场微调策略提升3D一致性。</p>
            
            <p><strong class="text-text-secondary">多模态提示</strong>：《PromptMAD》设计跨模态提示进行多类异常定位；另一篇研究在开放世界检测中利用语言提示动态调整视觉 backbone 特征，实现零样本迁移。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SimGraph：基于场景图的图像生成与编辑统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thanh-Nhan Vo，Trong-Thuan Nguyen，Tam V. Nguyen，Minh-Triet Tran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时实现基于场景图的图像生成与编辑，并保持空间一致性与语义连贯性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SimGraph，将场景图驱动的 token 生成与扩散编辑整合到单一模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明 SimGraph 在生成与编辑质量、空间一致性上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把场景图控制的生成与扩散编辑耦合为统一流程，实现对象关系与布局的精细同步调控。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要结构化场景控制的研究者提供高效一体化工具，推动可控生成与交互式编辑的发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管生成式 AI 在图像生成与编辑上均取得显著进展，但现有方法通常将二者割裂处理，导致空间一致性与语义连贯性难以保持。缺乏对物体关系与空间布局的结构化控制，进一步限制了用户对复杂场景的精准操控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimGraph 提出统一框架，将场景图同时驱动生成与编辑：先用图神经网络编码物体及关系，生成 token-based  latent 布局，再输入扩散模型完成图像合成；编辑阶段直接对图结构进行增删改，模型在共享 latent 空间内重采样，实现局部更新并继承未修改区域。整个流程端到端训练，损失函数联合优化重建、对抗与图一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome 与 COCO 数据集上的定量实验表明，SimGraph 在 FID、IS、关系准确率、布局一致性四项指标上均优于专用生成或编辑基线，用户研究偏好率提升 18% 以上；同一模型可在 512×512 图像上实现秒级编辑，且多次编辑后物体身份与位置漂移小于 2 像素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖输入场景图的准确性，若关系标注缺失或错误会直接影响生成质量；目前仅针对静态图像，尚未扩展到动态场景或文本驱动的开放词汇编辑；扩散分支计算开销仍高于纯 token 生成模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入文本-图联合 conditioning 支持开放词汇指令，并探索时序场景图以实现视频级别的长程一致编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注结构化生成、可控编辑、图神经网络与扩散模型融合的研究者，该文提供了统一的训练与推理范式以及可复现的代码基线，可直接作为对比基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113132" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lifelong Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">终身场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao He，Xin Hu，Tongtong Wu，Dongyang Zhang，Ming Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113132" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113132</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) aims to predict visual relationships between object pairs in an image. Existing SGG approaches typically adopt a one-time training paradigm, which requires retraining on the entire dataset when new relationship types emerge-an impractical solution that leads to catastrophic forgetting. In this work, we introduce Lifelong Scene Graph Generation (LSGG), a challenging and practical setting where predicates arrive sequentially in a streaming fashion. We propose ICSGG, a novel in-context learning framework that reformulates visual features into symbolic textual tokens compatible with pre-trained language models. To retain prior knowledge while adapting to new tasks, ICSGG employs a knowledge-aware prompt retrieval strategy that selects relevant exemplars as in-context demonstrations for each query. This enables effective continual learning through prompt-based reasoning. Extensive experiments on two large-scale benchmarks-Visual Genome (VG) and Open Images v 6 -demonstrate that our method significantly outperforms existing SGG models in both lifelong and conventional settings, e.g., with about 4 ∼ 5% points better than the state-of-the-art PGSG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决场景图生成在新增谓词时需全量重训、灾难性遗忘的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ICSGG框架，将视觉特征转为文本符号，用知识感知提示检索实现持续学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VG与OIv6上，终身与常规设定均优于SOTA约4–5个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把终身学习引入SGG，用上下文提示推理免微调即可增量吸收新关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系检测提供可扩展的持续学习范式，减少重训成本并保留旧知识。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有场景图生成(SGG)模型采用一次性训练范式，一旦新增谓词类别就必须在整个数据集上重新训练，既耗时又引发灾难性遗忘。随着视觉关系数据持续涌现，亟需一种无需重训即可持续吸收新关系的终身学习范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出终身场景图生成(LSGG)设定，使谓词类别以流式顺序到达，并设计ICSGG框架，将视觉特征转化为符号化文本token，从而直接调用预训练语言模型。ICSGG通过知识感知的提示检索策略，从历史样本中挑选与查询语义最相关的示例作为上下文，引导语言模型进行基于提示的推理，实现旧知识保持与新任务适应的平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Visual Genome和Open Images v6两大基准上，ICSGG在终身学习场景下比现有SGG方法平均提升4–5个百分点，同时在传统一次性训练设定中亦刷新最佳成绩，验证了符号化文本+上下文推理对持续关系预测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练语言模型，计算与存储开销较大；符号化转换过程可能丢失细粒度视觉细节；提示检索依赖历史示例质量，若旧数据分布与新任务差异过大，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索视觉-语言联合提示机制以减少信息损失，并引入动态网络结构或记忆巩固策略，进一步降低对语言模型规模的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为持续学习、视觉关系检测及多模态融合领域提供了可扩展的符号化提示范式，对研究灾难性遗忘、流式数据增量学习和场景图生成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22114v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SINA：一种基于人工智能的电路原理图到网表生成器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Saoud Aldowaish，Yashwanth Karumanchi，Kai-Chen Chiang，Soroosh Noorzad，Morteza Fayazi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22114v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将电路原理图图像自动转换为机器可读网表并克服元件识别与连线推断难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合深度学习检测元件、CCL提取连线、OCR读取位号及VLM校正位号分配</p>
                <p><span class="font-medium text-accent">主要发现：</span>SINA整体网表生成准确率达96.47%，为现有最佳方法的2.72倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言模型引入原理图位号分配，实现端到端开源自动网表生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为EDA、硬件逆向与文档自动化提供高精度、可复现的网表提取工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有电路原理图图像到网表的自动化流程在元件符号识别与导线连通性推断两方面准确率不足，导致手工返工成本高，阻碍了硬件逆向设计与文档数字化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SINA 采用级联式 AI 管线：首先用基于深度学习的对象检测网络定位并分类元件符号；随后对二值化走线层执行连通域标记(CCL)提取节点与导线拓扑；并行引入 OCR 读取图中所有字符，再由视觉-语言模型(VLM)将字符与对应元件实例进行语义匹配，从而恢复参考位号；最后将符号、引脚与节点信息整合为 SPICE 兼容的网表。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含 1 500 张手工与合成原理图的基准上，SINA 整体网表生成准确率达 96.47%，比现有最佳方法提高 2.72 倍；组件检测 F1 98.1%，连通性提取 F1 97.4%，参考位号分配准确率 95.8%，且端到端单张图平均耗时 1.3 s，验证了全流程自动化的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练数据与代码细节，难以复现；对多页/层次式原理图、弯曲导线或密集总线的鲁棒性未评估；VLM 依赖外部语言模型，可能在高噪声扫描图上出现语义错配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至 PCB 布局图网表提取，并引入图神经网络对跨页信号名进行全局一致性推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注硬件逆向、EDA 自动化或文档图像理解，SINA 提供了可落地的深度学习+图处理融合范式及开源基准，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Personalization Turing Test
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉个性化图灵测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rameen Abdal，James Burgess，Sergey Tulyakov，Kuan-Chieh Jackson Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以“像本人会发”而非“复制本人脸”来评测视觉个性化生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VPTT框架：10k人设基准+VPRAG生成器+仅用文本的VPTT评分，并用人类/VLM校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VPTT评分与人类/VLM判断高度相关，VPRAG在贴合度与原创性间取得最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图灵测试思想用于视觉个性化，用 perceptual indistinguishability 替代身份重建并给出可扩展文本指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私安全、可扩展的个性化生成提供新评测标准与基线模型，推动人机共创研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>个性化生成模型通常以“身份保真”作为金标准，却忽视了输出是否真正符合目标用户可能自行创作或分享的内容语境。作者认为，真正的“个性化”应达到人类（或校准的视觉-语言模型）无法区分生成内容与该用户真实创作的感知水平，因此提出以“感知不可区分性”取代传统身份复制指标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文定义了 Visual Personalization Turing Test (VPTT)：若受试者（人或 VLM）无法分辨生成样本与目标人可能发布的真实样本，则模型通过测试。为此构建了含 10k 虚拟人格的 VPTT-Bench，每个 persona 配有文本自述与真实多媒体；设计视觉检索增强生成器 VPRAG，在扩散过程中动态注入 persona 相关图文记忆；提出仅用文本对比的 VPTT Score，用人类与 VLM 的 0/1 判断作为监督信号进行校准，实现无需成对图像即可评估个性化逼真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>人类、GPT-4V 与 VPTT Score 三者判断的 Kendall τ&gt;0.82，验证了文本指标可作为高效感知代理；VPRAG 在 VPTT-Bench 上达到 68.3% 的“通过”率，显著优于 DreamBooth、LoRA 等基线，同时保持更高的内容原创度（FID 降低 1.8，CLIP-OD 提升 12%）。实验显示，VPRAG 在隐私保护（仅使用公开自述文本）与可扩展性（线性增加 0.3 s/样本）方面亦优于需多幅私照的对比方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VPTT-Bench 的 10k persona 以英语、北美语境为主，文化多样性有限；VPTT Score 依赖 VLM 的文本-视觉对齐能力，对低资源语言或反事实场景可能失效；实验仅评估了静态图像与短视频，尚未验证在 3D 资产、长视频等模态上的可靠性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展跨文化、多语言 persona 库并引入时序一致性约束，以检验长时序内容与交互式场景下的个性化逼真度；同时研究免检索、可端侧部署的轻量策略，实现移动设备上的实时个性化生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注个性化生成、隐私安全评估或视觉 Turing Test 的研究者，本文提供了可复用的 10k 人格基准、校准的文本评估协议以及检索增强框架，可直接比较新方法在“感知不可区分”标准下的表现，并避免收集敏感私照带来的伦理风险。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.36</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22754v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用视觉语言模型从工业故障排除指南中提取程序性知识</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guillermo Gil de Avalle，Laura Maruster，Christos Emmanouilidis
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22754v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动把工业故障排查流程图转化为机器可读的诊断知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用两种提示策略对比评估两款视觉-语言模型提取结构化知识的效果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在布局敏感度与语义鲁棒性间存在权衡，影响实际部署选择。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对工业流程图提出布局感知提示，量化VLM提取诊断知识性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建智能维修助手提供可扩展的知识获取方案，减少人工标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业维修指南通常以流程图式图表呈现诊断步骤，空间布局与专业术语共同承载知识，但人工将其转录为可机读格式耗时且易错。为把这类知识集成到车间操作员支持系统，需要自动提取并结构化流程知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选用两种 Vision-Language Model（VLM），在真实工业维修手册页面上对比两种提示策略：标准指令式提示与加入“布局模式线索”的增强提示。实验流程包括图像预处理、提示工程、模型推理、输出解析，并以人工标注的结构化流程图作为金标准进行评价。评估指标涵盖布局元素检测准确率、语义关系抽取 F1、以及整体流程图可执行性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>增强提示在布局敏感型 VLM 上显著提升节点与连边检测的 F1（+12–18 pp），但对语义鲁棒性强的模型反而引入噪声，导致文本实体识别下降 5–7 pp。两种模型呈现“布局敏感度–语义鲁棒性”权衡：高布局敏感度模型更适合版式规范的指南，而语义鲁棒型模型在图文混排复杂或低分辨率扫描件上更稳定。结果提示部署时应根据手册版式质量选择模型与提示策略。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖来自两家制造商的 367 页英文指南，语种与行业多样性不足；未对多栏、跨页及彩色高亮等复杂版式进行细分测试。作者未披露所用 VLM 的具体版本与参数量，也未评估长流程图中的长程依赖抽取效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多语种、多行业数据集，并引入领域自适应预训练以提升语义鲁棒性；同时探索将图神经网络与 VLM 融合，显式建模流程图的拓扑结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业知识图谱构建、流程图自动解析、或图文混合文档的智能抽取，该文提供了 VLM 在真实工业场景下的基准实验与提示工程经验，可直接借鉴其评估框架与权衡分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113132" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lifelong Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">终身场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao He，Xin Hu，Tongtong Wu，Dongyang Zhang，Ming Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113132" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113132</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) aims to predict visual relationships between object pairs in an image. Existing SGG approaches typically adopt a one-time training paradigm, which requires retraining on the entire dataset when new relationship types emerge-an impractical solution that leads to catastrophic forgetting. In this work, we introduce Lifelong Scene Graph Generation (LSGG), a challenging and practical setting where predicates arrive sequentially in a streaming fashion. We propose ICSGG, a novel in-context learning framework that reformulates visual features into symbolic textual tokens compatible with pre-trained language models. To retain prior knowledge while adapting to new tasks, ICSGG employs a knowledge-aware prompt retrieval strategy that selects relevant exemplars as in-context demonstrations for each query. This enables effective continual learning through prompt-based reasoning. Extensive experiments on two large-scale benchmarks-Visual Genome (VG) and Open Images v 6 -demonstrate that our method significantly outperforms existing SGG models in both lifelong and conventional settings, e.g., with about 4 ∼ 5% points better than the state-of-the-art PGSG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决场景图生成在新增谓词时需全量重训、灾难性遗忘的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ICSGG框架，将视觉特征转为文本符号，用知识感知提示检索实现持续学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VG与OIv6上，终身与常规设定均优于SOTA约4–5个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把终身学习引入SGG，用上下文提示推理免微调即可增量吸收新关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系检测提供可扩展的持续学习范式，减少重训成本并保留旧知识。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有场景图生成(SGG)模型采用一次性训练范式，一旦新增谓词类别就必须在整个数据集上重新训练，既耗时又引发灾难性遗忘。随着视觉关系数据持续涌现，亟需一种无需重训即可持续吸收新关系的终身学习范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出终身场景图生成(LSGG)设定，使谓词类别以流式顺序到达，并设计ICSGG框架，将视觉特征转化为符号化文本token，从而直接调用预训练语言模型。ICSGG通过知识感知的提示检索策略，从历史样本中挑选与查询语义最相关的示例作为上下文，引导语言模型进行基于提示的推理，实现旧知识保持与新任务适应的平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Visual Genome和Open Images v6两大基准上，ICSGG在终身学习场景下比现有SGG方法平均提升4–5个百分点，同时在传统一次性训练设定中亦刷新最佳成绩，验证了符号化文本+上下文推理对持续关系预测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练语言模型，计算与存储开销较大；符号化转换过程可能丢失细粒度视觉细节；提示检索依赖历史示例质量，若旧数据分布与新任务差异过大，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索视觉-语言联合提示机制以减少信息损失，并引入动态网络结构或记忆巩固策略，进一步降低对语言模型规模的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为持续学习、视觉关系检测及多模态融合领域提供了可扩展的符号化提示范式，对研究灾难性遗忘、流式数据增量学习和场景图生成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SimGraph：基于场景图的图像生成与编辑统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thanh-Nhan Vo，Trong-Thuan Nguyen，Tam V. Nguyen，Minh-Triet Tran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时实现基于场景图的图像生成与编辑，并保持空间一致性与语义连贯性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SimGraph，将场景图驱动的 token 生成与扩散编辑整合到单一模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明 SimGraph 在生成与编辑质量、空间一致性上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把场景图控制的生成与扩散编辑耦合为统一流程，实现对象关系与布局的精细同步调控。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要结构化场景控制的研究者提供高效一体化工具，推动可控生成与交互式编辑的发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管生成式 AI 在图像生成与编辑上均取得显著进展，但现有方法通常将二者割裂处理，导致空间一致性与语义连贯性难以保持。缺乏对物体关系与空间布局的结构化控制，进一步限制了用户对复杂场景的精准操控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimGraph 提出统一框架，将场景图同时驱动生成与编辑：先用图神经网络编码物体及关系，生成 token-based  latent 布局，再输入扩散模型完成图像合成；编辑阶段直接对图结构进行增删改，模型在共享 latent 空间内重采样，实现局部更新并继承未修改区域。整个流程端到端训练，损失函数联合优化重建、对抗与图一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome 与 COCO 数据集上的定量实验表明，SimGraph 在 FID、IS、关系准确率、布局一致性四项指标上均优于专用生成或编辑基线，用户研究偏好率提升 18% 以上；同一模型可在 512×512 图像上实现秒级编辑，且多次编辑后物体身份与位置漂移小于 2 像素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖输入场景图的准确性，若关系标注缺失或错误会直接影响生成质量；目前仅针对静态图像，尚未扩展到动态场景或文本驱动的开放词汇编辑；扩散分支计算开销仍高于纯 token 生成模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入文本-图联合 conditioning 支持开放词汇指令，并探索时序场景图以实现视频级别的长程一致编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注结构化生成、可控编辑、图神经网络与扩散模型融合的研究者，该文提供了统一的训练与推理范式以及可复现的代码基线，可直接作为对比基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21159v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像开放词汇语义分割的双向交叉感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianzheng Wang，Huan Ni
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21159v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using &#34;one-way injection&#34; and &#34;shallow post-processing&#34; strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>训练无关开放词汇遥感语义分割在高密度目标与复杂边界下精度不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SDCI框架：跨模型注意力融合、双向图扩散、超像素凸优化协同预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试优于现有无训练方法，消融验证超像素仍具价值。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双向跨感知注入与图扩散协同的无训练OVSS框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇解析提供即插即用的高精度方案，融合传统超像素与深度学习。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中地物密集、边界复杂，对几何定位与语义预测同时提出高要求；现有无需训练的开放词汇语义分割(OVSS)方法普遍采用“单向注入+浅后处理”融合CLIP与视觉基础模型，难以满足精细分割需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SDCI框架，在特征编码阶段用Cross-Model Attention Fusion(CAF)模块让CLIP与VFM的自注意力图互相注入，实现协同推理；随后设计Bidirectional Cross-Graph Diffusion Refinement(BCDR)，通过迭代随机游走扩散对双分支分割得分进行双向优化；最后引入低层超像素结构，提出基于凸优化的Superpixel Collaborative Prediction(CSCP)进一步精修对象边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个遥感语义分割基准上的实验表明，SDCI无需任何训练即可超越现有OVSS方法；消融实验证实传统面向对象的超像素策略在深度学习框架下依然有效，且CAF、BCDR、CSCP各模块均带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练CLIP与VFM，若基础模型在遥感域表征不足则性能受限；超像素参数与图扩散步数需人工调节，对巨幅影像的内存与计算开销尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应超像素生成与轻量化图扩散，以提升巨幅遥感影像的效率；或将SDCI扩展至时空序列与多源遥感数据，实现动态开放词汇监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无需训练的开放词汇遥感语义分割提供新基准，其跨模型双向协同与超像素再优化思路可直接迁移至其他遥感解析任务，对研究基础模型融合、边界精修或无监督域适应的学者具有参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21187v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FRISM：通过子空间级模型合并向视觉-语言模型注入细粒度推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyu Huang，Peng Ye，Xudong Tan，Jinhan Mu，Shenghe Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21187v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&#39;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在向VLM注入LRM推理能力时避免视觉能力退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SVD的子空间级任务向量分解+自适应缩放系数学习+无标签自蒸馏双目标优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项视觉推理基准上取得SOTA，同时保持原视觉性能不降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出子空间级细粒度模型合并框架，实现推理与视觉能力解耦注入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效融合大模型能力提供细粒度方法，兼顾性能与计算成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)与大型推理模型(LRM)的融合多停留在粗粒度层级别，导致推理能力注入与视觉能力保持之间出现此消彼长的权衡。作者观察到推理能力被编码在参数空间的特定子空间而非整个层，因此提出在子空间级别进行精细融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FRISM首先对LRM的任务向量进行奇异值分解(SVD)，得到若干正交子空间；随后为每个子空间引入可学习的缩放系数，通过无标签自蒸馏在通用视觉-语言感知数据上优化双目标函数——既保留VLM的感知性能又最大化推理增益；最终仅将加权后的子空间增量注入到原始VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MathVista、MMMU、MMBench等多样化视觉推理基准上，FRISM持续取得SOTA，平均提升3-5个百分点，同时保持与原模型在图像描述、VQA等感知任务上的零掉点表现；消融实验表明子空间级注入比层级别减少约40%的视觉能力损失。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对LRM任务向量的完整获取，闭源或仅提供API的推理模型难以适用；SVD分解带来的额外显存与计算开销随模型规模线性增长，对十亿参数级模型实施仍具挑战；目前仅在英文与通用领域验证，跨语言或专业领域泛化性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于低秩近似或稀疏掩码的更高效子空间选择，以及将FRISM扩展至多模态链式推理与自主智能体场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、模型融合或参数高效迁移，本文提供的子空间级视角和无监督自蒸馏策略可直接借鉴，用于在保持原有能力的同时为VLMs注入数学、科学或代码等特定推理技能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23159v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Segment Any Events with Language
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用语言分割任意事件</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seungjun Lee，Gim Hee Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23159v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>首次解决开放词汇事件实例分割（OV-EIS），实现语言驱动的事件像素级细粒度理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SEAL框架，用视觉-语言对齐与参数高效结构，统一完成事件分割与开放词汇掩码分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SEAL在自建四级基准上显著优于基线，兼顾高精度与速度，无需视觉提示的变体亦具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创语义感知的事件任意分割框架，支持实例/部件级开放词汇分类并构建OV-EIS评测基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为事件相机社区提供语言交互式细粒度理解工具与标准，推动低功耗动态场景智能应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>事件相机以微秒级延迟输出稀疏的亮度变化，天然适合高速、低功耗场景，但现有研究多停留在语义分类层面，缺乏像素级、开放词汇的细粒度理解。作者观察到图像/点云领域已广泛利用自由语言进行多粒度场景解析，而事件流在这一方向几乎空白，因此提出用自然语言驱动事件实例分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SEAL 框架将事件体素化后送入轻量级编码器提取时空特征，再用跨模态 Transformer 将语言查询与事件特征对齐，实现开放词汇的实例掩膜预测；同一网络可同时接受视觉提示（如首帧图像或手绘框）完成 promptable segmentation，也能在无提示模式下通过时序自回归完成通用时空分割。为训练 OV-EIS，作者构建了四档粒度（粗类→细类→实例→部件）的伪标签生成流程，利用大型视觉-语言模型在同步 RGB 帧上产生掩膜，再投影到事件空间进行弱监督学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四个基准上，SEAL 比纯事件或 RGB-E 混合基线的 AP 平均提升 8–15 个点，而参数量仅为其 1/3，推理速度提高 1.7×；开放词汇分类的 top-1 准确率在实例级达到 72 %，部件级仍保持 59 %，显著优于零样本 CLIP 事件迁移方案。无提示变体在 240 fps 事件流上实现在线分割，延迟低于 4 ms，展示了事件相机实时细粒度理解的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前伪标签依赖同步 RGB 帧，若场景光照极端或 RGB 缺失则标签质量下降；语言查询需为名词短语，对复杂关系或否定句式鲁棒性不足；此外，事件流对纹理缺失区域容易产生漏分割，导致细小部件召回率偏低。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索纯事件自监督伪标签生成以摆脱 RGB 依赖，并引入时序语言描述实现事件视频的持续指代分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注事件相机、开放词汇视觉任务或高效 Transformer 架构，SEAL 提供了首个可语言驱动的事件实例分割基准与代码，可直接作为实验对比或扩展多模态时序理解方法的基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迷失在空间中？视觉-语言模型在相对相机位姿估计上的困境</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ken Deng，Yifu Qiu，Yoni Kasten，Shay B. Cohen，Yftah Ziser
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型能否估计两帧图像间的相对相机位姿。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建VRRPI-Bench与VRRPI-Diag基准，用自然语言描述相机运动并测试多模型表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>主流VLMs严重落后几何基线与人类，深度与滚转估计尤差，多帧一致性仅约六成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化VLMs在真实3D相对位姿估计上的缺陷并公开语言化视频基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示VLMs缺乏3D空间推理能力，为提升多模态模型几何理解指明改进方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have achieved impressive results on 2D vision-language tasks, yet their grasp of 3D spatial relationships remains largely unexplored. Relative camera pose estimation—recovering the 6-DoF transformation between two viewpoints—is a classic 3D vision problem that demands geometric reasoning rather than lexical alignment. The authors hypothesize that VLMs’ language priors and 2D pre-training leave them ill-equipped for such multi-view, metric-space inference.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The study curates VRRPI-Bench, a large-scale benchmark extracted from unlabeled egocentric videos that are automatically annotated with natural-language descriptions of simultaneous translation and rotation around an object. To isolate failure modes, they introduce VRRPI-Diag, a synthetic dataset that varies one motion parameter (depth, roll, yaw, pitch, lateral shift) at a time. Prompts are standardized to force models to output scalar rotation angles and translation ratios, enabling direct comparison with classical structure-from-motion (SfM) pipelines and human subjects.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Even the strongest VLM (GPT-5) attains only 64 % accuracy on VRRPI-Bench, far below a simple geometric baseline (97 %) and human performance (92 %). Models collapse on depth changes and roll angles, regressing to 2D optical-flow heuristics such as “larger object means closer.” Multi-image prompting yields inconsistent gains (best 59.7 %), indicating that VLMs cannot reliably integrate spatial cues across frames. These results highlight a systematic inability to ground linguistic representations in metric 3D space.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The benchmark is limited to table-top, object-centric scenes and may not reflect outdoor or large-scale environments. Automatic language annotation relies on SLAM trajectories that themselves contain drift, potentially injecting label noise. The study only probes zero-shot inference; fine-tuning protocols or chain-of-thought prompting might partially mitigate observed failures.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore 3D-aware pre-training objectives that explicitly predict ego-motion from paired images, or hybrid architectures that fuse VLM tokens with differentiable pose regression layers.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating 3D scene understanding, embodied AI navigation, or geometric reasoning in multimodal models will find the benchmark and diagnostic splits useful for quantifying how much “3D awareness” current VLMs actually possess.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOTIF条件下二维目标检测的大型视觉-语言模型比较评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ji Zhou，Yilin Ding，Yongqi Zhao，Jiachen Xu，Arno Eichberger
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估大视觉-语言模型在SOTIF条件下2D目标检测的安全可用性</p>
                <p><span class="font-medium text-accent">研究方法：</span>用PeSOTIF数据集量化对比10种LVLM与YOLO在长尾与退化场景的表现</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级LVLM在复杂自然场景召回率超YOLO 25%，但在合成几何扰动上精度落后</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化LVLM与经典检测器在SOTIF安全感知中的互补性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶安全验证提供将LVLM作为高层语义校验器的实证依据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可靠的环境感知是自动驾驶安全落地的核心瓶颈，而SOTIF（预期功能安全）关注的是感知不足在雨雾、强光、长尾场景等不利条件下带来的残余风险。传统YOLO类检测器在这些情况下召回率骤降，亟需评估具备语义推理能力的大型视觉-语言模型（LVLMs）能否作为安全补充。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建并公开了面向SOTIF的PeSOTIF基准，涵盖长尾交通目标与18种视觉退化（合成+自然）。选取10个代表性LVLMs（Gemini、Doubao、Qwen-VL等），统一采用zero-shot链式思维提示进行2D目标检测。以YOLOv8x作为经典几何回归基线，采用COCO-style mAP、Recall、Precision以及SOTIF-sensitive的漏检率、误检率进行量化对比，并分场景做显著性检验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂自然退化场景下，最佳LVLMs（Gemini 3、Doubao）比YOLO基线召回率提升&gt;25%，对雨雾、逆光等表现出更强鲁棒性；但在合成几何扰动（旋转、缩放、补丁攻击）中，YOLO的定位精度仍显著优于LVLMs。实验揭示“语义推理-几何回归”互补曲线：LVLMs擅长凭常识补全遮挡/模糊目标，却缺乏像素级定位精度，适合作为高层安全验证器而非主检测器。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅评估了2D框检测，未涉及3D、速度、语义可解释性等更高阶安全需求；PeSOTIF目前规模有限，长尾类别与极端天气样本仍不足；LVLMs推理延迟高、成本大，距离车载实时应用尚有差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索LVLMs与YOLO的级联或混合架构，实现毫秒级推理下的高召回-高精度协同；同时扩展PeSOTIF至3D、多模态、可解释不确定性估计，以全面覆盖SOTIF风险空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次量化验证LVLMs在SOTIF条件下的检测效能，为研究感知长尾、鲁棒性及融合语义-几何方法的学者提供公开基准与实验洞察，可直接支撑安全验证、模型选择及标准制定。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22508v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoVA：面向视听内容的文本引导组合视频检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gyuwon Han，Young Kyun Jang，Chanho Eom
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22508v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有 CoVR 仅关注视觉差异，无法检索音频不同但视觉相似的视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 AV-Comp 基准与 AVT 模型，联合视频、音频、文本并动态对齐最相关模态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AVT 在跨模态检索中显著优于单模态融合，成为 CoVA 强基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将音频差异纳入组合视频检索，建立含视听变化的新任务与数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需同时考虑视听一致性的检索、生成与理解研究提供基准与方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Video Retrieval (CoVR) lets users search for a target video by giving a reference clip plus a text phrase that describes desired visual edits. Prior datasets and models assume the audio track is irrelevant; they therefore fail when the sought-after video looks similar but sounds different. The authors argue that real-world video retrieval must jointly reason over visual and auditory changes, motivating a new task and benchmark.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper defines CoVA—Composed retrieval for Video with its Audio—where the query text can specify both visual and audio modifications. A new dataset called AV-Comp is collected: 9 k triplets (reference video, target video, text) covering cross-modal differences such as &#34;same scene but add rain sound&#34; or &#34;same music but daytime visuals.&#34; To solve CoVA, an AVT Compositional Fusion model is proposed; it encodes video frames, audio waveform, and text into a shared embedding space, then uses a lightweight attention gate to emphasize the modality most relevant to the textual change before computing final similarity scores.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AVT yields 18–30 % relative gains in Recall@5 over naive concatenation or late-fusion baselines on AV-Comp, establishing the first strong benchmark for CoVA. Ablation shows that audio-only and visual-only branches each contribute, but selective alignment gives the largest boost, confirming the value of cross-modal reasoning. Human evaluation on 300 randomly sampled queries indicates 82 % preference for AVT rankings versus 64 % for the best unimodal competitor. The dataset and code release facilitate reproducibility and future comparisons.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>AV-Comp is still moderate in scale (9 k triplets) and English-only, so performance may not transfer to larger multilingual repositories. The AVT gate relies on pretrained audio and visual backbones; any bias in those encoders propagates to retrieval results. The benchmark focuses on short 5–10 s clips, leaving longer-form temporal compositionality unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can enlarge AV-Comp with longer videos and multilingual captions, and explore generative diffusion-based models that directly synthesize the retrieved video as additional supervision.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal retrieval, audio–visual representation learning, or text-guided video search will find CoVA a timely extension that explicitly tests auditory reasoning, offering a new dataset, metrics, and a strong reproducible baseline.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21193v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式召回与稠密重排序：面向高效文本到视频检索的多视角语义 ID 学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zecheng Zhao，Zhi Chen，Zi Huang，Shazia Sadiq，Tong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21193v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模文本-视频检索中，用生成式召回兼顾效率与召回质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GRDR：多视角语义 ID 生成式召回 + 稠密重排，联合训练文本引导分词器与生成检索器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GRDR 精度媲美稠密检索，存储降一个量级，全库检索加速 300 倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>为每视频分配多语义 ID，用共享码本桥接文本-视觉，生成式召回首次在两阶段 TVR 中超越稠密召回。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业级视频平台提供高准高效检索方案，缓解存储与延迟瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-to-Video Retrieval (TVR) is dominated by two-stage pipelines where a lightweight recall stage prunes the corpus before a heavy dense reranker. Dense recall scales linearly in storage and compute, making it impractical for real-time, billion-scale catalogs. Generative Retrieval (GR) promises constant-time/space decoding by mapping videos to compact semantic IDs, but existing GR treats each video as a single ID, ignoring its multi-faceted semantics and yielding poor recall quality.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRDR introduces a query-aware multi-view tokenizer that assigns each video K semantic IDs, each capturing a different semantic facet; the tokenizer and the generative retriever share a learnable codebook so that ID tokens act as a cross-modal bridge. During training, the tokenizer is optimized to maximize the probability that at least one of the K IDs is decoded from the relevant text query, while the retriever is trained with sequence-to-sequence loss constrained by a prefix trie. At inference, trie-guided beam search emits a small candidate set that is reranked by any state-of-the-art dense model without extra finetuning.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MSR-VTT, ActivityNet Captions and LSMDC, GRDR’s recall stage alone achieves 90–95 % of the dense retriever’s R@1 while storing the video index in 8–12 × fewer bytes and running full-corpus retrieval up to 300 × faster on GPU. When coupled with an off-the-shelf CLIP4Clip reranker, the two-stage system equals or surpasses single-stage dense baselines in nDCG and R@5, demonstrating that GRDR’s multi-view IDs effectively preserve fine-grained semantics for downstream ranking.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The number K of IDs per video is fixed and dataset-specific, risking over- or under-segmentation; the shared codebook size must be tuned to balance vocabulary coverage and decoding latency; and trie construction still requires a full scan of the ID vocabulary, which can become a bottleneck at billion-scale unless approximated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn adaptive K per video via reinforcement learning and extend GRDR to hierarchical or product-quantized codebooks to push memory and speed even further on web-scale corpora.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable cross-modal retrieval, generative search, or memory-efficient indexing will find GRDR a practical way to graft high-capacity rerankers onto near-constant-time recall, directly addressing the storage and latency pain points of large-scale video search.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22492v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PromptMAD：跨模态提示的多类别视觉异常定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Duncan McCain，Hossein Kashiani，Fatemeh Afghah
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22492v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多类无监督场景下精准定位细微与伪装视觉缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP跨模态提示+Focal损失+多尺度卷积-Transformer融合扩散精炼</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVTec-AD像素级AUC达98.35%，AP提升至66.54%，保持跨类别高效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义文本提示引入视觉重建，用Focal损失缓解像素级类别失衡</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供无需异常样本的高精度定位框架，可推广至多类缺陷检测</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多类别工业视觉异常检测长期受限于缺陷样本稀缺、纹理伪装与类别跨度大，传统基于重建或单模态特征的方法难以同时捕捉细微缺陷与语义差异。作者观察到，若能引入语言模态对“正常/异常”进行显式语义描述，可在无监督框架下缓解缺陷样本不足并提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PromptMAD 以 CLIP 为骨干，将类别特定的正常与异常文本提示映射为语义向量，与图像编码特征做跨模态对齐，把对齐得分作为重建网络的额外监督，从而把“语义上下文”注入像素级重建过程。网络主体为 CNN-Transformer 混合编码器-解码器，解码器输出多尺度特征后接入监督分割头，该头结合 Transformer 空间注意力与扩散式迭代细化，逐步精炼高分辨率异常概率图。训练阶段采用 Focal Loss 对像素损失重加权，迫使模型关注难检缺陷区域，整体框架完全无监督，仅文本提示需人工撰写。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MVTec-AD 的 15 类对象上，PromptMAD 将像素级平均 AUC 从先前最佳的 97.1% 提升到 98.35%，平均 AP 从 62.3% 提升到 66.54%，在纹理、金属、透明物体等难类别上增益最显著。语义提示的引入使模型对微小划痕、针孔等低对比度缺陷的召回率提高约 8%，而扩散细化步骤把定位边缘误差降低 1.2 像素。参数量与推理时间仍保持实时水平，验证了语义-视觉协同在工业场景的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的预训练域，若工业对象与 ImageNet 语义差距过大，文本提示可能失效；手工设计的正常/异常提示需要领域知识，提示微小变化会导致 1–2% 的性能波动。此外，扩散细化带来 30% 的额外推理延迟，对高速产线部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动提示学习或大型多模态模型生成领域专用描述，减少人工干预；将扩散步骤蒸馏为轻量级前馈网络，以满足毫秒级在线检测需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本/少样本异常检测、跨模态语义增强或工业 AI 的落地效率，PromptMAD 提供了可复用的“语言-视觉”协同范式与完整的代码基线，可直接迁移到医疗、半导体等新领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-object tracking of vehicles and anomalous states in remote sensing videos: Joint learning of historical trajectory guidance and ID prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视频中车辆与异常状态的多目标跟踪：历史轨迹引导与ID预测的联合学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Wang，Yuan Zhou，Haigang Sui，Guorui Ma，Peng Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在遥感视频中同时跟踪车辆及其受强干扰后的异常状态</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端联合学习历史轨迹引导与ID预测，含FFAM、HTFE、SCM和DMFU模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建VAS-MOT达HOTA 68.2%/MOTA 71.5%，IRTS-AG达70.9%/91.6%，均为当前最佳</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba历史轨迹流编码与稀疏注意语义聚类引入遥感车辆MOT，并构建异常状态数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事防御、交通管理与损伤评估提供高鲁棒长时小目标跟踪新基准与方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视频车辆多目标跟踪(MOT)在军事防御、交通流管理与战损评估中需求迫切，但强形变干扰下的异常状态持续跟踪仍是空白。现有方法将检测与重识别割裂，导致车辆一旦被遮挡、烟雾扰动或局部毁伤即发生ID断裂。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端联合学习框架，以Frame Feature Aggregation Module(FFAM)跨帧增强空间一致性，Historical Tracklets Flow Encoder(HTFE)利用Mamba块在潜在运动流中嵌入历史轨迹先验，Semantic-Consistent Clustering Module(SCM)通过稀疏注意力捕获全局语义，最终由Dual-branch Modulation Fusion Unit(DMFU)融合判别特征并同步输出检测框与ID。训练时采用检测-嵌入-预测三任务共享骨干，以历史轨迹作为额外监督信号，实现异常状态发生后的身份保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建VAS-MOT数据集上HOTA 68.2%、MOTA 71.5%，显著优于此前最佳基线约+5.3 HOTA；在公开IRTS-AG红外小目标长时序数据集上HOTA 70.9%、MOTA 91.6%，证明对复杂场景与小目标具有强鲁棒性。消融实验显示HTFE与SCM分别贡献约2.8与2.1 HOTA增益，验证了历史轨迹引导与全局语义一致性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧间运动流估计，在超低帧率或极端几何畸变下可能失效；Mamba块的序列建模带来额外显存开销，限制实时部署；VAS-MOT仅覆盖中分辨率光学与红外视频，缺乏SAR、多光谱等异构传感器验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或激光点云作为辅助模态，提升在烟雾、夜间等异常状态下的鲁棒性；研究轻量化状态空间模型以降低计算延迟，满足机载边缘实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事遥感视频解析、战损评估或鲁棒MOT，该文提供了首个面向异常状态保持身份的联合学习范式及公开数据集，可直接作为基准或扩展至多模态异常检测研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PathReasoner-R1：通过知识引导的策略优化将结构化推理注入病理视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songhan Jiang，Fengchun Liu，Ziyue Wang，Linghan Cai，Yongbing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让病理视觉-语言模型输出可验证、结构化的诊断推理链，而非直接结论。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含20K样本的PathReasoner数据集，用知识图对齐发现-推理-诊断，再以轨迹掩码SFT与知识感知强化学习训练PathReasoner-R1。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PathReasoner-R1在PathReasoner及公开基准上达SOTA，具备透明、临床可验证的推理能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模WSI推理数据集与知识图引导的多粒度奖励机制，实现逻辑一致而非仅结果匹配的病理链式思维。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为病理AI提供可信、可纠错的可解释推理框架，推动临床落地与专家协作。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前病理视觉-语言模型(VLM)虽视觉理解能力强，却普遍采用“端到端”直接输出诊断结论，缺乏可验证的证据链推理，导致临床可信度低且难以被专家纠错。为突破这一瓶颈，作者提出构建可解释、可追溯的病理推理范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建PathReasoner——首个面向全载玻片图像(WSI)的大规模推理数据集：借助医学知识图谱，将结构化病理发现、临床推理与诊断结论显式对齐，生成2万余条高质量指令样本。随后提出PathReasoner-R1模型，采用“轨迹掩码监督微调+面向推理的强化学习”双阶段训练，并设计知识感知多粒度奖励函数(含与知识图谱严格对齐的Entity Reward)，以逻辑一致性而非单纯结果匹配为优化目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PathReasoner自建基准及多个公开病理VQA/诊断数据集上，PathReasoner-R1在不同图像分辨率下均取得SOTA性能，同时输出具备临床可解释性的链式推理路径，显著提升了模型透明度与可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖部分癌种与染色方式，WSI级标注成本高昂导致数据规模仍受限；强化学习奖励依赖知识图谱完整性，若图谱缺失或存在噪声可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多癌种、多中心数据，并探索结合大语言模型自进化机制以动态修正知识图谱，实现持续学习与推理能力更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为构建可解释病理VLM提供了首个大规模推理数据集与知识引导训练框架，对致力于医学AI可解释性、链式推理或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23224v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-o3：面向长视频多跳推理的原生交错线索搜寻</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Zeng，Zhiqiu Zhang，Yuhan Zhu，Xinhao Li，Zikang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23224v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3&#39;s strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在冗长视频中主动定位稀疏关键证据并完成多跳推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>迭代式工具调用+任务解耦注意力掩码+可验证轨迹奖励，配合173K合成轨迹训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>MLVU达72.1%、Video-Holmes达46.5%，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现原生交错工具调用与自适应终止的长视频线索搜寻框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长视频理解提供可扩展的主动证据挖掘范式，推动多模态推理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大模型在处理长视频时普遍采用均匀采样+单轮推理，极易淹没在大量冗余帧中，难以定位稀疏却关键的多跳证据。Video-o3旨在让模型像人类侦探一样，边看边想、边调用工具，逐步锁定决定性片段，从而突破长视频多跳问答的性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架核心是一个可原生交错调用视觉工具（片段检索、慢放、放大、OCR等）的推理循环：每步先由任务解耦注意力掩码把“推理token”与“工具调用token”分离，防止异质信息互相干扰，又共享全局记忆；随后模型决定调用何种工具并解析返回结果，上下文长度随轮次增长，用可验证轨迹奖励在覆盖度与效率间动态权衡，达到证据充分即自动终止。为训练该策略，作者构建了17.3万条高质量工具交互轨迹Seeker-173K，结合有监督微调与强化学习，使模型学会何时、如何以及何时停止调用工具。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MLVU长视频理解基准上Video-o3达到72.1%准确率，比现有最佳方法提升约8个百分点；在更强调多跳推理的Video-Holmes数据集上取得46.5%准确率，领先次优模型12个百分点以上。消融实验表明，任务解耦注意力与轨迹奖励分别带来3.4%与4.8%的绝对增益，验证了原生工具调用对稀疏证据发现的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义的视觉工具集合，若任务需要新工具需额外标注与训练；多轮交互虽可控但仍使推理成本高于单轮模型，对实时场景不够友好；Seeker-173K虽规模大，但合成数据与真实人类侦探行为之间可能存在分布差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可扩展的工具库与元学习，使模型在测试时自主生成新工具；结合视频语义索引与层级记忆，进一步压缩上下文并提升实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及长视频理解、多跳推理、工具增强LLM或强化学习在视觉任务中的应用，Video-o3提供了可执行的交错推理范式、训练数据构造方案及两项可迁移的技术模块（任务解耦注意力+轨迹奖励），可直接对比或嵌入您的系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HierLoc：用于层次化视觉地理定位的双曲实体嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hari Krishna Gadi，Daniel Matos，Hongyi Luo，Lu Liu，Yongliang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23064v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模视觉地理定位中兼顾层级地理结构与高效推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用双曲空间嵌入地理实体，并以加权对比学习对齐图像与层级实体。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OSV5M上平均误差降19.5%，细粒度区域精度升43%，仅用24万实体嵌入。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双曲层级实体嵌入引入地理定位，显式编码球面距离与地理连续性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为亿级图像全球定位提供紧凑、可解释且高性能的新范式与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地理定位需在全球尺度、视觉歧义与地理层级结构下预测图像拍摄位置，传统检索需存储数百万图像嵌入，网格分类忽略地理连续性，生成模型难以保留细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以地理实体为中心的层级公式，将图像-图像检索替换为双曲空间中紧凑的地理实体层级；通过Geo-Weighted Hyperbolic对比学习把haversine距离直接纳入对比损失，使图像一次对齐国家、区域、子区域与城市实体；整个层级仅24万实体嵌入，无需保留5M参考图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OSV5M基准上，HierLoc将平均测地误差降低19.5%，细粒度子区域准确率提升43%，同时刷新SOTA；层级嵌入使预测可解释，推理存储与速度显著优于检索范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义且完整的地理实体层级，对行政边界变化或缺乏标注的地区敏感；双曲嵌入需仔细调参，对比学习受限于训练图像的地理分布偏差；极端细粒度（如街道级）仍可能因实体粒度不足而失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应层级生成与多模态（文本、GPS弱标签）联合嵌入，以提升无实体区域与街景级定位能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将双曲几何与层级对比学习引入地理定位，为研究大规模、层级结构感知、内存高效的视觉定位或跨域检索提供新范式与公开基准结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22809v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FarmMind：面向农田遥感图像的推理查询驱动动态分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haiyang Wu，Weiliang Mu，Jipeng Zhang，Zhong Dandan，Zhuofei Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22809v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破静态分割范式，在模糊农田遥感影像中实现专家式跨图推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>FarmMind 框架：先推理定位歧义根因，再按需动态查询外部辅助影像补全信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在农田遥感分割精度与泛化能力上均优于现有静态方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“推理-查询”机制引入遥感分割，模仿专家思维按需调用多源影像。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景遥感解析提供可解释、可扩展的动态分割新范式与开源基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>农田遥感影像（FRSI）语义分割是精准农业的核心技术，但现有静态范式仅依赖单张影像，难以应对尺度差异、时相变化与分辨率不足导致的视觉不确定性。人类专家在判读模糊地块时会主动检索高分辨率、更大范围或相邻时相的辅助影像进行交叉验证，这一认知过程尚未被算法化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FarmMind 构建了一个推理-查询驱动的动态分割框架：先由“推理器”对当前 patch 的预测不确定性进行归因分析，定位模糊来源（如边界混淆、类别相似或遮挡）；随后“查询策略网络”将归因结果映射为辅助影像类型与空间-时相参数，从外部库中按需拉取补充数据；最后“融合分割器”将原图与动态获取的辅助影像在特征层进行跨尺度、跨时相对齐与置信度加权，输出最终分割结果，实现“先思后问、边问边分”的闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的 5.2 万对多源 FRSI 测试集上，FarmMind 将 mIoU 从最佳静态方法的 72.4 提升到 80.1，边界 F1 提高 8.7 个百分点；跨域泛化实验表明，当目标区域影像分辨率或作物类型分布与训练集差异 30% 以上时，FarmMind 仍保持 ≥75% 相对性能，而基线方法降至 ≤60%。消融实验证实，推理-查询模块贡献了总增益的 65%，验证了“先归因再查询”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部辅助影像库的可用性与质量，若查询不到对应时相或分辨率数据则性能下降；推理-查询链引入额外计算与延迟，单幅推理时间增加约 2.3×，尚难满足实时监测需求；查询策略网络的可解释性仍较弱，难以验证其是否学到与人类专家一致的判读逻辑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线强化学习让查询策略在部署后持续适应本地数据分布，并探索边缘-云协同架构以缩短查询延迟；同时构建可解释性接口，使农业从业者能够干预与校准查询逻辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感语义分割的不确定性建模、多源数据融合或人类认知启发的动态推理机制，FarmMind 提供了一个可扩展的开源框架与数据集，可直接对比或嵌入新的归因、查询与融合模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22231v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Shi，Michael Birsak，Wenqing Cui，Zhenyu Li，Peter Wonka
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22231v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>位置编码在 ViT 中究竟是索引还是几何先验？</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计 token 级诊断指标，系统测试 14 个基础 ViT 的多视角几何一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PE 是因果性几何机制，决定空间结构；缺失或错位会显著削弱多视角一致与空间推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 PE 视为可测几何先验，提出量化其因果影响的多视角一致性探针。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解并改进 ViT 空间推理提供直接依据，指导 PE 设计与 3D 视觉模型开发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) split images into patches and add positional embeddings (PEs) to encode spatial order, but it is unclear whether these embeddings act only as indices or as deeper geometric priors. Recent work shows that ViTs can reason about 3-D geometry without explicit 3-D supervision, motivating a rigorous examination of how PEs shape spatial representations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors treat PEs as learnable geometric priors and derive token-level diagnostics that quantify multi-view geometric consistency across augmented views of the same scene. They freeze 14 foundation ViTs, ablate or shuffle PEs, and measure changes in epipolar consistency, depth ordering, and relative pose estimation. A causal intervention framework swaps or removes PEs at inference time to isolate their contribution to downstream spatial-reasoning probes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PEs are a decisive causal factor: removing or permuting them degrades multi-view consistency by 20–60 %, even when absolute accuracy stays high. Different architectures learn markedly different geometric priors—some encode metric scale, others only ordinal depth—indicating that PEs actively sculpt the representation’s spatial structure. Surprisingly, in several models spatial reasoning improves after dropping PEs, revealing that PEs can also inject harmful bias.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to frozen off-the-shelf models; results may change when PEs are jointly fine-tuned for geometric tasks. The diagnostics rely on synthetic multi-view datasets and may not generalize to in-the-wild imagery. Causal interventions manipulate PEs in isolation, ignoring coupled effects with other architectural components.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Design task-adaptive PEs that can be selectively activated or suppressed during inference to balance geometric bias and flexibility. Extend the framework to other modalities (audio, video, lidar) to test whether positional priors operate analogously across domains.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying 3-D vision from 2-D self-supervision, interpretability of attention-based models, or ways to inject geometric inductive biases without architectural redesign will find direct methodological inspiration and a reusable probe suite.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22666v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyi Hu，Tian Bai，Fengyi Wu，Wenyan Li，Zhenming Peng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22666v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在弱监督下实现细粒度开放词汇视觉-语言对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于多示例学习的期望对齐头与能量多尺度一致性正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LVIS minival AP_r 36.2，长尾检测与零样本分割显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>无额外标注的软MIL池化与拉格朗日约束自由能最小化正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级开放词汇定位提供新理论框架与实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇定位任务要求在仅有图像-句子对弱监督的条件下，实现视觉区域与文本 token 的细粒度对齐；现有方法要么使用全局句子嵌入丢失细节，要么依赖额外标注或重型跨注意力模块，难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将定位形式化为多示例学习（MIL），提出 Expectation Alignment Head，在 token-区域相似度矩阵上执行注意力式软 MIL 池化，隐式挑选关键 token 与实例而无需额外监督；进一步从拉格朗日约束自由能最小化出发，设计能量多尺度一致性正则，包括 Top-K 多正对比损失与几何感知一致性损失，以稳定对齐学习并抑制噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LVIS minival 上，ExpAlign 取得 36.2 AP_r，刷新同规模模型最佳纪录，对长尾类别的提升尤为显著；在开放词汇检测和零样本实例分割基准上均获得一致增益，且推理时仅引入轻量级对齐头，保持高效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在静态图像上验证，未探讨视频时序对齐；能量正则的超参数需针对新数据集重新调优；对极度稀有类别的增益仍受限于视觉 backbone 的表征能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将期望导向的 MIL 框架扩展至视频开放词汇定位，并结合大语言模型进行在线概念扩展；探索自适应能量正则以自动平衡不同数据分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督视觉-语言对齐、开放词汇检测或长尾识别，本文提供的可微 MIL 期望池化与能量正则思路可直接迁移并强化现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22730v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoshu Chen，Sihang Zhou，Ke Liang，Taichun Zhou，Xinwang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22730v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何压缩长链思维(CoT)以提升大模型推理效率并避免语言表层偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ImgCoT，将CoT渲染为图像作为自编码器重建目标，并辅以低似然文本关键步的松散混合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉令牌保留全局推理结构，混合少量文本关键步即可在更少令牌下保持细粒度细节，多数据集与模型均显著优于文本压缩基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉CoT替代文本重建目标，引入空间归纳偏置，并设计松散视觉-文本混合压缩策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效长链推理提供低语言偏差、高结构保真的新压缩范式，可推广至多步逻辑任务与资源受限场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型语言模型在多步推理时往往需要生成冗长的文本链式思维（CoT），带来高昂的计算与内存开销。近期研究尝试用自编码器把长CoT压缩成少量隐式token，但重建目标仍是原始文本，导致模型被迫保留词汇、语法等表层语言特征，反而淹没了真正的逻辑结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ImgCoT把重建目标从文本CoT改成将CoT渲染成图像后的视觉CoT，用Vision-AutoEncoder把长文本推理链压缩成极短的视觉潜token，引入空间归纳偏置而非语言偏置。为进一步保留关键细节，作者提出loose ImgCoT：先让LLM生成完整CoT，再按token似然度筛选最难预测的少量文本步骤，与视觉潜token共同作为推理提示。训练阶段，编码器-解码器仅优化图像重建损失，文本步骤不参与梯度回传，实现真正的“视觉抽象+文本补丁”混合推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GSM8K、MATH、ARC-E等5个推理数据集及LLaMA-2-7B、LLaMA-3-8B、GPT-J模型上的实验显示，ImgCoT用平均8-12个视觉token即可达到与完整CoT相当的准确率，比文本压缩基线平均提升4.1%并减少约70%的推理长度。loose ImgCoT在仅增加3-5个文本token的情况下，比纯视觉版本再提升1.8%，且显著降低重复、跳步等逻辑错误。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>视觉渲染方式与分辨率对空间结构编码效果敏感，极端公式或符号可能失真；loose ImgCoT仍需预生成完整CoT来挑选低似然步骤，压缩流程并非端到端，增加离线开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可微分渲染实现端到端视觉token学习，或引入动态预算机制在推理时自适应分配视觉与文本token比例。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究LLM高效推理、链式思维压缩、多模态语义抽象或视觉-语言协同机制的学者，ImgCoT提供了跳出“文本重建”框架的新范式及可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22529v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SHED Light on Segmentation for Dense Prediction
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seung Hyun Lee，Sangwoo Mo，Stella X. Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22529v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖显式分割监督的情况下，让单图稠密预测保持几何结构与边界一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SHED编码-解码架构，通过双向分层池化/反池化隐式引入分割先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型输出的深度边界更锐利、片段更连贯，跨域泛化与3D重建质量显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无监督分段层级嵌入稠密预测，实现像素级任务与全局几何联合推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D感知与机器人提供兼顾精度与结构一致性的新框架，可即插即用到现有网络。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense prediction tasks like depth estimation and semantic segmentation are typically framed as per-pixel regression or classification, ignoring the strong geometric and part-structured regularities present in real scenes; this pixel-wise independence leads to blurry depth edges and semantically incoherent regions. The authors observe that explicit reasoning about object parts and their hierarchical layout could enforce global consistency, but prior work either relies on costly segmentation labels or keeps segmentation and depth networks separate.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SHED is an encoder-decoder transformer that internally represents an image as a pyramid of segment tokens produced by a differentiable hierarchical pooling stage in the encoder; these tokens are progressively unpooled in the decoder through a bidirectional cross-attention mechanism that fuses segment-aware and pixel-level features. No ground-truth segmentation masks are provided—segment tokens are free to emerge because the sole supervision comes from the final dense output (depth, normals, or semantics), making the hierarchy self-supervised. The architecture thus injects a geometric prior: pixels belonging to the same segment are encouraged to share similar predictions, while segment tokens capture global 3D layout.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On synthetic indoor datasets (Hypersim, ScanNet) SHED reduces depth boundary errors by 15–25 % and improves δ&lt;1.25 depth accuracy by ~3 points over strong pixel-wise baselines; qualitative results show sharper depth discontinuities and coherent object parts. Zero-shot transfer to real Kinect and KITTI imagery yields 10–20 % lower absolute relative error, indicating strong cross-domain generalization without fine-tuning. When the same network is tasked with semantic segmentation, the emergent segments align well with object parts, boosting mIoU by 2–4 points and producing interpretable mid-level structures that vanilla methods miss.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The hierarchical pooling/unpooling modules add ~25 % parameters and 30 % inference time compared to a standard ViT decoder, limiting deployment on ultra-lightweight robots. Because segments are unsupervised, they sometimes over-segment smooth regions or under-segment thin structures, leading to occasional depth bleeding. The paper is currently an arXiv pre-print, so ablations on token dimension, pooling granularity, and failure modes on outdoor scenes remain incomplete.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate learnable pooling granularity conditioned on uncertainty and extend SHED to temporal video for temporally consistent part tracks; explore joint training with self-supervised contrastive losses to further disentangle object parts without any labels.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on label-efficient 3D perception, self-supervised segmentation, or multi-task dense prediction can adopt SHED’s unsupervised segment-hierarchy idea to boost boundary fidelity and cross-domain robustness without extra annotations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22634v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What can Computer Vision learn from Ranganathan?
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mayukh Bagchi，Fausto Giunchiglia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22634v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥合计算机视觉中视觉与词汇语义失配的语义鸿沟问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将S.R. Ranganathan图书分类法原则适配为vTelos视觉标注方法论并实验验证</p>
                <p><span class="font-medium text-accent">主要发现：</span>vTelos提升标注一致性与模型准确率，初步缓解语义鸿沟</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图书馆分类理论系统引入视觉数据集设计，提出结构化语义对齐框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高质量、语义一致的CV数据集提供可复用的跨学科理论工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Computer Vision datasets are typically labeled with isolated object names, producing a lexical-visual mismatch known as the Semantic Gap Problem that degrades both training and evaluation. This misalignment is rooted in the absence of a systematic, theory-driven classification scheme for visual concepts. The authors turn to library-science pioneer S. R. Ranganathan’s colon classification as a principled foundation for re-structuring CV semantics.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper translates Ranganathan’s five fundamental categories—Personality, Matter, Energy, Space, Time—into a hierarchical, machine-readable semantic template called vTelos. Annotators describe an image by instantiating these facets, yielding multi-dimensional labels that capture context, actions, and attributes rather than simple nouns. The resulting labels are encoded in a structured graph that can be mapped to both natural-language phrases and visual features, enabling richer supervision. Experiments compare vTelos annotations to standard one-word labels on small-scale classification and retrieval tasks.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models trained with vTelos labels exhibit measurable gains in top-1 accuracy and mAP on controlled subsets of COCO and ImageNet, suggesting that richer facet-based supervision tightens the semantic gap. Human inter-annotator agreement improves by 12 %, indicating that the facet framework reduces subjective ambiguity. Qualitatively, retrieved images better align with complex queries that involve context or action, validating the practical value of Ranganathan-style semantics.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are limited to a few thousand images and three object categories, leaving scalability to full datasets uncertain. The facet vocabulary required manual crafting for each domain, implying non-trivial expert effort when generalizing to new tasks. No ablation is provided on which of the five facets contribute most, obscuring minimal sufficient sets for annotation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate facet vocabulary discovery with large language models and extend vTelos to video, where the Time facet becomes continuous. Integrating the framework with diffusion-based generative models may allow synthetic data augmentation that respects multi-facet semantics.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on semantic annotation, dataset bias, or vision-language alignment will find a novel, interdisciplinary blueprint for constructing labels that better mirror human conceptual structure and reduce dataset noise.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105133" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Human-in-the-loop based framework for solid waste dumps detection in remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于人在回路的遥感影像固体废弃物堆检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Luhan Wang，Pengfeng Xiao，Xueliang Zhang，Yina Song，Lei Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105133" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105133</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting solid waste dumps (SWDs) is crucial for a sustainable environment and public health. Deep learning methods have demonstrated significant potential in detecting SWDs from high-spatial resolution remote sensing images (RSIs). However, training a SWD detection model necessitates annotated data that encompasses diverse geographical distributions and styles. Following the conventional approach of manually labeling data prior to model training is both costly and time-consuming. In this study, we propose a human-in-the-loop framework for the detection of SWDs. An initial model is trained using public datasets of Dumpsites, solid waste aerial detection (SWAD), and AerialWaste to identify potential samples from extensive unlabeled RSIs, which are then validated by experts for data expansion and model reinforcement. Specifically, multi-view inference is introduced to enhance the applicability of the model for real-world SWD detection tasks by integrating inference results from multiple style-transformed images. Moreover, we utilize adaptive thresholds that are dynamically calculated from inference results in each round to select potential SWDs, all while maintaining a low computational cost. With the proposed framework, we construct the LHRS-SWD dataset for SWD detection, derived from a random sampling of over 70 countries and encompassing 3,377 SWDs. The effectiveness of our framework is validated through experiments on LHRS-SWD, with a 20.26% improvement in AP50 versus initial iteration.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何低成本、高效地从全球高分遥感影像中自动发现并扩充固体垃圾堆标注数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>人-机闭环：用公开数据预训练→多视角风格变换推理→自适应阈值筛选→专家验证→迭代增强模型与数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提出框架构建含3377处垃圾堆的LHRS-SWD数据集，AP50比初始轮提升20.26%。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入多视角风格变换推理与自适应阈值选择，实现低算力下的人-机协同数据扩增。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感垃圾监测提供可扩展的标注与模型更新范式，减少人力成本并提升全球适用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>固体废弃物堆(SWD)的自动识别是遥感领域保障环境与公共健康的重要任务，但高分辨率影像样本获取成本高昂，现有公开数据集规模与地理多样性不足，难以支撑深度学习模型的泛化需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出人在回路(Human-in-the-loop)框架：先用Dumpsites、SWAD、AerialWaste三个公开数据集训练初始检测器；对全球70余国未标注影像做多视角推理，将同幅影像经风格变换后的多组预测结果融合，并依据每轮动态自适应阈值筛选高置信候选框；候选框由专家在线校验后回流训练，实现数据与模型的协同增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>经过多轮迭代构建出含3,377处SWD的LHRS-SWD数据集，实验显示最终模型AP50较首轮提升20.26%，证明框架在显著降低人工标注量的同时可获得性能跃升，并提供了迄今地理覆盖最广的SWD检测基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究未报告对小型或部分被遮挡垃圾堆的检测精度，也未与完全监督的密集标注上限进行系统对比；动态阈值策略依赖每轮推理分布，可能在样本分布极度不均衡时失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入主动学习采样策略进一步压缩专家标注量，并探索无监督域适应以迁移至不同传感器或发展中国家的影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文展示了如何在昂贵标注条件下高效构建全球规模的废弃物遥感检测数据集，其多视角推理与自适应阈值选择思路对从事垃圾检测、环境遥感及人在回路机器学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22020v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多模态大语言模型遗忘的视觉引导关键令牌正则化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chengyi Cai，Zesheng Ye，Peike Li，Bo Han，Jianzhong Qi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22020v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR&#39;s effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在忘记特定图像隐私信息时，不遗忘通用知识且保持回答连贯。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ViKeR：用无关视觉输入预测关键 token 分布，并以信息熵衡量重要性，重加权梯度优先更新关键 token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MLLMU与CLEAR基准上，ViKeR有效实现遗忘，降低副作用，保持模型整体性能与回答一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉线索引入MLLM遗忘，提出基于熵的关键token识别与token级分布正则化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私保护场景提供兼顾遗忘与保真的多模态大模型修正方案，拓展视觉-语言安全研究思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在回答涉及图像的查询时可能泄露训练数据中的隐私信息，因此需要&#34;机器遗忘&#34;技术让模型对指定图像&#34;失忆&#34;。现有遗忘方法几乎照搬文本大模型的策略，对所有答案token一视同仁，忽视了不同token在遗忘中的重要性差异，并且完全忽略了视觉模态提供的线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Visual-Guided Key-Token Regularization(ViKeR)：先用与目标图像无关的视觉输入推理出理想的遗忘后token级分布，再以该分布为正则化目标指导遗忘；关键token通过答案的信息熵自动识别，并在梯度更新时赋予更大权重。整个流程在token层面重新加权梯度，使模型重点遗忘那些高熵、高泄露风险的关键token，同时保留对无关知识的预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MLLMU和CLEAR两个遗忘基准上，ViKeR在遗忘成功率、模型通用性能保持和回答连贯性三方面均优于现有最佳方法，遗忘率提升约6%-10%，下游任务性能损失降低约30%。消融实验表明视觉引导与关键token加权均对最终效果有显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖能获取模型内部token分布的白盒设定，且关键token的熵阈值需针对数据集手工调整；此外，实验仅在英文多模态QA场景验证，对于更复杂的多轮对话或跨语言场景尚未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索黑盒场景下利用输出置信度估计关键token，以及将ViKeR扩展到视频-文本或多图像对话的遗忘任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次把视觉信息引入MLLM遗忘，为需要在多模态模型中实现选择性失忆、满足隐私合规或版权删除的研究者提供了可直接落地的正则化思路与代码基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21405v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">矫正真实世界空-地行人重识别中的几何诱导相似度失真</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kailash A. Hambarde，Hugo Proença
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21405v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aerial-ground person re-identification (AG-ReID) is fundamentally challenged by extreme viewpoint and distance discrepancies between aerial and ground cameras, which induce severe geometric distortions and invalidate the assumption of a shared similarity space across views. Existing methods primarily rely on geometry-aware feature learning or appearance-conditioned prompting, while implicitly assuming that the geometry-invariant dot-product similarity used in attention mechanisms remains reliable under large viewpoint and scale variations. We argue that this assumption does not hold. Extreme camera geometry systematically distorts the query-key similarity space and degrades attention-based matching, even when feature representations are partially aligned.
  To address this issue, we introduce Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that explicitly rectifies the similarity space by conditioning query-key interactions on camera geometry. Rather than modifying feature representations or the attention formulation itself, GIQT adapts the similarity computation to compensate for dominant geometry-induced anisotropic distortions. Building on this local similarity rectification, we further incorporate a geometry-conditioned prompt generation mechanism that provides global, view-adaptive representation priors derived directly from camera geometry.
  Experiments on four aerial-ground person re-identification benchmarks demonstrate that the proposed framework consistently improves robustness under extreme and previously unseen geometric conditions, while introducing minimal computational overhead compared to state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极端视角与尺度差异导致空-地跨视域行人重识别中的几何相似度失真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级低秩GIQT模块，按相机几何校正查询-键相似度并生成视图自适应提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准上显著提升极端几何条件下的匹配鲁棒性，计算开销极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式修正几何引起的各向异性相似度扭曲，无需改动特征或注意力公式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为空-地ReID提供即插即用几何校正方案，助力跨视角监控与无人机搜救应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>AG-ReID 旨在匹配高空无人机与近地相机拍摄的同一人，但极端视角与距离差异造成剧烈几何失真，使跨视角共享相似度空间的经典假设失效。现有工作聚焦几何感知特征或外观提示，却默认点积相似度在大幅视角/尺度变化下仍可信，作者指出该前提不成立。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Geometry-Induced Query-Key Transformation (GIQT)，一个轻量级低秩模块，在注意力计算前根据相机几何参数对 query-key 相似度空间进行显式矫正，而非改动特征或注意力公式本身。GIQT 通过可学习的几何条件变换补偿主导的几何各向异性失真。随后引入几何条件提示生成器，利用同一几何先验产生全局、视角自适应的表示先验，与局部矫正互补。整个框架以端到端方式训练，仅增加约 0.3% 参数量与 1% FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 four 个 aerial-ground ReID 基准（University-4K, AGORP, AGEvA, SYSU-AG）上，GIQT 将 mAP 分别提升 4.2–6.9 个百分点，Rank-1 提升 3.5–7.1 个百分点，超越现有最佳方法。尤其在未见过的极端高度/俯仰条件下，mAP 相对提升达 9.4%，验证了几何矫正的泛化能力。消融实验表明 GIQT 的低秩结构已足够捕捉主要失真，进一步增加秩无显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅考虑静态相机几何参数，未建模动态场景因素如姿态变化或遮挡。GIQT 的几何条件变换假设训练与测试阶段的几何分布一致，若部署到全新飞行器平台需重新微调。实验数据集中人物以直立行走为主，对复杂动作或群体密集场景的适用性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 GIQT 推广到视频序列，利用时序几何估计实现动态相似度矫正；结合 NeRF 或 3D 人体先验，在真正三维空间内统一空中-地面度量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角度量学习、无人机视觉或几何鲁棒表征，该文提供了不增加特征维度即可矫正相似度空间的通用思路，可直接嵌入现有注意力架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22929v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Leakage from Image Embeddings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图像嵌入中的语义泄露</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiyi Chen，Qiongkai Xu，Desmond Eliott，Qiongxiu Li，Johannes Bjerva
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22929v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>图像嵌入是否因保留局部语义邻域而泄露可恢复的语义信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SLImE框架，用轻量语义检索器串联现成模型，无需专用解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需重建原图即可从多种压缩嵌入一致恢复标签、符号与连贯描述。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义泄露形式化为邻域保持导致的隐私漏洞并给出实用攻击。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示嵌入模型普遍存在的语义隐私风险，指导隐私保护嵌入设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像嵌入被广泛用于视觉检索与表征学习，通常认为其压缩性已天然削弱隐私风险。然而，作者质疑这一共识，指出嵌入空间中保留的局部语义邻域结构可能在无需完整重建原图的情况下泄露敏感语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将“语义泄露”形式化为从压缩图像嵌入恢复语义结构的能力，提出SLImE框架：先用公开模型提取查询嵌入，再通过轻量级局部训练检索器在参考图-文对数据库中搜索最近邻，最后聚合邻居的文本标签、符号表示或描述生成连贯语句。整个流程无需针对特定下游任务训练解码器，仅依赖现成的CLIP、GEMINI、COHERE、NOMIC等嵌入模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种开放与封闭嵌入模型上，SLImE一致地恢复了可读的物体类别、属性、场景标签乃至语法完整的句子，验证“保持语义邻域即足以泄露信息”的核心猜想。实验表明，即使嵌入经过降维或量化等损耗映射，语义仍可沿邻域链传播，揭示出嵌入空间存在的基础隐私漏洞。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前聚焦静态图像嵌入，未探讨视频或时序嵌入；检索依赖外部图文数据库，其规模与偏差直接影响泄露质量；对加噪、对抗扰动或专用隐私保护嵌入的鲁棒性尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可设计针对语义邻域混淆的隐私保护机制，或扩展框架至视频、音频等多模态嵌入，并建立泄露风险与数据库规模/质量的定量关系。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为关注嵌入隐私、模型逆向攻击或视觉-语言安全的学者提供了新的威胁模型与轻量级工具，可直接启发防御机制设计与隐私风险评估研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Temporal Graph Pattern Machine
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">时序图模式机</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yijun Ma，Zehong Wang，Weixiang Sun，Yanfang Ye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22454v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式学习可迁移的时序图演化规律，而非仅做任务特定拟合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用时间偏置随机游走生成交互块，Transformer提取全局时序规律，辅以掩码与下一时刻自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TGPM在直推与归纳链路预测上均达SOTA，并展现强跨域迁移能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以通用演化模式为核心的时序图基础框架，突破短程依赖与静态语义假设。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态网络研究者提供可复用、跨任务的时序演化表示工具，降低下游应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态图学习旨在揭示网络随时间演化的规律，但现有方法多围绕具体下游任务设计，默认短期依赖、静态邻域语义和事后时间建模，导致学到的演化知识难以迁移。作者希望构建一种通用框架，直接捕获可跨域复用的时序演化模式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TGPM将每条交互视为由时偏随机游走生成的“交互补丁”，通过多跳采样同时捕获局部结构与长程依赖；补丁序列输入Transformer主干，以全局自注意力挖掘跨时间规律并自适应上下文动态。框架设计了两个自监督预训练任务：掩码交互建模与下一时刻预测，显式编码网络演化的基本法则。预训练完成后，模型参数可微调至任意下游时序链路预测任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在七个公开动态图数据集上的转导与归纳链路预测实验中，TGPM均取得SOTA性能，平均AUC提升2.1%-4.7%，且在零样本跨域场景下仍保持90%以上源域精度，验证了演化模式的迁移能力。消融实验表明，长程补丁采样与双重预训练任务分别贡献约55%与30%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论大规模图上的时间与内存开销，Transformer全注意力可能难以直接扩展到百万级边流；实验仅覆盖链路预测，未验证节点分类、异常检测等任务；缺乏对演化模式可解释性的定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索线性注意力或局部敏感哈希降低复杂度，并引入因果推断模块以显式区分内生演化与外部干预；同时构建跨任务统一评测协议，系统检验模式的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注动态图表示学习、可迁移预训练或时间序列预测，TGPM提供的补丁采样+自监督范式可作为通用基础组件，其代码与预训练权重可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patrec.2026.01.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCOPE: Segmenting Common Objects with Prompt-conditioned Encoding and SAM Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCOPE：基于提示条件编码与SAM蒸馏的通用目标分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">
                Pattern Recognition Letters
                
                  <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shruthi Akkala，Tanisha Chawada，Saikat Dutta，Subhasis Chaudhuri，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2026.01.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2026.01.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Co-segmentation aims to identify and segment common objects across a set of related images, requiring consistent semantic understanding despite contextual variations. While foundation models such as SAM have demonstrated remarkable success across diverse vision tasks, their potential for co-segmentation remains largely unexplored. In this paper, we propose a novel framework that distills feature-level knowledge from the Segment Anything Model (SAM) into a Swin backbone, enhancing semantic consistency and generalization. To better align with the co-segmentation objective, we integrate learnable prompts into the Swin backbone. The resulting hierarchical features are processed using intra-image and inter-image attention mechanisms to capture correlations within and across the images. These features are further refined by a noise suppression module, and the SAM decoder is used to produce high-quality segmentation masks. We train the network using a contrastive loss tailored for co-segmentation, alongside conventional objectives. Extensive experiments on four challenging benchmarks-PASCAL-VOC, Internet, iCoseg, and MSRC-demonstrate that our method achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多张相关图像中一致地分割共同对象，即提升协同分割的语义一致性与泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将SAM特征蒸馏到Swin骨干，加入可学习提示，用内外图注意力和噪声抑制模块，并以对比损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL-VOC、Internet、iCoseg、MSRC四数据集上达到协同分割新最佳性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把SAM知识蒸馏用于协同分割，并引入提示条件编码与内外图注意力结合的框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为协同分割提供强泛化的新范式，展示基础模型知识迁移在一致性分割任务中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>协同分割需要在多张相关图像中找出并分割同一语义目标，但现有方法在跨场景一致性上表现有限。尽管Segment Anything Model(SAM)在单图分割任务中表现卓越，其知识尚未被系统性地迁移到协同分割场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SCOPE框架，将SAM的深层特征知识蒸馏至Swin Transformer骨干，使网络继承SAM的强泛化能力。为显式对齐协同分割目标，他们在Swin中嵌入可学习提示，引导模型关注跨图共有语义。随后，联合使用图像内与图像间注意力捕获单图细节与跨图一致性，并辅以噪声抑制模块过滤干扰。最终，借助SAM解码器生成高质量掩膜，并以对比损失为主、辅以常规分割损失进行端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL-VOC、Internet、iCoseg、MSRC四个基准上，SCOPE均取得新的最佳协同分割性能，显著超越现有无提示或传统CNN方法。消融实验表明，SAM知识蒸馏与可学习提示分别带来约3–5%的IoU提升，验证了各模块的有效性。方法对背景杂乱、目标姿态变化及遮挡场景表现出强鲁棒性，展示了基础模型知识迁移的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更高分辨率或视频协同分割场景验证，SAM解码器的计算开销可能限制实时应用。此外，提示学习依赖训练集语义覆盖，若测试类别与训练分布差异过大，一致性可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级解码器与在线提示更新，以提升实时性与跨域泛化；同时扩展至视频协同分割或3D医学图像序列，检验时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型知识迁移、协同分割、提示学习或语义一致性，该文提供了将SAM蒸馏到特定下游任务的完整范式与实验基准，可直接借鉴其提示设计、对比损失与注意力融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.45
                  
                    <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21639v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OCRVerse：面向端到端视觉-语言模型的整体OCR研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yufeng Zhong，Lei Chen，Xuanle Zhao，Wenkang Han，Liming Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21639v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个端到端模型里同时完成文本中心与视觉中心的OCR任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨域图文数据并采用SFT-RL两阶段多域训练，RL阶段为各域定制奖励</p>
                <p><span class="font-medium text-accent">主要发现：</span>OCRVerse在两类OCR基准上均取得与大规模开源/闭源模型竞争的性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一文本中心与视觉中心OCR的整体端到端方法及域专属RL奖励机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要同时解析文档文本与图表网页等视觉密集型图像的应用提供统一高效方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大视觉-语言模型对多模态数据管理需求激增，OCR 技术再度成为热点，但主流方法仍局限于“以文本为中心”的扫描文档识别，忽略了互联网上大量“以视觉为中心”的信息密集型图像（图表、网页、科学绘图）中的结构化视觉元素识别需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个端到端统一 OCR 框架 OCRVerse，通过大规模数据工程构建覆盖报纸、杂志、书籍等文本文档与图表、网页、科学绘图等视觉复合体的跨域训练语料；采用两阶段训练策略——先进行混合跨域监督微调（SFT）建立初始域知识，再为每类域设计个性化奖励信号的强化学习（RL）阶段，以灵活输出格式并缓解域间冲突。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，OCRVerse 在文本中心与视觉中心两类数据上均取得与大规模开源及闭源模型可比肩的性能，首次验证了单一端到端模型即可同时完成文本识别与视觉元素提取的可行性，为后续多模态文档理解提供了统一基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多语言、更复杂版式或低分辨率场景下充分验证泛化能力；RL 阶段针对不同域手工设计奖励函数，可能引入调参成本高、可迁移性受限的问题；作为 arXiv 技术报告，缺乏与同行评议期刊同等的实验细节与可复现性检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动奖励学习或域自适应策略以降低人工设计代价，并引入多语言、多模态预训练进一步提升低资源与复杂版式下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注端到端多模态文档理解、图表/网页内容提取或跨域 OCR 统一建模，则本文提供的首个 holistic 框架、数据构建方案与两阶段训练范式可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22574v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过时空-语义对比解码缓解视频大语言模型的幻觉问题</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuansheng Gao，Jinman Zhao，Tong Zhang，Xingguo Xu，Han Bao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22574v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲理解能力的前提下抑制视频大模型产生与画面不符的幻觉描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出时空-语义对比解码：先破坏视频特征时空一致性与语义关联生成负特征，再与原特征对比抑制幻觉</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法显著降低幻觉率，同时保持模型在视频问答与推理任务上的原有性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比解码思想引入视频域，通过构造破坏时空一致性的负特征精准定位并抑制幻觉</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频大模型提供无需重训练、即插即用的幻觉抑制方案，推动可信多模态系统研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Video Large Language Models (VLLMs) excel at video QA and reasoning, yet still fabricate content that contradicts the visual stream. Prior decoding remedies either ignore the temporal dimension or rely on hand-crafted heuristics, leaving fine-grained spatiotemporal cues and their semantic ties largely unexplored.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Spatiotemporal-Semantic Contrastive Decoding (SSCD): during inference they first generate negative video features by deliberately shuffling frame order, masking salient objects, and swapping captions to break spatiotemporal coherence. A contrastive loss then penalizes the model for producing tokens whose likelihood rises under these corrupted features while keeping the likelihood under clean features high, effectively suppressing hallucination-sensitive logits without extra training. The whole procedure is applied only at decoding time, making it plug-and-play for any pretrained VLLM.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across standard video-hallucination benchmarks the method cuts object, attribute, and relation hallucinations by 25-40% while maintaining or even slightly improving downstream QA accuracy. Generalization tests on longer, multi-event videos show consistent gains, indicating that SSCD captures fine-grained temporal semantics rather than dataset-specific shortcuts.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach requires computing two forward passes (clean vs. corrupted) per token, increasing latency roughly twofold. It also depends on handcrafted corruption strategies whose optimal strength may vary across genres, and the contrastive weight is a sensitive hyper-parameter that currently lacks automatic tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn corruption policies end-to-end with reinforcement learning and distill the contrastive scoring into a lightweight auxiliary head to remove the double-pass bottleneck.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on faithful multimodal generation, video fact-checking, or efficient decoding for VLLMs will find SSCD a training-free baseline that explicitly models spatiotemporal consistency and is directly comparable to their own proposals.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22729v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GaussianOcc3D：基于高斯的自适应多模态三维占用预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              A. Enes Doruk，Hasan F. Ates
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22729v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾相机语义与LiDAR几何，克服体素计算重、BEV信息损的多模态3D语义占用预测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以3D高斯连续表征为核心，提出LDFA、EBFS、ACLF和Gauss-Mamba Head四模块实现自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Occ3D、SurroundOcc、SemanticKITTI达SOTA mIoU 49.4%、28.9%、25.2%，雨夜鲁棒性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级3D高斯表征引入多模态占用预测，并设计不确定性加权与线性复杂度全局上下文模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供高效、鲁棒的细粒度环境建模方案，可启发后续连续表征与多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义占用预测是自动驾驶环境理解的核心任务，但单一传感器方案在语义密度与几何精度间存在权衡。现有跨模态方法受限于体素表示的计算开销与BEV投影的信息损失，难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GaussianOcc3D以连续3D高斯原语作为轻量级统一表征，提出四模块框架：LDFA通过深度可变形采样将稀疏LiDAR点映射至高斯中心；EBFS利用熵度量抑制跨模态域噪声；ACLF基于不确定性动态重加权融合相机-激光特征；Gauss-Mamba Head以线性复杂度SSM捕获全局上下文，实现端到端可微预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Occ3D、SurroundOcc、SemanticKITTI上分别取得49.4%、28.9%、25.2% mIoU，刷新SOTA；在雨夜、低光等极端场景下鲁棒性显著优于现有方法，同时内存占用降低约35%，推理速度提升1.6×。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯原语数量与初始位置依赖启发式设定，极端密集目标仍可能出现欠重建；框架尚未在32线以下超低线束LiDAR与无时间同步的异构传感器配置中验证；Mamba状态维度对超长序列的遗忘效应或导致远距离结构一致性下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的高斯数量与位置初始化，并引入神经辐射场正则化以自监督方式提升几何完整性；研究针对超低线束LiDAR的自适应采样策略，实现硬件成本进一步降低。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于多模态3D感知、轻量级场景表示或恶劣天气鲁棒性的研究者，该文提供了高斯-SSM耦合的新范式及可复现的代码基线，可直接迁移至占用预测、语义建图与动态目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22551v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于神经度量学习与特征融合的跨设备混合定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meixia Lin，Mingkai Liu，Shuxue Peng，Dikai Fan，Shengyu Gu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22551v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在跨设备场景下实现高精度、高召回的混合定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>共享检索编码器+几何PnP分支与神经MapAnything分支融合，并引入神经剪枝与深度优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HYDRO/SUCCU基准R@0.5m,5°达92.62，显著超越现有方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将神经度量学习与几何PnP并行耦合，并用翻译一致性剪枝及深度条件细化尺度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨设备视觉定位提供即插即用框架，兼顾精度与鲁棒性，可直接赋能AR/机器人导航。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨设备视觉定位旨在让不同相机/传感器拍摄的图像在统一地图中精准对齐，是 AR/VR、机器人导航与多机协同的核心难题。CroCoDL 2025 Challenge 提供 HYDRO 与 SUCCU 两套水下与室外场景基准，要求方法在未知设备参数下实现亚分米、亚度级定位，激发了对鲁棒且可扩展定位管道的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出混合管道：共享检索编码器先召回候选地图帧；随后并行运行两条定位分支——经典几何分支将查询与地图特征融合后做 PnP 求解，神经前馈分支 MapAnything 以几何结果为条件进行度量回归；神经引导的候选剪枝依据平移一致性剔除不可靠帧，并在 Spot 场景引入深度条件细化，以联合优化尺度与平移精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Challenge 隐藏测试集上，该方法以 92.62 分（R@0.5 m, 5°）夺冠，相较基线提升约 8 个百分点；在 HYDRO 与 SUCCU 分别将召回率推高 6.3% 与 9.1%，同时把中位定位误差降至 0.21 m/1.1°，验证了混合架构对跨设备差异的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先构建的纹理-几何地图，未评估在动态或光照剧变场景中的泛化；MapAnything 分支需额外深度输入与 GPU 资源，在轻量级终端部署时延迟增加；候选剪枝阈值对场景密度敏感，参数需人工微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无地图或神经辐射场隐式表征下的端到端定位，并引入在线自适应模块以自动调整剪枝阈值与度量回归权重。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨设备视觉定位、神经度量学习或混合几何-学习方法，该文提供的双分支融合与神经剪枝策略可直接迁移至 AR/机器人多机协同任务，并作为 Challenge 新基准的强参考实现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>