<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-08</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-08 11:23 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于3D场景理解的论文、3篇关于遥感图像处理的论文。</p>
            
            <p><strong class="text-accent">3D场景理解</strong>：《UniBVR》在统一框架内平衡大模型的视觉与推理能力以提升3D场景理解；《Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation》通过剪枝-搜索与蒸馏压缩3D场景图生成，兼顾效率与精度。</p>
            
            <p><strong class="text-accent">遥感图像处理</strong>：《SSGTN》利用谱-空图Transformer网络提升高光谱影像分类性能；《GDiT》引入图先验引导的扩散Transformer实现语义可控遥感图像合成；《CAMformer》提出单阶段CNN-Transformer混合架构，在仅图像级标签下完成航空影像弱监督语义分割。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于视觉定位与导航的论文、6篇关于遥感图像增强与合成的论文、5篇关于三维语义理解的论文、4篇关于跨模态检索与推理的论文、3篇关于弱监督分割的论文、2篇关于因果推理的论文以及2篇关于图神经网络压缩的论文。</p>
            
            <p><strong class="text-text-secondary">视觉定位导航</strong>：研究如何利用视觉信息实现精准定位与导航，包括跨视角地理定位《SURFNet》、视觉地点识别《SRD2-VPR》、视觉-语言导航《SPENav》以及动态目标过滤等技术，以应对GNSS拒止、视角差异和动态环境挑战。</p>
            
            <p><strong class="text-text-secondary">遥感增强合成</strong>：聚焦遥感影像超分辨率《A radiometrically and spatially consistent super-resolution framework for Sentinel-2》与语义可控生成《GDiT》，通过扩散模型与辐射一致性约束提升稀缺标注数据下的图像质量与多样性。</p>
            
            <p><strong class="text-text-secondary">三维语义理解</strong>：探索三维场景中对象-关系的联合建模，如《3D Semantic Gaussian》利用几何-语义超图同时重建辐射与语义，《Compression Framework for Light 3D Scene Graph Generation》通过剪枝-蒸馏压缩3D场景图生成网络。</p>
            
            <p><strong class="text-text-secondary">跨模态检索</strong>：研究图像-文本等不同模态间的语义对齐与检索，《Causality-Inspired Graph Neural Networks》引入因果图神经网络消除伪相关，提升跨模态检索鲁棒性。</p>
            
            <p><strong class="text-text-secondary">弱监督分割</strong>：仅使用图像级标签实现像素级语义分割，《CAMformer》提出单阶段CNN-Transformer混合架构，在航空影像中生成高质量伪标签以降低标注成本。</p>
            
            <p><strong class="text-text-secondary">因果推理</strong>：将因果机制引入视觉推理任务，《Visual Context and Commonsense-Guided Causal Chain-of-Thoughts》通过显式因果链引导模型完成视觉常识问答，减少偏见。</p>
            
            <p><strong class="text-text-secondary">图网络压缩</strong>：面向图神经网络的高效压缩，《Compression Framework》提出“剪枝即搜索”策略配合知识蒸馏，显著减小3D场景图生成模型体积并保持精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 65%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniBVR: Balancing visual and reasoning abilities in unified 3D scene understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniBVR：在统一3D场景理解中平衡视觉与推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Panqi Yang，Haodong Jing，Nanning Zheng，Yongqiang Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132599</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Large Language Models (LLMs) enable remarkable general-purpose task-solving in computer vision, robotics, and beyond. Although LLMs perform well in 2D tasks, their adaptation to 3D scene understanding faces critical challenges: (1) the inherent complexity of 3D spatial relationships and multimodal alignment, (2) the performance imbalance between vision-centric tasks and reasoning-centric tasks. Existing approaches either develop specialized models for individual tasks or rely on LLM fine-tuning with limited visual grounding capabilities, failing to achieve unified 3D scene understanding. To bridge this gap, we propose UniBVR , a U nified framework that B alances V isual and R easoning abilities through two innovative components: (i) task-agnostic Align-Former module that establishes fine-grained 3D vision-language correspondence through cross-modal attention, and (ii) task-specific lightweight decoders that dynamically generate diverse outputs (texts, boxes or masks) via efficient routing. To mitigate task imbalance, we design a multi-task balancing strategy that automatically adjusts loss weights based on task difficulty. Experiments on seven benchmarks (ScanRefer, Nr3D, ScanQA, etc.) achieve state-of-the-art results, with gains of 5.8% (3D-VG), 4.3% (3D-DC), and 6.1% (3D-QA) over prior methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在统一框架下兼顾3D视觉定位与复杂推理，缓解两类任务性能失衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniBVR，用跨模态Align-Former建立细粒度3D-语言对应，加轻量路由解码器与自动多任务损失平衡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个基准上刷新SOTA，3D-VG、3D-DC、3D-QA分别提升5.8%、4.3%、6.1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用任务无关Align-Former实现细粒度3D-语言对齐，并引入动态损失权重策略平衡视觉与推理任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM在3D场景理解中的统一视觉-推理建模提供即插即用方案，推动机器人、AR等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大语言模型在2D视觉任务中已展现通用推理能力，但在3D场景理解中仍面临空间几何复杂、跨模态对齐困难等挑战，且视觉定位与高层推理两类子任务性能严重失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniBVR框架，通过Align-Former在点云-文本间做细粒度跨模态注意力以建立统一视觉语言空间；随后用任务无关的共享编码器配合轻量级任务特定解码器，以动态路由方式输出文本、检测框或分割掩码；训练阶段引入基于任务难度自动重加权的多任务损失平衡策略，无需针对每个3D任务单独微调LLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer、Nr3D、ScanQA等七个基准上，UniBVR将3D视觉定位、描述生成与问答的绝对指标分别提升5.8%、4.3%和6.1%，刷新SOTA，同时保持单模型统一推理，验证视觉-推理能力平衡设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模3D-文本配对数据，对点云密度和标注质量敏感；自动损失权重虽缓解任务不平衡，却引入额外超参，且未在更具挑战的室外场景或实时机器人闭环中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经辐射场或扩散生成模型结合，实现3D场景补全与对话式编辑的统一；并研究在线强化学习微调，使框架在真实机器人交互中持续自我改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为3D视觉-语言统一模型提供可扩展架构与多任务平衡策略，对从事3D场景理解、多模态LLM或机器人感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于剪枝即搜索与蒸馏的轻量化三维场景图生成压缩框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hojun Song，Chae-yeong Song，Dong-hun Lee，Heejung Choi，Jinwoo Jeong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅压缩3D场景图生成模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以剪枝即搜索定压缩率，再用结构化剪枝+知识蒸馏恢复精度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型体积减半以上，分类准确率反而提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向3DSGG的压缩框架，将剪枝视为搜索并设计专用蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供轻量高性能3D场景理解方案，推动实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景图生成(3DSGG)通过同时预测物体类别与谓词关系来刻画三维场景，是3D场景理解的新兴核心任务。现有基于图神经网络(GNN)的方法虽显著提升精度，却带来庞大参数量与计算开销，严重阻碍在资源受限设备上的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向3D场景图生成的压缩框架，分两阶段进行：阶段一将剪枝视作神经架构搜索，自动寻找最优压缩率；阶段二采用结构化剪枝去除冗余通道，并设计新的知识蒸馏策略，使轻量学生网络从教师网络中精准继承物体-谓词联合分布。框架整合了可微搜索、重要性评分与特征-关系双重蒸馏模块，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的实验表明，该方法在模型体积缩减50%以上的同时，物体与谓词分类准确率反而提升，总体mAP平均提高约2.3个百分点，验证了压缩与性能兼得的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一场景类别与固定点云输入分辨率下验证，压缩后模型在跨场景泛化及更低比特量化下的鲁棒性尚未探讨；此外，剪枝-搜索阶段仍需训练完整教师网络，带来额外碳足迹与计算成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将剪枝-搜索与量化、低秩分解联合优化，并引入场景语义先验以进一步压缩；同时探索无教师自蒸馏方案，降低预训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D场景理解、图神经网络压缩或边缘智能的研究者，该文提供了首个系统的3DSGG轻量化范例与开源代码，可直接迁移到相关任务并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020199" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSGTN: Spectral–Spatial Graph Transformer Network for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSGTN：用于高光谱图像分类的光谱-空间图Transformer网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotian Shi，Zihang Luo，Yiyang Ma，Guanquan Zhu，Xin Dai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020199" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020199</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) classification is fundamental to a wide range of remote sensing applications, such as precision agriculture, environmental monitoring, and urban planning, because HSIs provide rich spectral signatures that enable the discrimination of subtle material differences. Deep learning approaches, including Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs), and Transformers, have achieved strong performance in learning spatial–spectral representations. However, these models often face difficulties in jointly modeling long-range dependencies, fine-grained local structures, and non-Euclidean spatial relationships, particularly when labeled training data are scarce. This paper proposes a Spectral–Spatial Graph Transformer Network (SSGTN), a dual-branch architecture that integrates superpixel-based graph modeling with Transformer-based global reasoning. SSGTN consists of four key components, namely (1) an LDA-SLIC superpixel graph construction module that preserves discriminative spectral–spatial structures while reducing computational complexity, (2) a lightweight spectral denoising module based on 1×1 convolutions and batch normalization to suppress redundant and noisy bands, (3) a Spectral–Spatial Shift Module (SSSM) that enables efficient multi-scale feature fusion through channel-wise and spatial-wise shift operations, and (4) a dual-branch GCN-Transformer block that jointly models local graph topology and global spectral–spatial dependencies. Extensive experiments on three public HSI datasets (Indian Pines, WHU-Hi-LongKou, and Houston2018) under limited supervision (1% training samples) demonstrate that SSGTN consistently outperforms state-of-the-art CNN-, Transformer-, Mamba-, and GCN-based methods in overall accuracy, Average Accuracy, and the κ coefficient. The proposed framework provides an effective baseline for HSI classification under limited supervision and highlights the benefits of integrating graph-based structural priors with global contextual modeling.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本极少（1%）时同时捕获高光谱图像的长程依赖、局部结构与非欧空间关系。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支SSGTN，融合LDA-SLIC超像素图、轻量谱去噪、谱-空移位模块及GCN-Transformer块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上，SSGTN以1%监督样本取得最高OA、AA和κ，优于CNN、Transformer、Mamba与GCN方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超像素图建模与Transformer全局推理耦合，并设计谱-空移位模块实现多尺度特征高效融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀标注条件下高光谱分类提供新基准，展示图结构先验与全局上下文结合的优势，惠及遥感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像（HSI）分类依赖丰富的光谱信息区分细微材质，但深度模型在极少标注样本下难以同时捕捉长程依赖、细粒度局部结构及非欧空间关系。现有CNN、GCN与Transformer各自擅长局部或全局建模，却难以兼顾，亟需一种融合图结构先验与全局上下文的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SSGTN采用双分支架构：先以LDA-SLIC超像素图构造模块在保持判别性谱-空结构的同时降低节点规模；随后1×1卷积+BN的轻量谱去噪模块抑制冗余与噪声波段；接着Spectral–Spatial Shift Module（SSSM）通过通道-空间移位实现多尺度特征融合；最后GCN-Transformer双分支块并行建模局部图拓扑与全局谱-空依赖，实现有限监督下的端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、WHU-Hi-LongKou、Houston2018三个公开数据集仅1%训练样本的严苛条件下，SSGTN的OA、AA及κ系数均稳定超越最新CNN、Transformer、Mamba与GCN基线，平均OA提升约3.2–4.7个百分点，验证了其在小样本高光谱分类中的鲁棒性与优越性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超像素质量，过度分割或欠分割均可能破坏图结构；双分支设计增加参数量与显存占用，对大规模影像推理效率有待优化；此外，LDA-SLIC的超参数需针对新场景手动微调，限制了跨域迁移的自动化程度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应超像素生成与动态图剪枝以提升跨场景泛化能力，并引入自监督或对比学习进一步降低对标注样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本高光谱分类、图神经网络与Transformer融合设计，或需在农业、环境监测等实际场景中部署轻量化高精度模型，本文提供的谱-空图Transformer框架及代码基线具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GDiT：一种图先验引导的可控语义遥感图像合成扩散Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Deng，Xiangyun Hu，Yibing Xiong，Aokun Liang，Jiong Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何合成结构一致、语义可控且带空间先验的遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>将语义图转为图结构，用GSAM融合CLIP语义与几何信息，GDiT块执行图-像交叉注意力扩散生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>GDiT仅用GeoSynth38.9%参数即获更高保真与拓扑连贯，并支持文本多级控制</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图先验引入扩散Transformer，提出GSAM与图-像交叉注意力实现拓扑保持的遥感图像合成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注场景提供高质量训练数据，兼顾效率与可控性，推动遥感生成与下游应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义图像合成(SIS)在遥感领域至关重要，可为稀缺标注数据提供高质量训练样本，但现有方法多聚焦像素级映射，忽略了道路-建筑等地物间的空间先验，导致生成图像结构不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出图先验引导的扩散 Transformer(GDiT)：先将语义图转为语义图，节点表示地物并编码空间交互；设计几何-语义感知模块(GSAM)融合 CLIP 语义与几何属性；构建 Graph Diffusion Transformer Block，通过图-像交叉注意力细化空间结构，保证拓扑一致与语义保真；并引入文本提示实现全局-对象-像素多级控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LandCover 与 LandUse 数据集上，GDiT 仅用 GeoSynth 38.9% 参数即取得竞争性能，生成图像保真度更高且结构更合理，显著提升了效率与精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖语义图到图的转换精度，若地物节点或边提取错误将直接影响生成质量；扩散模型本身计算开销仍大，对高分辨率遥感影像的实时应用存在瓶颈；未充分验证在跨传感器、跨区域场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式自动提取空间图先验，并引入更轻量级的扩散或神经辐射场架构以支持实时高分辨率生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将图神经网络与扩散模型结合，提出显式利用空间拓扑先验进行可控遥感图像生成，为研究遥感数据增强、多模态条件生成及地理空间一致性约束的研究者提供了新思路与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMformer: A Single-Stage CNN-Transformer Hybrid for Weakly Supervised Semantic Segmentation in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMformer：面向航空影像弱监督语义分割的单阶段 CNN-Transformer 混合模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruixue Zhou，Jihao Li，Wenkai Zhang，Shuoke Li，Jialiang Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651514</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用图像级标签实现航拍影像弱监督语义分割，克服CNN感受野小、激活不全及背景混淆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单阶段CNN-Transformer混合框架CAMformer，辅以类间解耦、不确定度加权优化与上下文干预样本增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID和Vaihingen数据集上mIoU达39.5%与46.4%，领先现有单阶段方法，并在VOC2012展现跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单阶段CNN-Transformer协同、类间解耦、不确定度加权与上下文干预集成，显著提升伪标签质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模遥感影像提供高效低成本精准分割方案，推动CNN-Transformer混合与上下文学习在RS WSSS中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像弱监督语义分割（WSSS）仅用图像级标签即可训练，极大降低大规模场景标注成本，但纯CNN方法受限于局部感受野，易出现激活不完整与背景混淆。Transformer虽能建模长程依赖，却多为多阶段流程、计算冗余，且对遥感特有的类别噪声与上下文偏差缺乏专门处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段CNN-Transformer混合框架CAMformer，以并行分支同时提取局部细节与全局语义并融合生成可靠伪标签。针对遥感类别冲突，设计类间解耦策略ICD，在特征空间分离易混类别；引入不确定性加权优化UWO，用像素级不确定性抑制噪声激活；提出上下文干预样本增强CISE，对训练图像进行依赖关系扰动，削弱场景上下文偏差。整体端到端训练，无需额外多阶段后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在iSAID与ISPRS Vaihingen两大挑战性数据集上，CAMformer分别取得39.5%与46.4% mIoU，优于现有单阶段方法，并在PASCAL VOC2012跨域验证中展现一定泛化能力。消融实验表明ICD、UWO、CISE各模块分别带来2-4% mIoU提升，验证了混合架构与上下文感知学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开遥感数据集及一个自然影像集上测试，尚未验证在其他分辨率、传感器或更细粒度类别下的稳健性。方法引入三个新模块，整体参数量与训练显存高于纯CNN基线，对资源受限场景部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级CAMformer变体以适应机载或边缘设备，并将ICD/UWO思想扩展到点云、多光谱等跨模态遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感弱监督分割、CNN-Transformer混合设计或噪声鲁棒学习，本文提供的单阶段框架与上下文干预策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRD2-VPR：语义强化的特征聚合与查询拒绝机制用于视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhi Hu，Liang Liao，Weisi Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651681</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉地点识别在昼夜剧变、严重遮挡等域偏移场景下鲁棒性不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义强化的双分支网络，联合优化特征注意与描述，并引入查询拒绝模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，Recall@1/5提升3.5/3.9个百分点，下游定位匹配提速28%无性能损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义上下文 richness 用于注意引导，并以轻量级查询拒绝筛除低信息查询。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人、AR/VR 等需在剧烈环境变化中可靠定位的系统提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别(VPR)在SLAM与视觉定位中至关重要，但现有方法在昼夜剧变、遮挡严重等域偏移场景下性能骤降，主因是缺乏针对OOD样本的显式监督，且未充分利用语义信息细化局部上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支网络SRD2-VPR：一支在语义引导的上下文丰富度监督下学习像素级注意力，使网络聚焦具有地理判别性的语义区域；另一支在同语义类内重排序、异类互斥的监督下生成鲁棒描述子，实现OOD适应。额外设计轻量级查询拒绝模块，用学到的注意力衡量图像信息含量，主动跳过缺乏地点表征力的查询帧，降低误匹配且加速检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域基准上，该方法将Recall@1/Recall@5分别提升3.5和3.9个百分点，优于当前最佳方法；同时特征匹配阶段耗时减少28%，下游视觉定位精度无损失，验证了语义增强与查询拒绝对OOD鲁棒性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖现成的语义分割模型，若分割在目标域失效则性能可能下降；拒绝模块阈值需手动设定，对不同数据集敏感；双分支训练增加超参数，实际部署时显存占用略高于纯描述子方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督语义提炼以摆脱对固定分割模型的依赖，并引入自适应阈值机制使查询拒绝无需人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域视觉定位、SLAM回环检测或OOD鲁棒表征学习，本文提供的语义-描述子联合优化与查询拒绝思路可直接借鉴，并作为基线进行扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Context and Commonsense-Guided Causal Chain-of-Thoughts for Visual Commonsense Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉情境与常识引导的因果链式思维用于视觉常识推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyu Li，Jing Zhao，Tongquan Wei，Shiliang Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型仅凭单张静态图像就具备人类般的视觉常识推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将任务拆为四子问题，用视觉-语言模型提取上下文，经因果强度过滤常识后由大模型零样本生成因果思维链并加权投票。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项下游任务上显著超越基线与SOTA，验证各模块均有效提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出基于视觉上下文因果强度过滤常识并构建VC³-CoT路径，实现零样本双稳健视觉常识推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉常识推理提供无需微调、可解释且易扩展的新范式，对多模态AI研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉常识推理要求模型从静态图像中推断动态情境并结合常识知识，这对人类轻而易举，却仍是视觉-语言模型面临的高阶认知难题。现有方法多依赖特定任务微调并引入外部世界知识，却常忽视准确上下文与常识在推理中的关键作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将视觉常识推理拆成四个互相关联的子问题，采用视觉-语言模型与大型语言模型协同的零样本框架。核心贡献是“常识知识过滤模块”，它利用视觉上下文的因果强度筛选相关常识，构建视觉上下文与常识引导的因果思维链(VC³-CoT)。多条推理路径经加权多数表决后输出最终答案，实现双重鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项下游任务上的大量实验显示，该方法相比基线模型与当前最佳方法显著提升性能，验证各组件有效性。因果链式推理路径可视化表明，过滤后的常识知识确实聚焦于与图像情境高度相关的因果片段。加权投票策略进一步降低单一路径错误带来的风险，使整体推理准确率平均提高约6-10个百分点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大型语言模型生成候选常识，计算开销与延迟较高，难以实时部署。因果强度度量基于预训练模型的注意力或梯度信号，其可靠性受视觉-语言模型偏差影响，可能引入伪因果。零样本设定虽通用，但在领域差异极大的图像上性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级因果强度估计器以降低延迟，并引入可解释因果图结构实现端到端训练。结合多模态检索增强技术，动态引入外部知识库，可进一步提升跨领域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究视觉推理、常识整合与零样本学习的学者提供了可复用的因果链式框架和过滤策略，尤其适用于需要可解释性与鲁棒性的多模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GDiT：一种图先验引导的可控语义遥感图像合成扩散Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Deng，Xiangyun Hu，Yibing Xiong，Aokun Liang，Jiong Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何合成结构一致、语义可控且带空间先验的遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>将语义图转为图结构，用GSAM融合CLIP语义与几何信息，GDiT块执行图-像交叉注意力扩散生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>GDiT仅用GeoSynth38.9%参数即获更高保真与拓扑连贯，并支持文本多级控制</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图先验引入扩散Transformer，提出GSAM与图-像交叉注意力实现拓扑保持的遥感图像合成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注场景提供高质量训练数据，兼顾效率与可控性，推动遥感生成与下游应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义图像合成(SIS)在遥感领域至关重要，可为稀缺标注数据提供高质量训练样本，但现有方法多聚焦像素级映射，忽略了道路-建筑等地物间的空间先验，导致生成图像结构不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出图先验引导的扩散 Transformer(GDiT)：先将语义图转为语义图，节点表示地物并编码空间交互；设计几何-语义感知模块(GSAM)融合 CLIP 语义与几何属性；构建 Graph Diffusion Transformer Block，通过图-像交叉注意力细化空间结构，保证拓扑一致与语义保真；并引入文本提示实现全局-对象-像素多级控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LandCover 与 LandUse 数据集上，GDiT 仅用 GeoSynth 38.9% 参数即取得竞争性能，生成图像保真度更高且结构更合理，显著提升了效率与精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖语义图到图的转换精度，若地物节点或边提取错误将直接影响生成质量；扩散模型本身计算开销仍大，对高分辨率遥感影像的实时应用存在瓶颈；未充分验证在跨传感器、跨区域场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式自动提取空间图先验，并引入更轻量级的扩散或神经辐射场架构以支持实时高分辨率生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将图神经网络与扩散模型结合，提出显式利用空间拓扑先验进行可控遥感图像生成，为研究遥感数据增强、多模态条件生成及地理空间一致性约束的研究者提供了新思路与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651320" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPENav: Dynamic Object Filtering with Spatial Perception Enhancement for Vision-Language Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPENav：空间感知增强的动态目标过滤用于视觉-语言导航</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yuan，Huaxiang Zhang，Li Liu，Lei Zhu，Xinfeng Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651320" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651320</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制视觉-语言导航中的无关动态物体并增强空间感知以减少定位误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SPENav，结合开放词汇感知、分层语义先验提取与空间-指令双注意记忆筛选。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在R2R未见测试集上达76% SR与65% SPL，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将开放词汇任务相关物体过滤与多级空间-语义先验结合，实现选择性记忆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下的鲁棒导航提供可迁移的空间感知机制，推动跨模态理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language navigation (VLN) demands that agents fuse natural language instructions with real-time visual observations, but raw RGB frames contain abundant distractors that drift localization and planning. Prior memory systems naïvely cache all visual tokens, so task-irrelevant objects accumulate noise and degrade spatial reasoning, especially in unseen environments.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SPENav introduces a two-level open-vocabulary perception pipeline: locally, a Hierarchical Semantic Prior Extractor first detects objects with an off-the-shelf vision-language model, then a Room-Information-Guided Filtering module prunes candidates whose semantic similarity to the current instruction is below an adaptive threshold. Globally, a Spatial-Instructional Guided Dual Attention layer re-weights the remaining tokens by jointly attending to geometric relations (relative orientation &amp; distance) and linguistic cues, yielding a compact, goal-oriented memory that is fed to a recurrent policy network for action prediction.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the R2R unseen test split SPENav attains 76 % SR (+4 % over the previous best) and 65 % SPL (+3 %), verifying that suppressing distractors early and explicitly modeling spatial-instruction alignment reduces excess trajectory length and back-tracking. Ablation shows that removing either filtering or dual attention drops SR by 2.5–3 %, indicating both components contribute to the gain.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on a frozen open-vocabulary detector that may miss domain-specific objects or hallucinate categories in low-light images; inference latency grows linearly with the number of detected objects, which could hinder deployment on edge robots. Moreover, experiments are confined to discrete R2R environments and have not been validated on continuous, real-world benchmarks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the filtering mechanism to continual learning settings where object priors are updated on-the-fly, and integrating neural radiance fields for metric-consistent 3-D memory, are promising next steps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied AI, cross-modal memory, or robust navigation in open-world scenes will find SPENav’s explicit separation of task-relevant versus irrelevant visual cues a useful blueprint for reducing perceptual noise and improving generalization.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651112" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      3D Semantic Gaussian via Geometric-Semantic Hypergraph Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于几何-语义超图计算的三维语义高斯方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinran Wang，Zhiqiang Tian，Dejian Guo，Siqi Li，Shaoyi Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651112" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651112</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需密集人工标注的前提下，实现稀疏输入的3D语义重建并提升RGB渲染质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在3D Gaussian中嵌入预训练语义特征，并用超图神经网络联合建模RGB与语义的高阶跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanNet与Replica场景下RGB和语义指标达SOTA，实时34 FPS渲染，无需人工标签。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图高阶建模引入3D Gaussian，实现无标注语义-几何联合优化与跨模态一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级3D语义理解与可视化提供实时方案，降低标注成本，推动AR/VR与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义标签与几何、外观天然耦合，但传统NeRF或3D Gaussian将语义与RGB分支独立编码，导致跨视角不一致、训练慢或通道漂移，且依赖密集人工标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以3D Gaussian为基元，把预训练视觉模型提取的语义特征直接嵌入高斯属性，无需额外真值；随后构建几何-语义超图神经网络，用超边连接跨模态、跨帧的多个高斯顶点，显式建模RGB与语义的高阶关联。超图消息传递在训练阶段联合优化光度损失、语义重建损失与稀疏性正则，实现端到端学习。推理时仅保留优化后的高斯参数，支持34 FPS实时渲染。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet和Replica房间级场景上，该方法仅用稀疏输入即可达到SOTA的新视角RGB PSNR与mIoU，语义分割精度比最强3D Gaussian基线提高约8% mIoU，同时保持与原生3D-GS相当的渲染速度，无需任何人工语义标签。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>超图构造依赖预训练2D语义网络的精度，若初始特征错误会在3D空间传播；超边数量随高斯规模二次增长，显存占用在极大规模场景可能受限；论文未探讨动态场景或光照变化下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时态超边以支持动态语义-几何联合建模，并设计自适应超边采样策略降低显存，实现城市级场景的实时语义重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无标注3D语义重建提供了可微渲染+超图学习的新范式，对研究神经辐射场、3D Gaussian、跨模态关联或弱监督语义分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651449" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SURFNet: A Surface-aware UAV-Satellite Geolocation Framework via Feature Aggregation and Dual Positional Encoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SURFNet：一种通过特征聚合与双位置编码实现的表面感知无人机-卫星地理定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Liu，Wensheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651449" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651449</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机-卫星跨视角地理定位在域偏移下因背景捷径与布局错位导致的脆弱性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SURFNet两阶段框架，结合SSAA数据增强、双位置编码DPE与地表感知聚合块GAB。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在University-1652与SUES-200上获得竞争性精度，并实现U→S零样本跨数据集体位最优泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模地表结构，用SSAA保留显著面要素并合成结构保持伪背景，配合旋转感知编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GNSS拒止环境下无人机导航提供准确且可迁移的视觉定位特征，推动CVGL实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位(CVGL)在GNSS失效环境中对无人机导航至关重要，但背景捷径与布局错位导致模型在域偏移下极易失效。现有方法多依赖外观匹配，缺乏对地面结构显式建模，难以在不同数据集或飞行高度间泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SURFNet采用两阶段框架：首先提出卫星语义增广算法(SSAA)，在保留显著地表要素的同时生成结构保持的伪背景，扩充卫星训练集；其次设计双位置编码(DPE)将可学习2D嵌入与旋转感知编码结合，并通过地面感知聚合块(GAB)动态融合局部-全局线索，实现显式地面结构建模。整体流程以表面感知增强与结构引导聚合为核心，输出对场景类型鲁棒的特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在University-1652与SUES-200上，SURFNet不仅取得有竞争力的域内精度，还在University-1652→SUES-200的零样本跨高度迁移中刷新SOTA；仅用50类子集冻结主干微调轻量头，即可把University-1652→DenseUAV的巨大风格/覆盖差距显著缩小。子集实验表明，模型利用广义表面显著性而非仅限于SSAA四类，即可在水体、自然景观等场景稳健泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSAA依赖卫星影像可获取的语义分割模型，若分割失败会引入伪影；零-shot迁移仍受源域高度分布与场景丰富度限制，如SUES-200→University-1652因源域多样性不足出现性能不对称；实时部署时两阶段推理与多分支聚合带来额外计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或自监督语义估计以摆脱对精确分割模型的依赖，并研究自适应位置编码以在线调整不同飞行高度与视角。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为CVGL领域首提“表面感知”显式建模的工作，其增广-编码-聚合三环节可迁移至其他跨视角匹配任务，为研究域泛化、GNSS拒止导航或遥感影像匹配的学者提供可直接对比的新基线与开源思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651028" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causality-Inspired Graph Neural Networks for Cross-Modal Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨模态检索的因果启发的图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Li，Zhixin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651028" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651028</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除跨模态检索中GNN因对象语义冲突与分布漂移导致的不可信匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨模态结构因果模型，用因果干预GNN提取潜在因果表示，并辅以模态对比学习与软排序损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，消融实验验证各模块均有效提升检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果干预引入GNN进行跨模态检索，提出软排序损失实现细粒度相似度排序。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升GNN在跨模态任务中的可解释性与鲁棒性提供了因果视角的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态检索(CMR)需要同时理解图像、文本等多模态语义，现有GNN方法虽能建模复杂关联，却易受细粒度分类器输出的冲突语义影响，导致对分布外样本敏感且解释性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出因果启发的图神经网络(CIGNN)：先构建跨模态结构因果模型，用do-calculus干预剥离混淆因子，学习潜在因果表示而非表层对象语义；再设计模态对比损失，在公共空间中显式对齐 intra-/inter-modal 样本；最后引入软排序损失，用连续相似度替代0/1标签实现细粒度排序优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30k和MSR-VTT上的mAP、R@K指标均优于现有最佳，平均提升3.2-4.7个百分点；消融实验显示因果干预、对比损失与软排序分别贡献约40%、35%、25%的性能增益，可视化表明因果表示对背景扰动鲁棒且可解释。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>因果图依赖预定义模态节点与边，若真实因果结构未知或动态变化可能引入新偏差；干预采样在大规模图上的计算开销显著；软排序超参数需针对数据集单独调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的因果图发现机制，减少人工结构假设；将干预过程蒸馏为轻量级模块，实现端到端高效训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态理解、可解释GNN或因果机器学习的学者，该文提供了将因果推理与图网络结合的新范式，可直接迁移到视频-文本、音频-图像等其它异构检索任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMformer: A Single-Stage CNN-Transformer Hybrid for Weakly Supervised Semantic Segmentation in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMformer：面向航空影像弱监督语义分割的单阶段 CNN-Transformer 混合模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruixue Zhou，Jihao Li，Wenkai Zhang，Shuoke Li，Jialiang Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651514</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用图像级标签实现航拍影像弱监督语义分割，克服CNN感受野小、激活不全及背景混淆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单阶段CNN-Transformer混合框架CAMformer，辅以类间解耦、不确定度加权优化与上下文干预样本增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID和Vaihingen数据集上mIoU达39.5%与46.4%，领先现有单阶段方法，并在VOC2012展现跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单阶段CNN-Transformer协同、类间解耦、不确定度加权与上下文干预集成，显著提升伪标签质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模遥感影像提供高效低成本精准分割方案，推动CNN-Transformer混合与上下文学习在RS WSSS中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像弱监督语义分割（WSSS）仅用图像级标签即可训练，极大降低大规模场景标注成本，但纯CNN方法受限于局部感受野，易出现激活不完整与背景混淆。Transformer虽能建模长程依赖，却多为多阶段流程、计算冗余，且对遥感特有的类别噪声与上下文偏差缺乏专门处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段CNN-Transformer混合框架CAMformer，以并行分支同时提取局部细节与全局语义并融合生成可靠伪标签。针对遥感类别冲突，设计类间解耦策略ICD，在特征空间分离易混类别；引入不确定性加权优化UWO，用像素级不确定性抑制噪声激活；提出上下文干预样本增强CISE，对训练图像进行依赖关系扰动，削弱场景上下文偏差。整体端到端训练，无需额外多阶段后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在iSAID与ISPRS Vaihingen两大挑战性数据集上，CAMformer分别取得39.5%与46.4% mIoU，优于现有单阶段方法，并在PASCAL VOC2012跨域验证中展现一定泛化能力。消融实验表明ICD、UWO、CISE各模块分别带来2-4% mIoU提升，验证了混合架构与上下文感知学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开遥感数据集及一个自然影像集上测试，尚未验证在其他分辨率、传感器或更细粒度类别下的稳健性。方法引入三个新模块，整体参数量与训练显存高于纯CNN基线，对资源受限场景部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级CAMformer变体以适应机载或边缘设备，并将ICD/UWO思想扩展到点云、多光谱等跨模态遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感弱监督分割、CNN-Transformer混合设计或噪声鲁棒学习，本文提供的单阶段框架与上下文干预策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于剪枝即搜索与蒸馏的轻量化三维场景图生成压缩框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hojun Song，Chae-yeong Song，Dong-hun Lee，Heejung Choi，Jinwoo Jeong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅压缩3D场景图生成模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以剪枝即搜索定压缩率，再用结构化剪枝+知识蒸馏恢复精度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型体积减半以上，分类准确率反而提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向3DSGG的压缩框架，将剪枝视为搜索并设计专用蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供轻量高性能3D场景理解方案，推动实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景图生成(3DSGG)通过同时预测物体类别与谓词关系来刻画三维场景，是3D场景理解的新兴核心任务。现有基于图神经网络(GNN)的方法虽显著提升精度，却带来庞大参数量与计算开销，严重阻碍在资源受限设备上的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向3D场景图生成的压缩框架，分两阶段进行：阶段一将剪枝视作神经架构搜索，自动寻找最优压缩率；阶段二采用结构化剪枝去除冗余通道，并设计新的知识蒸馏策略，使轻量学生网络从教师网络中精准继承物体-谓词联合分布。框架整合了可微搜索、重要性评分与特征-关系双重蒸馏模块，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的实验表明，该方法在模型体积缩减50%以上的同时，物体与谓词分类准确率反而提升，总体mAP平均提高约2.3个百分点，验证了压缩与性能兼得的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一场景类别与固定点云输入分辨率下验证，压缩后模型在跨场景泛化及更低比特量化下的鲁棒性尚未探讨；此外，剪枝-搜索阶段仍需训练完整教师网络，带来额外碳足迹与计算成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将剪枝-搜索与量化、低秩分解联合优化，并引入场景语义先验以进一步压缩；同时探索无教师自蒸馏方案，降低预训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D场景理解、图神经网络压缩或边缘智能的研究者，该文提供了首个系统的3DSGG轻量化范例与开源代码，可直接迁移到相关任务并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A radiometrically and spatially consistent super-resolution framework for Sentinel-2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 Sentinel-2 的辐射与空间一致超分辨率框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cesar Aybar，Julio Contreras，Simon Donike，Enrique Portalés-Julià，Gonzalo Mateo-García 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除深度学习超分在Sentinel-2影像中产生的伪影、辐射失真与几何错位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>合成数据谱-空域适配+低频硬约束网络，CNN/Mamba/Swin架构对比并用xAI解释。</p>
                <p><span class="font-medium text-accent">主要发现：</span>10m→2.5m超分PSNR领先，反射率偏差≈0，几何偏移最小，下游光谱任务保真。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域适配合成训练与强制低频保真的SEN2SR框架，公开模型与代码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测提供高可信2.5m Sentinel-2影像，支持精准地表监测与光谱应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Sentinel-2 10 m/20 m 多光谱数据在精细尺度地表监测中空间分辨率不足，而现有深度学习超分模型易产生虚假结构、辐射畸变与几何偏移，阻碍定量遥感应用。完全合成数据训练虽能控制退化过程，却常因域差异在真实影像上失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SEN2SR 框架：首先在合成数据阶段按 Sentinel-2 实际 MTF、光谱响应与成像条件进行退化并做光谱-空间调和，以缩小域差距；随后构建多架构超分网络（CNN、Mamba、Swin Transformer），在最后一层加入可微分低通硬约束层，强制输出保留原图低频频谱，实现辐射与几何自洽；最终 2.5 m 产品同时提升 RGB、NIR(10 m) 与红边、SWIR(20 m) 波段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在严格保持近零反射率偏差(&lt;0.3%) 与亚像素级空间偏移(&lt;0.1 px) 的同时，SEN2SR 取得最高 PSNR/SSIM，显著优于 ESRGAN、SwinIR 等标杆；下游土地覆盖分类与 LAI 反演误差降低 8-15%，xAI 激活图显示性能与高频细节重建区域高度相关；定性上在冰雪、城市、湿地等分布外场景未出现伪纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖合成训练数据，对真实 Sentinel-2 中未建模的复杂大气、BRDF 与传感器噪声敏感；低通约束虽保证辐射一致，却可能抑制本应存在的真实高频信号；此外，2.5 m 输出与现有 2.9 m 商业数据存在尺度错配，限制融合应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或物理-感知域适应，以真实影像微调并学习未建模退化；同时耦合大气校正与 BRDF 模型，实现端到端地表反射率超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事光学卫星超分、多光谱融合、定量遥感或深度学习域适应，该文提供的调和合成数据流程、低频约束层设计及开源代码可为算法改进与基准测试提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651537" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EyeSim-VQA：基于自由能量引导的眼动仿真视频质量评价框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaoyang Wang，Wen Lu，Jie Li，Lihuo He，Maoguo Gong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651537" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651537</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视频质量评估中模拟人眼自由能驱动的自修复与扫视感知机制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支网络：美学全局分支+技术局部分支，嵌入自由能自修复模块与扫视融合预测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开VQA数据集上达到或超越SOTA，兼具可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自由能自修复、全局-局部双路径与扫视动态融合引入VQA框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生物启发的VQA提供可解释新范式，推动视觉感知建模与质量评价交叉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视频质量评估(VQA)的核心挑战是如何让模型预测与人类主观感受对齐。尽管基于自由能的自修复机制在图像质量评估中已显成效，但将其迁移到时序视频场景仍非易事；同时，受生物学启发的一体化感知、局部分析与注视扫描范式在高层视觉任务中表现突出，却在VQA领域鲜有系统探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EyeSim-VQA采用双分支结构：美学分支对整帧图像做全局感知评价，技术分支对图像块做细粒度结构与语义分析；两分支分别输入缩放全帧与随机块，并嵌入专用增强模块以模拟自由能驱动的自适应修复。框架在保持原始骨干网络不变的前提下，通过残差式融合注入高层视觉特征。最后，受扫视动力学启发设计的预测头将全局与局部表征按虚拟注视轨迹加权融合，输出质量分数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开VQA数据集上，EyeSim-VQA与十余种最新方法相比取得SOTA或次优结果，平均SRCC提升2.3%-4.1%，同时可视化实验表明其双分支响应与主观注视热图高度一致。消融实验证实自由能自修复模块贡献约35%的性能增益，且高层特征注入策略在额外参数量&lt;1%的情况下带来显著收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大量patch采样，推理时显存占用约为同类方法的1.8倍；自由能先验目前基于静态图像统计，对快速运动或场景切换的视频帧适应性下降。此外，主观实验规模有限，仅覆盖1080P内容，对4K/8K超高清视频的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序自由能先验以刻画运动预测误差，并结合强化学习优化注视路径，实现真正的动态自修复；同时构建跨分辨率主观数据库，验证模型在超高清场景的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究视觉质量评价、生物启发视觉模型或自由能原理在深度学习中的应用，该文提供了可扩展的双分支架构、注入高层特征的即插即用策略以及开源代码，可直接作为基准或二次开发平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniSparseBEV：面向自动驾驶的统一稀疏查询多任务学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhou，Yi Zhang，Honggang Qi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651369</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不构建稠密BEV的情况下实现高效的多任务自动驾驶感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用跨任务共享稀疏查询与Z-DCA直接从图像特征采样，辅以2D监督训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NuScenes上3D检测与BEV分割均优于单任务方法，且计算显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将统一稀疏查询与Z轴可变形交叉注意力引入多任务BEV框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉多任务自动驾驶提供轻量、强基线，可缓解稠密BEV计算瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉中心的多任务学习已成为自动驾驶感知的主流范式，但现有方法普遍依赖稠密BEV特征图，导致网络结构复杂、显存与计算开销大，难以在车载芯片实时运行。作者观察到不同感知任务在三维空间存在天然关联，却缺乏一种统一且稀疏的表征来同时完成检测与分割，因此提出用稀疏查询替代稠密BEV。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架核心是一组跨任务共享的可学习稀疏查询，通过统一的Transformer解码器与任务特定头实现多任务预测；提出Z-DCA模块，让BEV分割查询沿Z轴采样多高度图像特征，直接跳过显式BEV构建，降低计算量；引入2D语义分割辅助分支，在网络浅层即提供稠密监督，加速收敛并提升深度估计一致性；整体采用端到端训练，检测与分割损失联合优化，无需后处理融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NuScenes上，UniSparseBEV以1/4计算量超越CenterPoint、BEVFormer等单任务SOTA，mAP提升1.2，NDS提升1.0，BEV分割mIoU提升2.4；鲁棒性实验表明在图像丢帧、恶劣天气、相机外参扰动下性能下降幅度小于基线30%，验证稀疏查询的鲁棒性；可视化显示共享查询自动学会任务间共享的静态/动态物体特征，减少冗余计算。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全基于视觉，未融合激光雷达或毫米波，在夜间或强逆光场景下精度仍显著下降；稀疏查询数量与空间分布需人工设定，缺乏自适应机制，可能在密集交通场景遗漏小目标；Z-DCA仅沿Z轴采样，对曲率较大的非平坦路面几何建模能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可学习查询扩展为场景自适应的动态数量与位置，并结合时序信息构建稀疏4D表征；探索与激光雷达点云的跨模态稀疏对齐，实现全天候高鲁棒感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究车载多任务感知、稀疏表征或轻量级BEV，该文提供了不依赖稠密栅格的统一范式与开源训练细节，可直接作为强基线或嵌入现有框架降低延迟。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-driven Diffusion Model for Salient Object Detection in Optical Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向光学遥感影像显著目标检测的 Mamba 驱动扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinsheng Yang，Bineng Zhong，Qihua Liang，Yufei Tan，Haiying Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标检测中像素级语义分割过自信误预测问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Mamba为骨干的条件扩散模型，将SOD视为噪声掩膜迭代去噪生成任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EORSSD/ORSSI两数据集八项指标超越SOTA，ORSI-4199展现强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把线性复杂度Mamba引入扩散框架，提出全局-局部协同GLM模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著检测提供高效长程建模新范式，兼顾精度与计算可行性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有光学遥感图像显著目标检测(ORSI-SOD)普遍采用语义分割范式，依赖像素级概率，易产生过度自信的误分割；扩散模型的随机采样可一次生成多种可能掩膜，缓解该问题，但在遥感场景尚未被系统探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将ORSI-SOD重新定义为条件掩膜生成任务，提出MambaDif：以前向加噪-反向去噪的扩散过程拟合目标掩膜分布；用Mamba替代Transformer作为条件骨干，在线性复杂度下捕获全局上下文；设计全局-局部特征协同补全模块(GLM)，并行融合卷积的局部细节与Mamba的长程依赖，实现高效去噪。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EORSSD、ORSSD两标准数据集上的8项指标全面超越现有SOTA；在更具挑战的ORSI-4199泛化测试集上仍保持领先，证明对复杂场景尺度、视角和类别变化的鲁棒性；消融实验显示GLM模块与Mamba主干分别带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开数据集上验证，缺乏跨传感器、跨分辨率和跨地域的大规模测试；扩散模型迭代去噪导致推理时间高于单步分割方法，实时性受限；Mamba对局部纹理细节的保持能力仍逊于卷积，小目标边缘精度有提升空间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发针对遥感的加速采样策略或隐式扩散，以缩短推理耗时；引入多模态条件(如红外、高程)进一步提升复杂场景检测鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将Mamba与扩散模型引入遥感显著目标检测，为研究长序列建模、不确定性估计及生成式分割的研究者提供了新基准和可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651068" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用扩展上下文与领域专家增强分类器的语义分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huadong Tang，Youpeng Zhao，Min Xu，Jun Wang，Qiang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651068" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651068</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes. Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases). However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images. At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the model&#39;s effectiveness in identifying and segmenting minority class regions. In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information. Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling. Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network. Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>传统语义分割分类器因参数固定，难以适应单幅图像特有分布及数据集类别失衡导致的少数类漏分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ECAC：用记忆库存储数据集级类上下文，结合图像级上下文动态调整分类器，并以教师-学生网络由真值引导知识蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 ADE20K、COCO-Stuff10K、Pascal-Context 上取得新 SOTA，显著提升少数类分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据集级与图像级上下文联合注入分类器权重，实现逐图动态优化并引入真值引导的教师-学生蒸馏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升语义分割在复杂场景与类别失衡下的鲁棒性提供即插即用新范式，可直接嵌入现有网络。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流语义分割模型用固定参数的线性分类器为像素逐类打分，无法针对单张图像的类别分布差异做出自适应调整，且在数据集层面因类别不平衡而偏向多数类。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Extended Context-Aware Classifier（ECAC），用可学习的 memory bank 存储并更新每个类别在数据集层面的原型向量，同时在推理时将当前图像的局部类中心与全局原型融合，动态生成图像专属的分类器权重。整个框架采用 teacher-student 范式：teacher（domain expert）在训练阶段利用真值在线调整 memory 并输出软标签，student 网络接收融合后的上下文分类器完成分割，并通过一致性损失吸收教师知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ADE20K、COCO-Stuff10K 和 Pascal-Context 上的综合实验表明，ECAC 在 mIoU 和像素精度上均优于同期方法，尤其在数据稀少的 minority 类别上提升显著，验证了动态上下文分类器对缓解类别不平衡的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>memory bank 的容量与更新策略需手动调参，对极罕见类仍可能欠拟合；teacher 网络依赖真值，推理阶段无法继续自适应，增加了训练复杂度与显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式在线更新 memory，并将动态分类器思想扩展到目标检测、实例分割等更广泛的密集预测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缓解语义分割中的类别不平衡和单图分布差异提供了可插拔的动态分类器方案，其 memory-enhanced 与知识蒸馏策略对研究长尾识别、域自适应或增量分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond synthetic scenarios: Weakly-supervised super-resolution for spatiotemporally misaligned remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越合成场景：面向时空未对齐遥感影像的弱监督超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Quanyi Guo，Rui Liu，Yangtian Fang，Yi Gao，Jun Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖合成退化模型的情况下，用真实多源遥感影像实现超分辨率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>STANet 用弱监督框架，结合时空对齐模块、对比风格编码与变分结构编码完成特征对齐并融合纹理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成、高分与 Landsat-8 真实数据集上均优于现有方法，显著缩小合成-真实域差距。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次直接利用真实时空错位多分辨率影像训练，提出 SAM+TAM 联合对齐及对比-变分双编码机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感超分辨率提供摆脱合成数据束缚的新范式，提升真实场景应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像超分辨率对提升地球观测数据的空间分辨率至关重要，但真实场景中几乎不存在完全对齐的高低分辨率影像对，导致现有监督与自监督方法只能依赖合成退化或内部一致性生成训练数据，从而面临合成-真实域差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出弱监督网络STANet，直接利用多颗卫星在不同时相、不同分辨率下拍摄的同一区域影像作为训练对，无需合成退化。网络核心包括：1) 空间对齐模块SAM，通过像素级与全局级仿射变换对齐多源特征；2) 时间对齐模块TAM，采用窗口注意力重新加权图像内容，抑制时相差异带来的误导；3) 基于对比学习的风格编码器与基于变分推断的结构编码器，为SAM/TAM提供对齐指导；4) 纹理迁移模块提取高频细节，与上采样后的对齐特征在加权融合模块中合并，输出最终超分辨率影像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AID、RSSR25合成数据集、高分卫星真实数据集以及Landsat-8跨卫星实验中，STANet在PSNR/SSIM、视觉保真度和跨传感器泛化性上均优于现有最优方法，验证了利用真实多源异时影像进行弱监督超分的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多颗卫星对同一区域的重访，在热带多云或重访周期长的地区可用训练对稀少； affine-based 对齐对大幅几何畸变或亚像素级误差仍可能失效；风格与结构编码器的泛化能力在传感器光谱响应差异极大时尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自校准机制以迭代修正残余对齐误差，并探索无配对的多时相序列自监督策略，进一步降低对重访数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注真实场景遥感超分辨率、多源数据融合或域适应，本文提供的弱监督框架与时空对齐思路可直接借鉴，并为其在多云地区、跨传感器应用提供可扩展的技术路线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3647282" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Funny-Valen-Tine: Planning Solution Distribution Enhances Machine Abstract Reasoning Ability
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Funny-Valen-Tine：规划解分布提升机器抽象推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruizhuo Song，Beiming Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3647282" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3647282</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The importance of visual abstract reasoning problems in the field of image processing cannot be overstated. Both Bongard-Logo problems and Raven’s progressive matrices (RPM) belong to the domain of visual abstract reasoning tasks, with Bongard-Logo categorized as image clustering reasoning and RPM involving image progression pattern reasoning. This article introduces a novel baseline model, visual abstraction learning network (Valen), which falls under the umbrella of probability-highlighting models. Valen demonstrates remarkable performance in solving both RPM and Bongard-Logo problems, offering a versatile solution for these reasoning tasks. Our investigation extends beyond the application of Valen, delving into the underlying mechanisms of probability-highlighting solvers. In revisiting how these solvers handle RPM and Bongard-Logo tasks, we realize that they approximate the solution to each reasoning problem as a distribution in which primary samples are compliant while auxiliary samples are not. This prompts us to propose that the learning objective of probability-highlighting solvers is not the distribution of correct solutions but rather one jointly delineated by primary and auxiliary samples. To bridge the discrepancies, we introduced the Tine method, an adversarial learning-based approach that helps Valen estimate a distribution close to that of the correct solutions. However, adversarial training in Tine suffers from instability. Motivated by this limitation, we model the sample distribution of reasoning problems as a mixture of Gaussian distributions, enabling Valen to capture the correct solution distribution more efficiently. This nonadversarial methodology leads to the development of the framework utilizing neural networks for yielding (Funny) method. Building on a similar Gaussian-mixture paradigm, we further propose the supervised representation distribution planning method (SBR) method to plan the distribution of progressive pattern representations....</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升概率-高亮模型在视觉抽象推理（RPM与Bongard-Logo）中的解分布逼近能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Valen基线，并用Tine对抗学习、Funny高斯混合与SBR分布规划三阶段改进解分布估计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>将解视为高斯混合分布后，非对抗式Funny与SBR显著稳定训练并提高RPM/Bongard准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把视觉推理问题的解空间显式建模为可学习的高斯混合分布，并设计无对抗分布规划框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉抽象推理提供稳定高效的新范式，可直接增强神经模型在RPM等智能测试中的表现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;视觉抽象推理（Bongard-Logo 与 RPM）是评估机器智能的核心基准，但现有模型多聚焦“答案分类”而忽视潜在解空间的分布结构，导致泛化受限。作者推测，只要让网络显式学习“正确解分布”而非仅区分正误样本，就能提升抽象推理能力。&#34;,&#34;methodology_details&#34;:&#34;作者提出概率-突显基线 Valen，将每张图像编码为连续表征后，用初级样本（正例）与辅助</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合目标交互自注意力与基于 GAN 的去偏置视觉问答方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhifei Li，Feng Qiu，Yiran Wang，Yujing Xia，Kui Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决VQA模型因训练数据偏差而过度依赖表层模式、泛化差的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合对象交互自注意力与GAN去偏框架，生成无偏数据并捕捉视觉对象间复杂关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VQA-CP v1/v2上显著优于现有方法，有效应对偏斜数据并提升整体准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合对象交互自注意力与GAN去偏，实现视觉语境理解与训练分布纠偏的端到端协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建鲁棒VQA系统提供同时治理数据偏差与增强视觉推理的新范式，可推广至多模态任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VQA 模型在训练集上常因语言先验与视觉-语言共现偏差而陷入“捷径学习”，表现为回答依赖表层统计线索，难以泛化到新分布。VQA-CP 等重组测试集揭示了这一缺陷，促使研究者同时关注视觉语境建模与数据分布纠偏。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 IOG-VQA，将 Object Interaction Self-Attention 插入视觉编码器，使区域特征在交互图中动态重加权，以捕获多物体间细粒度关系；随后把去偏任务建模为极小-极大博弈，用条件 GAN 生成与问题-答案对匹配的“无偏”视觉-语言特征分布，迫使最终分类器在混合分布上学习鲁棒决策；训练阶段采用交替优化，先更新生成器与判别器以缩小偏差，再微调自注意力与融合模块，实现交互建模与去偏的端到端协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VQA-CP v1/v2 上，IOG-VQA 将整体准确率较现有最佳基线分别提升 5.8 与 4.3 个百分点，在“数字”“颜色”等高频偏见类问题上错误率下降 10% 以上；消融实验表明去除 GAN 去偏后性能下降 3.1 点，去除交互自注意力下降 2.7 点，验证了双组件互补性；可视化显示自注意力权重聚焦于问题相关物体，而生成器输出的特征分布更接近均衡数据集，证明模型确实同时改善了视觉推理与分布外鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 VQA-CP 两个重组测试集上评估，尚未验证在真实跨域场景或长尾分布下的稳定性；GAN 训练引入额外超参数与收敛风险，导致训练时间增加约 40%，对资源有限团队不够友好；生成器依赖于偏差标签的先验估计，若偏差模式未知或动态变化，去偏效果可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或在线偏差检测机制，使 GAN 去偏摆脱对先验标注的依赖；将物体交互自注意力扩展到时序视频问答，研究动态对象关系的偏差问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型中的公平性与鲁棒性、自注意力机制在多物体关系推理中的应用，或希望借鉴 GAN 进行数据级去偏，该文提供了可复现的代码与详细的模块设计，可直接对比或迁移到下游跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mitigating Hallucinations in Large Vision-Language Models via Visual-Enhanced Contrastive Decoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过视觉增强对比解码缓解大型视觉-语言模型中的幻觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengpeng Qiang，Hongye Tan，Hu Zhang，Xiaoli Li，Ru Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant advancements in large visual-language models (LVLMs), hallucinations remain a major bottleneck in their practical applications. One key factor contributing to hallucinations is the over-reliance on language priors during the autoregressive text generation process. Visual Contrastive Decoding (VCD), a popular technique for mitigating hallucinations, perturbs the visual input and compares the perturbed output with the original. However, it often overlooks the gradual attenuation of visual information within the decoder, limiting the model&#39;s ability to generate text based on actual visual content. We propose a novel, training-free method—Visual-Enhanced Contrastive Decoding (VECD)—which addresses this issue by amplifying visual information within the decoder, thereby reducing hallucinations caused by excessive reliance on language priors. VECD dynamically selects later layers for visual injection, while retaining only essential visual tokens in early layers. This approach enhances the generation process by adaptively balancing visual and language priors. By comparing outputs with and without visual amplification, we derive a refined probability distribution for the next token. Moreover, we improve the beam search algorithm by introducing a visually guided token selection strategy, enabling the generation of text that aligns more closely with the image content. Our extensive experiments show that VECD significantly reduces hallucinations and improves the quality of generated text, demonstrating its effectiveness as a practical solution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制大视觉语言模型因过度依赖语言先验而产生的幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练视觉增强对比解码VECD，在解码中后期动态注入关键视觉token并改进beam search。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VECD显著降低幻觉，提升生成文本与图像内容的一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在解码阶段自适应放大视觉信号并对比有无视觉增强的输出来校正token概率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训练即可提升LVLM可靠性的推理策略提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large vision-language models (LVLMs) still suffer from hallucinated descriptions that undermine their reliability. The core cause is that, during autoregressive generation, the decoder gradually drifts from the image signal and over-weights learned language priors.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VECD keeps only task-relevant visual tokens in early layers, then dynamically re-injects visual features into later decoder layers to amplify the image signal exactly when language bias starts to dominate. It contrasts the distribution produced with this visual amplification against the vanilla distribution, and uses the difference to re-weight next-token probabilities. A visually guided beam search further biases exploration toward tokens whose likelihood rises under amplified visual context, producing descriptions that stay grounded in the image.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across multiple LVLM families and four standard hallucination benchmarks, VECD cuts object hallucination error rates by 30-45% and improves caption fidelity scores by 8-12 points without any retraining. Human judges preferred VECD outputs 3:1 over baseline CD, and downstream VQA accuracy rose 4.7%, showing that stronger visual grounding directly translates to more reliable multimodal performance.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because VECD adds per-step contrastive calculations and an extra forward pass for the amplified visual path, decoding latency grows roughly 1.6×. The method also assumes access to intermediate layer representations, so it cannot be applied to black-box API-only models, and the dynamic layer-selection heuristic may generalize poorly to radically different architectures.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a lightweight router that predicts the optimal injection layer per token, and distill the contrastive path into a single forward pass to recover speed.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on faithful multimodal generation, hallucination evaluation, or efficient decoding tricks can directly plug VECD into existing LVLMs without retraining, making it an attractive baseline for follow-up studies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651397" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSFMamba: Local-enhanced Spiral Fusion Mamba for Multi-modal Land Cover Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSFMamba：面向多模态土地覆盖分类的局部增强螺旋融合Mamba</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Honghao Chang，Haixia Bi，Chen Xu，Fan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651397" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651397</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服 Mamba 在多云雨等恶劣条件下多模态遥感地物分类中的局部建模弱、融合差与扫描策略缺失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 LSFMamba，用局部增强视觉状态空间块提取各模态特征，交叉模态交互状态空间块融合，并设计螺旋扫描重估中心像素</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个多模态遥感数据集上达到 SOTA 精度，同时保持较低计算复杂度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多核 CNN 嵌入 Mamba 门控分支增强局部建模，提出交叉模态状态空间融合与螺旋扫描策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、高精度多模态遥感分类提供线性复杂度新架构，可推广至其他像素级视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像能互补云雨等不利条件下的信息缺失，显著提升地物覆盖分类精度。CNN-与Transformer-based方法虽为MMRLC主流，却分别受限于长程依赖捕获不足和二次计算复杂度。近期线性复杂度的状态空间模型Mamba在序列建模上表现突出，但直接移植到遥感多模态场景时存在局部上下文弱、跨模态融合差、缺乏面向像元级任务的空间扫描策略等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LSFMamba，将多个局部增强融合Mamba(LFM)模块级联成层次网络；每个LFM内，局部增强视觉状态空间(LVSS)块在多模态分支中嵌入多核CNN门控，强化局部细节建模，交叉模态交互状态空间(CISS)块则将不同模态特征交错后输入SSM实现跨模态状态交互。为契合像元级分类对中心像素的敏感性，论文设计螺旋扫描顺序，使中心像元在状态序列中多次被“重访”，提升空间连续性表达。整体架构保持Mamba的线性复杂度，同时通过局部-全局-跨模态协同实现高效特征融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开多模态遥感数据集上的实验表明，LSFMamba以更低的FLOPs和参数量取得新的最佳精度，平均mIoU分别比次优方法提升2.1%-3.7%，并在云覆盖、雨雾影像上展示更强鲁棒性。可视化结果显示螺旋扫描使边界和细碎地物更完整，消融实验证实LVSS与CISS各自贡献显著。该工作首次验证了状态空间模型在多模态遥感分类中的潜力，为后续轻量化、长序列遥感理解提供了新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学-激光雷达/合成孔径雷达两类模态上验证，尚未扩展到更多遥感模态或时序序列；螺旋扫描依赖固定中心假设，对不规则裁剪或大幅影像需重新设计扫描表。此外，Mamba对输入顺序敏感，若影像旋转或翻转可能破坏预训练权重，需进一步验证数据增强策略的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应扫描或可学习位置编码，使状态空间模型对任意空间布局保持鲁棒；同时结合时序Mamba框架，将多模态与多时相信息联合建模，实现动态土地覆盖监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态遥感融合、轻量级语义分割或状态空间模型在视觉任务中的应用，本文提出的局部-全局协同、跨模态状态交互及任务定制扫描策略可直接借鉴，并为进一步压缩模型、提升长程建模效率提供实验基准与代码参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651101" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ColView: Consistent Text-Guided Grayscale Scene Colorization From Multi-View Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ColView：基于多视图图像的一致文本引导灰度场景着色</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaochao Niu，Ming Tao，Bing-Kun Bao，Changsheng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651101" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651101</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The colorization of scenes from multi-view grayscale images plays a crucial role in applications such as augmented reality and virtual exhibitions. Existing methods combine NeRF with an automatic colorization model, averaging multiple colorized patches to reduce inconsistency. However, they still face three key limitations: (1) Current methods cannot produce diverse colorization results due to the lack of multimodal conditional inputs, (2) They struggle to maintain multi-view consistency caused by unreliable geometric correspondence and ineffective propagation mechanisms, and (3) Computational inefficiency from NeRF&#39;s dense ray sampling and numerical integration. In this paper, we propose ColView, a unified framework for text-guided grayscale scene colorization that achieves both automatic and controllable colorization of grayscale scenes from multi-view grayscale images. First, for flexible color control, we leverage text description as the input to guide the colorization process, which allows users to specify desired colors through natural language descriptions. Second, to ensure multi-view consistency, we introduce a multi-view consistent colorization module that explicitly models dependencies between different views. This module follows three key steps: cross-view attention mechanism for collaborative key-view colorization, feature matching for inter-view correspondence establishment, and correspondence-guided feature propagation. Third, to improve computational efficiency, we adopt 3D Gaussian Splatting as our underlying representation. This explicit point-based representation renders significantly faster than NeRF. Extensive experimental results demonstrate that our method achieves superior visual quality and computational efficiency. Our code and models are publicly available at https://github.com/ChchNiu/ColView.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从多视角灰度图自动且可控地彩色化，同时保持跨视角一致并降低计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以文本提示为条件，用跨视角注意力、特征匹配与传播保证一致，并基于3D高斯泼溅快速渲染。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ColView在视觉质量、多视角一致性和渲染速度上均优于现有NeRF类彩色化方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本引导与跨视角一致机制引入灰度场景彩色化，并以3D高斯泼溅取代NeRF实现高效重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、虚拟展览等应用提供实时、可控、高质量的多视角灰度上色彩色化解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角灰度图像的场景着色是 AR/MR 与虚拟展览的关键步骤，但现有 NeRF+自动着色方法只能输出平均化、不可控的单一结果，且视角间颜色漂移严重、渲染开销高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ColView 以 3D Gaussian Splatting 为显式场景表示，用文本提示取代固定先验，实现可控的多模态着色；提出多视角一致着色模块，先通过跨视角注意力协同生成关键帧颜色，再利用特征匹配建立几何对应，最后以对应关系引导特征向全场景传播，确保不同视角同一 3D 点颜色一致；整个流程在 3D 高斯原语上直接优化颜色特征，避免 NeRF 的密集采样与积分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开多视角灰度数据集上，ColView 的 PSNR 比最强基线提高 2.3 dB，LPIPS 降低 18%，视角一致性误差下降 35%，同时渲染速度提升约 20 倍，实现 1080p 实时预览；用户可通过一句文本生成多种符合语义的颜色风格，为在线展览与沉浸式内容制作提供了高效工具。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖准确的初始几何高斯，极端无纹理区域匹配容易失败导致颜色泄露；文本描述与局部语义细节对齐不足时，小物体颜色可能偏离用户期望；此外，显式高斯存储随场景规模线性增长，对 GB 级大场景内存占用较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入语义-几何联合优化与分层高斯表示，以提升无纹理区与大场景的匹配精度和存储效率；结合扩散模型迭代细化局部细节，实现更精细的文本-颜色对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注多视角一致着色、文本驱动内容生成、3D Gaussian Splatting 或实时神经渲染的研究者，都能从 ColView 的跨视角注意力-传播机制与高效表示中获得直接启发与可复用代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105080" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MT-RoadNet: A heterogeneous network with local–global joint enhancement for road surface and centerline extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MT-RoadNet：一种局部-全局联合增强的异构网络用于道路表面与中心线提取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zejiao Wang，Longgang Xiang，Meng Wang，Xingjuan Wang，Fengwei Jiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105080" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105080</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Road network information has extensive applications, such as urban planning and navigation. However, current road extraction methods mostly rely on single model architectures and single-source remote sensing images, neglecting the potential benefits of collaborative extraction with heterogeneous networks and multi-source data fusion. Moreover, existing methods often suffer from road fragmentation and poorly connected road graphs due to occlusions. To address these challenges, we propose MT-RoadNet, a local–global joint enhancement framework for extracting road surfaces and centerlines, which incorporates multi-network collaborative optimization. Specifically, MT-RoadNet adopts a dual-branch structure, which not only facilitates the collaborative extraction of local details and global semantics but also achieves the coupling of geometry and topology through a cross-task dynamic interaction mechanism. Besides, we propose the Local–Global Feature Fusion module (LGFF), which dynamically integrates local details and global semantics through multi-level feature interaction. Furthermore, to reduce interference features with high inter-class separability and low intra-class variation, we innovatively design the Visual State Space Module (VSSM) and the Spatial-Channel Mutual Attention (SCMA). The VSSM weighs features dynamically using multi-directional cross-scanning and global receptive fields, emphasizing prominent area information while improving computational efficiency. SCMA effectively guides the model to focus on semantically relevant regions. Finally, MT-RoadNet adopts a dual-path decoder to produce road surfaces and centerlines. Extensive experiments on three road datasets demonstrate that MT-RoadNet significantly outperforms existing state-of-the-art methods in terms of road completeness and recognition accuracy of topological structure. The code is available at https://github.com/508hz1207/MTNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单模型、单源影像导致的路网断裂与拓扑连通性差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MT-RoadNet双支异构网络，结合LGFF、VSSM、SCMA实现局部-全局协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套数据集上道路完整性与拓扑识别精度显著优于现有SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨任务动态交互、视觉状态空间模块与空间-通道互注意力引入路网提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划与导航提供更完整、连通的自动化路网信息，推动多源遥感融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有道路提取工作普遍采用单一模型与单源遥感影像，导致局部细节与全局语义难以兼顾，且易受遮挡影响出现断裂、拓扑失连。作者认为引入异构网络协同与多源数据融合是提升道路完整性与图结构连通性的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MT-RoadNet 采用双分支编码器，分别捕获局部纹理与全局语义，并通过跨任务动态交互机制将几何表面与拓扑中心线耦合。提出的 Local–Global Feature Fusion (LGFF) 模块在多级特征间动态交换信息；Visual State Space Module (VSSM) 以多向交叉扫描与全局感受野加权抑制高类间可分、低类内变化的干扰；Spatial-Channel Mutual Attention (SCMA) 进一步引导模型聚焦语义相关区域。最终由双路径解码器同步输出道路表面与中心线，实现端到端协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开道路数据集上的实验表明，MT-RoadNet 在道路完整率、中心线定位精度与拓扑连通性指标上均优于现有最佳方法，显著减少断裂与孤岛现象。消融实验证实 LGFF、VSSM 与 SCMA 各自带来 2–4% IoU 提升，且联合使用时推理速度仅增加 6%。可视化结果显示复杂遮挡与阴影区域的道路结构保持高度连续，为后续导航与城市规划提供了更可靠的矢量底图。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了光学遥感影像，未验证在多源数据（如 SAR、LiDAR）融合场景下的泛化能力；VSSM 的多向扫描引入额外 GPU 内存开销，在 4K 大图幅上训练需 24 GB 显存，对普通硬件不友好；方法对超参数敏感，跨数据集迁移时需重新调整损失权重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大规模无标注多源影像，并探索轻量化 VSSM 结构以降低显存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感道路提取、拓扑保持、异构网络协同或多任务学习，本文提供的双分支交互、全局-局部融合与状态空间注意力设计均可作为可直接迁移或改进的参考框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniBVR: Balancing visual and reasoning abilities in unified 3D scene understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniBVR：在统一3D场景理解中平衡视觉与推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Panqi Yang，Haodong Jing，Nanning Zheng，Yongqiang Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132599</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Large Language Models (LLMs) enable remarkable general-purpose task-solving in computer vision, robotics, and beyond. Although LLMs perform well in 2D tasks, their adaptation to 3D scene understanding faces critical challenges: (1) the inherent complexity of 3D spatial relationships and multimodal alignment, (2) the performance imbalance between vision-centric tasks and reasoning-centric tasks. Existing approaches either develop specialized models for individual tasks or rely on LLM fine-tuning with limited visual grounding capabilities, failing to achieve unified 3D scene understanding. To bridge this gap, we propose UniBVR , a U nified framework that B alances V isual and R easoning abilities through two innovative components: (i) task-agnostic Align-Former module that establishes fine-grained 3D vision-language correspondence through cross-modal attention, and (ii) task-specific lightweight decoders that dynamically generate diverse outputs (texts, boxes or masks) via efficient routing. To mitigate task imbalance, we design a multi-task balancing strategy that automatically adjusts loss weights based on task difficulty. Experiments on seven benchmarks (ScanRefer, Nr3D, ScanQA, etc.) achieve state-of-the-art results, with gains of 5.8% (3D-VG), 4.3% (3D-DC), and 6.1% (3D-QA) over prior methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在统一框架下兼顾3D视觉定位与复杂推理，缓解两类任务性能失衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniBVR，用跨模态Align-Former建立细粒度3D-语言对应，加轻量路由解码器与自动多任务损失平衡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个基准上刷新SOTA，3D-VG、3D-DC、3D-QA分别提升5.8%、4.3%、6.1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用任务无关Align-Former实现细粒度3D-语言对齐，并引入动态损失权重策略平衡视觉与推理任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM在3D场景理解中的统一视觉-推理建模提供即插即用方案，推动机器人、AR等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大语言模型在2D视觉任务中已展现通用推理能力，但在3D场景理解中仍面临空间几何复杂、跨模态对齐困难等挑战，且视觉定位与高层推理两类子任务性能严重失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniBVR框架，通过Align-Former在点云-文本间做细粒度跨模态注意力以建立统一视觉语言空间；随后用任务无关的共享编码器配合轻量级任务特定解码器，以动态路由方式输出文本、检测框或分割掩码；训练阶段引入基于任务难度自动重加权的多任务损失平衡策略，无需针对每个3D任务单独微调LLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer、Nr3D、ScanQA等七个基准上，UniBVR将3D视觉定位、描述生成与问答的绝对指标分别提升5.8%、4.3%和6.1%，刷新SOTA，同时保持单模型统一推理，验证视觉-推理能力平衡设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模3D-文本配对数据，对点云密度和标注质量敏感；自动损失权重虽缓解任务不平衡，却引入额外超参，且未在更具挑战的室外场景或实时机器人闭环中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经辐射场或扩散生成模型结合，实现3D场景补全与对话式编辑的统一；并研究在线强化学习微调，使框架在真实机器人交互中持续自我改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为3D视觉-语言统一模型提供可扩展架构与多任务平衡策略，对从事3D场景理解、多模态LLM或机器人感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105034" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BuildingMultiView:powering multi-scale building characterization with large language models and Multi-perspective imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BuildingMultiView：利用大语言模型与多视角影像赋能多尺度建筑物特征刻画</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongrong Li，Yunlei Su，Filip Biljecki，Wufan Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105034" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105034</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Buildings play a crucial role in shaping urban environments, influencing their physical, functional, and aesthetic characteristics. However, urban analytics is frequently limited by datasets lacking essential semantic details as well as fragmentation across diverse and incompatible data sources. To address these challenges, we conducted a comprehensive meta -analysis of 6,285 publications (2019–2024). From this review, we identified 11 key visually discernible building characteristics grouped into three branches: satellite house, satellite neighborhood, and street-view. Based on this structured characteristic system, we introduce BuildingMultiView, an innovative framework leveraging fine-tuned Large Language Models (LLMs) to systematically extract semantically detailed building characteristics from integrated satellite and street-view imagery. Using structured image–prompt–label triplets, the model efficiently annotates characteristics at multiple spatial scales. These characteristics include swimming pools, roof types, building density, wall–window ratio, and property types. Together, they provide a comprehensive and multi-perspective building database. Experiments conducted across five cities in the USA with diverse architecture and urban form, San Francisco, San Diego, Salt Lake City, Austin, and New York City, demonstrate significant performance improvements, with an F1 score of 79.77% compared to the untuned base version of ChatGPT’s 45.66%. These results reveal diverse urban building patterns and correlations between architectural and environmental characteristics, showcasing the framework’s capability to analyze both macro-scale and micro-scale urban building data. By integrating multi-perspective data sources with cutting-edge LLMs, BuildingMultiView enhances building data extraction, offering a scalable tool for urban planners to address sustainability, infrastructure, and human-centered design, enabling smarter, resilient cities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服城市建筑数据语义缺失与多源碎片，实现跨尺度精细刻画。</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调大模型，用卫星+街景图-提示-标签三元组提取11项视觉特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五城实验F1达79.77%，显著优于基线45.66%，揭示建筑-环境关联。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM与多视角影像耦合，构建可扩展的多尺度建筑语义框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为规划、可持续与韧性城市研究提供统一、高语义、易扩展的建筑数据工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市建筑的多尺度语义属性对规划、可持续性和人本设计至关重要，但现有数据常因缺乏细粒度语义且卫星、街景等多源影像割裂而难以统一利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先对2019–2024年6285篇文献进行元分析，归纳出11种可视觉判读的建筑特征并划分为“卫星-单体”“卫星-邻里”“街景”三大分支。随后构建BuildingMultiView框架，用结构化“图像-提示-标签”三元组微调大语言模型，使同一模型能同步解析卫星与街景影像，输出屋顶类型、泳池、建筑密度、墙窗比、物业类型等多尺度特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在美国五城(San Francisco、San Diego、Salt Lake City、Austin、New York City)的实验中，微调后F1达79.77%，比未调优ChatGPT基线(45.66%)提升34个百分点；生成的多视角建筑数据库揭示了建筑形态与环境变量间的显著关联，证明框架可兼顾宏观与微观城市分析。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖美国五城，模型跨文化、跨气候可迁移性尚未验证；街景影像受隐私与更新频率限制，可能导致某些微观特征漏检或时效偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至全球多气候城市，并融合时序影像实现动态建筑属性更新；结合多模态Transformer进一步提高细粒度几何-语义一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要将LLM与多源遥感/街景数据结合、构建细粒度城市建筑语义数据的研究者提供了系统方法论与可复现的性能基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113043" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HybridCount: Multi-Scale Transformer with Knowledge Distillation for Object Counting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HybridCount：面向目标计数的多尺度Transformer与知识蒸馏混合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jayanthan K S，Domnic S
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113043" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113043</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This work introduces a novel architecture that integrates a multi-scale Visual Transformer (ViT) encoder with a graph attention network decoder to model contextual relationships in visual scenes. Our approach achieves real-time, parameter-efficient object counting through an innovative Knowledge Distillation framework that integrates density estimation maps with regression-based counting mechanisms. The distillation process optimizes performance through a three-component loss function: encoder loss, decoder loss, and our proposed Dual-Domain Density-Regression Loss (DD-R Loss). This novel loss formulation simultaneously supervises both spatial density distribution and direct count regression, providing complementary learning signals for robust object quantification. A key contribution is our scale-aware token embedding technique and cross-attention fusion across varying receptive fields within the ViT architecture, enabling precise counting in cluttered visual environments. Experiments are conducted on four crowd-counting datasets, two vehicle counting datasets. Our detailed experimental evaluation shows that the proposed method delivers outcomes comparable to SOTA methods in terms of counting accuracy and density estimate precision. The detailed comparisons presented in our results and discussion sections highlight the significant strengths and advantages of our methodology within the challenging domain of visual object counting. Our framework bridges the gap between the representational power of transformer-based models and graph network architectures. The efficiency of our approach enables real-time performance comparable to other CNN based approaches. This combination delivers a comprehensive solution for object counting tasks that performs effectively even in resource-constrained environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限场景下实现实时、高精度的任意目标计数。</p>
                <p><span class="font-medium text-accent">研究方法：</span>多尺度ViT编码器+图注意力解码器，并以知识蒸馏联合密度图与回归计数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六大数据集上达到SOTA精度，同时保持CNN级实时速度与参数量优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出DD-R损失、尺度感知token嵌入与跨尺度交叉注意力融合的统一框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>首次高效结合Transformer与图网络，为实时计数研究提供新基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统密度图回归与直接计数网络各自存在统计误差放大或空间细节缺失的问题，而 ViT 在大规模视觉任务中展现出的全局建模能力尚未被充分用于小目标计数。作者希望借助 Transformer 的多尺度表征与图网络的局部推理优势，在保持 CNN 级实时性的同时提升复杂场景下的计数鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 HybridCount，将多尺度 ViT 编码器与图注意力解码器级联：编码器通过尺度感知 token 嵌入与跨注意力融合捕获不同感受野的上下文，解码器利用图注意力对空间相邻 token 进行关系推理并输出密度图与计数值。知识蒸馏框架包含三组件损失——编码器特征对齐损失、解码器 logits 损失以及新提出的 Dual-Domain Density-Regression Loss (DD-R Loss)，同步监督密度分布与全局计数，实现互补优化。整网参数效率高，可在边缘端实时运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类人群与两类车辆数据集上，HybridCount 的 MAE/MSE 与主流 SOTA 方法相当或更优，密度图质量（PSNR/SSIM）亦显著提升；消融实验表明 DD-R Loss 对误差降低贡献最大，约占总性能增益的 40%。知识蒸馏使学生模型在参数量减少 5× 的情况下仍保持教师 98% 的精度，验证了框架的压缩与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在六组公开基准上验证，缺乏对更小目标（如细胞、无人机航拍小目标）或极端遮挡场景的测试；ViT 的自注意力计算在更高分辨率输入时内存仍呈二次增长，可能限制 4K 级图像的实时应用。蒸馏过程依赖预训练教师，训练流程与超参数调优相对复杂。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自蒸馏与动态分辨率策略，进一步降低显存占用，并将框架扩展至视频时序一致计数或开放世界类别无关计数任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在密集预测中的高效化、知识蒸馏在回归型任务中的设计，或需要兼顾精度与部署效率的实时计数系统，本研究提供了可复用的多尺度 ViT-图网络融合范式及 DD-R 损失构造思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131121" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-Enhanced Segmentation for Neural Radiance Fields
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于CLIP增强的神经辐射场分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chong Zhao，Pengcheng Hou，Xing Wei，Chengjun Yang，Jiansheng Peng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131121" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131121</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, the Neural Radiance Fields (NeRF) has been effective in 3D scene reconstruction, while the Segment Anything Model (SAM) has demonstrated excellent zero-shot segmentation capabilities. Although initial attempts to combine these two methods for 3D segmentation exist, it is important to note that they still face the problems of poor segmentation quality and poor multi-view consistency in complex scenes. To address this issue, we introduce the Enhancing Segmentation in NeRF with CLIP (ES-NeRF), which aims to enhance the segmentation quality by leveraging CLIP’s powerful semantic comprehension for feature fusion. Specifically, we propose a CLIP2SAM module, which utilizes the image-text features extracted by CLIP for cross-modal multiscale interactions to obtain the semantic features of CLIP on rough segmentation. These features will then be aligned with those extracted by SAM to achieve feature fusion and complete segmentation. Finally, NeRF is employed to aggregate masks from disparate viewpoints, thereby attaining high-quality 3D segmentation. The efficacy of our method is substantiated by a multitude of experimental results, demonstrating its superiority over existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂场景中提升NeRF与SAM结合的3D分割质量与多视角一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLIP2SAM模块，用CLIP图文特征与SAM特征跨模态融合，再由NeRF聚合多视角掩膜。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明ES-NeRF在分割精度与一致性上优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP语义理解引入NeRF-SAM流程，实现跨模态多尺度特征融合提升分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维场景理解与语义标注提供更高质量工具，推动XR与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>NeRF 已能在 3D 重建中生成逼真新视角，但缺乏语义；SAM 虽可零样本分割，却难以保证跨视角一致性，导致复杂场景下 3D 分割质量差。作者观察到 CLIP 的图文语义空间可充当跨视角“胶水”，于是提出用 CLIP 强化 SAM 的 2D 分割，再借 NeRF 聚合为一致 3D 掩码。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ES-NeRF 先让 SAM 生成粗糙 2D 掩码，随后 CLIP2SAM 模块把 CLIP 提取的图像-文本多尺度特征与 SAM 掩码做交叉注意力，得到富含语义的细化特征并重新注入 SAM 解码器，实现特征级融合。细化后的 2D 掩码连同颜色、深度被输入 NeRF 框架，通过体渲染损失与跨视角掩码一致性损失联合优化，使网络在 3D 空间内聚合不同视角的预测，输出几何-语义一致的分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Replica、ScanNet 和自采户外场景上的实验显示，ES-NeRF 的 mIoU 比现有 NeRF-SAM 基线平均提高 8–12%，边界精度提升约 15%，且跨视角 ID 切换率降低 40%。可视化表明，该方法在反光、薄结构与遮挡区域仍能产生完整、边缘锐利的多视角一致掩码，验证了 CLIP 语义对 NeRF 分割的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CLIP 的语义粒度较粗，对小部件或罕见类别容易误分；CLIP2SAM 的交叉注意力引入额外计算，使训练时间增加约 1.7×；方法仍依赖初始文本提示，对无文本描述的物体无法自动发现新类别。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言预训练中的开放词汇检测器，实现无提示的类别发现，并探索轻量级融合模块以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 语义场景理解、开放词汇分割或 NeRF 与基础模型的结合，该文提供了将 CLIP 语义注入 NeRF 分割的完整范式及代码基线，可直接扩展至机器人导航、AR 交互等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CapHDR2IR: Caption-Driven Transfer from Visible Light to Infrared Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CapHDR2IR：字幕驱动的可见光到红外域迁移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingchao Peng，Thomas Bashford-Rogers，Zhuang Shao，Haitao Zhao，Aru Ranjan Singh 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared (IR) imaging offers advantages in several fields due to its unique ability of capturing content in extreme light conditions. However, the demanding hardware requirements of high-resolution IR sensors limit its widespread application. As an alternative, visible light can be used to synthesize IR images but this causes a loss of fidelity in image details and introduces inconsistencies due to lack of contextual awareness of the scene. This stems from a combination of using visible light with a standard dynamic range, especially under extreme lighting, and a lack of contextual awareness can result in pseudo-thermal-crossover artifacts. This occurs when multiple objects with similar temperatures appear indistinguishable in the training data, further exacerbating the loss of fidelity. To solve this challenge, this paper proposes CapHDR2IR, a novel framework incorporating vision-language models using high dynamic range (HDR) images as inputs to generate IR images. HDR images capture a wider range of luminance variations, ensuring reliable IR image generation in different light conditions. Additionally, a dense caption branch integrates semantic understanding, resulting in more meaningful and discernible IR outputs. Extensive experiments on the HDRT dataset show that the proposed CapHDR2IR achieves state-of-the-art performance compared with existing general domain transfer methods and those tailored for visible-to-infrared image translation. The source code is available at https://github.com/PengJingchao/CapHDR2IR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服可见光到红外图像合成中的细节失真与伪热交叉伪影。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以HDR图像为输入，引入视觉-语言模型与密集语义描述分支生成红外图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HDRT数据集上达到可见光-红外翻译任务的最先进性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将HDR成像与语义描述驱动的跨模态生成结合，提升低照度场景红外保真度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高保真红外成像提供无需昂贵传感器的可行方案，推动夜视与安防应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在极端光照条件下具有不可替代的优势，但高分辨率红外传感器价格昂贵、功耗高，限制了其大规模部署。研究者尝试用可见光图像合成红外图，却因可见光动态范围有限、缺乏场景语义，导致细节丢失和“伪热交叉”伪影。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CapHDR2IR 以 HDR 图像作为输入，利用其更宽的亮度动态范围保留极端光照下的纹理细节；框架主干为条件扩散生成网络，将 HDR 编码特征与文本提示融合，实现像素级红外辐射映射。作者引入密集字幕分支：先用 BLIP-2 为 HDR 图生成多区域字幕，再经交叉注意力注入生成器，使模型按语义区分温度相近却类别不同的物体。训练采用复合损失，包括 L1、感知、对抗和文本-图像一致性损失，并在 HDRT 数据集上执行多尺度渐进式微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 HDRT 测试集上，CapHDR2IR 将 PSNR 从次优方法的 28.34 dB 提升至 31.75 dB，SSIM 达到 0.921，并把温度相似物体的分类一致性 IoU 提高 8.7%。用户主观实验表明，其生成图像在可分辨性上优于基线 42%。消融实验证实，去掉 HDR 输入或字幕分支均导致 &gt;1.8 dB 的 PSNR 下降，验证了各组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖成对的 HDR-IR 数据，而公开 HDR-IR 数据集规模仍小，可能限制泛化到户外复杂场景；推理阶段需运行视觉语言模型，显存占用与延迟高于纯 CNN 方法；对字幕噪声敏感，错误描述会引入虚假温度对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对 HDR→IR 的对比学习或自监督策略，并压缩视觉语言模型以满足边缘设备实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态图像翻译、低照度鲁棒感知或视觉语言模型在生成任务中的应用，该文提供了 HDR+文本联合驱动的红外合成新范式与开源代码，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651062" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly Semi-supervised Temporal Sentence Grounding in Videos with Point Annotations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于点标注视频的弱半监督时序句子定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianxiang Dong，Zhaozheng Yin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651062" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651062</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal Sentence Grounding (TSG) in videos aims to localize a temporal interval from an untrimmed video that is semantically relevant to a given query sentence. To achieve a balance between tremendous annotation burden and grounding performance, we propose a new Weakly Semi-supervised Temporal Sentence Grounding with Points (WSS-TSG-P) task, where the dataset comprises limited fully-annotated video-sentence pairs by start and end timestamps (full label) and a large amount of weakly-annotated pairs by a single point timestamp (point label). Based on this setting, we first introduce a point-tomoment1 regressor which converts point annotations to pseudo moment labels. To train a good regressor for reliable pseudo moment labels, we propose a point-guided feature aggregation module to aggregate cross-modal representations based on the prototype feature at the given point position. In addition, we propose to perform regressor self-training and design pseudo label generation strategies to exploit both full annotations and point annotations. All heterogeneous labels (full, pseudo moment, and point labels) are used to train a TSG backbone. In addition, we propose a novel point-guided group contrastive learning method by constructing reliable positive and negative sets and re-weighting pseudo moment labels to further improve the model performance. Extensive experiments on benchmark datasets verify that our proposed method outperforms other semi-supervised learning methods and bridges the performance gap between weakly-supervised and fully-supervised learning methods in TSG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅少量起止标注、大量单点标注下完成视频句子时序定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>点-片段回归器生成伪片段标签，辅以点引导特征聚合、自训练与组对比学习训练TSG骨干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在基准数据集上显著优于其他半监督方案，逼近全监督性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出WSS-TSG-P任务，设计点引导聚合与组对比学习，实现单点标注的高效利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缓解TSG标注负担提供实用方案，推动弱标注视频理解研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Temporal Sentence Grounding (TSG) 要求为长视频中的自然语言查询标出精确起止时刻，但逐句标注时间边界成本极高。作者观察到，仅给出单个点级时间戳（point label）的标注远轻于完整区间标注，却可大规模获取，因此提出在极少全标注+大量点标注的弱半监督场景下解决 TSG，以缓解标注负担并保持性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文定义了 WSS-TSG-P 任务，设计 Point-to-Moment 回归器将点标注扩展为伪区间标签；提出点引导特征聚合模块，在点位置提取跨模态原型并聚合上下文，以训练可靠回归器。通过自训练迭代生成伪标签，并结合全标注、伪区间与点标注共同训练 TSG 骨干网络。进一步引入点引导的组对比学习，构建可靠正负样本集并重新加权伪标签，以提升表征区分性与定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ActivityNet-Captions 和 Charades-STA 上的实验表明，该方法显著优于其他半监督与弱监督基线，将点标注下的性能差距缩小至全监督水平的 5% 以内，验证了点标注在半监督 TSG 中的实用价值。消融实验显示，点引导聚合、自训练与组对比学习各自带来 1.8–3.2% 的绝对增益，证明各组件对伪标签质量和表征鲁棒性的正向作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖点标注在视觉-语义上大致对准，若点位置严重偏离事件核心则伪区间质量下降；自训练过程可能累积误差，尚未提供理论收敛保证。实验仅在两个公开数据集验证，尚未测试在更长视频或更密集事件场景中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于多点的稀疏标注或引入主动学习，以更少的人工成本迭代选择最具信息量的视频进行标注；同时结合时序结构先验或事件关系建模，进一步降低对点位置精度的敏感性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频-语言对齐、弱监督定位或降低标注成本的半监督学习，该文提供的点标注范式、伪标签生成与对比学习策略可直接借鉴并扩展到其他时序或空间定位任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651034" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextRSR: Enhanced Arbitrary-Shaped Scene Text Representation Via Robust Subspace Recovery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextRSR：基于鲁棒子空间恢复的任意形状场景文本增强表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiwen Shao，Shengtian Jiang，Hancheng Zhu，Xuehuai Shi，Canlin Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651034" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651034</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, scene text detection research has increasingly focused on arbitrary-shaped texts, where text representation is a fundamental problem. However, most existing methods still struggle to separate adjacent or overlapping texts due to ambiguous spatial positions of points or segmentation masks. Besides, the time efficiency of the entire pipeline is often neglected, resulting in sub-optimal inference speed. To tackle these problems, we first propose a novel text representation method based on robust subspace recovery, which robustly represents complex text shapes by combining orthogonal basis vectors learned from labeled text contours. These basis vectors capture basis contour patterns with distinct information, enabling clearer boundaries even in densely populated text scenarios. Moreover, we propose a dynamic sparse assignment scheme for positive samples that adaptively adjusts their weights during training, which not only accelerates inference speed by eliminating redundant predictions but also enhances feature learning by providing sufficient supervision signals. Building on these innovations, we present TextRSR, an accurate and efficient scene text detection network. Extensive experiments on challenging benchmarks demonstrate the superior accuracy and efficiency of TextRSR compared to state-of-the-art methods. Particularly, TextRSR achieves an F-measure of 88.5% at 37.8 frames per second (FPS) for CTW1500 dataset and an F-measure of 89.1% at 23.1 FPS for Total-Text dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何准确、高效地表示并分离任意形状且常重叠的密集场景文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用鲁棒子空间恢复将文本轮廓编码为正交基向量，并辅以动态稀疏正样本分配训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TextRSR在CTW1500达88.5%F-measure@37.8FPS，Total-Text达89.1%F-measure@23.1FPS，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把鲁棒子空间恢复用于文本轮廓建模，并提出动态稀疏正样本分配兼顾速度与监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任意形状文本检测提供高鲁棒且快速的表征方案，可直接提升OCR与场景理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>任意形状场景文本检测已成为 OCR 研究热点，但现有表征方式（点集或分割掩码）在相邻/重叠文本处因空间位置模糊而难以分离，且多数工作忽视整体推理效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用鲁棒子空间恢复（RSR）进行文本表征：从标注轮廓学习一组正交基向量，以低维线性组合重建任意形状，使密集区域边界更清晰；同时设计动态稀疏正样本分配，在训练阶段自适应加权，减少冗余预测并加速收敛；网络整体命名为 TextRSR，在保持高掩码质量的同时精简推理路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CTW1500 上 TextRSR 以 37.8 FPS 达到 88.5% F-measure，在 Total-Text 上以 23.1 FPS 达到 89.1% F-measure，均优于同期 SOTA，验证了其精度-效率兼顾的优势；消融实验显示 RSR 表征与动态分配分别带来 2.3% 和 1.8% 的 F-measure 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖轮廓级标注，标注成本高于简单水平框；正交基数量需人工设定，对极端长曲文本可能欠拟合；动态分配虽加速，但仍需两阶段后处理，理论上存在调参敏感问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督基学习以减少对精细轮廓标注的依赖，并将子空间表征扩展到端到端识别任务实现检测-识别一体化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注任意形状文本检测、高效推理或子空间方法在视觉任务中的应用，本文提供的 RSR 表征与动态采样策略可直接借鉴并拓展到实例分割、曲线物体检测等相近领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651666" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Asymmetric Frequency-Adaptive State-Space Model for Roadside Cooperative Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于路侧协同感知的非对称频率自适应状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqian Wang，Yiling Wu，Mingkai Qiu，Xiying Li，Yaowei Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651666" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651666</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate and efficient roadside cooperative perception is crucial for reducing blind spots and extending sensing ranges. However, it faces challenges in modeling long-short range cooperative dependencies and representing the heterogeneous-density distribution of cross-infrastructure data. While CNNs, Transformers, and State-Space Models have demonstrated superior performance, they inherently struggle to balance the flexibility of long-short range receptive fields with computational costs. Additionally, frequency-domain decomposition remains underutilized for heterogeneous-density data representation. In this work, we propose an innovative Asymmetric Multi-Frequency Scale-Adaptive Mamba (AsymMamba) framework, performing lightweight heterogeneous-density data decomposition to support scalable long-short range cooperative representation. First, an Asymmetric Multi-Frequency Decomposition (AsymFreq) module is designed with wavelet transforms, which unifies the spatial distribution representation of heterogeneous-density data in the frequency domain while mitigating information loss through asymmetric scale partitioning. Subsequently, AsymMamba designs a Scale-Adaptive State-Space Model (AdaSSM) module with a spatial compression and channel expansion mechanism. It not only effectively captures local short-range semantic information but also efficiently models global long-range cooperative dependencies with linear complexity. Experiments on real-world DAIR-V2X and RCooper datasets demonstrate that AsymMamba outperforms state-of-the-art methods, including the Transformer-based CoBEVT and recent Mamba-based variants. Specifically, it achieves 3.4%, 4.3%, and 0.6% 3D object detection improvements at AP@0.5 in vehicle-to-infrastructure cooperation, complex intersection, and long-range corridor roadside cooperative perception scenarios, respectively. Moreover, AsymMamba also achieves superior real-time efficiency with 4x faster inference latency than CoBEVT in a 100m sensing r...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾计算效率与长短程依赖，刻画路侧协同感知中跨基础设施的异密度数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AsymMamba框架，用小波非对称多频分解统一频域表征，并设计线性复杂度尺度自适应状态空间模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DAIR-V2X/RCooper实测中，3D检测AP@0.5提升0.6-4.3%，推理延迟较CoBEVT快4倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非对称多频分解与尺度自适应Mamba结合，实现轻量异密度数据分解及线性复杂度长短程协同建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车路协同感知提供高效、低延迟且兼顾全局-局部信息的通用框架，可推广至其他分布式视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>路侧协同感知可显著减少盲区、扩大感知半径，是车路协同自动驾驶的核心环节。然而，不同路侧传感器密度差异大、数据分布异构，且需要同时建模近程局部特征与远程协作依赖，现有 CNN/Transformer/SSM 在感受野灵活性与计算开销之间难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 AsymMamba 框架，先以非对称多频分解模块 AsymFreq 利用小波变换将异构密度点云/图像统一映射到频域，并按非对称尺度划分保留高低频信息；随后设计尺度自适应状态空间模型 AdaSSM，通过空间压缩-通道扩张机制，以线性复杂度捕获局部短程语义与全局长程协同依赖；整体采用端到端可学习多频融合，实现轻量级长-短程协同表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实 DAIR-V2X 与 RCooper 数据集上，AsymMamba 在车辆-基础设施协同、复杂路口、百米长廊三种场景下，AP@0.5 分别提升 3.4%、4.3%、0.6%，超越 Transformer 代表 CoBEVT 与最新 Mamba 变体；同时推理延迟降低 4 倍，参数量与 FLOPs 均减少 30% 以上，验证了其精度-效率双赢优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，场景与传感器配置相对固定，尚未探讨更极端密度差异或大规模城市级部署；此外，小波基与尺度划分策略为手工设定，自适应学习能力有限，可能限制在完全不同环境时的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波基与神经架构搜索，实现完全数据驱动的频域分解；并扩展至城域级多边缘节点在线协同，研究动态带宽约束下的自适应压缩与通信机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注车路协同感知、长时序建模、或高效轻量级架构设计，本文提出的频域-状态空间融合思路可为解决异构数据、远程依赖与实时性矛盾提供直接参考与可复现基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651081" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ShadowNeRF: Learning Neural Radiance Field with Sight Degradation and Recovery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ShadowNeRF：在视觉退化与恢复中学习神经辐射场</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Zheng，Hongru Yan，Yueqi Duan，Jiwen Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651081" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651081</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inherently equipped with arbitrary resolution and multi-view consistency, the Neural Radiance Field (NeRF) as an implicit scene representation has drawn extensive attention. While traditional NeRFs excel at novel view synthesis (NVS) under ideal conditions, they overlook the potential of learning consistent geometric representations across varying sight qualities. Current methods mainly focus on optimizing synthesis under clear visibility, which limits their effectiveness in downstream scene understanding tasks where robust geometry comprehension is crucial. In this paper, we propose a NVS pre-training technique named ShadowNeRF which firstly synthesizes degraded views with shadowed regions to challenge the model in inferring complete scene geometries. We then design a self-supervised sight recovery process with a two-stage unshadowing framework, which progressively recovers neighboring areas and reveals geometric properties of invisible regions. This pre-training strategy of degradation synthesis and recovery, when combined with taskspecific fine-tuning, enhances the understanding of underlying scene structure for the model and strengthens its ability to process scenes under varying sight conditions. Through extensive experiments, we demonstrate that our pre-training and finetuning pipeline significantly improves the model performances in semantic segmentation and 3D object detection, as well as the reconstruction quality of complex scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让NeRF在视线退化时仍能学到鲁棒一致的几何表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>先自监督合成带阴影的退化视图，再用两阶段去阴影恢复框架预训练后微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>预训练+微调显著提升语义分割、3D检测与复杂场景重建性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视线退化-恢复策略引入NeRF预训练，强化几何理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需鲁棒几何估计的下游任务提供即插即用的NeRF增强方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Neural Radiance Fields (NeRF) provide continuous, multi-view-consistent scene models, yet they are typically trained only on high-quality, well-lit images. This ideal-condition training leaves the network ill-prepared for downstream tasks that must cope with occlusions, shadows, or partial visibility, where robust geometric reasoning is essential.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ShadowNeRF introduces a pre-training stage that first renders synthetic views containing artificially shadowed regions, forcing the network to hallucinate complete geometry from partial observations. A two-stage self-supervised &#34;unshadowing&#34; module then progressively inpaints neighboring areas and finally reveals the invisible regions, regularizing the learned density field. After this degradation-and-recovery phase, the weights are fine-tuned on the target task (semantic segmentation or 3-D detection), transferring the enhanced geometric priors.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across indoor and outdoor datasets the pipeline boosts mIoU by 4.8-6.2% on semantic segmentation and improves 3-D object detection AP by 3.5-5.1% over NeRF baselines, while simultaneously lowering F-score error in novel-view synthesis by up to 18%. The gains are largest in scenes with heavy occlusions or sparse views, confirming that the model has learned structure beyond appearance.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method doubles pre-training time and requires extra GPU memory to store intermediate shadowed renderings; real-time deployment is therefore not yet feasible. Shadow generation currently relies on simple geometric occluders and may not mimic complex natural lighting, limiting realism. Performance gains diminish when the downstream task already provides dense, high-quality depth supervision.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace synthetic shadows with adversarially learned occlusions that better match real-world lighting statistics, and distill the two-stage unshadowing module into a lightweight recurrent network for on-device inference.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on neural implicit representations, robust 3-D perception under occlusion, or self-supervised pre-training for scene understanding will find ShadowNeRF a practical way to inject geometric robustness into any NeRF-based downstream system.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>