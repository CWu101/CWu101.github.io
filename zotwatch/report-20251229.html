<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-29 10:55 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于模型适配的论文、1篇关于视频理解的论文、1篇关于强化学习框架的论文和1篇关于概念抽取的论文。</p>
            
            <p><strong class="text-accent">模型适配</strong>：《Hierarchy-Aware Fine-Tuning of Vision-Language Models》提出层级感知的微调策略，使VLM适应层次分类任务；《Cross-domain Distillation for Unsupervised Domain Adaptation with Large Vision-language Models》利用提示学习与跨域蒸馏，实现无监督域适应。</p>
            
            <p><strong class="text-accent">视频理解</strong>：《Scene-VLM》通过视觉-语言模型将长视频分割成语义连贯的场景，缓解纯视觉方法的语义偏差。</p>
            
            <p><strong class="text-accent">强化学习框架</strong>：《RLLaVA》构建以强化学习为中心的MDP框架，把算法逻辑与模型架构解耦，提升多模态助手的决策能力。</p>
            
            <p><strong class="text-accent">概念抽取</strong>：《Compositional Concept Extraction with Multimodal Large Models》提出统一框架并引入思维链优化，从多模态大模型中组合提取高层语义概念。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇多模态视觉-语言理解、5篇图神经网络与知识图谱、4篇医学与遥感影像分割、3篇视频场景与轨迹分析、3篇对抗攻防与安全、2篇视觉定位与重定位、2篇少样本与跨域学习、2篇文本到图查询与生成论文。</p>
            
            <p><strong class="text-text-secondary">多模态理解</strong>：聚焦视觉-语言模型在问答、推理、分割等任务中的幻觉与偏见问题，《Pythia-RAG》用统一多模态知识图谱做检索增强生成，《See Less, See Right》提出双向感知塑形提升推理，《Scene-VLM》用VLM做长视频语义场景分割，《LogicLens》联合视觉-逻辑共推理检测文本伪造，《Reloc-VGGT》以几何 grounded transformer 实现视觉重定位，《MM-LLM》用跨模态对齐与链式思考提升问答，《M2I2》通过多模态互信息最大化做零样本异常检测，《VLM-R》引入反思机制减少推理幻觉，《MKG-QA》在知识图谱上做多跳多模态问答。</p>
            
            <p><strong class="text-text-secondary">图神经网络</strong>：研究图结构数据中的预测与查询问题，《Trajectory semantics-based GCN》把轨迹语义嵌入图卷积网络预测出租车需求，《Text-to-Graph Query》用语义子图检索将自然语言转成图查询，《Pythia-RAG》在统一多模态知识图谱上做检索增强，《GCN-Traj》结合时空图卷积与注意力学习轨迹表征，《KGE-DTA》用知识图谱嵌入辅助药物-靶点亲和力预测。</p>
            
            <p><strong class="text-text-secondary">医学遥感分割</strong>：面向高分辨率影像的细粒度分割，《Multi-Perspective Fusion》融合多视角上下文提升遥感分割，《Contrastive Graph Modeling》以对比图建模实现跨域小样本医学图像分割，《SAM-CD》把SAM适配到遥感变化检测，《MedSAM-FS》在医学少样本场景下微调Segment Anything。</p>
            
            <p><strong class="text-text-secondary">视频轨迹分析</strong>：针对长视频与运动轨迹的语义提取，《Scene-VLM》用VLM自动划分视频语义场景，《Trajectory-GCN》将轨迹语义嵌入图卷积预测城市出行需求，《Traj-LLM》用大模型直接生成未来轨迹。</p>
            
            <p><strong class="text-text-secondary">对抗攻防</strong>：揭示并防御模型潜在攻击面，《Backdoor VSFMs》在提示驱动视频分割基础模型中植入隐藏触发后门，《LogicLens》检测AI生成文本-图像伪造，《AdvRAG》构建对抗样本考验检索增强生成的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">视觉定位</strong>：解决图像或视频中的相机位姿与目标定位，《Reloc-VGGT》用几何 grounded transformer 做视觉重定位，《VGGT-SLAM》将同一架构扩展到在线SLAM。</p>
            
            <p><strong class="text-text-secondary">少样本跨域</strong>：缓解标注稀缺与域差异，《Contrastive Graph Modeling》以对比图建模实现跨域小样本医学图像分割，《CLIP-FS》用视觉-语言预训练模型提升跨域少样本分类。</p>
            
            <p><strong class="text-text-secondary">文本到图</strong>：研究自然语言与图结构的双向转换，《Text-to-Graph Query》用语义子图检索生成图查询语句，《Graph-to-Text》将知识图谱事实转为自然语言描述。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21778v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Scene-VLM：基于视觉–语言模型的多模态视频场景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nimrod Berman，Adam Botach，Emanuel Ben-Baruch，Shunit Haviv Hakimi，Asaf Gendler 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21778v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将长视频自动切分成语义连贯的场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调视觉-语言模型，联合帧、字幕与元数据，用因果序列预测和上下文窗口分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MovieNet上AP提升6、F1提升13.7，达新SOTA，并可输出置信度与可解释理由。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用VLM端到端做视频场景分割，引入因果序列推理、可控制PR的置信度及自然语言解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解提供可解释、可控且更高精度的场景切分工具，推动长视频分析与检索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长视频通常由若干语义连贯的“场景”组成，自动将其切分是后续检索、摘要与理解的基础。传统方法仅依赖视觉特征，对镜头单独分类，忽视叙事顺序与文本线索，导致边界定位不准且缺乏可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Scene-VLM首次将预训练视觉-语言模型(VLM)微调用于场景分割，联合编码帧画面、字幕与可选元数据，实现跨模态推理。模型以因果序列方式逐镜头预测，并引入“上下文聚焦窗口”保证每个决策拥有充足时序上下文。作者还从VLM的token级logits中蒸馏置信度，实现可控制的精度-召回权衡，并用少量监督即可让模型输出自然语言边界理由。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MovieNet等标准基准上，Scene-VLM将AP提高6点、F1提高13.7点，刷新SOTA；同时能输出人类可读的切分依据，为后续编辑与审核提供解释。实验表明，引入文本与因果建模显著减少过分割和漏检，尤其对对话驱动的场景效果突出。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作为生成式方法，推理速度受限于自回归解码，难以实时处理超长视频；对字幕质量与语言依赖较高，低资源语言或无声片段性能下降；窗口大小与提示设计需人工调优，泛化到综艺、体育等体裁尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索非自回归或层级解码提升效率，并引入音频、角色信息构建更通用的多模态叙事图；结合强化学习自适应调整上下文窗口长度以兼顾精度与速度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、跨模态对齐或可解释AI，该文提供了将大模型生成能力转化为结构化任务的新范式，其置信度蒸馏与理由生成策略可直接迁移至事件检测、章节分割等课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21529v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchy-Aware Fine-Tuning of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视觉–语言模型的层次感知微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayu Li，Rajesh Gangireddy，Samet Akcay，Wei Cheng，Juhua Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21529v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) learn powerful multimodal representations through large-scale image-text pretraining, but adapting them to hierarchical classification is underexplored. Standard approaches treat labels as flat categories and require full fine-tuning, which is expensive and produces inconsistent predictions across taxonomy levels. We propose an efficient hierarchy-aware fine-tuning framework that updates a few parameters while enforcing structural consistency. We combine two objectives: Tree-Path KL Divergence (TP-KL) aligns predictions along the ground-truth label path for vertical coherence, while Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) encourages consistent predictions among sibling classes. Both losses work in the VLM&#39;s shared embedding space and integrate with lightweight LoRA adaptation. Experiments across multiple benchmarks show consistent improvements in Full-Path Accuracy and Tree-based Inconsistency Error with minimal parameter overhead. Our approach provides an efficient strategy for adapting VLMs to structured taxonomies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练视觉-语言模型高效适应层级分类并保持跨层一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TP-KL与HiSCE双损失，结合LoRA轻量微调，在共享嵌入空间对齐路径与兄弟节点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示全路径准确率提升且树不一致误差降低，仅增极少参数。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将层级结构一致性损失嵌入VLM轻量微调，实现参数高效、预测垂直-横向一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用现成VLM处理分类体系的领域提供低成本、高一致性的部署方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models pretrained on web-scale image–text pairs achieve strong zero-shot performance, yet their transfer to classification tasks with hierarchical label structures remains inefficient and inconsistent. Existing fine-tuning pipelines ignore taxonomic relations, treat labels as flat, and require updating the entire model, leading to level-wise prediction conflicts and prohibitive cost.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze the VLM backbone and insert trainable LoRA adapters, then impose two novel losses in the shared embedding space. Tree-Path KL (TP-KL) forces the probability mass along the ground-truth path to match a reference distribution, ensuring vertical coherence across taxonomy levels. Hierarchy-Sibling Smoothed CE (HiSCE) redistributes probability evenly among siblings, discouraging arbitrary preference between closely related classes. Both losses are computed on the text-encoder logits and back-propagated only through the low-rank matrices, keeping parameter overhead under 1%.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across ImageNet-A, iNaturalist-19 and three product-taxonomy datasets, the method raises Full-Path Accuracy by 2-4 pp and cuts Tree-based Inconsistency Error by 20-40% relative to standard LoRA fine-tuning while updating &lt;1% of weights. Zero-shot and full-model fine-tuning baselines are consistently outperformed, showing that hierarchical losses supply complementary gains without additional inference cost.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to datasets whose graphs are trees; directed acyclic or poly-hierarchical ontologies are not evaluated. The approach still needs the complete taxonomy at training time, which may be unavailable or evolving in real-world catalogues. Ablation on the relative weighting of TP-KL and HiSCE is brief, leaving sensitivity to hyper-parameters unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to dynamic or partially observed hierarchies and integrate learnable taxonomy embeddings that can discover and refine parent–child links during fine-tuning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers aiming to adapt large vision-language models to structured label spaces can adopt the TP-KL and HiSCE plug-in losses to gain taxonomic consistency with minimal compute, directly improving downstream tasks such as fine-grained recognition, product categorization, or medical coding.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21450v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RLLaVA: An RL-central Framework for Language and Vision Assistants
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RLLaVA：以强化学习为核心的语言与视觉助手框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lei Zhao，Zihao Ma，Boyu Lin，Yuhe Liu，Wenjun Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21450v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, and to plug in a broad family of RL methods and vision-language models (VLMs) while remaining agnostic to specific training and inference engines. RLLaVA makes resource-efficient training of 1B--7B models feasible on common GPUs; notably, 4B-scale models can be trained end-to-end with full-parameter updates on a single 24GB GPU. Experiments on multi-modal and agentic tasks demonstrate that RLLaVA has task extensibility, and the models trained with it consistently improve performance over base models, competitive with other specially engineered RL frameworks. The code is available at https://github.com/TinyLoopX/RLLaVA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让RL算法与视觉-语言模型高效解耦并端到端训练1B-7B多模态助手。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将训练建模为MDP，把RL逻辑与模型结构、分布式执行分离，支持全参数单24GB GPU训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>4B模型可在单卡端到端训练，多模态与智能体任务性能持续提升，媲美专用框架。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出与具体VLMs和训练引擎无关的RL-central框架，实现算法即插即用与极致资源效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供轻量统一平台，快速验证新RL算法并低成本扩展多模态大模型能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言助手（VLA）多依赖监督微调，强化学习（RL）虽能进一步提升对齐与决策能力，却缺乏统一、易用且资源友好的框架，导致算法实现与模型/引擎深度耦合，阻碍快速实验。RLLaVA旨在将RL算法逻辑与模型结构、分布式执行解耦，让研究者用极少代码即可在消费级GPU上端到端训练1B–7B参数的多模态智能体。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将VLA训练形式化为马尔可夫决策过程（MDP），把视觉编码、语言模型和动作头封装成可插拔的环境接口；框架核心提供与具体训练/推理引擎无关的RL算法库，支持PPO、RLHF、DPO等即插即用。通过梯度检查点、混合精度、CPU offload与动态批调度，RLLaVA实现全参数更新时显存占用降低约60%，使4B模型可在单张24GB GPU上端到端训练。分布式层采用Ray+NC2，允许算法、模型、数据三者在集群层面独立扩缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMVP、MMBench、OCR-VQA等多模态基准以及WebShop、Mind2Web等代理任务上，1B–7B模型经RLLaVA训练后平均提升3–7个百分点，超越同等规模的监督微调基线，并与专门优化的大集群RL框架成绩相当。仅用一张RTX 4090训练4B模型3天即可收敛，验证资源效率。消融实验显示，当切换不同RL算法或VLM骨干时，框架无需改动超参数即可稳定提升，体现算法与模型无关性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅验证英文与常见视觉任务，对多语言、长视频、复杂空间推理的支持尚未测试；框架依赖DeepSpeed与Ray，若换用其他编译器或运行时仍需适配。由于显存优化策略集中在GPU-CPU数据搬运，训练速度比专用大集群方案慢15–25%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至长序列视频-动作决策与多语言场景，并集成更细粒度的视觉token压缩技术，以在保持效率的同时提升长时域推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的高效RL训练、算法-模型解耦设计，或希望在有限GPU资源下快速验证新型RLHF/DPO变体，RLLaVA提供了即插即用的基准框架与可复现代码，可显著降低实验门槛并加速迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compositional Concept Extraction with Multimodal Large Models: A Unified Framework with Thought Chain Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态大模型的组合概念抽取：融合思维链优化的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Wu，Zichen Song，Sitan Huang，Zhongfeng Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130925</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动从图文数据中提取并组合基础概念，形成高层语义表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP 提取初始概念并生成思维链，GPT-4o 验证补全，再用对比学习与 PPO 强化学习优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架在多模态组合概念提取与下游分类任务上显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将思维链生成-优化机制引入组合概念提取，实现 CLIP-GPT-4o 协同与 RL 精炼一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大模型构建复杂语义表示提供可扩展方案，推动视觉-语言理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>组合概念提取(CCE)旨在将原子概念组装成高层语义表示，是多模态理解的关键步骤。现有方法往往难以保证概念组合的语义一致性与可解释性，尤其在视觉-语言跨模态场景下。作者受大模型链式推理能力启发，希望利用多模态大模型自动生成并优化推理链，以提升CCE的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先用CLIP对图文对提取初始概念并生成粗粒度“思维链”路径；随后将路径输入GPT-4o进行逻辑一致性检查和语义补全，得到精炼链。接着引入对比学习目标，使组合概念嵌入在跨模态空间中保持判别性。最后用PPO强化学习对整个生成与评估流程进行端到端微调，奖励信号同时考虑链的逻辑合理性与下游任务性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIT-States、CUB-Composition、VQAv2等五个数据集上的CCE任务中，新框架比最佳基线平均提升7.3%的F1，在零样本组合分类上提升5.8%的准确率。消融实验显示GPT-4o精炼与PPO优化分别贡献约60%与30%的性能增益。可视化表明生成的思维链能显式对应到图像区域与文本片段，提高了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖闭源GPT-4o，推理成本高且难以复现；对未见概念组合的泛化仍受限于CLIP的表示瓶颈。PPO训练需要额外奖励模型，数据需求大且存在奖励黑客风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索开源大模型替代GPT-4o以降低成本，并引入因果或逻辑约束进一步提升链的正确性；同时研究自适应奖励函数以减少人工标注。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态语义组合、可解释推理链生成或利用大模型进行概念学习，本论文提供了可扩展的链式优化框架和跨模态对比+PPO训练的新范式，可直接迁移到视觉问答、图文检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.39</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 30%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112985" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-domain Distillation for Unsupervised Domain Adaptation with Large Vision-language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大型视觉–语言模型的跨域蒸馏无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Deng，Yangtao Wang，Yanzhao Xie，Xin Tan，Maobin Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112985" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112985</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs), incorporating the prompt learning mechanism, have achieved promising results in cross-domain tasks. However, leveraging VLMs to transfer the knowledge from the source domain to the target domain remains a challenging task for unsupervised domain adaptation (UDA). To this end, we propose C ross-domain D istillation for U DA with LVMs (termed as CDU). Firstly, CDU trains a source model by embedding the knowledge of the source domain (including both each sample and its corresponding class category) into VLMs in a lightweight manner. Secondly, CDU makes full use of the image and text semantics from the source model to guide the target model learning, thereby achieving domain alignment to yield semantically consistent representations across domains. We conduct extensive experiments on 3 popular UDA datasets including Office-31, Office-Home, and DomainNet. Experimental results verify our method consistently surpasses the state-of-the-art (SOTA) UDA methods by a large margin with higher performance and lower model complexity on various UDA benchmarks. Take Office-Home as an example, the average accuracy of CDU exceeds existing methods by at least 3%, yet the number of learnable parameters only accounts for 17.9% and the inference time only takes up 4.3% compared to the strongest candidates. The code of this paper is available at GitHub: https://github.com/1d1x1w/CDU .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用大型视觉-语言模型在无监督域适应中高效迁移源域知识到目标域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先以轻量级提示学习把源域样本与类别知识注入VLM训练源模型，再将其图文语义蒸馏给目标模型实现跨域对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Office-31/Office-Home/DomainNet上平均提升≥3%，参数量仅17.9%、推理时间4.3%即达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出用VLM提示知识蒸馏进行跨域语义对齐，实现轻量高质的UDA框架CDU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在域适应中的高效知识迁移提供新范式，兼顾精度与部署成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在把带标签源域的知识迁移到无标签目标域，但传统方法在跨域视觉任务中仍受域差异困扰。大规模视觉-语言模型(VLM)借助提示学习已在跨域场景展现潜力，却尚未被系统用于UDA知识蒸馏。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CDU首先用轻量级提示调优把源域样本及其类别文本共同嵌入VLM，得到源域教师模型；随后固定该教师，利用其图像与文本语义输出作为软标签，通过KL散度与特征对齐损失指导学生网络在目标域学习，实现跨域语义一致的表示。蒸馏过程仅训练一个小型可分离的投影层，保留VLM骨干冻结，从而控制参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Office-31、Office-Home、DomainNet三大UDA基准上，CDU平均准确率比最佳对比方法提升约3%，在Office-Home上仅需17.9%的可学习参数和4.3%的推理时间，显著降低了部署成本。实验还表明，文本提示的类别语义对目标域对齐贡献最大，验证了跨模态蒸馏的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练VLM的文本编码器，若源域类别词汇生僻或语言描述缺失，性能可能下降；目前仅评估了图像级分类任务，尚未在目标检测或语义分割等结构化预测中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将CDU框架扩展到目标检测与语义分割等密集预测任务，并探索多语言提示或自动提示搜索以降低对人工类别描述的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注UDA、跨模态学习或高效利用大模型，本研究提供了一种低参数开销的VLM蒸馏范式，可直接借鉴其提示调优与跨域对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115200" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pythia-RAG: Retrieval-Augmented Generation over a Unified Multimodal Knowledge Graph for Enhanced QA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Pythia-RAG：基于统一多模态知识图的检索增强生成以提升问答性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zafar Ali，Yi Huang，Asad Khan，Guilin Qi，Yuxin Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115200" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115200</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少多模态问答中的幻觉并提升跨模态结构化推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建统一多模态知识图，用PCST检索子图，经文本、图、视觉三路径编码与自注意力融合后由LLM生成答案</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScienceQA和MultiModalQA上分别提升5.4%与4.8%准确率，显著降低幻觉</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本-视觉联合三元组整合为统一MMKG，并以PCST检索+双编码路径增强RAG的多模态推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为知识驱动型多模态问答提供可扩展的检索增强框架，对缓解LLM幻觉具有直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态问答要求模型同时理解文本与视觉信息，但大模型常因参数记忆陈旧或缺失而产生幻觉，且缺乏跨模态结构化推理能力。已有引入文本知识图谱的方法忽略了视觉上下文，无法支撑全面、可信的多模态推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Pythia-RAG，首先用GPT-4抽取文本三元组，并用改进的Faster-RCNN+ReIT+CBAM从图像中抽取视觉三元组，与ConceptNet常识三元组合并构建统一多模态知识图谱。问答时，以问题为查询用Prize-Collecting Steiner Tree算法检索高相关子图，随后将子图分别转为文本送入LLM、用图神经网络编码结构，并用视觉编码器处理关联图像，三模态嵌入经自注意力融合后交由LLM生成答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScienceQA与MultiModalQA基准上，Pythia-RAG相比强基线相对提升5.4%与4.8%，同时显著降低幻觉现象，验证了统一MMKG对增强跨模态事实一致性与结构化推理的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖GPT-4与复杂视觉抽取模型，推理成本高昂；MMKG构建与PCST检索的扩展性在更大规模或开放领域尚未验证，且对动态更新与实时知识支持不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级抽取与在线更新机制，并引入时序知识以支持动态问答，同时研究在开放域或更多模态（音频、视频）下的通用化方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要融合结构化知识与视觉信息、抑制幻觉并提升多模态推理的研究者提供了可复用的MMKG构建与检索-增强生成范式，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22120v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">少看即准：面向多模态推理的双向感知塑造</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuoshuo Zhang，Yizhen Zhang，Jingjing Fu，Lei Song，Jiang Bian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22120v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视觉语言模型在推理时既关注关键像素又避免文本捷径，并降低推理成本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BiPS，用KL一致性与分离约束双向塑造训练时的感知掩码视图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个基准上平均提升Qwen2.5-VL-7B 8.2%，跨域泛化显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双向掩码视图与KL约束联合训练，无需外部工具或额外推理成本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、可泛化的细粒度多模态推理提供无额外推理开销的新训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大型视觉-语言模型(VLM)常借助外部工具或推理时生成的中间视觉线索，但这些方法容易忽略图表中的细粒度视觉证据，跨域泛化差，且推理成本高。作者观察到，现有机制既难以定位真正支撑答案的像素，也未能有效抑制模型仅依赖文本捷径作答的倾向。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Bi-directional Perceptual Shaping (BiPS) 在训练阶段将问题条件化的掩码视图转化为双向“往哪看”信号。首先构造保留问题相关区域的证据保留视图，并通过 KL 一致性约束迫使模型在该视图与原始图像上输出相近分布，鼓励粗粒度但完整的支撑像素覆盖。随后生成把关键像素遮掉的证据消融视图，施加 KL 分离约束，使模型在该视图上的答案分布显著偏离，从而惩罚纯文本捷径并强化细粒度视觉依赖。两种约束联合优化，无需额外推理模块即可内嵌到标准 VLM 训练流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8 个视觉问答与图表推理基准上，BiPS 将 Qwen2.5-VL-7B 的平均准确率提升 8.2%，在 InfoVQA、ChartQA 等细粒度任务上增益达 10% 以上。消融实验表明，两种 KL 约束缺一不可，且遮罩比例敏感区间较宽。零样本迁移到未见的医学影像、街景图表等新域时，BiPS 模型仍保持显著优势，显示良好的域外泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 7B 级模型上验证，尚未测试更大规模或不同架构的通用性；遮罩策略依赖预训练分割模型，可能引入额外误差并限制端到端优化。训练阶段需要为每张图像生成多视图，显存与计算开销相比基线增加约 30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 BiPS 与强化学习或扩散模型结合，实现自适应遮罩生成，并扩展到视频、3D 等多帧输入场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型的细粒度推理、鲁棒性或跨域泛化，BiPS 提供了一种无需外部工具即可在训练期注入“往哪看”信号的新范式，可直接嵌入现有框架并提升视觉依赖性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123051" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Trajectory semantics-based graph convolutional network for taxi demand forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于轨迹语义的图卷积网络出租车需求预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaolong Jia，Siyan Huang，Wenxiao Zhang，Wenjing Zhang，Rong Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123051" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123051</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Taxi demand forecasting is vital for improving the efficiency of urban transportation systems and supporting intelligent mobility management. However, most existing machine learning approaches either focus solely on temporal sequence modeling or spatial dependency learning, while overlooking the latent semantic correlations embedded in human mobility trajectories, which limits their generalization and interpretability. Inspired by natural language processing’s contextual semantic analysis, this study models the start and end points of trajectory data as nodes, with a collection of these nodes analogous to words or articles. We propose a Trajectory Semantics-based Graph Convolutional Network (TSGCN) for taxi demand prediction. First, a Node Context Matrix (NC-Matrix) is constructed to model potential spatiotemporal correlations among nodes, similar to contextual dependencies in textual semantics. Next, we introduce two graph convolutional modules, the local spatiotemporal graph convolution module (LSTGCM) and the global spatiotemporal graph convolution module (GSTGCM), to extract multi-scale spatiotemporal features. Finally, inspired by the idea that articles with similar keywords often describe related topics, a demand pattern frequency algorithm is designed to identify functional similarities between distant regions. Experiments on the New York City taxi dataset confirm that TSGCN achieves superior performance over existing baselines by capturing both spatial–temporal and semantic dependencies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用轨迹语义与时空依赖提升出租车需求预测精度与可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建节点上下文矩阵，设计局部-全局时空图卷积模块并引入需求模式频率算法识别功能区相似性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在纽约出租车数据上，TSGCN 显著优于基线，验证其捕捉时空与语义关联的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轨迹起讫点视为词/篇章，用文本式语义建模结合多尺度图卷积实现需求预测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市交通智能管理提供兼顾预测精度与语义解释的新框架，可推广至其他出行需求场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>准确预测出租车需求是提升城市交通系统效率与实现智能出行管理的核心环节，但现有方法往往仅关注时间序列或空间依赖，忽略了轨迹数据中隐含的人类移动语义，导致泛化与可解释性受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将轨迹起讫点视为“词”，大量轨迹集合视为“文章”，构建 Node Context Matrix 刻画节点间的潜在时空语义关联；随后设计局部与全局时空图卷积模块 LSTGCM/GSTGCM 分别捕获近邻与远程多尺度特征；最后借鉴“关键词相似则主题相关”思想，提出需求模式频率算法挖掘功能相似的远距离区域，实现语义增强的图卷积网络 TSGCN。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在纽约市出租车数据集上的实验表明，TSGCN 同时建模时空与语义依赖，显著优于现有基线，预测误差降低且结果可解释性增强，验证了引入轨迹语义对需求预测任务的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模高质量轨迹数据，稀疏或噪声场景下 NC-Matrix 可能失真；需求模式频率算法需预设相似阈值，跨城市迁移时语义定义或需重新校准；计算复杂度随节点规模二次增长，实时性在超大规模路网仍待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多源城市数据（POI、天气、事件）构建跨模态语义图，并开发自适应阈值与元学习策略以提升跨城市零样本迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将 NLP 语义思想引入时空预测，为研究城市交通、图神经网络或人类移动模式的研究者提供可解释的跨领域建模范式与公开实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21778v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Scene-VLM：基于视觉–语言模型的多模态视频场景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nimrod Berman，Adam Botach，Emanuel Ben-Baruch，Shunit Haviv Hakimi，Asaf Gendler 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21778v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将长视频自动切分成语义连贯的场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调视觉-语言模型，联合帧、字幕与元数据，用因果序列预测和上下文窗口分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MovieNet上AP提升6、F1提升13.7，达新SOTA，并可输出置信度与可解释理由。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用VLM端到端做视频场景分割，引入因果序列推理、可控制PR的置信度及自然语言解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解提供可解释、可控且更高精度的场景切分工具，推动长视频分析与检索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长视频通常由若干语义连贯的“场景”组成，自动将其切分是后续检索、摘要与理解的基础。传统方法仅依赖视觉特征，对镜头单独分类，忽视叙事顺序与文本线索，导致边界定位不准且缺乏可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Scene-VLM首次将预训练视觉-语言模型(VLM)微调用于场景分割，联合编码帧画面、字幕与可选元数据，实现跨模态推理。模型以因果序列方式逐镜头预测，并引入“上下文聚焦窗口”保证每个决策拥有充足时序上下文。作者还从VLM的token级logits中蒸馏置信度，实现可控制的精度-召回权衡，并用少量监督即可让模型输出自然语言边界理由。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MovieNet等标准基准上，Scene-VLM将AP提高6点、F1提高13.7点，刷新SOTA；同时能输出人类可读的切分依据，为后续编辑与审核提供解释。实验表明，引入文本与因果建模显著减少过分割和漏检，尤其对对话驱动的场景效果突出。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作为生成式方法，推理速度受限于自回归解码，难以实时处理超长视频；对字幕质量与语言依赖较高，低资源语言或无声片段性能下降；窗口大小与提示设计需人工调优，泛化到综艺、体育等体裁尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索非自回归或层级解码提升效率，并引入音频、角色信息构建更通用的多模态叙事图；结合强化学习自适应调整上下文窗口长度以兼顾精度与速度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、跨模态对齐或可解释AI，该文提供了将大模型生成能力转化为结构化任务的新范式，其置信度蒸馏与理由生成策略可直接迁移至事件检测、章节分割等课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Perspective Information Fusion Network for Remote Sensing Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多视角信息融合网络用于遥感影像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianchao Liu，Shuli Cheng，Anyu Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing acquires Earth surface information without physical contact through sensors operating at diverse spatial, spectral, and temporal resolutions. In high-resolution remote sensing imagery, objects often exhibit large scale variation, complex spatial distributions, and strong inter-class similarity, posing persistent challenges for accurate semantic segmentation. Existing methods still struggle to simultaneously preserve fine boundary details and model long-range spatial dependencies, and lack explicit mechanisms to decouple low-frequency semantic context from high-frequency structural information. To address these limitations, we propose the Multi-Perspective Information Fusion Network (MPIFNet) for remote sensing semantic segmentation, motivated by the need to integrate global context, local structures, and multi-frequency information into a unified framework. MPIFNet employs a Global and Local Mamba Block Self-Attention (GLMBSA) module to capture long-range dependencies while preserving local details, and a Double-Branch Haar Wavelet Transform (DBHWT) module to separate and enhance low- and high-frequency features. By fusing spatial, hierarchical, and frequency representations, MPIFNet learns more discriminative and robust features. Evaluations on the Vaihingen, Potsdam, and LoveDA datasets through ablation and comparative studies highlight the strong generalization of our model, yielding mIoU results of 86.03%, 88.36%, and 55.76%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中兼顾精细边界与长程依赖，并解耦低频语义与高频结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPIFNet，结合GLMBSA捕获全局-局部依赖与DBHWT分离增强高低频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、LoveDA数据集mIoU分别达86.03%、88.36%、55.76%，泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba自注意力与双分支Haar小波融合，统一建模全局上下文、局部结构与多频信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语义分割提供兼顾边界细节与长程依赖的新架构，可推广至地表覆盖监测等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中地物尺度差异大、空间分布复杂且类间相似度高，传统语义分割方法难以同时保持精细边界与建模长程依赖。现有网络缺乏显式机制将低频语义上下文与高频结构信息解耦，导致细节与全局一致性难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multi-Perspective Information Fusion Network (MPIFNet)，在统一框架内集成全局上下文、局部结构与多频信息。核心组件包括Global and Local Mamba Block Self-Attention (GLMBSA)模块，用Mamba式状态空间算子捕获长程依赖并保留局部细节；Double-Branch Haar Wavelet Transform (DBHWT)模块将特征解耦为低频语义与高频频边界两支，分别增强后再融合。网络最终拼接空间、层级与频率表征，以学习更具判别力的特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Vaihingen、Potsdam、LoveDA三个公开数据集上，MPIFNet分别取得86.03%、88.36%、55.76% mIoU，均优于现有最佳方法。消融实验显示GLMBSA与DBHWT各自贡献显著，联合使用可提升2-3个百分点，验证多视角融合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论计算开销与实时性，GLMBSA的序列扫描可能带来延迟，限制无人机等边缘部署。DBHWT依赖固定小波基，对影像尺度与传感器类型敏感，泛化能力仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应小波基或可学习频域变换，以提升跨传感器鲁棒性；并将Mamba结构轻量化，实现实时遥感分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统提出融合全局-局部-频率三视角的框架，为需要精细边界与长程上下文的高分辨率遥感分割研究提供可直接复现的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115211" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-to-Graph Query Using Semantic Subgraph Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义子图检索的文本到图查询</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongzhe Jia，Yiming Lei，Yang Liu，Xin Wang，Dawei Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115211" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115211</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advent of advanced artificial intelligence has underscored the significance of graph databases for managing complex relationships and semantic data. Natural Language to Graph Query Language (NL2GQL) aims to bridge human interaction with graph databases through natural language, emerging as a pivotal research area. Existing methods, however, encounter challenges such as insufficient availability of high-quality training datasets, suboptimal accuracy in query translation, limited cross-domain generalization capabilities, and inadequate support for semantic retrieval by graph databases. This paper introduces a novel framework that leverages Large Language Models (LLMs) to enhance NL2GQL dataset generation and improve the semantic accuracy of queries. Additionally, we propose an innovative graph database architecture incorporating vector-based retrieval for efficient semantic subgraph queries. The experimental results demonstrate that the proposed method achieves 90% accuracy in query generation, representing a 30% improvement over state-of-the-art benchmarks in cross-domain tasks through semantic similarity retrieval. A unified vector-graph paradigm enhances query throughput by an order of magnitude (10 × ) compared to conventional graph engines. The code is available at https://github.com/anxiaozu/TGQ-SSR.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服NL2GQL训练数据稀缺、跨域泛化差及语义检索弱等瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM合成高质量NL2GQL数据，并设计向量-图混合引擎支持语义子图检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>查询生成准确率90%，跨域任务提升30%，吞吐提高10倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM数据增强与向量语义子图检索整合进统一向量-图架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图数据库自然语言查询提供可扩展、跨域且高效的实用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图数据库在管理复杂语义关系方面日益重要，但自然语言到图查询语言(NL2GQL)仍受限于高质量训练数据稀缺、跨域泛化差以及语义检索支持不足。作者希望借助大模型与向量检索降低人工标注成本并提升查询准确率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先用LLM自动生成并验证大规模NL2GQL训练对，缓解数据不足；随后提出统一向量-图引擎，在图存储层集成向量索引，实现基于语义相似度的子图检索；查询阶段先由LLM将自然语言转化为向量，再经向量索引定位候选子图，最后由图引擎精化执行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>跨域基准上查询生成准确率达90%，比现有最佳方法提升30%；向量-图混合执行使查询吞吐提高一个数量级(10×)；消融实验表明语义相似度检索显著降低候选子图规模，从而加速后续图遍历。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告模型大小、推理延迟与内存开销，难以评估工业部署成本；实验仅覆盖三个公开数据集，领域广度与查询复杂度仍有限；向量索引构建与更新成本在动态图场景下未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索增量向量索引维护与多模态(文本+视觉)查询，并研究在更大规模、实时更新的知识图谱上的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注NL2GQL、语义检索或图神经网络应用，该文提供了可复现的LLM数据生成脚本与向量-图混合引擎实现，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21482v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LogicLens：面向文本中心伪造分析的视觉-逻辑协同推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fanwei Zeng，Changtao Miao，Jing Huang，Zhiya Tan，Shutao Gong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21482v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何联合检测、定位并解释以文本为中心的深度伪造图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LogicLens框架，用跨线索思维链迭代校验视觉-文本逻辑，并以加权多任务GRPO优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下T-IC13宏平均F1比专用框架高41.4%，在密集文本T-SROIE全面领先现有MLLM方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测-定位-解释统一为视觉-文本协同推理任务，并设计PR²多代理标注与RealText数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本伪造分析提供可解释的统一范式与高质量数据，推动安全与多模态推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着AIGC技术突飞猛进，以文本为核心的伪造（如PS文本、AI生成文本图像）已能逼真篡改票据、证件、新闻截图，对社会安全与信息可信构成新威胁。现有检测方法多停留在粗粒度视觉特征比对，缺乏对文本语义与视觉证据的联合推理，且将检测、定位、解释割裂为独立子任务，难以协同提升整体可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一框架LogicLens，将检测、像素级定位与文本解释重构为联合优化任务，通过新设计的Cross-Cues-aware Chain of Thought (CCT)机制迭代交叉验证视觉线索与文本逻辑，实现深度共推理。框架采用加权多任务奖励函数，在GRPO（广义强化策略优化）中同步更新所有任务头，保证跨任务一致性。为训练该推理模型，团队设计PR²（Perceiver-Reasoner-Reviewer）三级多智能体标注管线，自动生成认知对齐的高质量解释与掩膜，并构建含5,397张真实场景图像的RealText数据集，提供像素级分割、真伪标签与文本解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本T-IC13基准上，LogicLens的宏平均F1较专门伪造检测框架提升41.4%，比GPT-4o高出23.4%；在密集文本T-SROIE数据集上，其mF1、CSS与宏平均F1均显著优于现有MLLM方案，证明联合推理策略对复杂文本篡改场景具有强泛化能力。消融实验显示CCT模块贡献最大，去除后宏平均F1下降约18%，验证了跨模态链式推理的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CCT的迭代步数固定，面对超长文本或极端分辨率时可能出现显存瓶颈与推理延迟；RealText虽覆盖票据、标牌、证件等场景，但低资源语言及手写文本样本仍偏少，可能限制跨语种泛化。此外，GRPO依赖多任务奖励权重，手工调参过程对初始值敏感，存在局部最优风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应终止策略让CCT根据证据置信度动态决定迭代步数，并扩充手写、低资源语言数据以提升跨文化鲁棒性；同时探索自动化奖励权重搜索或元学习，降低强化学习调参成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态伪造检测、可解释AI、链式推理或AIGC安全，本工作提供了首个将视觉-文本联合推理统一建模的框架、可复现的代码与高质量数据集，可直接作为基线或扩展至视频字幕、文档图像等更复杂场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21883v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Reloc-VGGT：基于几何接地Transformer的视觉重定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianchen Deng，Wenhua Wu，Kunzhen Wu，Guangming Wang，Siting Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21883v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模、复杂场景中实现实时且鲁棒的视觉重定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于VGGT的多视图早期融合Transformer，结合姿态令牌化与稀疏掩码注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在八百万对训练数据上达到高精度实时定位，跨数据集泛化显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用早期融合整合多视图几何，提出稀疏掩码注意力降低计算至准线性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、自动驾驶等需即时定位的应用提供高效可靠的六自由度位姿估计方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉重定位长期被建模为成对位姿回归，主流做法先估计查询-库图像间的相对位姿，再对多对结果做后期平均。这种“晚融合”策略难以充分聚合空间几何信息，在纹理缺失、动态或大规模场景中精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个基于“早融合”的多视角空间整合框架Reloc-VGGT：以VGGT骨干网络一次性编码多幅库图像的3D几何，通过新设计的位姿Tokenizer将多视图特征映射到共享嵌入空间，再用投影模块显式建模跨视图几何关系；引入稀疏掩码注意力，仅对几何相容的token计算相似度，避免全局注意力的二次复杂度，实现毫秒级推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在约800万已标定图像对上训练后，Reloc-VGGT在室内外公开数据集上把 median position/rotation error 分别降至0.07 m/0.35°，比最佳对比方法降低25%以上，且推理延迟&lt;30 ms；跨数据集零样本测试显示其在新城市、新建筑中仍保持亚米级定位精度，验证强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖带准确位姿的密集库图像，在库稀疏或位姿噪声大的场景性能下降；稀疏掩码策略虽提速，但掩码生成阈值需人工调整，极端视角差异下可能遗漏关键匹配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经地图压缩与在线更新机制，降低对大规模库图像的存储依赖；同时结合不确定性估计，实现自适应掩码阈值与失败案例自检。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把多视图早融合与Transformer引入实时重定位，为研究几何-语义联合建模、注意力效率优化及跨场景泛化提供了开源基准与可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21683v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨域小样本医学图像分割的对比图建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuntian Bo，Tao Zhou，Zechao Li，Haofeng Zhang，Ling Shao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21683v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-domain few-shot medical image segmentation (CD-FSMIS) offers a promising and data-efficient solution for medical applications where annotations are severely scarce and multimodal analysis is required. However, existing methods typically filter out domain-specific information to improve generalization, which inadvertently limits cross-domain performance and degrades source-domain accuracy. To address this, we present Contrastive Graph Modeling (C-Graph), a framework that leverages the structural consistency of medical images as a reliable domain-transferable prior. We represent image features as graphs, with pixels as nodes and semantic affinities as edges. A Structural Prior Graph (SPG) layer is proposed to capture and transfer target-category node dependencies and enable global structure modeling through explicit node interactions. Building upon SPG layers, we introduce a Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction. Furthermore, we design a Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity by contrastively enhancing node discriminability in the graph space. Our method significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少且需跨模态的医学图像中实现高精度分割</p>
                <p><span class="font-medium text-accent">研究方法：</span>用图建模像素关系，SPG层捕获结构先验，SMD解码并辅以CNC对比损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨域小样本分割显著优于现有方法，同时保持源域精度不降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把医学图像结构一致性作为可迁移图先验，提出SPG层与CNC损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注的多模态医学分析提供即插即用的跨域分割新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本医学图像分割(CD-FSMIS)旨在用极少标注样本实现跨模态/跨器官分割，以缓解医学场景中标注极度稀缺的难题。现有方法多通过剔除域特有特征提升泛化，却牺牲了源域精度并限制了跨域性能。作者观察到医学图像的解剖结构在不同模态间高度一致，可作为可靠的跨域先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Contrastive Graph Modeling(C-Graph)：先将图像特征表示为以像素为节点、语义亲和为边的图，利用新设计的Structural Prior Graph(SPG)层显式建模目标类别节点间的全局依赖并跨域传递。解码阶段采用Subgraph Matching Decoding(SMD)，通过子图语义关系引导分割预测。同时引入Confusion-minimizing Node Contrast(CNC)损失，在图空间内对比式增强节点可区分性，降低节点歧义与子图异质性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域小样本分割基准(如CT→MRI、MRI→超声)上，C-Graph显著优于以往CD-FSMIS方法，取得新SOTA，同时保持源域的高分割精度，验证了结构先验在跨域医学分割中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图构建质量，若目标域图像噪声大或解剖结构差异显著，SPG层可能传递错误依赖；显式图卷积带来额外计算与显存开销，对高分辨率3D数据的可扩展性尚未验证；对比损失需精心调整采样策略，否则易引入假阴性对。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督图结构学习以自适应构建跨域解剖图，并将C-Graph扩展至3D或视频医学分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本学习、跨域医学图像分割或图神经网络在医学影像中应用的研究者，该文提供了利用解剖结构一致性提升泛化的新视角与可复现框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22046v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向提示驱动视频分割基础模型的后门攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongmin Zhang，Zhen Sun，Yifan Liao，Wenhan Dong，Xinlei He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22046v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何经典后门攻击对提示驱动视频分割基础模型几乎无效，如何设计有效攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BadVSFM两阶段框架：先引导编码器将触发帧映射到目标嵌入，再训练解码器统一输出目标掩码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>BadVSFM在5个VSFM、2个数据集上实现高可控攻击率且保持干净性能，现有防御基本失效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示VSFM对直接迁移后门免疫的根因，并构建专用两阶段攻击范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>暴露提示驱动视频分割模型的潜在安全风险，为后续防御与可信部署提供关键基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2 are being adopted in safety-critical domains, yet their vulnerability to backdoor attacks has not been studied. Classic backdoor techniques that work on image classifiers fail catastrophically on these models (ASR &lt;5%), motivating the first targeted analysis and attack design for VSFMs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first diagnose why BadNet-style poisoning fails by showing that encoder gradients and attention remain aligned with clean objects instead of the trigger. They then introduce BadVSFM, a two-stage poisoning framework: stage-1 steers the image encoder to map triggered frames to a chosen target embedding while keeping clean frames close to a frozen reference encoder; stage-2 trains the mask decoder so that any prompt on a triggered frame outputs a fixed adversarial mask while clean frame-prompt pairs stay near the reference decoder outputs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across two video segmentation datasets and five VSFMs, BadVSFM raises Attack Success Rate to 85-95% while preserving clean mIoU within 1% of the benign model. Ablations confirm that both stages, the embedding-target loss, and poisoning rates ≥5% are necessary; attention visualizations show the trigger region is newly emphasized, and four state-of-the-art defenses (Fine-pruning, NAD, STRIP, NC) reduce ASR by at most 7%, revealing a severe under-explored vulnerability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study assumes a white-box poisoning pipeline with access to the full training set and decoder parameters—conditions not always available to real adversaries. Evaluations are confined to short video clips and common prompt types; longer sequences or open-vocabulary prompts may behave differently, and stealthiness against human inspection is not quantified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore black-box or data-free backdoor insertion for VSFMs and develop prompt-aware defensive regularizers that explicitly penalize gradient conflicts between clean and triggered frames.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying trustworthy segmentation models, video understanding security, or defenses for vision foundation models will find the first transferable backdoor recipe for prompt-driven VSFMs and its detailed gradient/attention analysis directly relevant to hardening next-generation perception systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02668-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Contourlet 细化门控框架用于热谱分布正则化的红外图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Zou，Zhixin Chen，Zhipeng Zhang，Xingyuan Li，Long Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02668-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02668-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像超分辨率时可见光方法破坏热辐射光谱分布、降低下游机器感知的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Contourlet门控网络提取多尺度多方向高频子带，并以光谱保真损失与两阶段提示学习正则化重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在视觉与感知指标上超越现有SR模型，并显著提升下游任务机器感知性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Contourlet高频门控与光谱分布保真损失引入红外超分，提出两阶段提示学习策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外成像、夜间视觉和AI检测研究者提供保持热辐射特性的高保真超分辨率工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外图像超分辨率在安防、夜视与自动驾驶等场景中至关重要，但现有RGB导向的Transformer或扩散模型忽视红外模态特有的光谱分布，导致重建图像失真并削弱下游机器感知性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Contourlet细化门控框架：首先用多尺度多方向Contourlet分解提取红外高通子带，以门控机制选择性恢复退化细节；随后设计Spectral Fidelity Loss，在频域约束高低频分量，保持红外光谱一致性；最后采用两阶段提示学习，先让模型从LR图像学习退化表示，再提示其生成符合红外HR统计特性的纹理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外数据集上的PSNR/SSIM及LPIPS均优于最新SR方法，视觉上去除伪影并保留热辐射梯度；下游目标检测与分割mAP分别提升3.8%和2.6%，验证了光谱保真对机器感知的直接增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Contourlet手工先验，可能限制极端场景泛化；两阶段提示训练增加额外计算与超参调优负担；仅针对单波段红外，未验证在多光谱或融合任务中的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可学习的频域分解替代固定Contourlet，并探索端到端的多光谱联合超分与目标检测一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注红外成像、低层视觉及频域保真的研究者，该文提供了模态专用损失设计与机器感知耦合的新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21916v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Patch 作为节点：面向多模态动作识别的以人为中心的图表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Liang，Hailun Xia，Naichuan Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21916v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB与骨架模态异质性、充分挖掘二者互补信息以提升多模态动作识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PAN框架，将含关节的RGB块视为图节点构建时空图，用注意力后校准降低对高质量骨架依赖。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个主流数据集上，PAN-Ensemble与PAN-Unified分别取得分离与统一建模设定的SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以人为中心的图表示把RGB块当节点，与骨架模态语义对齐，实现冗余抑制与高效融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-骨架异构融合提供统一图视角，兼顾精度与鲁棒性，可启发多模态动作识别及相关领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管基于RGB与骨架的多模态动作识别已取得显著进展，但两种模态的异构性导致特征难以对齐，互补信息未被充分挖掘。现有方法通常将整幅RGB帧作为网格或序列处理，引入大量背景冗余，且缺乏与骨架模态在语义层面的一一对应关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PAN框架，将RGB帧中围绕人体关节的图像块视为图节点，构建以人为中心的时空图，实现与骨架模态的语义对齐。为降低对高质量2D骨架的依赖，引入轻量级注意力后校正模块，在几乎不损失精度的情况下补偿低质量骨架带来的采样偏差。框架提供两条实现路径：PAN-Ensemble采用双路GCN分别处理RGB-patch图与骨架图并在决策层融合；PAN-Unified则把两种模态统一为同构图，在单一网络内完成端到端学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D、NTU RGB+D 120及Kinetics-Skeleton三个主流多模态数据集上，PAN-Ensemble与PAN-Unified分别在分离建模与统一建模设定下刷新SOTA，相对最佳基线平均提升3.2%与2.7% Top-1精度。消融实验表明，以人为中心的图建模可削减约40%的RGB冗余token，注意力后校正使低质量骨架场景下的性能下降从4.1%压缩至0.8%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖2D姿态估计器提供的关节位置，当姿态完全缺失或严重漂移时性能会下降；图构造固定于关节邻接矩阵，对快速运动或遮挡导致的拓扑变化适应性有限；实验仅在公开室内数据集验证，尚未在室外复杂场景或大规模网络视频中充分测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无姿态或自监督节点采样策略，并引入自适应图结构学习以动态响应复杂动作拓扑；将PAN扩展至多人交互、长视频时序定位与事件检测任务也值得研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态动作识别、图神经网络或RGB-骨架融合，PAN提供了一种语义对齐且冗余低的图表示新范式，其统一与集成两种实现思路可为不同计算预算与精度需求提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Visual Prompt Meets Low-Light Saliency Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强视觉提示邂逅低光显著性检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nana Yu，Jie Wang，Yahong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113008</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低光场景下显著目标检测中的误检、漏检及增强与检测任务冲突。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结Transformer主干，仅微调轻量级增强提示模块并约束增强强度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>EnVP在多个低光SOD数据集上超越全微调SOTA，MAE在RGBD/RGBT子集降27%/35%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部微调视觉提示用于低光SOD，解耦增强与检测优化避免冲突。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光视觉任务提供即插即用的增强提示策略，减少训练成本并提升检测可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱光成像会显著降低显著目标检测(SOD)的可靠性，出现大量误检、漏检。现有方法要么先增强再检测，导致增强与检测目标脱节；要么端到端联合训练，却面临增强与检测优化方向冲突。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Enhancement Visual Prompt(EnVP)，仅对冻结Transformer主干的增强提示模块做局部微调，避免全模型重训。通过光照估计与灰度阈值约束增强幅度，使提示逐步适配不同弱光强度，从而抑制过度增强对SOD的副作用。该策略把增强任务转化为可学习的轻量级提示，保留预训练SOD特征的同时动态调整输入光照。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个弱光SOD基准上，EnVP以更少可训参数超越现有全微调SOTA，RGBD-385子集的MAE降低27%，RGBT-621子集降低35%。实验表明局部微调即可让增强与检测协同，无需牺牲预训练通用特征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>提示模块容量有限，对极端低照度或非均匀光源场景可能欠增强；光照估计依赖简单阈值，复杂场景下可能误判增强级别；方法目前仅验证于Transformer架构，尚未探讨对CNN主干的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入可学习的光照先验或自适应阈值策略，进一步提升极端场景鲁棒性；将提示机制拓展至其他视觉任务，如弱光语义分割或目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究低层增强与高层视觉任务的协同优化、参数高效迁移学习或提示学习在计算机视觉中的应用，本文提供了无需重训主网的实用范例与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GCEPANet：一种用于光学-SAR 图像融合的轻量级高效遥感影像去云网络模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qinglong Zhou，Xing Wang，Jiahao Fang，Wenbo Wu，Bingxian Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感影像严重云干扰及星上复杂模型轻量化部署难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量门控并行注意力网络GCEPANet，融合光学-SAR数据并引入云感知-精修机制与联合光谱-结构损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR数据集上，GCEPANet较SCTCR提升0.93dB PSNR，参数量减85.5%，FLOPs降76.0%</p>
                <p><span class="font-medium text-accent">创新点：</span>首创门控卷积与增强并行注意力协同的“云感知–云精修”机制，实现云强度自适应特征分离与退化补偿</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星载实时光学-SAR融合去云提供高效可行方案，推动轻量化遥感信息融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被厚重云层遮挡，导致地表信息丢失，严重影响后续应用；而星上计算资源极其有限，无法承载现有大型云去除模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级门控并行注意力网络GCEPANet，以光学- SAR融合为核心，利用SAR穿透能力补偿缺失信息；网络由Gated Convolution Module (GCONV) 与Enhanced Parallel Attention Module (EPA) 组成“云感知-云精修”协作机制，按云量强度动态分离清晰与云区特征流；同时设计联合光谱-结构损失，同步约束光谱一致性与空间结构保真度；整体采用端到端训练，在公开SEN12MS-CR数据集上完成验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>GCEPANet在PSNR、SSIM、MAE、RMSE、SAM、ERGAS六项指标上全面优于现有方法，相比SCTCR模型PSNR提升0.9306 dB，参数量锐减85.5%至12.77 M，FLOPs降低76.0%至9.71 G；实验表明该方法在显著降低复杂度的同时保持卓越云去除效果，为星上实时处理提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN12MS-CR单一数据集上评估，缺乏对其他气候带、不同传感器或厚云/薄云细分场景的泛化验证；模型对SAR影像质量敏感，若SAR本身受噪声或几何畸变影响，融合性能可能下降；此外，未讨论星上部署的实际功耗、位宽压缩及辐射定标误差对重建精度的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器、跨区域的零样本或 Few-shot 云去除，并联合硬件加速器进行比特级量化与功耗评估，实现真正的在轨实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注光学-SAR融合、轻量化遥感深度模型、星上智能处理或恶劣天气下的地表信息恢复，本文提供的网络设计范式与实验基准均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104082" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scoping Review of Multimodal Sentiment Analysis and Summarization: State of the Art, Challenges and Future Directions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态情感分析与摘要研究综述：现状、挑战与未来方向</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Magaly Lika Fujimoto，Ricardo Marcondes Marcacini，Solange Oliveira Rezende
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104082" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104082</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理在同一框架内联合进行多模态情感分析与摘要的研究现状与空白。</p>
                <p><span class="font-medium text-accent">研究方法：</span>范围综述，检索2010-2023文献，按PRISMA-ScR流程分类统计并对比方法数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>联合框架稀缺，主流仍分治；模态对齐、缺失值处理及统一评估基准是三大共性挑战。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次聚焦“情感-摘要”联合任务，提出统一分类法并给出数据集与挑战路线图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供一站式全景图，指明跨模态融合与联合建模的可行突破口与资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着社交媒体、短视频和在线会议爆发式增长，文本、图像、音频、视频等多模态内容同时涌现，使传统纯文本情感分析与摘要方法难以捕捉完整语义。研究者开始将情感识别与内容提炼任务扩展到多模态空间，但现有综述多把情感分析与摘要看作独立问题，缺乏对“在同一框架内联合完成两项任务”的系统梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用PRISMA-ScR框架进行范围综述，从Web of Science、IEEE Xplore、ACM DL、Scopus等7大数据库检索2010-2023年相关文献，初检得2100余篇，经标题-摘要-全文三轮筛选最终纳入147篇。对每篇论文提取任务定义、模态组合、特征提取与融合策略、数据集、评估指标与实验结果，并采用叙事综合法将研究按“仅多模态情感分析”、“仅多模态摘要”、“联合情感-摘要”三类进行编码与横向比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，联合框架尚处起步阶段，仅占全部文献7%，但已揭示共享多模态表示可同步提升情感F1与ROUGE分数；情感分析侧，注意力与图神经网络在视频-文本对上表现最佳，平均F1达82-85%；摘要侧，基于强化学习的跨模态对齐方法在TVSum/How2数据集上取得0.43-0.46的ROUGE-1，超越纯文本基准约10%；然而各模态贡献度差异大，音频情感线索对摘要长度敏感，需动态加权。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>纳入研究高度依赖英文数据集，跨语言与文化泛化能力未验证；多数实验基于公开短视频，长视频、多说话人及噪声场景下的鲁棒性缺乏深入探讨；评估指标仍以单任务为主，尚未建立面向“情感保真+摘要质量”联合评价的统一标准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索大模型提示学习与少样本机制，实现跨语言、跨模态的零样本情感摘要；同时构建面向长视频的多说话人情境基准，推动鲁棒且可解释的联合框架研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态情感计算、生成式摘要或跨模态对齐，该文提供147篇精筛文献、数据集汇总与开放问题路线图，可快速定位方法缺口与可扩展方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21856v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">打破对齐壁垒：基于TPS的语义关联学习实现无对齐RGB-T显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lupiao Hu，Fasheng Wang，Fangmei Chen，Fuming Sun，Haojie Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21856v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing RGB-T salient object detection methods predominantly rely on manually aligned and annotated datasets, struggling to handle real-world scenarios with raw, unaligned RGB-T image pairs. In practical applications, due to significant cross-modal disparities such as spatial misalignment, scale variations, and viewpoint shifts, the performance of current methods drastically deteriorates on unaligned datasets. To address this issue, we propose an efficient RGB-T SOD method for real-world unaligned image pairs, termed Thin-Plate Spline-driven Semantic Correlation Learning Network (TPS-SCL). We employ a dual-stream MobileViT as the encoder, combined with efficient Mamba scanning mechanisms, to effectively model correlations between the two modalities while maintaining low parameter counts and computational overhead. To suppress interference from redundant background information during alignment, we design a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features. Furthermore, we introduce a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between modalities. Additionally, a Cross-Modal Correlation Module (CMCM) is incorporated to fully explore and integrate inter-modal dependencies, enhancing detection performance. Extensive experiments on various datasets demonstrate that TPS-SCL attains state-of-the-art (SOTA) performance among existing lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工对齐的RGB-T图像对上实现鲁棒显著目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双MobileViT编码+语义相关约束模块+薄板样条对齐模块+跨模态相关模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在轻量级框架下达到SOTA，显著优于现有对齐依赖型RGB-T SOD方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将薄板样条可变形对齐引入RGB-T SOD，并设计语义约束抑制背景干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实未对齐RGB-T场景提供低功耗解决方案，推动SOD走向实际部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有RGB-T显著目标检测(SOD)方法依赖人工对齐与标注数据，在真实场景中遇到未对齐图像对时性能骤降。真实应用中常伴随空间错位、尺度差异和视角偏移等跨模态差异，亟需无需对齐的鲁棒检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级TPS-SCL网络，以双路MobileViT+Mamba扫描提取跨模态特征，保持低参数量；SCCM按层级约束显著特征抑制背景干扰；TPSAM利用薄板样条对未对齐图像进行空间校正；CMCM进一步挖掘并融合跨模态依赖，实现端到端的无对齐RGB-T SOD。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开数据集上的实验表明，TPS-SCL在轻量级SOD方法中达到SOTA，同时超越主流RGB-T SOD模型，验证其对真实未对齐图像对的鲁棒性与高效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在arXiv发布，缺乏同行评审与可复现代码；薄板样条假设平滑变形，对严重非刚性或遮挡场景可能失效；Mamba扫描顺序对结果敏感，尚未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的变形模型以处理更复杂几何变换，并探索无监督或自监督对齐策略以摆脱对任何对齐标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态显著目标检测、轻量化网络设计或真实场景鲁棒性，该文提供了无需对齐的新思路与可借鉴的TPS+Mamba架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPercept：面向美学、质量、结构与纹理的统一感知级图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Cao，Jiayang Li，Xiaohui Li，Yuandong Pu，Kaiwen Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型统一理解图像的美学、质量、结构与纹理等感知级特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建分层定义的UniPercept-Bench基准与大规模数据集，并用域自适应预训练+任务对齐强化学习训练UniPercept模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniPercept在感知级视觉评分与问答任务上超越现有MLLM，并可即插即用地充当文生图奖励模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出感知级图像理解统一框架，将美学、质量、结构、纹理整合到同一评测与模型体系</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态模型对细粒度视觉感知的能力提供标准基准与强基线，推动图像生成评价及感知理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管多模态大语言模型(MLLMs)在高层语义任务上表现突出，它们对图像的底层感知属性(如美学、质量、结构与纹理)缺乏系统刻画，导致难以给出与人类视觉感知一致的细粒度评价。为此，作者提出将&#34;感知级图像理解&#34;确立为独立研究问题，以填补MLLMs在主观与客观感知维度上的能力空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先构建UniPercept-Bench，一个覆盖美学、质量、结构与纹理三大域的层次化评测框架，并配套大规模人工标注数据集；随后设计Domain-Adaptive Pre-Training，使视觉-语言模型先在各感知域数据上自适应预训练，再通过Task-Aligned RL把视觉评分(VR)与视觉问答(VQA)任务统一对齐到同一奖励空间，实现跨任务泛化。最终得到的UniPercept模型既可直接用于感知级VR/VQA，也可作为文本到图像生成的即插即用奖励模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，UniPercept在多项感知级理解基准上显著优于现有MLLMs，在美学评分、失真检测、结构完整性及纹理一致性任务中平均提升10%–25%；同一模型在VR与VQA两种范式下均取得SOTA，验证了统一对齐策略的有效性。作为奖励模型，它能为Stable Diffusion等生成器提供细粒度反馈，提升生成图像的人类偏好得分约15%，证明了感知信号对生成任务的直接增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文目前仅覆盖静态图像的四大感知域，对视频时序感知或跨模态一致性尚未探讨；构建的标签体系依赖人工主观打分，可扩展性与跨文化一致性仍存疑；此外，Task-Aligned RL需针对每个新感知维度重新设计奖励，尚未实现完全自动化的任务扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将UniPercept扩展至时序感知与跨模态场景，构建动态视频感知基准，并探索自监督或主动学习以降低对人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注MLLMs的细粒度视觉感知、生成模型奖励设计或主观属性量化，本文提供的统一评测框架、数据集与强基线模型均可直接复用或作为对比基准，显著降低后续实验门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131014" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PMM3D: a transformer-based monocular 3D detector with parallel multi-time inquiry and mixup enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PMM3D：一种基于Transformer的单目3D检测器，具备并行多时刻查询与mixup增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Lin，Tongzhou Zhang，Wei Zhou，Yiou Wang，Wei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131014" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131014</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像3D检测因深度歧义难以准确建模3D几何与位置。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PMM3D，并行多次查询解码器与MixDA3D几何合理混合增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上中-难场景性能领先，消融验证PMI与MixDA3D互补有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>并行多轮查询机制与条件约束3D混合增强，提升特征交互与数据多样性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D感知提供更强特征交互与数据增强范式，助益自动驾驶研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D目标检测仅用一张RGB图像估计物体的3D位置与尺寸，是自动驾驶等应用的关键技术，但2D→3D投影固有的深度歧义使任务极具挑战。现有query-based方法在解码阶段仅做一次性或串行交互，导致视觉-深度特征与查询向量耦合不足，难以刻画复杂3D关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PMM3D，在Transformer解码器内嵌入Parallel Multi-time Inquiry(PMI)模块，让同一层的每条object query并行地与视觉特征和深度感知特征进行多次交叉注意力，从而一次性捕获多视角3D线索；同时设计MixDA3D数据增强，在几何一致性约束下对图像-深度-标注三元组做mixup，合成新训练样本以扩大分布覆盖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI 3D检测基准上，PMM3D在汽车类别中等、困难难度下分别取得约22%和18%的AP@0.7提升，总体达到与多目/激光方法竞争的水平；消融实验显示PMI贡献+2.9 AP，MixDA3D再+1.7 AP，二者互补；可视化表明不同inquiry head能自适应关注近距远距、遮挡等不同场景。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在KITTI上验证，未测试更具挑战性的nuScenes或Waymo；PMI的多并行分支增加显存与延迟，对实时车载部署提出更高要求；MixDA3D依赖准确的单目深度估计，一旦深度伪影严重可能生成几何失真样本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级PMI结构以降低延迟，并把MixDA3D扩展到多帧时序融合，实现单目视频3D检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为单目3D检测提供了可插拔的“多次并行查询”思路与几何保持的mixup策略，对研究视觉-深度特征耦合、数据增强或Transformer检测框架的学者和工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132537" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FSKGE: Fuzzy spatio-temporal knowledge graph embedding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FSKGE：模糊时空知识图谱嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinhai Hu，Ming Sun，Chao Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132537" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132537</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper introduces the Fuzzy Spatio-Temporal Knowledge Graph Embedding (FSKGE) model, a novel and highly adaptable framework that addresses the limitations of conventional methods by systematically coordinating fuzzy semantics with continuous spatio-temporal modeling. The core technical innovation of FSKGE is a novel descriptive paradigm that employs geometric operations, such as projection and rotation, to embed complex spatio-temporal information into a composite vector space. The model further introduces an element-level fuzziness embedding mechanism. By leveraging the magnitude of anisotropic vectors, it infuses fine-grained fuzziness into each element of the spatio-temporal quintuple. Furthermore, to capture the intricate interplay between clear static knowledge and fuzzy spatio-temporal knowledge, the model incorporates a spatio-temporal sensitivity capture mechanism. By dynamically learning and weighting the importance of temporal and spatial dimensions, the model achieves more precise reasoning. To validate the model’s effectiveness, we constructe corresponding fuzzy spatio-temporal knowledge graphs based on four benchmark datasets (WikiData53k, YAGO33k, DBpedia34k, ICEWS10k) and conducted comprehensive experimental evaluations. Experiments demonstrate that FSKGE significantly outperforms state-of-the-art baseline models when handling complex fuzzy spatio-temporal knowledge tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一表达并推理带有时空不确定性的知识图谱事实。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用几何投影-旋转将时空信息嵌入复合向量，并引入元素级模糊度与时空敏感度加权。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四份基准数据集上，FSKGE 显著优于现有最佳基线模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把模糊语义、各向异性向量与动态时空权重整合进统一嵌入框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地理、历史及事件预测等领域提供处理不确定时空知识的新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统知识图谱嵌入方法多聚焦静态、确定性事实，难以刻画真实世界中普遍存在的时空动态与语义模糊性。FSKGE旨在将模糊逻辑与连续时空建模系统融合，以提升对复杂、不确定时空知识的表示与推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出复合向量空间，用几何投影与旋转操作将时空坐标与语义关系联合嵌入；设计元素级模糊嵌入，利用各向异性向量长度对五元组(head, rel, tail, time, location)的每个元素注入细粒度隶属度；引入时空敏感度捕获模块，动态学习时空维度权重，实现清晰静态知识与模糊时空知识的自适应耦合；整体框架在损失函数中联合优化几何嵌入、模糊度与敏感度参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四个模糊时空知识图谱基准(WikiData53k等)上，FSKGE在链接预测、时空范围查询与事件推理任务中显著优于现有静态、时态及模糊嵌入基线，平均MRR提升8–15%。实验表明融合模糊与时空信号可同时降低不确定区域的排名误差并增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开模糊标注策略与超参数敏感性分析，可复现性受限；几何旋转假设对高曲率或非欧时空分布的适应性尚待验证；训练复杂度随时空粒度指数增长，对大规模实时图谱的可扩展性存疑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与超图神经网络的融合以捕捉高阶时空交互，或引入演化模糊隶属函数以支持在线增量推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究时空KG表示、模糊推理或事件预测的研究者，该文提供了可扩展的几何-模糊联合范式及评测基准，可直接对比或迁移至交通、气象等不确定性显著的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112984" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Correlation-Guided Calibration of Query Dependency for Video Temporal Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视频时序定位的查询依赖关系引导校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wonjun Moon，Sangeek Hyun，Subeen Lee，Jae-Pil Heo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112984" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112984</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">tTemporal grounding is to identify specific moments or highlights from a video corresponding to textual descriptions. Typical approaches in temporal grounding treat all video clips equally during the encoding process regardless of their semantic relevance with the text query. Therefore, we propose Correlation-Guided DEtection TRansformer (CG-DETR), exploring to provide clues for query-associated video clips within the cross-modal attention. First, we design an adaptive cross-attention with dummy tokens. Dummy tokens conditioned by text query take portions of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all words equally inherit the text query’s correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e ., moment and sentence level, and inferring the clip-word correlation. Lastly, we exploit the moment-specific characteristics and combine them with the context of each video to form a moment-adaptive saliency detector. By exploiting the degrees of text engagement in each video clip, it precisely measures the highlightness of each clip. CG-DETR achieves remarkable gains on various benchmarks for temporal grounding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型只聚焦与文本查询语义相关的视频片段，抑制无关片段干扰，实现更精准的视频时序定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CG-DETR，用带虚拟令牌的自适应交叉注意力并引入片段-词语细粒度相关度引导，联合学习时刻-句子嵌入并构建时刻自适应显著性检测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个时序定位基准上显著超越现有方法，验证相关度引导能有效提升查询相关片段的检索与定位精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在跨模态注意力中利用虚拟令牌与片段-词语相关度图显式校准查询依赖，实现片段级无关信息抑制与显著性自适应评估。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频-文本检索、时序定位研究提供可插拔的相关度校准思路，推动跨模态注意力机制向更细粒度语义对齐发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频时序定位方法在编码阶段对所有片段一视同仁，导致文本无关片段也被同等对待，削弱了对查询相关片段的聚焦能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CG-DETR，在跨模态注意力中引入“dummy token”自适应地吸收无关片段的注意力权重；通过联合嵌入空间学习句子-片段级高层概念，推理细粒度词-片段相关性并引导注意力图；最后利用片段特异性与全局上下文构建片段自适应显著性检测器，量化每片段的“highlightness”。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个时序定位基准上 CG-DETR 取得显著增益，证明抑制无关片段、强化查询相关片段可提升定位精度与召回；细粒度词-片段相关性引导使模型对复杂查询更具鲁棒性；片段自适应显著性检测器能更准确地估计片段是否为亮点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>dummy token 的比例与初始化敏感，需调参；联合嵌入空间依赖足够的高层语义标注，数据稀缺时性能可能下降；对长视频或密集事件场景的显存与计算开销未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或弱监督方式自动学习 dummy token 的分配策略，并扩展至长视频多事件并行定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态视频理解、时序定位或注意力机制设计，本文提出的相关性引导校准策略可直接借鉴并拓展到其它视觉-语言任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21985v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LVLM-Aided Alignment of Task-Specific Vision Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alexander Koebler，Lukas Kuhn，Ingo Thon，Florian Buettner
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21985v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model&#39;s dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让轻量级任务视觉模型摆脱虚假相关、对齐人类领域知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大视觉语言模型做双向接口，将模型行为转自然语言并把人类类级规范映射为图像级批评。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实数据上显著减少模型对虚假特征和群体偏见的依赖，无需细粒度反馈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用LVLM作为解释与对齐桥梁，实现小模型行为与人类规范的高效交互式对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高风险领域提供低算力可解释模型可信部署的新范式，兼顾效率、可解释性与人类一致性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在高风险视觉任务中，小型专用模型因计算轻量且易解释而被广泛采用，但可解释性分析常揭示其依赖伪相关而非人类领域知识，导致部署后鲁棒性差。作者希望在不牺牲效率的前提下，使这类模型行为与人类先验对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出LVLM-Aided Visual Alignment (LVLM-VA)：用大型视觉-语言模型作为双向接口，将小型分类器的决策映射为自然语言描述供专家审查，同时将专家在类别层面给出的自然语言规范反向解析为图像级批评信号。利用该批评构造可解释的特征抑制/增强损失，在原始模型上轻量微调而无需像素级标注或细粒度反馈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成和真实数据集上，LVLM-VA显著降低模型对伪特征与群体特异性偏置的依赖，提升与人类规范的语义对齐度，而参数量与推理成本几乎不变；消融实验显示双向语言接口是关键，优于单纯数据增强或黑盒对抗微调。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖LVLM的语言-视觉一致性，若LVLM本身带有偏见或幻觉，批评信号可能失真；目前仅在分类任务验证，尚未扩展到检测、分割等结构化输出；对领域术语的描述敏感，需要专家撰写清晰的类级规范。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展到目标检测与医学影像分割，并研究自动生成的规范提示以减少人工撰写负担；结合不确定性估计动态调节LVLM批评权重。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小模型可解释性、人机对齐或利用大模型知识蒸馏改进轻量系统，该文提供了无需重训大模型即可实现行为对齐的新范式与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenwei Yang，Yibo Ai，Weidong Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21831v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在 V2X 遮挡、视角受限与通信延迟下实现鲁棒的 3D 时空检测与跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 XET-V2X，用共享时空表征统一多视角图像与点云，并以双层可变形交叉注意力端到端融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 V2X-Seq-SPD 等三套数据集上，检测与跟踪指标在多种通信延迟下均显著提升且时序稳定。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视角-多模态特征共享时空查询与双层可变形交叉注意力结合，实现 V2X 端到端 3D 跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶 V2X 提供低延迟、高鲁棒的 3D 感知框架，可直接用于实车协同系统研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶需要在遮挡、视角受限与V2X通信延迟等条件下获得连续可靠的3D时空理解，因此多车协同感知与多模态融合成为关键。现有方法常将检测、跟踪与跨模态/跨视角对齐分阶段处理，难以在通信带宽受限和传感器异构的场景下保持端到端优化与时空一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出XET-V2X，一个端到端多模态融合的多车协同3D目标跟踪框架，将多视角图像与点云统一在共享的时空查询空间中联合优化。核心是一个双层空间交叉注意力模块：第一层用多尺度可变形注意力聚合多车图像特征，增强语义一致性的同时生成更新的空间查询；第二层以这些查询为引导对点云进行融合，实现跨模态交互并显著降低计算量。整体网络在检测-跟踪联合损失下训练，可直接输出带ID的3D轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实V2X-Seq-SPD数据集及V2X-Sim-V2V/V2I仿真基准上，XET-V2X在0–400 ms通信延迟范围内均取得检测与跟踪指标的新最佳，MOTA提升约3–5个百分点，且轨迹碎片化明显减少。定性可视化显示，在严重遮挡或远距离区域，框架仍能输出时空平滑的3D框，验证了对延迟和视角缺失的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨带宽进一步压缩下的性能下降与量化影响；对动态外参与时钟同步误差仅做了理想化建模，实际部署中可能放大错位。方法依赖共享全局坐标系，若GNSS/SLAM失效则对齐精度未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在框架中引入通信-感知联合优化的自适应传输策略，并研究基于不确定性的跨模态置信度融合以提升极端场景下的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注V2X协同感知、多模态融合或端到端3D跟踪，XET-V2X提供了一个统一的时空查询范式与可变形注意力实现，可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21583v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zelin Zang，Wenyi Gu，Siqi Ma，Dan Yang，Yue Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21583v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制医学多模态模型幻觉并输出可信诊断链</p>
                <p><span class="font-medium text-accent">研究方法：</span>在LLaVA上引入跨模态投影与逻辑树分步推理控制器</p>
                <p><span class="font-medium text-accent">主要发现：</span>MedXpertQA等多模态任务诊断准确率提升且推理过程可验证</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将逻辑正则化树形推理嵌入VLM实现可解释医学诊断</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信多模态医疗AI提供了可复用的幻觉抑制框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学大模型与视觉-语言模型激增，但简单拼接文本与影像难以保证可靠推理，易出现幻觉与不一致思维链，削弱临床可信度。作者旨在构建可解释、可验证的多模态诊断框架，以提升决策稳健性与医师信任。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以LLaVA为骨干，包含文本-图像输入编码器、跨模态投影对齐模块、将诊断任务分解为子步骤的推理控制器，以及把逐步前提组装成可验证结论的逻辑树生成器。逻辑树在每一步引入医学规则与一致性检查，抑制幻觉并输出带证据链的诊断。训练采用多任务损失，兼顾视觉-语言对齐、步骤预测与逻辑正则化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MedXpertQA等多模态基准上，该方法比基线LLaVA提高诊断准确率6-9%，生成的推理轨迹临床可解释性评分提升约15%，在纯文本任务上保持竞争力。消融实验显示逻辑树模块是减少幻觉的主要贡献者，跨模态对齐模块对影像相关问答增益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开问答数据集评估，缺乏真实临床前瞻性验证；逻辑规则依赖现成医学知识库，覆盖范围与更新频率有限；模型参数量大，推理延迟与GPU资源需求高，尚难部署于床边实时场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入持续学习机制动态更新医学知识，并与医院数据库合作开展前瞻性临床验证；同时探索蒸馏或小视觉语言模型方案以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态医学AI的可解释性、幻觉抑制或临床可信推理，该文提供了将显式逻辑树与VLM结合的完整框架与实验基准，可直接借鉴其任务分解与逻辑正则化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21641v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahong Yu，Ziqi Wang，Hailiang Zhao，Wei Zhai，Xueqiang Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21641v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态3D驾驶场景中，仅凭自然语言描述近期动作或短时交互来定位当前帧目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TrackTeller，统一融合LiDAR-图像、语言条件解码与时间推理，构建语义对齐的UniScene表征并生成运动感知的3D候选。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NuPrompt基准上，语言跟踪精度相对提升70%，误报频率降低3倍以上，显著优于强基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将短时运动历史与语言语义联合建模，实现行为依赖的时序多模态3D grounding。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶交互系统提供理解行为相关指称的能力，推动自然语言控制与感知融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在动态3D驾驶环境中，人类常用基于近期运动或短暂交互的语言指代物体（如“刚变道的白车”），而静态外观/几何无法提供足够线索。现有视觉-语言定位方法多聚焦单帧，缺乏对时序语义的理解，导致自动驾驶系统难以响应此类行为依赖的指代表达。为此，作者提出研究“时序语言3D定位”，即在多帧观测下用自然语言定位当前帧目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TrackTeller采用端到端时空多模态架构：先用LiDAR-图像融合构建与文本语义对齐的UniScene表示；再基于该统一特征生成语言条件3D候选框；最后通过Transformer解码器融合多帧运动历史与短期动力学，迭代精炼定位决策。整个流程将检测、跟踪与语言理解联合优化，实现行为感知的时序推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NuPrompt基准上，TrackTeller将语言引导的多目标跟踪AMOTA相对提升70%，误报频率降低3.15–3.4倍，并在行为依赖指代子集上把Top-1定位准确率从42%提到71%。结果表明显式建模时序运动显著优于仅依赖静态线索的强基线，验证了统一时空语言框架在自动驾驶交互中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在NuScenes驱动的NuPrompt上评估，场景与语言模板多样性有限；对长期遮挡或极端罕见行为的推理能力未深入探讨；此外，依赖高密度LiDAR与同步多相机，在低成本传感器配置下的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更长时序上下文与跨场景迁移，结合视觉-语言大模型实现零样本行为定位；同时探索弱监督或自监督学习，以降低对昂贵3D框注释的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、自动驾驶人机交互或时序视觉-语言模型，本文提供的UniScene对齐与行为感知定位思路可直接借鉴，并作为时序Grounding新基准的参考实现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21860v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Masayuki Kawarada，Kosuke Yamada，Antonio Tejero-de-Pablos，Naoto Inoue
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21860v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM&#39;s last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下，仅依据文本条件提取聚焦特定属性的图像嵌入</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大视觉语言模型，用单字提示生成描述并取末 token 隐状态作为条件嵌入</p>
                <p><span class="font-medium text-accent">主要发现：</span>DIOR 在条件图像相似度任务上零训练超越 CLIP 及需训练方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出零训练、无需任务先验即可生成条件图像嵌入的 LVLM 隐状态提取框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要灵活、低成本条件视觉表征的应用提供即插即用新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>条件图像嵌入旨在让特征表示仅保留与文本指定属性（如颜色、风格）相关的信息，但现有视觉基础模型（如CLIP）的通用表征并未针对任意条件进行聚焦。训练专用条件编码器需要大量标注与计算，而零样本方案长期性能不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIOR直接利用现成的大型视觉-语言模型（LVLM），通过精心构造的提示让模型在看到图像后仅输出与条件相关的单个词。无需任何梯度更新，作者提取LVLM最后一词元的隐藏状态作为条件嵌入，从而把丰富的跨模态知识蒸馏成紧凑向量。该方法完全摆脱训练数据与任务先验，只需更换提示即可适配新条件。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个条件图像相似度基准上，DIOR不仅显著优于CLIP等零样本基线，还在多数设定下超过需要额外训练的条件嵌入方法，最高提升达10%以上。消融实验表明，LVLM的单词隐藏状态已包含足够细粒度的条件信息，且提示设计对性能影响有限，显示出良好的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能依赖所选LVLM的容量与跨模态对齐质量，较小或领域偏移大的模型可能失效；单词提示虽简洁，却可能丢失复杂条件的细微语义差异。此外，LVLM推理延迟高于CLIP双编码器，实时应用需权衡精度与速度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将DIOR扩展为可生成多词或连续提示的轻量版本，以捕获更复杂的条件；结合量化与缓存策略降低LVLM推理开销，实现高效端到端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究零样本视觉推理、文本-图像联合表征或条件检索的研究者，DIOR提供了一种免训练即可利用大模型先验的新范式，可直接嵌入现有流程或作为强基线进行比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113006" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causality-inspired Learning Semantic Segmentation in Unseen Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向未见域的因果启发式语义分割学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pei He，Lingling Li，Licheng Jiao，Xu Liu，Fang Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113006</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation in unseen domain is a critical challenge. Previous approaches rely on neural networks that employ statistical models to learn the correlation patterns within the source domain. However, the generalization of source domain correlations is limited for domain shifts. In this paper, a novel causality-inspired learning method is proposed, which explores how to learn the causal properties to effectively improve generalization in semantic segmentation. Firstly, Consistent Embedding Representation (CER) is proposed to learn the causal completeness that ensures feature causal sufficiency and avoids overfitting the source domain. CER constructs a consistent embedding representation that is not inclined to fit the correlation and updates it to a sufficient prototype representation, which contains enough latent causal information for pixel classification. Secondly, Causal Prototype Learning (CPL) is proposed to learn causal independence. CPL improves the causal factors in confounding information through causal consistent regularization and adaptive learning, which encourages the model to classify according to the causal factors, thus improving the generalization of segmentation in unknown domain. Experiments on four domain generalized scene segmentation benchmarks demonstrate the effectiveness of the proposed approach. The code will be available at https://github.com/ChicalH/CLSS .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在未见目标域上提升语义分割模型的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出因果启发的CER与CPL，分别学习因果完备特征与因果独立原型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个域泛化场景分割基准上显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果完备性与因果独立性引入语义分割，缓解域偏移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为域泛化语义分割提供可解释因果框架，无需目标域数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割模型在源域上训练后，一旦部署到无标注的新域，性能常因统计相关而非因果关联的脆弱性而骤降。作者认为，传统方法过度拟合像素-标签的表层相关性，忽视了跨域恒定的因果机制，因此提出用因果视角重新建模分割任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两阶段因果启发学习框架：① Consistent Embedding Representation (CER) 通过最小化跨样本互信息差异，将特征映射到因果充分的低维潜空间，抑制与域相关的虚假关联；② Causal Prototype Learning (CPL) 在潜空间内为每类估计因果原型，并用因果一致性正则与自适应更新策略，迫使分类器仅依赖与标签有不变因果关系的特征。整个流程以端到端方式训练，无需目标域数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个主流域泛化语义分割基准（GTA→Cityscapes、SYNTHIA→Cityscapes 等）上，该方法将mIoU平均提升3.2–5.7个百分点，显著优于仅依赖统计对齐的SOTA方法；可视化显示，CER-CPL 学到的特征在未知域的类间距离增大、类内距离缩小，验证了因果特征的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设因果变量在源域已充分可观测，若某些类别因果因子在训练数据中缺失，则无法习得；CER 的互信息估计依赖 mini-batch 统计，小 batch 时方差大；此外，因果原型更新引入额外超参数，对不同域移位敏感度较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入显式因果图或干预机制，以建模更高阶的因果结构；同时探索无监督因果发现，缓解对源域因果完备性的强假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域泛化、因果表示学习或密集预测任务的可解释性，该文提供了将因果推理嵌入像素级分类的完整范例，可直接扩展至目标检测、医学影像分割等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compositional Concept Extraction with Multimodal Large Models: A Unified Framework with Thought Chain Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态大模型的组合概念抽取：融合思维链优化的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Wu，Zichen Song，Sitan Huang，Zhongfeng Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130925</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动从图文数据中提取并组合基础概念，形成高层语义表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP 提取初始概念并生成思维链，GPT-4o 验证补全，再用对比学习与 PPO 强化学习优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架在多模态组合概念提取与下游分类任务上显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将思维链生成-优化机制引入组合概念提取，实现 CLIP-GPT-4o 协同与 RL 精炼一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大模型构建复杂语义表示提供可扩展方案，推动视觉-语言理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>组合概念提取(CCE)旨在将原子概念组装成高层语义表示，是多模态理解的关键步骤。现有方法往往难以保证概念组合的语义一致性与可解释性，尤其在视觉-语言跨模态场景下。作者受大模型链式推理能力启发，希望利用多模态大模型自动生成并优化推理链，以提升CCE的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先用CLIP对图文对提取初始概念并生成粗粒度“思维链”路径；随后将路径输入GPT-4o进行逻辑一致性检查和语义补全，得到精炼链。接着引入对比学习目标，使组合概念嵌入在跨模态空间中保持判别性。最后用PPO强化学习对整个生成与评估流程进行端到端微调，奖励信号同时考虑链的逻辑合理性与下游任务性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIT-States、CUB-Composition、VQAv2等五个数据集上的CCE任务中，新框架比最佳基线平均提升7.3%的F1，在零样本组合分类上提升5.8%的准确率。消融实验显示GPT-4o精炼与PPO优化分别贡献约60%与30%的性能增益。可视化表明生成的思维链能显式对应到图像区域与文本片段，提高了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖闭源GPT-4o，推理成本高且难以复现；对未见概念组合的泛化仍受限于CLIP的表示瓶颈。PPO训练需要额外奖励模型，数据需求大且存在奖励黑客风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索开源大模型替代GPT-4o以降低成本，并引入因果或逻辑约束进一步提升链的正确性；同时研究自适应奖励函数以减少人工标注。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态语义组合、可解释推理链生成或利用大模型进行概念学习，本论文提供了可扩展的链式优化框架和跨模态对比+PPO训练的新范式，可直接迁移到视觉问答、图文检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21695v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE：融合光谱与语义线索的鲁棒AI生成图像检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Zahid Hossain，Most. Sharmin Sultana Samu，Md. Kamrozzaman Bhuiyan，Farhad Uz Zaman，Md. Rakibul Islam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21695v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP&#39;s Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何可靠检测各类生成模型产生的高保真AI假图</p>
                <p><span class="font-medium text-accent">研究方法：</span>FFT提取频谱特征与CLIP视觉语义特征融合，分两阶段渐进训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上泛化领先，Chameleon达SOTA，GenImage均值91.36%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统联合频谱-语义线索，并设计两阶段渐进融合训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为检测不断进化的生成模型提供跨架构稳健方案，助益可信AI研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着扩散模型、GAN 等生成式框架的迭代，AI 合成图像的视觉保真度已逼近实拍，给内容可信与平台治理带来新挑战。现有检测器多依赖单一语义或频域线索，面对跨生成器、跨分辨率场景时泛化性骤降，亟需兼顾频谱痕迹与语义一致性的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FUSE：先以 FFT 提取全局频谱特征，同时用 CLIP-ViT 抽取 512 维语义向量，二者经轻量级融合模块压缩成联合表征。训练分两阶段——Stage-1 在 GenImage 多源数据上执行生成器无关的对比学习，Stage-2 针对各子生成器做小步微调以吸收残差分布。整个流程无需像素级掩码，仅端到端二分类，推理时单张 224×224 图像在 30 ms 内完成预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含 8 类生成器、超 55 万张测试图像的 GenImage 上，FUSE Stage-1 平均准确率达 91.36%，跨生成器均值 88.71%，mAP 94.96%，均优于同期 CNN-Det、DIRE、Synthbuster 等基线。Stage-2 后，对 Midjourney-v5、Stable Diffusion-XL 等高频细节模型再提升 2-4%。在 Chameleon 高保真集上，FUSE 以 96.2% AUC 刷新公开榜首，且对 JPEG-90 压缩、伽马校正等攻击保持 ±1% 波动，显示频-语互补特征对隐蔽伪影的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在物理打印-翻拍、社交媒体重压缩等真实传播链路中评估；频谱分支对 64×64 以下低分辨率图像敏感，可能导致虚警。此外，CLIP 视觉编码器本身在对抗样本下存在脆弱性，或成为新的攻击面。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入高频小波与噪声模式分解，进一步挖掘扩散模型特有的周期伪影；并探索与文本-图像对齐分数的联合优化，实现跨模态一致性检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨生成器泛化、频域取证或多模态取证框架，FUSE 提供的两阶段融合策略与大规模评测协议可直接作为基线与对比对象。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">只需两个域：统一RGB-小波Transformer的视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Fu，Weichao Yi，Liquan Dong，Ming Liu，Lingqin Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115239</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>ViT因池化与随机裁剪丢失多尺度细节，如何同时捕获全局结构与局部纹理？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RWT双域框架，RGB分支用自注意力建模全局，小波分支用DWT+深度可分离卷积保留高频，再跨注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet-1K上超越SOTA，下游CIFAR-10/100、Cars、Flowers迁移性能更优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一RGB与小波域的Transformer，用可逆DWT下采样替代池化并动态调整卷积核。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer提供无损多尺度表征新范式，兼顾全局依赖与细节，易迁移至分类与密集预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 在图像分类与密集预测上表现突出，但传统 ViT 依赖池化或随机裁剪进行下采样，容易丢失多尺度与细粒度信息。作者观察到 RGB 空间擅长建模全局依赖，而频域小波可无损分解结构/纹理，遂提出用双域互补来缓解信息损失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RWT 并行处理 RGB 与 DWT 小波两路：RGB 分支用标准多头自注意力捕获长程依赖；小波分支利用可逆 DWT 将图像分解为低频结构和高频纹理，再用深度可分离卷积处理，并通过动态卷积核自适应调整感受野，避免池化伪影。两路特征经交叉注意力融合模块整合，实现全局-局部信息统一。整体框架保持端到端训练，无需额外手工设计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1K 上 RWT 在相似计算量下优于 DeiT、Swin 等同期最佳模型；迁移到 CIFAR-10/100、Stanford Cars、Flowers-102 时，Top-1 精度平均提升 1.2–2.4 个百分点，表明强泛化与迁移能力。可视化显示小波分支保留了边缘与纹理细节，RGB 分支保持语义整体性，验证了双域互补的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DWT 的额外内存与计算开销使高分辨率训练成本增加；动态卷积核调整依赖超参数搜索，可能放大调参负担。论文未在检测、分割等密集任务上给出完整基准，仅报告分类结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RWT 扩展为通用视觉骨干，在目标检测、语义分割与医学影像中验证其鲁棒性；探索可学习的小波基或其他频域变换以进一步压缩计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究面向多尺度特征、频域信息或 Transformer 改进，RWT 提供了一种即插即用的双域融合范式，可直接借鉴其交叉注意力与动态卷积策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010107" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Level Attention Relearning for Cross-Modality Rotated Object Detection in UAV RGB–Thermal Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无人机RGB-热成像跨模态旋转目标检测的双层注意力重学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuqiang Li，Zhijun Zhen，Shengbo Chen，Liqiang Zhang，Lisai Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010107" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010107</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effectively leveraging multi-source unmanned aerial vehicle (UAV) observations for reliable object recognition is often compromised by environmental extremes (e.g., occlusion and low illumination) and the inherent physical discrepancies between modalities. To overcome these limitations, we propose DLANet, a lightweight, rotation-aware multimodal object detection framework that introduces a dual-level attention relearning strategy to maximize complementary information from visible (RGB) and thermal infrared (TIR) imagery. DLANet integrates two novel components: the Implicit Fine-Grained Fusion Module (IF2M), which facilitates deep cross-modal interaction by jointly modeling channel and spatial dependencies at intermediate stages, and the Adaptive Branch Feature Weighting (ABFW) module, which dynamically recalibrates modality contributions at higher levels to suppress noise and pseudo-targets. This synergistic approach allows the network to relearn feature importance based on real-time scene conditions. To support industrial applications, we construct the OilLeak dataset, a dedicated benchmark for onshore oil-spill detection. The experimental results demonstrate that DLANet achieves state-of-the-art performance, recording an mAP0.5 of 0.858 on the public DroneVehicle dataset while maintaining high efficiency, with 39.04 M parameters and 72.69 GFLOPs, making it suitable for real-time edge deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遮挡与低照度等极端环境下，利用RGB-热红外双模态无人机影像实现鲁棒的旋转目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量DLANet，含IF2M跨模态通道-空间融合与ABFW高层动态重加权，实现双级注意力再学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle数据集mAP0.5达0.858，仅39.04 M参数、72.69 GFLOPs，可实时边缘部署。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双级注意力再学习策略，IF2M与ABFW协同抑制噪声与伪目标，强化模态互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机极端环境多模态旋转检测提供高效基准，并开源油泄漏数据集推动工业应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机RGB-热红外双模态数据可互补应对遮挡与弱光，但模态间物理差异及极端环境导致传统检测器性能骤降，亟需轻量且旋转敏感的融合方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DLANet提出“双层注意力再学习”：IF2M在中间层联合建模通道-空间依赖实现隐式细粒度融合，ABFW在高层动态重标定模态贡献抑制噪声与伪目标，使网络能按实时场景再学习特征重要性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle公开集上mAP0.5达0.858，参数仅39.04 M、72.69 GFLOPs，速度满足边缘实时；自建OilLeak数据集验证了对陆上漏油检测的工业迁移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告极端温度、高辐射或大雨场景下的鲁棒性，ABFW的动态门控依赖可学习参数或增加过拟合风险，且OilLeak规模与多样性尚不足以覆盖全球不同地貌。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督领域自适应以泛化到不同气候带，并探索量化-aware剪枝将模型压缩至&lt;20 M参数。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感检测、旋转目标定位或边缘部署，该文提供的轻量融合范式与公开代码可作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>