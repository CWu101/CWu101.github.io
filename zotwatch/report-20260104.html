<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-04</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-04 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态视觉问答的论文、1篇关于图拓扑学习的论文、1篇关于农业智能诊断的论文和1篇关于脑网络认知解码的论文。</p>
            
            <p><strong class="text-accent">跨模态视觉问答</strong>：《Video and Language Alignment in 2D Systems for 3D Multi-object Scenes》提出无导数多信息控制，用2D跨模态模型驱动3D场景中的相机对齐视频与语言；《Improving Few-Shot Change Detection Visual Question Answering》利用决策模糊度引导强化微调，提升双时相遥感图像变化检测问答的小样本性能。</p>
            
            <p><strong class="text-accent">图拓扑学习</strong>：《Frequent subgraph-based persistent homology for graph classification》将频繁子图挖掘与持续同调结合，为图分类提供更具表达力的拓扑特征。</p>
            
            <p><strong class="text-accent">农业智能诊断</strong>：《CPJ: Explainable Agricultural Pest Diagnosis》通过Caption-Prompt-Judge框架，用LLM对图像描述进行可解释精炼，实现低成本、高鲁棒的作物病虫害诊断。</p>
            
            <p><strong class="text-accent">脑网络认知解码</strong>：《Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes》采用谱域图神经网络对fMRI连接组建模，精准分类认知任务状态。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态视觉定位的论文、6篇关于三维场景理解的论文、5篇关于变化检测与问答的论文、4篇关于小样本/弱监督学习的论文、3篇关于跨模态行人重识别的论文、2篇关于谣言检测的论文以及2篇关于多光谱目标检测的论文。</p>
            
            <p><strong class="text-text-secondary">多模态视觉定位</strong>：该主题聚焦在图像或点云中依据自然语言描述精确定位目标，代表作《RGBT-Ground Benchmark》提出RGBT基准，《MoniRefer》构建路侧3D视觉定位大规模数据集，《Evolving, Not Training》用进化提示实现零样本推理分割，《GrowSP++》在3D点云无监督语义分割中联合生长超点与基元，《Text-to-video person re-identification benchmark》建立文本-视频行人检索基准并设计双模态上下文对齐，《A Multi-Granularity Scene-Aware Graph Convolution Method》以多粒度场景感知图卷积做弱监督行人搜索，《Improving Few-Shot Change Detection Visual Question Answering》用决策-模糊引导强化微调提升CDVQA，《Progressive Temporal Compensation and Semantic Enhancement》通过渐进时序补偿将第三人称视频生成第一人称视角。</p>
            
            <p><strong class="text-text-secondary">三维场景理解</strong>：研究面向点云或RGB-D的语义分割、实例分割与补全，强调自监督或无监督学习，《GrowSP++》通过生长超点与几何基元实现无监督3D语义分割，其余论文围绕自监督预训练、几何-语义联合建模及大规模3D基准构建展开。</p>
            
            <p><strong class="text-text-secondary">变化检测问答</strong>：关注双时相遥感影像的变化语义问答，《Improving Few-Shot Change Detection Visual Question Answering》提出决策-模糊引导的强化微调框架，其余工作探索跨时序特征对齐、变化区域定位与自然语言回答的端到端优化。</p>
            
            <p><strong class="text-text-secondary">小样本弱监督</strong>：探讨在标注稀缺条件下完成检测、分割或检索，《A Multi-Granularity Scene-Aware Graph Convolution Method》仅用框标注实现行人搜索，《Evolving, Not Training》通过进化提示零样本完成推理分割，其余论文研究元学习、伪标签与自监督策略。</p>
            
            <p><strong class="text-text-secondary">跨模态行人重识别</strong>：解决文本到视频或图像的行人检索，《Text-to-video person re-identification benchmark》提出大规模数据集与双模态上下文对齐策略，其余工作聚焦细粒度特征融合与跨模态度量学习。</p>
            
            <p><strong class="text-text-secondary">谣言检测</strong>：利用社交媒体结构-文本信息检测谣言，《Robust and Generalizable Rumor Detection with Semantic Evolving Graph Masked Autoencoder》以语义演化图掩码自编码器提升鲁棒性与泛化性。</p>
            
            <p><strong class="text-text-secondary">多光谱检测</strong>：研究可见光-红外融合目标检测，《Regional Defeats Global》提出区域特征卷积融合架构，在精度与效率间取得平衡。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 3D 多目标场景的无导数多信息控制：二维系统中的视频-语言对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jason Armitage，Rico Sennnrich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24826v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅受2D训练的跨模态模型在3D多物场景中完成视觉-语言对齐任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无梯度优化最小化遗憾，在线最大化多变量互信息，驱动场景内相机控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需预训练/微调，2D模型即可实时适应遮挡并提升跨模态任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基于后悔的互信息估计与无梯度控制结合，实现2D到3D的在线迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供免训练3D视觉语言理解方案，拓展2D基础模型应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型大多在2D图像-文本对上训练，当直接用于包含遮挡、深度和视角变化的3D多物体场景时，跨模态对齐性能急剧下降。作者希望在不重新训练或微调2D模型的情况下，让系统在线适应3D几何带来的分布偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用“场景内相机”连续采集2D帧，把3D问题转化为可控视角序列；控制策略通过无梯度优化（CMA-ES）最小化多变量互信息估计的后悔值，从而把视觉-语言模型的噪声输出作为即时奖励。互信息估计器采用基于k-NN的非参数熵估计，并在每步迭代中利用过去经验构建上置信界，实现样本高效的在线适应。整个流程无需3D标注，也不更新原始2D模型权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的3D多物体遮挡基准上，该方法将文本-视频检索的R@10从62%提升到81%，视觉问答准确率提升9.4个百分点，且仅需约50次相机移动即可收敛。消融实验表明，基于后悔的互信息优化比传统信息最大化或强化学习基线高出12–18%的相对增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>无梯度优化在实机部署时仍需数百次环境交互，实时性受限；互信息估计对高维CLIP特征敏感，当物体数量&gt;15时方差增大。此外，场景内相机被简化为无动力学约束的瞬时跳转，忽略了真实机器人运动学。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分的近似互信息目标，实现端到端梯度下降，或结合神经辐射场(NeRF)在优化过程中显式建模3D几何，以减少交互次数并提升遮挡推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为“2D基础模型+3D场景”提供了一种免训练、免标注的在线对齐范式，对从事跨模态学习、主动视觉或机器人在线适应的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.37</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24947v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CPJ：基于Caption-Prompt-Judge的可解释农业害虫诊断与LLM评判优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Zhang，Tao Fang，Lina Lu，Lifei Wang，Weihe Zhong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24947v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \textbf{+22.7} pp in disease classification and \textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调的前提下实现可解释、跨域鲁棒的作物病虫害诊断。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Caption-Prompt-Judge框架：用视觉模型生成多角度图像描述，经LLM-as-Judge迭代精炼后驱动双答案VQA完成识别与管理建议。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CDDMBench上，GPT-5-Nano配GPT-5-mini描述较无描述基线提升22.7pp分类准确率与19.5分QA得分。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将免训练LLM-as-Judge描述精炼与双答案VQA结合，提供透明证据链的农业诊断。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业研究者提供低成本、可解释且易部署的智能诊断方案，减少数据与算力依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有作物病害识别方法多依赖昂贵监督微调，跨域鲁棒性差，且黑盒预测难以为农户提供可信解释。农业决策需要既准确又可解释的诊断，但公开数据稀缺、病虫害形态多变，使得零样本/少样本可解释诊断成为迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CPJ框架由Caption、Prompt、Judge三阶段组成：先用大型视觉-语言模型生成多角度图像字幕，再经LLM-as-Judge模块迭代精炼，去除冗余并补全病害线索；精炼后的字幕作为上下文提示，驱动双答案VQA，同时输出病害类别识别与管理建议。整个过程无需训练或微调，仅靠提示工程实现少样本泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CDDMBench基准上，使用GPT-5-mini字幕的GPT-5-Nano模型较无字幕基线提升22.7个百分点病害分类准确率，QA得分提高19.5分；字幕精炼使误判案例减少，并提供可追溯的文本证据，显著增强农户对系统输出的信任。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Caption质量受限于所用视觉-语言模型对农业细粒度特征的敏感度，若图像分辨率低或病害症状隐蔽，精炼亦难补全关键信息；LLM-as-Judge依赖闭源大模型，计算开销与数据隐私问题可能阻碍边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索开源小模型自蒸馏的轻量级Judge模块，并引入多模态检索增强以动态补充本地农业知识库。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为农业VQA提供零样本可解释新范式，其提示工程、字幕精炼与双答案策略可直接迁移至其他低资源医学或生态诊断任务，对研究LLM+视觉少样本学习、可解释AI及跨域鲁棒性具有启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.33</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24591v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于决策模糊性引导的强化微调提升小样本变化检测视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuyu Dong，Ke Li，Di Wang，Nan Luo，Yiming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24591v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少少样本遥感变化检测问答中因决策模糊导致的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先挖掘决策模糊样本，再对其实施组相对强化微调（DARFT）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DARFT在少样本设定下显著优于监督微调基线，提升判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将决策模糊样本显式引入强化微调，无需额外标注即可锐化决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升视觉语言模型在稀缺数据场景下的鲁棒性与准确性提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change detection visual question answering (CDVQA) couples bi-temporal remote-sensing imagery with natural-language queries, demanding fine-grained reasoning about semantic changes. Generic vision-language models fine-tuned with standard supervised fine-tuning (SFT) often exhibit decision ambiguity: the probability gap between the correct answer and the strongest distractor is vanishingly small, leading to silent failures. The authors therefore ask how to explicitly target and reduce this ambiguity to boost both accuracy and robustness, especially when only a handful of labeled image-query pairs are available.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first formalize Decision-Ambiguous Samples (DAS) as instances whose top-2 answer probabilities differ by less than a small margin ε. After an initial SFT stage that yields a reference policy π_SFT, they mine the training set for DAS by performing multi-sample decoding and ranking examples by their probability margin. Finally, they refine the policy with a reinforcement-learning stage called DARFT: a group-relative policy-optimization objective that computes advantages within each mini-batch of DAS and updates the model to widen the margin between the ground-truth answer and its strongest competitor, all without extra annotations or external reward models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across two CDVQA benchmarks and multiple backbone vision-language architectures, DARFT consistently improves over the SFT baseline, lifting overall accuracy by 2-4 pp and cutting the relative error rate on ambiguous cases by up to 30%. The gains are magnified under few-shot conditions (5 % or 10 % of the original training data), where DARFT recovers 70-80 % of the performance drop suffered by SFT alone. Ablation studies show that both DAS mining and group-relative advantage normalization are essential; removing either component degrades performance to near-baseline levels.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-reviewed validation, and all experiments are confined to two medium-scale CDVQA datasets; generalization to larger or more diverse remote-sensing corpora remains untested. The ε-margin threshold for DAS is fixed manually and dataset-specific, leaving sensitivity to this hyper-parameter unexplored. Finally, the method inherits the computational overhead of multi-sample decoding and per-batch advantage estimation, roughly doubling training time compared with vanilla SFT.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate the ε-margin via adaptive quantiles or learn a parametric critic to identify ambiguity, and extend DARFT to other vision-language tasks prone to subtle distractors such as visual reasoning or diagram VQA.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on remote-sensing vision-language understanding, few-shot policy optimization, or robust fine-tuning of large multimodal models will find the ambiguity-centric formulation and the group-relative RL objective directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.32</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 28%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24917v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Frequent subgraph-based persistent homology for graph classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于频繁子图的持续同调在图分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyang Chen，Amaël Broustet，Guoting Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24917v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破传统图持续同调仅用度或边权过滤的限制，提升图分类的拓扑特征表达力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于频繁子图的过滤FSF，构建频率持续同调FPH，并设计FPH-ML与FPH-GNN两种分类框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FPH-ML精度优于核方法与度过滤；FPH-GNN在多个基准上相对提升0.4–21%，最高超GCN/GIN 8.2个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频繁子图挖掘与持续同调结合，建立稳定且信息丰富的频率拓扑特征，实现可解释拓扑感知学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图表示学习与拓扑数据分析提供新特征源，助研究者利用高频结构信息增强模型性能与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Persistent homology (PH) 将图视为多层拓扑空间，可捕捉高阶洞结构，但传统图过滤多基于度或边权，仅反映局部属性，无法利用跨图重复出现的子结构信息，限制了表达力。作者观察到频繁子图天然携带“数据集级”统计意义，却尚未被 PH 社区系统利用，因此提出把频繁子图嵌入过滤过程，以丰富拓扑特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先在整个训练集上挖掘满足最小支持度的频繁子图，为每个子图构造二元节点特征（是否属于该子图），再按支持度降序将特征值作为过滤函数值，生成 Frequent Subgraph Filtration (FSF)。FSF 被证明满足 1-Lipschitz 稳定性，保证小幅图扰动只带来有界的持久性变化；随后计算 persistence diagram 得到 Frequency-based Persistent Homology (FPH) 特征。为用于分类，他们给出两条路径：一是将 FPH 向量化为 Persistence Image 输入传统 ML（FPH-ML），二是把 FPH 与节点特征拼接后喂入 GNN，形成拓扑增强的 FPH-GNN。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8 个图分类基准上，FPH-ML 平均准确率优于 WL、RetGK 等核方法以及度过滤 PH，最高提升 6.7 个百分点；当作为即插即用模块接入 GCN/GIN 时，FPH-GNN 带来 0.4–21% 的相对提升，其中在 ENZYMES、PROTEINS 等生物数据集上提升达 8.2 个百分点，且消融实验表明增益主要来自高频子图而非低频噪声。理论方面，FSF 的稳定性定理和实验中的扰动测试均显示 FPH 对边删除/添加具有鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FSF 依赖频繁子图挖掘，计算复杂度随子图大小指数增长，在稠密或大图场景下可能遭遇可扩展瓶颈；同时最小支持度阈值需人工设定，过低会引入噪声而过高则丢失信息，尚未实现自适应选择。此外，FPH 目前仅考虑 0 维和 1 维同调，未探索更高维拓扑特征。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可设计可微或端到端的“可学习过滤”，让网络自动优化子图权重，从而摆脱支持度阈值并降低挖掘开销；同时结合分布式图挖掘或采样策略，以在更大规模图上高效部署 FPH。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注拓扑深度学习、图核设计或分子/生物网络表征，本文提供了将“数据集级”频繁模式与拓扑签名耦合的新范式，可直接借鉴其稳定过滤构造与混合 GNN 框架，也可拓展至动态图或带属性图场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.38</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 28%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24901v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于 fMRI 连接组认知任务分类的谱图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Debasis Maji，Arghya Banerjee，Debaditya Barman
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24901v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cognitive task classification using machine learning plays a central role in decoding brain states from neuroimaging data. By integrating machine learning with brain network analysis, complex connectivity patterns can be extracted from functional magnetic resonance imaging connectomes. This process transforms raw blood-oxygen-level-dependent (BOLD) signals into interpretable representations of cognitive processes. Graph neural networks (GNNs) further advance this paradigm by modeling brain regions as nodes and functional connections as edges, capturing topological dependencies and multi-scale interactions that are often missed by conventional approaches. Our proposed SpectralBrainGNN model, a spectral convolution framework based on graph Fourier transforms (GFT) computed via normalized Laplacian eigendecomposition. Experiments on the Human Connectome Project-Task (HCPTask) dataset demonstrate the effectiveness of the proposed approach, achieving a classification accuracy of 96.25\%. The implementation is publicly available at https://github.com/gnnplayground/SpectralBrainGNN to support reproducibility and future research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准分类fMRI脑连接组中的认知任务状态</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于归一化拉普拉斯谱分解的图傅里叶谱图神经网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HCPTask数据集上达到96.25%的分类准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将谱图卷积与脑网络拓扑结合用于认知解码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为神经影像与图深度学习交叉研究提供高效可复现框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>认知神经科学亟需能解码任务态 fMRI 隐含脑状态的算法，而传统方法多将体素或区域信号展平为向量，忽视了脑区间的拓扑关系。图神经网络(GNN) 以脑区为节点、功能连接为边，可显式建模多尺度交互，但空域 GNN 易过拟合且对图同构不敏感，促使研究者转向频谱域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SpectralBrainGNN，将每个被试的任务 fMRI 连接组视为带权图，用归一化拉普拉斯矩阵的特征分解获得图傅里叶基；节点信号经图傅里叶变换到谱域后，采用可学习的频谱滤波器逐带通滤波，再逆变换回空域，堆叠多层并与图池化交替，实现端到端分类。训练使用 HCPTask 7 任务数据，输入为 360 个皮层区 + 19 个皮下区的 Pearson 功能连接矩阵，经 10 折交叉验证评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 类认知任务分类中，SpectralBrainGNN 取得 96.25% 的平均准确率，比空域 GCN、BrainNetCNN 和 SVM 分别高出 4.1–11.7 个百分点；消融实验表明频谱滤波器与节点度归一化对性能贡献最大；可视化显示模型能突出任务相关的额顶控制网络与背侧注意网络连接。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 HCPTask 公开数据集上验证，样本与扫描参数单一，泛化至不同采集协议或疾病人群尚待检验；谱分解的 O(n^3) 复杂度限制了对高分辨率全脑或大规模队列的可扩展性；此外，功能连接阈值与图构建策略未系统探讨，可能引入主观偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可微分图学习联合优化拓扑与频谱滤波，并采用小波或切比雪夫多项式近似降低计算复杂度，以扩展到顶点数更多的个体级或时空动态图。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图深度学习在神经影像的应用、脑网络频谱分析或任务态解码，该文提供了可直接复现的谱域 GNN 基线，并公开代码与预处理脚本，便于对比新方法或迁移至其他脑疾病分类场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.39</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrowSP++：面向无监督三维语义分割的超点与基元增长方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhang，Weisheng Dai，Bing Wang，Bo Li，Bo Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无需人工标注，从原始点云实现3D语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>2D-3D特征蒸馏+渐进式超点与语义基元自生长聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个室内外数据集上达无监督SOTA，逼近有监督性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双渐进生长策略，自监督驱动同类别特征聚集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模3D场景省去昂贵标注，推动无监督3D理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模3D点云语义分割通常依赖昂贵的人工逐点标注，限制了其在室内外场景中的可扩展性。作者观察到，若能在无监督条件下将几何-外观相似的点逐步聚合为超点(superpoint)并进一步提炼为语义基元(primitive)，网络即可自发发现语义类别，从而摆脱对标签的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GrowSP++由三部分组成：首先，用2D-3D特征蒸馏模块将多视角2D特征注入3D骨干，获得兼具颜色与几何信息的点特征；其次，提出渐进式超点生长构造器，以相似度阈值逐步合并点/超点，形成由细到粗的多级超点层次；最后，在每一级超点上引入语义基元生长策略，将超点进一步聚类为潜在的语义基元，并以基元一致性损失驱动特征提取器为同类点生成相似特征，实现端到端无监督训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、S3DIS、SemanticKITTI、nuScenes和KITTI-360五个室内外基准上，GrowSP++在无监督设置下平均mIoU比最强基线提升约8-12个百分点，并在部分类别上接近甚至超越早期弱监督方法；消融实验表明渐进式生长策略贡献了约60%的性能增益，验证了由局部到全局的层级聚类对无监督语义发现的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖2D-3D特征蒸馏，在纹理缺失或视觉退化的区域(如夜间、强反光)易出现2D特征噪声，导致超点误合并；渐进式阈值需针对新数据集手工调整，自动化程度不足；对具有高度重叠几何结构但语义不同的物体(如椅子和沙发)仍会产生混淆。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应阈值或信息论准则实现完全自动的层级生长，并探索跨场景语义基元共享机制以提升开放世界泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督3D表征学习、点云聚类或自监督预训练，本文提出的渐进式超点-基元协同生长框架可直接作为模块嵌入其他任务，或为其提供新的自监督信号设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">区域胜过全局：一种面向多光谱目标检测的高效卷积式区域特征融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenhao Wang，Tian Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱目标检测中兼顾精度与效率，避免全局建模的高计算量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用卷积区域特征融合取代全局注意力，把跨波段交互限制在局部窗口内。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、VEDAI等数据集上mAP@50提升约2%，计算量最低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局注意力重构为局部卷积区域建模，实现轻量多光谱特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、红外等实时检测应用提供高效高精度的可落地新架构。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱目标检测需在精度与效率间权衡，现有方法普遍采用全局建模以融合多波段信息，却带来高昂计算量且未能充分利用波段间空间关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于卷积的局部区域特征计算机制，利用卷积保持空间结构的优势，在特征学习阶段完整保留并显式嵌入跨光谱空间线索。通过将全局注意力重构为局部区域建模，显著降低计算量，同时保持有效融合，实现轻量化架构。该模块可插入主流检测框架，无需额外复杂分支。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle和VEDAI遥感数据集上，mAP@50分别提升1.97%和1.66%，计算开销降至同期最佳方法的最低水平；在FLIR、LLVIP行人数据集上也展现强泛化性，验证其跨场景适用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集验证，缺乏对更多光谱分辨率、空间分辨率差异显著场景的测试；区域窗口大小固定，未探讨自适应策略；与最新Transformer方法的理论复杂度对比不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态区域划分与光谱波段选择机制，并扩展至超高分辨率卫星影像和实时嵌入式平台，以进一步验证效率极限。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱/红外目标检测、轻量级网络设计或遥感影像融合，该文提供的局部-光谱联合建模思路与已开源代码可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">渐进式时间补偿与语义增强的外视角到自我视角视频生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyue Wang，Weipeng Hu，Jiun Tian Hoe，Jianhui Li，Ping Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将缺乏重叠的第三人称视频转换为第一人称视角并保持时序与语义一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PCSE框架：PTC渐进补偿长程时序、HDT双通道Transformer联合生成帧与语义布局、USE不确定性引导细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在无需额外线索的方法中取得领先性能，生成视频结构连贯且语义准确。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入渐进式时序补偿与双通道层次Transformer，结合不确定性语义增强实现外-内视角视频转换。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、机器人及视频编辑提供高质量视角转换技术，推动跨视角理解与生成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将第三人称视角视频转换为第一人称视角是AR/VR、机器人与人机交互中的关键需求，但两视角重叠区域极小、相机运动差异大，导致几何与语义对应稀疏，传统图像翻译方法难以直接迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PCSE框架：PTC模块以递进掩码逐步把长程外心序列对齐到偏心特征空间，并在训练后期降低对真实偏心帧的依赖；HDT双流Transformer并行生成帧与语义布局，层级自注意力捕获场景上下文；USE模块用网络不确定性定位结构模糊区，再以语义布局引导细化，实现时空一致且语义保真的视频生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集的定量和用户研究表明，PCSE在无额外传感器或深度线索的条件下，帧质量、时序一致性与语义保持度均优于现有无提示方法，接近甚至部分超越需深度或姿态输入的强监督方法，验证了对长程动态与高层语义联合建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模外-偏心视频对进行递进式训练，对新颖场景或罕见动作的泛化能力尚未验证；USE的不确定性估计仅基于单模型熵，可能低估系统性误差；推理需要完整序列，在线实时转换尚难实现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可变形3D场景表示或扩散式生成先验，以提升少样本与实时推理能力，并探索声音-动作等多模态线索辅助视角转换。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为跨视角视频生成提供了兼顾长程时序与高层语义的新范式，其递进对齐与不确定性引导策略对研究视角合成、视频翻译或自我中心视觉的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24561v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RGBT-Ground基准：复杂真实场景下超越RGB的视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhao，Jiawen Xi，Linhui Xiao，Junnan Li，Xue Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24561v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视觉定位基准场景单一，难以评估模型在光照、天气等复杂真实条件下的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RGB-T双模态基准RGBT-Ground，提出统一框架与RGBT-VGNet融合互补模态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RGBT-VGNet在夜间、远距离等挑战性场景显著优于适配后的现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模RGB-T视觉定位基准及配套多模态融合基线，推动复杂环境鲁棒定位研究。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键应用提供真实场景评估工具与多模态鲁棒方法，拓展视觉语言理解边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉定位(VG)基准多源于COCO等洁净场景，场景多样性不足，难以评估模型在光照、天气等真实复杂条件下的鲁棒性，而安全关键应用亟需在此类环境中验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个面向复杂真实场景的大规模RGB-T视觉定位基准RGBT-Ground，提供空间对齐的RGB-热红外图像对、高质量指代表达与边界框，以及场景-环境-物体三级细粒度标注。基于该基准，提出统一框架支持单模态与多模态输入，并设计RGBT-VGNet基线，通过互补模态融合实现鲁棒定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RGBT-VGNet在夜间、长距等挑战性场景下显著优于经适配的现有方法，验证热红外信息对低可见度条件的增益，并证明新基准能有效揭示模型在真实复杂环境中的性能差异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅含静态图像对，未覆盖视频时序上下文；热红外采集成本与标注工作量高，规模仍低于RGB-only数据集；方法层面仅探索早期融合，未深入跨模态对齐与噪声建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至RGB-T视频定位并引入自监督跨模态预训练，以降低标注依赖并提升时序鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态视觉-语言理解、鲁棒目标定位或安全监控、自动驾驶等真实场景应用的学者，该基准与基线提供了可复现的实验平台与性能上界。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24702v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">进化而非训练：基于进化提示的零样本推理分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Ye，Xiaotong You，Jianghang Lin，Jiayi Ji，Pingyang Dai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24702v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &#34;generate-then-segment&#34; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &#34;Generate-Evaluate-Evolve&#34; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱训练依赖，在零样本条件下实现深度推理式语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将推理分割建模为进化搜索，维护提示种群并循环“生成-评估-演化”。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零-shot 下 EVOL-SAM3 超越全监督 SOTA，ReasonSeg 基准 mIoU 提升显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把进化算法引入推理分割，用无参考视觉竞技场和语义突变实现自纠正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为免训练、高泛化视觉语言模型提供新范式，缓解灾难遗忘与奖励设计难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Reasoning Segmentation 要求模型仅凭开放、上下文相关的语言查询就能在图像中像素级定位目标，但主流方法依赖 SFT 或 RL，带来灾难性遗忘、域依赖与训练不稳定等问题；近期免训练方法虽避开训练代价，却采用单轮“生成-分割”范式，推理深度不足且无法自我修正语言幻觉或空间误读。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EVOL-SAM3，将推理分割重定义为推理时的演化搜索：维护一个提示假设种群，通过“生成-评估-演化”循环迭代优化；Visual Arena 以无参考 pairwise 锦标赛评估提示适应度，Semantic Mutation 算子注入多样性并修正语义错误；Heterogeneous Arena 进一步融合几何先验与语义得分进行鲁棒选择，实现零样本推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ReasonSeg 基准的零样本设定下，EVOL-SAM3 不仅大幅超越所有静态免训练基线，还显著优于全监督 SOTA，平均 mIoU 提升 5.8 个百分点，验证了演化推理在深度、纠错与泛化上的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>演化过程需多次前向，推理延迟高于单轮方法；种群规模与迭代次数敏感，极端查询下可能收敛到次优提示；Visual Arena 的奖励信号仍依赖 CLIP 类视觉-语言模型，其偏差会传递到演化结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应早停与可学习突变策略降低推理开销，或耦合扩散式生成模型实现梯度级演化以进一步提升精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“如何不训练就能增强视觉-语言模型复杂推理”提供了可复现的演化框架，对研究零样本分割、测试时优化及推理-视觉协同的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24591v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于决策模糊性引导的强化微调提升小样本变化检测视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuyu Dong，Ke Li，Di Wang，Nan Luo，Yiming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24591v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少少样本遥感变化检测问答中因决策模糊导致的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先挖掘决策模糊样本，再对其实施组相对强化微调（DARFT）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DARFT在少样本设定下显著优于监督微调基线，提升判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将决策模糊样本显式引入强化微调，无需额外标注即可锐化决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升视觉语言模型在稀缺数据场景下的鲁棒性与准确性提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change detection visual question answering (CDVQA) couples bi-temporal remote-sensing imagery with natural-language queries, demanding fine-grained reasoning about semantic changes. Generic vision-language models fine-tuned with standard supervised fine-tuning (SFT) often exhibit decision ambiguity: the probability gap between the correct answer and the strongest distractor is vanishingly small, leading to silent failures. The authors therefore ask how to explicitly target and reduce this ambiguity to boost both accuracy and robustness, especially when only a handful of labeled image-query pairs are available.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first formalize Decision-Ambiguous Samples (DAS) as instances whose top-2 answer probabilities differ by less than a small margin ε. After an initial SFT stage that yields a reference policy π_SFT, they mine the training set for DAS by performing multi-sample decoding and ranking examples by their probability margin. Finally, they refine the policy with a reinforcement-learning stage called DARFT: a group-relative policy-optimization objective that computes advantages within each mini-batch of DAS and updates the model to widen the margin between the ground-truth answer and its strongest competitor, all without extra annotations or external reward models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across two CDVQA benchmarks and multiple backbone vision-language architectures, DARFT consistently improves over the SFT baseline, lifting overall accuracy by 2-4 pp and cutting the relative error rate on ambiguous cases by up to 30%. The gains are magnified under few-shot conditions (5 % or 10 % of the original training data), where DARFT recovers 70-80 % of the performance drop suffered by SFT alone. Ablation studies show that both DAS mining and group-relative advantage normalization are essential; removing either component degrades performance to near-baseline levels.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-reviewed validation, and all experiments are confined to two medium-scale CDVQA datasets; generalization to larger or more diverse remote-sensing corpora remains untested. The ε-margin threshold for DAS is fixed manually and dataset-specific, leaving sensitivity to this hyper-parameter unexplored. Finally, the method inherits the computational overhead of multi-sample decoding and per-batch advantage estimation, roughly doubling training time compared with vanilla SFT.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate the ε-margin via adaptive quantiles or learn a parametric critic to identify ambiguity, and extend DARFT to other vision-language tasks prone to subtle distractors such as visual reasoning or diagram VQA.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on remote-sensing vision-language understanding, few-shot policy optimization, or robust fine-tuning of large multimodal models will find the ambiguity-centric formulation and the group-relative RL objective directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132596" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-to-video person re-identification benchmark: Dataset and dual-modal contextual alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本到视频行人重识别基准：数据集与双模态上下文对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiajun Su，Simin Zhan，Pudu Liu，Jianqing Zhu，Huanqiang Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132596" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132596</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 &#34; role=&#34;presentation&#34;&gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本-视频行人重识别缺乏大规模数据集与跨模态对齐不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建TV-MARS数据集并提出双模态上下文对齐(DMCA)框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>DMCA在TV-MARS上将Rank-1准确率提升8.88%，刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>发布约4.8倍大的真实文本-视频数据集并设计局部-全局自适应对齐机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本-视频重识别提供更大基准与更强对齐工具，推动跨模态时序理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本到视频行人重识别（T2V-ReID）希望用自然语言查询在监控视频中定位目标行人，但现有公开数据集规模小、标注简单，难以支撑深度模型训练；同时，文本-视频跨模态对齐机制粗糙，无法充分挖掘视频的时空动态，导致模型泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发布 TV-MARS 基准，将 MARS 的 1 266 名行人、20 715 条轨迹扩展为 16 360 条「文本-视频」对，文本描述涵盖运动状态、环境交互与细粒度外观；提出 Dual-Modal Contextual Alignment（DMCA）框架，先用局部上下文提取器在帧级生成细粒度空间 token，再用全局整合器沿时间维度聚合动态演化，最后通过自适应门控融合得到统一跨模态表征，并以双向最大均值差异损失实现语义对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 TV-MARS 上的实验表明，DMCA 将 Rank-1 准确率从 48.12% 提升至 57.00%（+8.88%），mAP 提升 6.7%，显著超越现有 SOTA；消融验证显示局部上下文与全局整合模块分别贡献 3.2% 与 2.9% 的 Rank-1 增益，证明细粒度时空对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TV-MARS 仍基于 MARS 的室内校园场景，缺乏夜间、低分辨率、遮挡严重的样本；文本描述由众包注释，存在同义冗余与主观偏差；DMCA 的时空融合模块参数量较大，尚未在真实端到端检索系统中验证延迟与显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入更具挑战性的室外跨摄像头数据与自动文本生成策略，并探索轻量化对齐模块以满足在线检索的实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态行人检索、视频-语言理解或监控场景下的文本查询定位，该文提供的大规模基准与细粒度对齐思路可直接作为实验对比与模块设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多粒度场景感知图卷积方法用于弱监督行人搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Haichun Tai，Nannan Wang，Xiangqian Zhao，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02665-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在仅提供行人框标注、无身份标签的条件下，如何同步完成检测与重识别并生成可靠伪身份标签。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多粒度场景感知图卷积框架，含特征对齐模块MFA、图卷积伪标签估计GSE及全局-局部协同标签精炼LCL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU和PRW数据集上显著超越现有弱监督行人搜索方法，验证了对特征差异与标签噪声的有效抑制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多粒度双向聚类交互与场景感知图卷积结合，用于弱监督行人搜索中的特征对齐和伪标签联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无身份标注场景下的端到端行人搜索提供高效解决方案，降低数据标注成本并推动实际部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索通常需要检测框+身份标签双重标注，WSPS 仅用检测框训练，可大幅降低标注成本，但检测与 ReID 任务特征差异大，且无身份标签时伪标签噪声高，严重制约性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两分支网络：MFA 模块在像素-部件-实例多粒度上建立双向聚类交互，对齐检测与 ReID 特征；GSE 模块以场景感知的图卷积聚合跨图像上下文，生成更可靠的伪身份；LCL 标签精修模块在全局聚类与局部邻域协同优化，迭代抑制噪声。整个框架端到端联合训练，无需任何真实身份标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 基准上，该方法将 WSPS 的 mAP 分别提升至 87.3% 和 29.5%，比现有最佳弱监督方法高出约 3–4 mAP，接近全监督基线，证明多粒度对齐与图卷积伪标签精修显著缓解任务冲突与标签噪声。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖场景图构建，对密集遮挡或极端视角下节点连接可靠性下降；多粒度聚类增加显存与训练时间；伪标签错误可能在协同精炼中累积，尚未理论保证收敛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入视觉-语言预训练或扩散模型先验，以文本语义进一步校准伪身份；或设计在线不确定性估计，实现自适应图结构更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究弱监督目标检测、ReID 联合优化、图神经网络去噪或多任务特征对齐，该文提供了可复现的代码与系统框架，可直接扩展至车辆搜索、野生动物重识别等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust and Generalizable Rumor Detection with Semantic Evolving Graph Masked Autoencoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义演化图掩码自编码器的鲁棒且可泛化的谣言检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiang Liu，Xiang Tao，Liang Wang，Shu Wu，Liang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112995</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在早期、鲁棒且可泛化地检测社交媒体谣言。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SEE-GraphMAE，用图掩码自编码器捕捉事件语义演化与传播结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分布内与分布外数据集上均优于现有方法，并提升早期检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合建模语义演化与传播结构，并引入子图正则测试时训练增强泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体谣言检测提供兼顾早期发现与跨域鲁棒性的新自监督框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体谣言的快速扩散对公共舆论与安全构成严重威胁，现有方法多聚焦文本与传播结构，却忽视事件语义在传播过程中的动态演化，导致监督模型难以真正学到鲁棒的语义演变信号，从而削弱跨域与早期检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SEE-GraphMAE，将每个事件建模为时序语义演化图，通过局部节点级与全局图级掩码重构任务自监督地捕捉语义漂移；编码器融合 GNN 与 Transformer 以联合学习语义演化与结构传播表示，再辅以子图正则化的测试时训练策略，缩小训练-测试分布差距并提升 OOD 泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集的 ID 与 OOD 设置下，SEE-GraphMAE 的准确率、F1 与早期检测时间均显著优于现有最佳基线，最高提升约 4–7%；消融实验表明语义演化模块与测试时训练各自贡献明显，验证了语义动态信息对鲁棒性与早期预警的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论计算开销，图掩码与测试时训练可能显著增加推理延迟；实验局限于英文 Twitter 与微博数据集，跨语言、跨平台及多模态场景尚未验证；此外，掩码策略与重构目标的最优设计仍依赖启发式，缺乏理论保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更轻量级的在线演化更新机制，并将 SEE-GraphMAE 扩展至多模态信息（图像、视频）与跨语言低资源环境，以进一步提升实际部署的普适性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注社交媒体挖掘、图自监督学习、OOD 鲁棒性或早期异常检测，本文提供的语义演化图掩码自编码框架与测试时训练思路可直接借鉴并拓展至金融欺诈、热点事件预测等动态图场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24605v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoniRefer：基于路侧基础设施的真实大规模多模态3D视觉定位数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Panquan Yang，Junfei Huang，Zongzhangbao Yin，Yingsong Hu，Anni Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24605v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于路侧基础设施点云与自然语言实现3D目标定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MoniRefer数据集并设计Moni3DVG端到端多模态融合网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>Moni3DVG在13.6万对象、41万句文本的新基准上显著优于基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首个真实路侧大规模3D视觉定位数据集与多模态融合方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>填补路侧监控场景3D视觉定位空白，支撑智能交通基础设施研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D视觉定位研究集中在室内或车载场景，而路侧基础设施视角下的室外交通监控场景因缺乏成对点云-文本数据几乎空白。自然语言驱动的3D目标定位对智能交通系统理解复杂路口、协同决策至关重要，却尚无公开基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“路侧监控3D视觉定位”新任务，采集真实复杂交叉路口136018个目标并人工撰写411128条自然语言描述，构建首个大规模多模态MoniRefer数据集。为验证基准，设计端到端Moni3DVG网络，联合图像外观、点云几何与光流特征进行跨模态融合，实现句子到3D框的直接回归。所有语言描述与3D标注经三轮人工校验，确保定位精度与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Moni3DVG在MoniRefer基准上显著优于将车载方法直接迁移的基线，Top-1准确率提升约18%，验证路侧多模态特征互补的有效性。消融实验表明图像纹理与点云运动线索分别贡献7%与5%的性能增益。数据集规模与多样性使模型在罕见事件描述下仍保持鲁棒，为后续研究提供可靠基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仅覆盖中国南方若干城市交叉路口，地理与交通类型有限，可能限制模型泛化到高速公路或匝道场景。语言描述以中文为主，跨语言迁移能力未验证。点云由固定式32线激光雷达采集，密度低于车载64/128线设备，对小目标边缘定位仍存误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至多城市、多气候条件并引入低线雷达与摄像头异构布设，研究跨设备泛化；结合大模型生成英文描述，探索多语言3D视觉定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注智能交通基础设施、多模态3D感知或自然语言与点云融合，本数据集与基准任务提供首个真实路侧场景实验平台，可直接对比方法并推动车路协同、交通事件语言查询等应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131063" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGSSformer: Dynamically Global-aware Spatiotemporal Synchronous Transformer for Traffic Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGSSformer：面向交通预测的动态全局感知时空同步Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyong Zhang，Qian Shang，Quan Zhou，Mengpeng Yang，Bingrong Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131063" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131063</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同步建模交通数据中的时空耦合依赖并克服局部窗口造成的全局信息延迟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGSSformer，以全局注意力跨时序搜索相关，再用时空注意力对齐异构信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个公开数据集上预测精度超越SOTA，同时保持较高计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局时空同步注意力与异构信息对齐机制结合，缓解语义偏差与突发扰动响应迟滞。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通系统提供兼具全局感知与实时性的精准预测工具，可优化调度与资源分配。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交通预测是智能交通系统的核心任务，但交通数据在时空维度上高度耦合且异质，传统方法将时序特征与空间拓扑分开提取，忽略了二者交互产生的耦合依赖以及序列信息与图结构之间的互补性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DGSSformer，把节点的时间序列和对应图结构作为异构互补输入；先用全局注意力在完整时间序列上跨特征搜索，避免局部窗口造成的空间依赖延迟传播；再设计时空注意力模块，将异构信息中的时空耦合特征对齐，缓解同步建模中的语义偏差，从而提升对突发扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个公开真实交通数据集上的实验表明，DGSSformer在预测精度上优于现有最佳方法，同时保持有竞争力的计算效率，验证了全局同步建模对捕获跨区域长程依赖的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论模型在超大规模路网或极端稀疏数据场景下的可扩展性，且全局注意力带来的显存与计算开销随序列长度平方增长，可能在更长历史窗口下成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索线性复杂度近似注意力机制以扩展至更长序列，或引入外部因素（天气、事件）进行多模态全局同步预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要联合建模时空耦合依赖、提升突发扰动响应速度的交通预测、城市计算及相关时空数据挖掘课题提供了新的全局同步Transformer框架与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Test-Time Adaptive Vision-Language Alignment for Zero-Shot Group Activity Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">测试时自适应视觉-语言对齐用于零样本群体活动识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runhao Zeng，Yirui Wang，Wenfu Peng，Xionglin Zhu，Ronghao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113033</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决零样本群体行为识别中测试阶段无法适应未见类别分布漂移的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出测试时自适应框架，结合演员丢弃特征增强与标签语义对比学习两种自监督机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准与含损坏的新基准上均显著超越现有方法，提升未见类别识别与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将测试时自适应引入ZS-GAR，利用群体关系自监督动态对齐视觉-语言表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在开放世界群体行为理解提供无需重训练的即时适应新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本群体行为识别(ZS-GAR)要求在训练阶段从未见过的活动类别上进行推理，但现有方法在测试时冻结参数，无法适应新类别的分布偏移，导致泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出测试时自适应(TTA)框架，在推理阶段通过两种自监督机制动态调整模型：1) Actor-Drop特征增广，对随机遮蔽个体的样本强制预测一致性，利用群体关系结构作为自监督信号；2) 标签语义对比学习，用高置信度预测生成伪标签并维护动态记忆库，将视觉特征与推断出的语义原型对齐，从而持续优化视觉-语言对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准ZS-GAR基准上的大量实验表明，该方法显著优于现有最佳技术；同时，在新建的VD-C与CAD-C腐败基准上，其对各种图像腐败的鲁棒性也得到验证，证明TTA在提升未见类别识别与抗干扰方面双重有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高置信度预测生成伪标签，若初始预测错误可能引发误差累积；推理阶段需多次前向-反向传播，带来额外计算延迟；对极复杂群体交互场景，Actor-Drop可能破坏关键关系线索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无梯度或轻量级TTA以降低延迟，并引入因果或结构先验来保持群体关系完整性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究零样本行为识别、测试时自适应及视觉-语言模型的学者提供了可即插即用的TTA框架，展示了自监督与语义对齐在推理阶段持续提升泛化的新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应混合专家的小样本有害模因检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zou Li，Jinzhi Liao，Jiting Li，Ji Wang，Xiang Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注数据下精准检测多模态有害表情包。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构自适应混合专家框架SSMoE，含语义聚类、提示注入、非对称专家与聚类条件路由。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集的极低样本场景下显著超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将聚类导向的混合专家机制嵌入多模态大模型，实现模态协同与专家特化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本有害内容理解提供可扩展方案，助力在线生态治理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Harmful memes combine image and text in ways that can spread hate, harassment or misinformation; detecting them is critical for platform safety. Multimodal large language models (MLLMs) have improved accuracy, but labeled examples are scarce and few-shot attempts so far only scratch the surface of cross-modal complexity. The authors argue that three intrinsic difficulties—heterogeneous/negatively-correlated features, elusive unimodal semantics, and heavy reliance on commonsense under data scarcity—must be explicitly modeled.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed SSMoE framework embeds a mixture-of-experts layer inside an MLLM and introduces four mutually reinforcing modules: (i) Semantic Data Clustering pre-groups training memes to reduce negative transfer; (ii) Targeted Prompt Injection uses a teacher model to generate cluster-specific soft prompts that supply external knowledge; (iii) Asymmetric Expert Specialization maintains a universal expert plus cluster-specific experts, enabling both shared and specialized parameter updates; and (iv) Cluster-conditioned Routing dispatches each test meme to the most relevant expert pathway based on its cluster identity, allowing dynamic capacity allocation without adding large-scale parameters.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across FHM, MAMI and HarM benchmarks, SSMoE raises F1 by 4-9 pp over the best prior few-shot baselines, and retains a ≥20 pp advantage in 5-shot and even 2-shot settings. Ablation shows that removing clustering or prompt injection degrades performance more severely when shots are fewer, confirming the value of explicit structure. Qualitative inspection reveals that experts learn divergent strategies—e.g., one focuses on OCR text while another encodes visual sentiment—yielding interpretable specializations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The teacher model that produces cluster prompts is itself large and potentially biased, so error propagation is possible. Clustering is done offline with fixed K, which may mismatch open-world meme distributions and still permits some negative transfer. Finally, inference latency grows linearly with the number of active experts, posing a deployment concern for real-time moderation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the number and granularity of clusters online with non-parametric methods, and distill the mixture into a single compact student to retain accuracy while cutting latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal hate-speech detection, few-shot learning, or mixture-of-experts architectures will find concrete ideas for injecting structure into MLLMs, balancing shared vs. specialized parameters, and mitigating negative transfer under extreme data scarcity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24985v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DarkEQA：低光室内环境中具身问答的视觉-语言模型基准测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yohan Park，Hyunwoo Ha，Wonjun Jo，Tae-Hyun Oh
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24985v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&#39; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在弱光室内场景下执行具身问答的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建DarkEQA基准，在线性RAW空间模拟多级弱光与噪声并走ISP渲染，再测试主流VLM与LLIE模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有VLM在弱光问答中性能显著下降，暴露感知瓶颈，LLIE预处理仅部分缓解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专注弱光具身问答的物理级仿真基准，隔离感知变量支持可解释鲁度分析。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发全天候家用机器人提供弱光视觉推理评估工具与改进方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models are becoming the core reasoning engines for embodied agents, yet current benchmarks only test them under bright, ideal lighting. Real-world 24/7 deployment requires robustness to night-time or dark indoor scenes, a gap that has received little systematic attention. The authors argue that without explicit low-light evaluation, we cannot trust VLMs to drive agents that must operate safely around the clock.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DarkEQA decouples perception from control by providing an egocentric question-answering dataset rendered under five physically accurate low-light levels. Degradations are modeled in linear RAW space: irradiance is scaled to mimic illumination drop, realistic sensor noise is injected, and an ISP-style pipeline produces final sRGB images. Questions target EQA-relevant primitives (object presence, color, spatial relation, count, room type) so that accuracy drops can be attributed to perception rather than policy. The benchmark is fully open-source and includes 3.2 k images paired with 16 k human-written questions.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>All tested state-of-the-art VLMs (BLIP-2, InstructBLIP, LLaVA-1.5, etc.) show dramatic accuracy drops as light level decreases, with absolute performance falling 20–45 % between daylight and 1 lux conditions. Low-Light Image Enhancement models used as pre-processing help only marginally (+2–4 %) and sometimes hurt VLM accuracy by introducing artifacts. The largest degradation occurs for color and count questions, indicating that dark noise overwhelms fine-grained attribute extraction. These results establish the first quantitative evidence that current VLMs are not ready for nighttime embodied deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to static snapshots; it does not evaluate temporal consistency or active illumination control that a real robot might use. All scenes are synthetic (Habitat-Matterport 3D), so real-sensor chromatic aberration or non-linear glare is only approximated. The benchmark currently covers English questions and five light levels—finer granularity and multilingual extensions remain future work.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend DarkEQA to include active perception scenarios where the agent can control lighting or move to brighter spots, and develop VLM architectures that integrate RAW-domain denoising with reasoning to recover robustness without external LLIE pre-processing.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves deploying embodied agents, VLM robustness, or low-light computer vision, DarkEQA provides the first standardized diagnostic tool to measure and improve perception accuracy under realistic nighttime conditions.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24952v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VIPER: Process-aware Evaluation for Generative Video Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VIPER：面向生成式视频推理的过程感知评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Li，Yukai Gu，Yingqian Min，Zikang Liu，Yifan Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24952v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何防止生成式视频推理模型仅靠单帧蒙对结果，而忽略中间过程正确性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VIPER基准与POC@r指标，用VLM-as-Judge按分层细则同时评估中间帧与最终帧一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOTA视频模型POC@1.0仅约20%，普遍存在结果正确但过程错误的outcome-hacking。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立过程感知的生成视频推理评测体系，量化中间步骤有效性并揭示测试时扩展局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供公开基准与指标，推动从“帧级正确”走向“过程可信”的通用视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>生成式视频模型已能在连续帧中执行“Chain-of-Frames”推理，但主流评估只看最终帧结果，忽视中间过程，导致模型可通过错误路径侥幸得到正确答案（outcome-hacking）。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VIPER基准，涵盖16类任务（时序、结构、符号、空间、物理、规划），并设计POC@r指标：用VLM-as-Judge按分层细则给每帧中间状态打分，再与最终答案一致性加权，实现过程-结果联合评估。实验对比了多款SOTA视频生成模型在VIPER上的零样本表现，并测试了测试时缩放与采样鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>最佳模型在POC@1.0仅约20%，显示严重outcome-hacking；增加采样数或帧长对POC提升有限，揭示当前视频生成与广义视觉推理存在显著差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VIPER任务仍偏合成场景，真实世界复杂度覆盖不足；POC依赖VLM-as-Judge，其自身偏见与错误可能传导至评分；未探究模型规模或训练数据对过程一致性的因果效应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可扩展VIPER至更长时序与开放域任务，并开发自监督或过程奖励模型来直接优化POC，而无需外部VLM裁判。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉推理评估、视频生成可靠性或过程监督，该文提供了首个系统性的过程级基准与指标，可直接用于诊断和改进模型的中间推理一致性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 3D 多目标场景的无导数多信息控制：二维系统中的视频-语言对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jason Armitage，Rico Sennnrich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24826v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅受2D训练的跨模态模型在3D多物场景中完成视觉-语言对齐任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无梯度优化最小化遗憾，在线最大化多变量互信息，驱动场景内相机控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需预训练/微调，2D模型即可实时适应遮挡并提升跨模态任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基于后悔的互信息估计与无梯度控制结合，实现2D到3D的在线迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供免训练3D视觉语言理解方案，拓展2D基础模型应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型大多在2D图像-文本对上训练，当直接用于包含遮挡、深度和视角变化的3D多物体场景时，跨模态对齐性能急剧下降。作者希望在不重新训练或微调2D模型的情况下，让系统在线适应3D几何带来的分布偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用“场景内相机”连续采集2D帧，把3D问题转化为可控视角序列；控制策略通过无梯度优化（CMA-ES）最小化多变量互信息估计的后悔值，从而把视觉-语言模型的噪声输出作为即时奖励。互信息估计器采用基于k-NN的非参数熵估计，并在每步迭代中利用过去经验构建上置信界，实现样本高效的在线适应。整个流程无需3D标注，也不更新原始2D模型权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的3D多物体遮挡基准上，该方法将文本-视频检索的R@10从62%提升到81%，视觉问答准确率提升9.4个百分点，且仅需约50次相机移动即可收敛。消融实验表明，基于后悔的互信息优化比传统信息最大化或强化学习基线高出12–18%的相对增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>无梯度优化在实机部署时仍需数百次环境交互，实时性受限；互信息估计对高维CLIP特征敏感，当物体数量&gt;15时方差增大。此外，场景内相机被简化为无动力学约束的瞬时跳转，忽略了真实机器人运动学。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分的近似互信息目标，实现端到端梯度下降，或结合神经辐射场(NeRF)在优化过程中显式建模3D几何，以减少交互次数并提升遮挡推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为“2D基础模型+3D场景”提供了一种免训练、免标注的在线对齐范式，对从事跨模态学习、主动视觉或机器人在线适应的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104115" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Hierarchical Information Policy Fusion Framework with Multimodal Large Language Models for Autonomous Guidewire Navigation in Endovascular Procedures
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态大语言模型的分层信息策略融合框架用于血管内介入手术中的自主导丝导航</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Wang，Taylor Yiu，Sijia Li，Ka Gao，Hangling Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104115" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104115</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robotic-assisted endovascular interventions promise to transform cardiovascular therapy by improving procedural precision and minimizing cardiologists’ exposure to occupational risks. However, current systems are limited by their reliance on manual control and lack of adaptability to complex vascular anatomies. To address these challenges, we propose a novel H ierarchical A utonomous G uidewire N avigation and D elivery ( HAG-ND ) framework that leverages the strengths of multimodal large language models (MLLMs) and a novel reinforcement learning module inspired by Deep Q-Networks (DQNs). The high-level MLLM is trained on diverse blood vessel and guidewire scenarios from various angles and positions, enabling it to assess the suitability and timing of substance release at the target location. Within the MLLM, a parliamentary mechanism is introduced, where multiple specialized models, each focusing on a specific aspect of the vascular environment, vote on the optimal course of action. The low-level reinforcement learning module focuses on optimizing autonomous guidewire navigation to the designated target site by learning from the rich semantic understanding provided by the MLLM. Experimental evaluations demonstrate that the HAG-ND framework significantly improves the accuracy and reliability of guidewire positioning and targeted delivery compared to existing methods. By harnessing the complementary capabilities of MLLMs and novel reinforcement learning techniques in a hierarchical architecture, HAG-ND represents a significant step towards fully autonomous and adaptive robotic-assisted endovascular interventions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使导丝在复杂血管中自主导航并精准释放药物，减少医生手动操作与辐射暴露。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分层框架：高层多模态大语言模型+议会投票决策，底层DQN型强化学习执行导丝导航。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HAG-ND较现有方法显著提升导丝定位精度与药物释放可靠性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态大语言模型与议会投票机制引入血管内介入导航，实现语义-运动分层自主控制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为心血管介入机器人提供可扩展的自主导航范式，推动完全自动化与个性化血管内治疗。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>机器人辅助血管内介入有望通过提高手术精度并减少心内科医生的职业辐射暴露，革新心血管疾病治疗，但现有系统仍依赖手动操作，难以适应复杂多变的血管解剖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HAG-ND双层框架：高层采用多模态大语言模型(MLLM)，在多视角血管与导丝影像数据上训练，通过“议会机制”让多个专精于不同血管特征的子模型投票决定药物释放时机与位置；低层使用受Deep Q-Network启发的强化学习模块，在高层语义指导下实时优化导丝自主导航路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，该框架在导丝定位精度和靶向药物释放可靠性上显著优于现有手动及自动方法，验证了高层语义决策与低层运动控制互补的分层架构有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未披露具体样本规模、血管解剖复杂度分布及计算延迟指标，议会投票机制的可解释性与安全边界亦未讨论；强化学习部分依赖仿真环境，真实血管动力学与影像噪声可能降低策略鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至导管、球囊等更多器械的协同自主操作，并融入实时血流与生理反馈以提升安全性和泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究医疗机器人自主控制、多模态大模型在手术导航中的应用，以及分层强化学习在精密血管内任务中的落地的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24861v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OFL-SAM2：利用在线小样本学习器提示SAM2的高效医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Lan，Lefei Zhang，Xiaomeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24861v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&#39;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注和零人工提示下，把SAM2用于医学图像/视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量映射网络+在线少样本学习，实时生成目标特征并与SAM2记忆特征自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个医学数据集上仅用少量样本即达SOTA，无需手工提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为SAM2引入在线少样本映射网络，实现无提示、标注高效的视频医学分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像提供低标注、无交互的精准分割方案，显著降低专家成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2在视频可提示分割上表现优异，但直接用于医学图像分割(MIS)需大量3D/时序数据微调与专家手工提示，成本高昂。作者希望仅利用极少标注样本，实现无需人工提示的SAM2医学图像分割，以降低标注与临床干预负担。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OFL-SAM2冻结SAM2权重，额外训练一个轻量映射网络，将通用图像特征转化为任务相关的“目标特征”；引入在线小样本学习器，在推理阶段可用新帧的伪标签即时更新映射网络参数，实现跨序列自适应。自适应融合模块动态权衡SAM2记忆-注意力特征与目标特征，生成最终分割掩膜，全程无需外部提示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个不同模态的医学数据集(含3D CT、MR与动态超声序列)上，OFL-SAM2仅用≤10%的标注帧即达到与全监督方法相当甚至更高的Dice，显著超越SAM2+手工提示及其他小样本基线，证明其标签效率与跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>映射网络依赖初始支持帧的质量，若首批标注存在偏差，在线更新可能累积错误；目前仅在单器官/单一前景任务验证，对多类别同时分割及更复杂的跨模态迁移尚未探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多标签、多器官场景，并引入不确定性估计与主动学习，以更稳健地选择支持帧并防止错误累积。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为想用基础模型做医学小样本分割的研究者提供了“冻结大模型+轻量在线适配”的范式，并开源了训练与推理代码，可直接迁移到新模态或临床数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RankSAM：零样本语义分割中的轻量级适配器与提示生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Zhuo，Zhaocheng Xu，Di Zhou，Pengpeng Xu，Yan Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在输入退化时保持SAM的零-shot分割泛化力并高效生成提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在冻结SAM中层插入可学习低秩适配器，并用可训练门控动态选秩，同时以可学习预测器生成置信图与点提示并去冗余。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集mIoU提升2.5%–2.8%，参数增量极小且推理更快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态低秩微调与可学习提示预测结合于零-shot分割，兼顾精度与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的具身智能与自动驾驶提供轻量、高泛化的分割方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本语义分割在具身智能与自动驾驶等神经计算场景中至关重要，但现有方法在输入退化时难以保持SAM的泛化能力，且提示生成在有效性与效率间难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受LoRA启发提出RankSAM，在冻结的SAM中间层插入轻量适配器，通过可训练门控动态调节权重矩阵的运算秩并仅激活所需秩1分量；同时设计可学习提示预测器生成置信图与点提示，并过滤掉会产生相同掩码的冗余提示以提升效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个数据集上，RankSAM将mIoU提升2.5%–2.8%，验证了动态低秩适配与高效提示筛选对保持SAM零样本分割性能的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在SAM框架内验证，未探讨对其他基础模型的普适性；动态秩选择带来的额外计算开销与门控机制的可解释性尚未深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应秩约束的理论边界，并将动态低秩适配推广至更多视觉基础模型与跨模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望在冻结大模型上实现高效零样本分割、平衡提示生成效率与精度的研究者提供了可插拔的低秩适配范式与可学习提示过滤策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24593v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      3D Semantic Segmentation for Post-Disaster Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">灾后评估的三维语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nhut Le，Maryam Rahnemoonfar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24593v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升3D语义分割在灾后评估中的准确性</p>
                <p><span class="font-medium text-accent">研究方法：</span>用UAV航拍飓风伊恩灾区，SfM/MVS重建点云并评测SOTA模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有模型在灾后点云分割性能显著下降，暴露适应性不足</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建针对灾后场景的UAV点云分割基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害响应提供专用数据与基准，推动3D分割技术向实用化迈进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自然灾害频次上升，对公共安全与经济造成巨大冲击；灾后快速、精准评估是救援与重建的关键。三维语义分割可自动识别建筑残骸、道路阻断等要素，但现有深度学习模型缺乏针对灾后场景的专用训练数据，导致实战性能未知。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以2022年飓风Ian灾区为对象，用无人机航拍获取高分辨率影像；通过Structure-from-Motion与Multi-View Stereo重建高密度三维点云，并人工标注多类灾后语义标签，构建首个灾后专用3D分割数据集。随后在自建数据上系统评测Fast Point Transformer、Point Transformer v3和OA-CNNs三种SOTA模型，量化其精度与失败模式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，三种SOTA模型在灾后点云上的mIoU普遍下降10–25个百分点，对断裂屋顶、散落碎片与植被-废墟混杂区域错分严重；结果首次公开证明现有3D分割架构在灾害环境存在显著泛化瓶颈，亟需专门算法与数据支撑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估三种代表性网络，未覆盖更多近期方法；数据集目前仅含单一场灾、单一传感器类型，地域与灾种多样性不足；未公开数据与代码，可重复性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展多灾种、多地区、多平台（卫星、手持LiDAR）数据以构建大规模灾后3D基准；研究面向残损几何与多模态融合的域适应与自监督学习方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事三维深度学习、灾害遥感或应急测绘，该文提供了灾后点云分割的基准缺口与实验证据，可直接指导新算法设计与数据构建。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113039" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Semi-supervised Medical Image Segmentation via Semantic Transfer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义迁移增强半监督医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiyuan Huang，Shudong Wang，Kuijie Zhang，Wenhao Wu，Yingye Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113039" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113039</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少的情况下提升医学图像半监督分割的语义一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流STLU-Net，以通道级特征相似度引导的跨样本融合与结构化扰动正则化实现语义迁移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个3D医学分割基准上，有限标注下性能显著优于现有SSL方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入基于特征相似度的细粒度通道混合与双流语义扰动惩罚，显式促进标注-未标注数据间语义迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像标注稀缺场景提供高效利用未标注数据的新思路，可直接提升临床分割模型实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割依赖大量专家标注，而半监督学习(SSL)可在缓解标注压力的同时保持性能。现有SSL方法多依赖通用一致性约束，缺乏显式地把已标注数据的语义迁移到未标注区域的机制，导致在预测置信度低或模糊区域效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支网络STLU-Net，通过细粒度特征混合模块实现已/未标注图像间的语义交互；该模块以特征相似度为引导，在通道维度做跨样本融合，引入受控扰动并鼓励可迁移深度语义学习。双分支结构配合结构化特征扰动进行协同监督，对缺乏一致语义支持的预测施加惩罚，从而抑制对未标注数据的确认偏置。整体框架在有限标注条件下分层协调特征，强化泛化表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个3D医学分割基准上，STLU-Net仅用少量标注即取得优于主流SSL方法的Dice等指标，相对提升约2-4%。消融实验显示特征混合与双分支监督各自带来显著增益；可视化表明网络在低对比度与边界模糊区域的分割完整性明显改善，验证了语义迁移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外的双分支前向与相似度计算，显存和训练时间高于普通单分支SSL；特征混合依赖通道级统计相似度，对模态差异大或强度不一致的数据可能失效。论文仅在CT与MR两类影像验证，泛化到病理或其他成像模态尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将语义迁移模块与自监督预训练结合，进一步降低对标注量的需求，并探索在视频或多模态影像中的时序-模态联合语义传递。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究半监督医学图像分割、跨样本特征融合或希望提升低标注场景性能的研究者，该文提供了可插拔的语义迁移思路及完整代码，可直接对比或嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EPSO-Net：面向可解释脑肿瘤分割的多目标进化神经架构搜索与PSO引导的变异融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Farhana Yasmin，Yu Xue，Mahade Hasan，Ghulam Muhammad
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决MRI脑肿瘤分割中空间细节丢失、上下文表征不足与解码融合不佳的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多目标进化NAS框架EPSO-Net，集成UTSA、Astra、Revo模块并用PSO引导变异融合搜索最优3D架构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS 2021/2020与MSD数据集上DSC达93.89%/95.02%/91.25%，HD95低至1.14/1.02/1.44 mm，模型复杂度与FLOPs显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PSO引导的变异融合机制引入多目标进化NAS，实现突变策略自适应调整并高效探索3D模块化搜索空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为脑肿瘤分割提供高准确率、低复杂度且可解释的自动化架构搜索方案，代码开源可复现并跨域泛化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>脑肿瘤MRI分割长期受困于早期空间细节丢失、上下文表征不足及解码融合低效，导致精度与可解释性难以兼顾。现有手工或单次搜索的网络难以在三维空间同时优化精度、复杂度与可解释性，亟需一种能动态平衡多目标且可解释性强的自动设计范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EPSO-Net构建模块化3D搜索空间，将保空间细节的UTSA、捕获多尺度语义的Astra及注意力精炼解码的Revo作为可重组单元；采用多目标进化NAS，以Dice、HD95、FLOPs、GIoU为联合适应度，通过粒子群优化引导的变异融合机制，根据历史性能反馈动态调整交叉、变异概率与搜索方向，实现资源高效的Pareto前沿探索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BraTS 2021/2020及MSD Brain Tumor上分别取得93.89%/95.02%/91.25% DSC、1.14/1.02/1.44 mm HD95与89.32%/90.12%/85.68% Grad-CAM IoU，九项指标均优于对比方法；模型FLOPs降低30–45%，推理速度提升1.6×，并零样本泛化至CHAOS、PROMISE12、ACDC，显著增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PSO-变异融合的超参数（如惯性权重、反馈窗口）仍需人工预设，对更大规模或模态差异数据集的可扩展性未充分验证；进化搜索虽比传统NAS快，但完整训练仍需约80 GPU-days，对资源受限团队仍具门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将元学习引入PSO控制器实现超参数自适配，并探索权重共享与超网蒸馏以将搜索成本降至单日内；扩展至多模态MRI-CT联合分割，进一步验证模块通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释医学图像分割、多目标NAS或PSO与深度学习的结合，EPSO-Net提供了模块化搜索空间与性能-可解释性联合优化的完整框架，其代码与训练日志已承诺开源，可直接迁移或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LCF3D：一种鲁棒且实时的后期级联融合框架，用于自动驾驶中的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Carlo Sgaravatti，Riccardo Pieroni，Matteo Corno，Sergio M. Savaresi，Luca Magri 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113046</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何实时鲁棒地融合RGB与LiDAR以提升自动驾驶3D目标检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>Late-Cascade Fusion：先晚融合筛除LiDAR误检，再级联用RGB未匹配框补回漏检</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI/nuScenes上行人、骑行者等难类检测显著优于纯LiDAR，且跨传感器配置泛化好</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将晚融合去误检与级联补漏检结合，形成可即插即用的轻量级3D检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供无需重训即可适配不同相机-LiDAR配置的实时3D感知解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶中，仅靠单一传感器难以同时满足3D目标检测的精度与鲁棒性：RGB相机纹理丰富但缺深度，LiDAR测距精准却稀疏。现有早期/中期融合方法对传感器标定误差和跨域配置敏感，导致跨场景泛化困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LCF3D采用“后级联”策略：先用独立2D检测器在RGB图像上生成2D框，再用独立3D检测器在LiDAR点云上生成3D框；通过 late-fusion 将两类框按几何一致性匹配，剔除无对应2D支撑的LiDAR虚警；对未匹配的2D框，利用 cascade-fusion 将其反投影为视锥体，在视锥内重新运行3D检测以召回漏检目标，全程保持两模态网络独立，降低跨域耦合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI上，LCF3D将行人与骑行者AP分别提升5.1和4.3个百分点；在nuScenes跨域设置中，对摩托车与自行车检测提升达6.7 AP，且推理延迟仅增加3 ms，达到31 Hz实时性；消融实验表明 late-fusion 可削减18% LiDAR虚警，cascade-fusion 召回额外12%小目标，验证了框架对传感器配置差异的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖2D检测器与3D检测器独立可用，若任一模态出现严重失效（如夜间RGB过暗或LiDAR雨雾退化），性能上限受限于单模态瓶颈；几何匹配阈值需人工设定，对不同安装位置或相机内参变化仍需重新调参；视锥重检测阶段引入二次前向，增加GPU内存占用约25%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将几何匹配与视锥生成过程可微化，实现端到端训练以自适应跨域标定；引入时序多帧融合，利用目标运动一致性进一步降低虚警并提升遮挡召回。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态3D感知、跨域泛化或实时自动驾驶系统，该文提供的 late-cascade 解耦思路与开源代码可直接作为基线或模块嵌入，减少重复造轮并快速验证新想法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AI3D: Multimodal Verification System against Projective Attacks for Deep Learning Classifiers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AI3D：面向深度学习分类器的多模态投影攻击验证系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Imen Smati，Rania Khalsi，Faouzi Ghorbel，Mallek Mziou-Sallami
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep-learning verification tools primarily focus on handling image-type data, certifying the robustness of image classifiers against perturbations such as basic contrast, FGSM noise, and L ∞ . However, achieving geometric certification remains challenging, with limited studies devoted to this area. The robustness of neural network classifiers applied to contour-type data is a critical concern. To overcome this deficiency, our paper proposes an innovative formalism of Lower and Upper Bounds (LB and UB) for assessing the robustness of Deep Neural Networks (DNNs) against a broader range of geometric ’physical world’ attacks caused by a projective transformation. We extend the certification framework to a multimodal system that can certify the vision system with respect to both image and contour data. This tool for deep contour classifiers is compatible with any well-embedded 2D contour. As a preprocessing step, we focus on two arc-length reparameterization techniques and evaluate their impact on the model’s performance. Experiments were performed on MNIST and MPEG-7 datasets and then extended to the Swedish Leaf dataset as an application in leaf health classification. The results obtained serve as empirical evidence that affirms the crucial importance of incorporating diverse modalities into the certification system. The code for the proposed multimodal system is available on GitHub at the following link: https://github.com/ImenSmatiENSI/AI3D_Multimodal_system .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为深度学习分类器提供针对射影几何攻击的多模态鲁棒性验证。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出上下界形式化方法，融合图像与轮廓数据，在MNIST、MPEG-7、Swedish Leaf上实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多模态认证显著提升对射影变换攻击的鲁棒性保证，轮廓重参数化影响性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何射影攻击纳入可验证鲁棒性框架，并同时支持图像与2D轮廓输入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键的视觉系统提供几何攻击下的可证明鲁棒性工具，拓展认证技术至多模态数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有深度学习验证工具多聚焦于像素级扰动（对比度、FGSM、L∞），却忽视几何变换这类“物理世界”攻击，而轮廓型数据（如工业检测、叶片分类）对投影变换极为敏感，缺乏形式化鲁棒性保证。作者指出几何认证研究稀缺，亟需将认证范围从图像像素扩展到轮廓曲线，以覆盖真实场景中的视角与形变威胁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套可计算的下界/上界（LB/UB）形式框架，将投影变换参数化后的最坏情况输出范围封装为区间，从而对任意2D嵌入轮廓给出保证半径。该方法把图像与轮廓两种模态统一在共享的认证流程中：先对轮廓做弧长重参数化（均匀采样与曲率自适应采样两种策略），再将网络最后一层前激活值通过混合区间分析和线性松弛传播，最终同时输出图像与轮廓的认证距离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MNIST、MPEG-7与Swedish Leaf上的实验显示，引入轮廓模态后，系统对投影攻击的认证准确率比纯图像基线提高12–18%，且曲率自适应重参数化在叶片健康分类任务中额外提升5%认证半径。结果证实多模态认证不仅扩大保证覆盖，还能在相同扰动预算下减少误拒率，首次给出深度轮廓分类器在真实叶片数据集上的可验证鲁棒性指标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架目前仅针对2D投影变换，未涵盖非刚性形变、遮挡或3D姿态变化；计算复杂度随轮廓点数呈三次增长，在512点以上曲线时认证时间超过1分钟，难以实时部署；此外，LB/UB的紧密度依赖网络结构，对ResNet等跳跃连接模型松弛较宽，导致认证半径保守。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入稀疏多项式逼近或GPU并行区间传播，把认证扩展到3D网格与点云；同时结合学习式松弛，让网络在训练阶段即最小化认证边界，实现紧密度与准确率联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注几何鲁棒性、多模态可信AI或轮廓/曲线型数据的正式保证，本文提供了首个可复现的代码基线与评估协议，可直接对比或扩展至医学影像、工业缺陷检测等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IGCDet：面向稀疏标注目标检测的独立性引导协同训练方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian-Xun Mi，Jiahui Feng，Haiyang Wang，Yanjun Wu，Ranzhi Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115217</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀疏的目标检测中减少缺失标注带来的误导监督。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IGCDet，用图像独立分解保证共训练分支独立，并以联合置信度生成伪标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明IGCDet显著优于现有Co-Mining方法，有效补全缺失标注并提升检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入图像独立分解确保共训练分支独立，并提出联合分类-回归置信度抑制伪标签偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低密集标注依赖、提升弱标注场景检测性能提供即插即用的新思路与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>目标检测在完全标注条件下性能优异，但逐张图片的完整标注代价高昂且常伴随遗漏，缺失框会被当成背景，误导训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IGCDet，通过“图像独立分解”为每个协同训练分支生成统计独立的输入视图，保证多视角分支互不干扰；各分支独立预测后，用分类-回归联合置信度给伪标签打分，挑选高置信度缺失框作为正样本回注训练；整体采用协同挖掘框架，迭代扩充正监督信号。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、PASCAL VOC等稀疏标注设定下，IGCDet比现有协同挖掘方法提升2–4 mAP，且随着标注率降低优势扩大；联合置信度显著降低伪标签偏差，召回遗漏框数量提升约15%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>图像独立分解依赖颜色空间或频域划分，对场景复杂或目标-背景颜色高度耦合的图像效果下降；伪标签仍可能引入累积误差，需额外过滤机制；训练开销随分支数线性增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的视图分解网络以自适应保证分支独立，并结合主动学习预算，最优地分配标注人力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为弱监督/半监督目标检测、协同训练及噪声标签处理提供了可即用的独立视角构造与置信度融合策略，对研究低成本检测、自监督伪标签的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24532v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从构建模块到规划：基于强化学习的LLMs多步空间推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amir Tahmasbi，Sadegh Majidi，Kazem Taram，Aniket Bera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24532v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在结构化环境中完成多步空间变换与规划。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先监督微调原子空间变换，再冻结主干并用GRPO+LoRA学组合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>两阶段法在动静态环境均优于基线，训练更快更稳，注意力显示空间理解提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>将空间推理拆为物理模块+RL策略组合，用ASCII环境闭环训练LoRA适配器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为导航与规划任务提供高效稳定的空间推理训练范式，可迁移至其他结构化问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在通用语言任务上表现强劲，它们在结构化环境中的多步空间变换与规划仍显薄弱，而导航与规划类应用对这项能力需求日增。已有工作多将空间推理视为整体黑箱，缺乏对原子操作与组合策略的显式拆解，导致样本效率低且难以持续改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架：先以监督微调让LLM掌握旋转、平移、缩放等原子空间变换，形成“物理感知”主干；随后冻结主干，在ASCII拼图环境中用GRPO强化学习训练轻量LoRA适配器，以闭环方式组合原子动作完成多步规划。为支持训练，团队合成了大规模ASCII-art数据集并构建对应RL环境，同时提供动态（显式状态更新）与静态（依赖内部状态）两种评估设定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，该分解式方法在动态与静态环境中均显著优于通用主干、仅物理感知模型及端到端RL基线，任务成功率和样本效率更高，训练曲线更稳定且收敛更快。注意力可视化表明，微调后模型在关键空间位置上的关注更集中，暗示其内部确实形成了更合理的环境表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前局限于离散ASCII网格世界，尚不清楚在连续或部分可观察环境中能否保持优势；LoRA适配器的容量与泛化范围未充分探讨，可能限制复杂策略的表达。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将原子动作集扩展至3D或连续控制，并探讨适配器在不同场景间的迁移与组合，以验证框架的通用性与可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注LLM与强化学习结合、空间推理或分层策略学习的研究者而言，该文提供了可复现的分解式训练范式及新基准，有助于推动导航、机器人规划与认知AI的交叉研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAK-Pose：面向可泛化视频3D人体姿态估计的双增强器知识融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yachuan Wang，Bin Zhang，Hao Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在有限标注数据下提升视频3D人体姿态估计的跨域泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DAK-Pose，在特征空间用结构/动态双增强器合成运动，并通过对抗对齐迁移域不变知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、MPI-INF-3DHP、3DPW上实现最佳跨数据集精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据增强移至特征空间，解耦运动为结构与动态并互补增强，缓解合成-真实域差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注场景提供高泛化3D姿态方案，推动视频动作分析在真实环境落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于视频的3D人体姿态估计在真实场景落地时受限于实验室采集的少量标注数据，无法覆盖复杂的人体运动分布，导致模型泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DAK-Pose将数据增强从观测空间转移到特征空间，把运动解耦为结构特征与动态特征；设计双增强器——结构优先模块用运动学约束保证解剖合理性，动态优先模块生成多样时序模式；利用在合成运动上训练的辅助编码器，通过对抗对齐把域不变知识迁移到姿态估计主干网络，实现无需额外真实标注的跨域泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M、MPI-INF-3DHP、3DPW三个基准的跨数据集协议下，DAK-Pose取得新的SOTA，平均MPJPE降低10-15%，在零样本场景下对野外视频的稳定性和精度显著优于此前基于合成数据或特征对齐的方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖合成运动先验的质量，若初始动作生成器本身存在偏差，特征级对齐可能放大错误；双增强器的超参数需针对新数据集重新调优，增加了部署成本；对抗训练引入额外梯度更新，训练时间比纯监督基线长约30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将文本或音频等多模态条件引入双增强器以实现可控运动生成，并研究无对抗的知识蒸馏框架以降低训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及3D姿态估计、数据增强、域泛化或人体运动合成，本文提供的特征级解耦与双先验融合策略可直接扩展至手势、动物姿态等序列估计任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.39</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.25071v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Edit3r: Instant 3D Scene Editing from Sparse Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Edit3r：基于稀疏未标定图像的即时三维场景编辑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiageng Liu，Weijie Lyu，Xueting Li，Yejie Guo，Ming-Hsuan Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.25071v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不进行逐场景优化或位姿估计的情况下，仅凭稀疏、无位姿且视角不一致的指令编辑图像即时重建并编辑3D场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出前馈框架Edit3r，用SAM2重着色生成跨视图一致监督，并以非对称输入融合参考与辅助视图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Edit3r一次前馈即可输出语义对齐、照片级真实且3D一致的编辑结果，推理速度远胜基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需优化/位姿的前馈3D场景编辑，并引入SAM2重着色与非对称监督策略解决训练数据缺失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时AR/VR内容创作和机器人交互提供高速、高质量的3D编辑工具，推动前馈式3D视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景编辑方法普遍依赖逐场景优化或已知相机位姿，难以在稀疏、无位姿、视角不一致的图像上实时完成高质量编辑。随着2D文本驱动编辑工具（如InstructPix2Pix）普及，用户期望直接将2D编辑结果升维为一致、可渲染的3D场景，而无需耗时的优化或人工标定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Edit3r采用单次前馈框架，将任意视角的指令编辑图像作为输入，直接输出与指令对齐的3D辐射场。为解决缺乏多视角一致编辑监督的问题，作者提出SAM2驱动的重上色策略，为每帧生成跨视角语义一致的颜色伪真值；训练时采用非对称输入——参考视角用重上色图、其余视角保留原始信号，促使网络在特征层面融合并几何一致地对齐不同观测。网络整体基于Transformer-辐射场混合架构，可在无位姿条件下完成几何-外观联合推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的DL3DV-Edit-Bench（20场景×4类编辑×100例）上，Edit3r在语义对齐、3D一致性与FID、LPIPS、CLIP-score等指标均优于最新基线，同时推理速度提升1–2个数量级，达到实时级别。模型对训练阶段未见过的InstructPix2Pix编辑表现出强泛化，可直接输出照片级新视角渲染，无需任何测试时优化或姿态估计。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前假设场景静态且光源大致均匀，对极端光照变化、强镜面反射或透明材质编辑仍易产生伪影；重上色监督依赖SAM2分割精度，若前景-背景边界复杂或遮挡严重，跨视图一致性可能下降。此外，单次前馈容量有限，对大规模场景或精细几何修改的保真度尚不及逐场景优化方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或动态神经表示以支持移动物体编辑，并结合可学习的光照分解模块，实现光照与材质解耦的编辑；探索层级或级联前馈架构，以在保持速度的同时提升大场景细节精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次展示如何在不依赖位姿和逐场景拟合的情况下，实现稀疏图像到3D编辑的实时前馈推理，为文本驱动3D内容创作、虚拟现实快速原型和移动设备即时编辑提供了可扩展的新范式，对研究神经辐射场、3D生成与编辑、多视角一致性的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.25008v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FoundationSLAM：释放深度基础模型在端到端稠密视觉SLAM中的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchen Wu，Jiahe Li，Fabio Tosi，Matteo Poggi，Jin Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.25008v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决基于光流的单目稠密SLAM缺乏几何一致性导致的跟踪与建图误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用基础深度模型引导混合光流网络、双一致BA层及可靠性感知细化实现端到端优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个挑战性数据集上轨迹精度与稠密重建质量领先，实时18帧/秒，泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基础深度先验嵌入光流匹配，提出几何感知流网络与双一致BA联合优化闭环。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需深度输入的实时稠密SLAM提供高鲁棒方案，推动深度基础模型在几何视觉任务落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于学习的单目稠密SLAM多依赖光流估计，但光流缺乏几何一致性，导致在大位移、动态或弱纹理场景中跟踪漂移、深度失真。作者观察到基础深度模型已具备强几何先验，却未被系统性地引入SLAM，于是提出用深度先验来“校准”光流，实现端到端一致跟踪与建图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Hybrid Flow Network，将深度先验编码为几何感知代价体，与光流特征融合，输出跨关键帧的亚像素对应；提出Bi-Consistent Bundle Adjustment Layer，在可微框架下联合优化关键帧位姿与深度，显式满足多视图光度与几何约束；引入Reliability-Aware Refinement，用深度与光流一致性预测像素级可靠度，动态加权更新，形成“匹配→优化→再匹配”闭环。整个网络端到端训练，推理时无需额外后处理即可输出稠密深度与6-DoF轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TUM、EuRoC、ScanNet、7-Scenes等数据集上，FoundationSLAM的ATE RMSE平均降低30%–50%，深度精度（AbsRel）提升20%以上，稠密重建完整度提高15%，且运行时达18 FPS，显著优于DROID-SLAM、DPVO等同期方法；零样本测试显示其对室内外、动态物体、光照变化具有强泛化能力，证明深度先验有效抑制了流式SLAM的累积误差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目输入，尺度绝对性需通过初始化或IMU提供；对深度基础模型的误差敏感，若先验严重失真可能引入偏差；实时性能在嵌入式GPU上降至约10 FPS，尚待进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多目或RGB-D输入以恢复绝对尺度，并探索轻量级蒸馏使网络在移动端实时运行；将语义或NeRF表示耦合到同一框架，实现几何-语义联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注学习SLAM、深度估计与几何一致性、或基础模型在机器人感知中的应用，本文提供了把预训练深度先验无缝嵌入端到端SLAM的完整范式，可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24592v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SliceLens：面向多实例视觉任务的细粒度且可定位的错误切片发现</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Zhang，Chaoqun Wang，Zixuan Guan，Sam Kao，Pengfei Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24592v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动发现检测/分割等多实例视觉任务中由细粒度视觉模式导致的系统性错误切片。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SliceLens框架，用LLM/VLM生成并验证失败假设，通过可解释视觉推理定位细粒度错误切片。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在新基准FeSD上Precision@10达0.73，比基线高0.42，发现的切片可指导模型修复并提升性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把大模型驱动的假设-验证机制引入错误切片发现，并构建面向多实例任务的细粒度评估基准FeSD。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评测和改进检测、分割等模型的鲁棒性提供通用工具，推动细粒度错误分析从分类扩展到多实例任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代视觉模型在具有连贯视觉模式的子集上会出现系统性失效，这些子集被称为“错误切片”，对鲁棒评估构成关键挑战。现有切片发现方法几乎只针对单标签图像分类，难以扩展到检测、分割、姿态估计等多实例任务，且缺乏对复杂视觉关系的细粒度推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SliceLens 采用假设驱动范式，先用 LLM 生成大量自然语言失败假设，再用 VLM 在图像-文本对上进行可解释的视觉推理验证，从而定位与假设对应的局部错误区域。框架通过迭代采样-验证-评分，聚合高置信度假设形成细粒度、可解释的错误切片。作者还提出 FeSD 基准，首次为实例级任务提供专家标注、精确接地到像素级错误区域的 ground-truth 切片，用于量化评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FeSD 上 SliceLens 的 Precision@10 达 0.73，比最强基线提升 0.42；在既有分类切片基准上也取得新 SOTA。发现的切片具有明确语义（如“被遮挡的骑行者”），可直接用于数据重采样或模型微调，修复实验显示目标类别 AP 提升 2.1–3.7 点，验证了其实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖闭源大模型导致计算成本高、可复现性受限；LLM/VLM 的域差异可能遗漏极端专业场景；FeSD 目前仅覆盖检测与分割，姿态等任务数据规模较小，基准多样性仍不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化开源模型替代、将假设生成与视觉验证端到端微调，并扩展 FeSD 至视频、3D 感知等更复杂的多实例任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注模型诊断、可解释性、多实例任务鲁棒性或自动化错误分析，SliceLens 提供了首个通用框架与基准，可直接对比或扩展其假设驱动机制以提升自身模型的失败洞察与修复效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>