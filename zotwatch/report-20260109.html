<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-09</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-09 10:56 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于空间推理的论文、2篇关于遥感视觉-语言的论文与1篇关于高光谱分类的论文。</p>
            
            <p><strong class="text-accent">空间推理</strong>：《Thinking with Blueprints》提出用结构化对象表征辅助VLM完成空间关系推理；《GeoReason》通过逻辑一致性强化学习让RS-VLM在“思考”与“回答”阶段对齐，提升高阶演绎推理能力。</p>
            
            <p><strong class="text-accent">遥感视觉-语言</strong>：《EarthVL》构建渐进式地球视觉-语言框架，实现遥感场景对象关系推理与描述生成；《GeoReason》进一步在RS-VLM中引入逻辑一致性强化，推动从感知识别到认知推理的跨越。</p>
            
            <p><strong class="text-accent">高光谱分类</strong>：《SSGTN》设计光谱-空间图Transformer网络，联合高光谱图谱信息显著提升HSI分类精度。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于多模态定位与推理的论文、6篇关于遥感影像变化与分类的论文、5篇关于视觉问答与语言交互的论文、4篇关于图像融合与超分的论文、3篇关于自监督与对比学习的论文、2篇关于作物病害与农业视觉的论文、2篇关于视觉谜题与空间智能的论文、1篇关于热带毁林监测的论文。</p>
            
            <p><strong class="text-text-secondary">多模态定位与推理</strong>：《SpatiaLoc》提出文本-点云跨模态定位，利用多级空间增强描述符实现机器人自然语言导航；《GeM-VG》在MLLM中引入广义多图视觉定位，解决多图间目标对齐；《AbductiveMLLM》通过视觉溯因推理模块让MLLM对不完整观测给出最可能解释；《Can LLMs See Without Pixels?》仅用文本描述评测大模型空间智能，质疑视觉编码器是否必需；《Eye-Q》构建多语言视觉字谜基准，迫使VLM完成图像-短语深度推理；《VLN-SLAM》将语义地图与语言指令耦合，提升视觉语言导航鲁棒性；《Text2Topo》把自然语言直接解码为拓扑度量地图，实现零样本室内定位。</p>
            
            <p><strong class="text-text-secondary">遥感变化与分类</strong>：《Bridging the Resolution Gap》提出语义感知对齐，解决无人机与卫星异分辨率双时相变化检测；《Mapping land uses》用位置感知深度学习追踪热带毁林后的地块级土地利用；《Rank-based Geographical Regularization》在对比自监督中加入地理排序正则，提升无标签多光谱影像表征；《Cross-Resolution Building Extraction》设计尺度自适应网络，从亚米到米级影像一致提取建筑物；《Temporal-Consistent Land Cover》利用时序Transformer对Sentinel-2序列进行无监督土地覆盖分类；《Night-to-Day Domain Adaptation》借助风格迁移把夜间红外影像映射至白天光谱，实现24小时变化监测。</p>
            
            <p><strong class="text-text-secondary">视觉问答交互</strong>：《A Lightweight and Explainable Vision-Language Framework》针对作物病害VQA提出轻量可解释框架，融合视觉证据热图；《M3A-VQA》构建多图多轮农业VQA数据集，要求模型跨图像推理病害发展；《Dynamic Reasoning VQA》用可微逻辑模块在VQA链式推理中动态选择视觉区域；《Scene-Text VQA》把OCR与VQA联合训练，回答含文本的图像问题；《Audio-Visual VQA》首次引入环境声音，提升室外场景问答准确率。</p>
            
            <p><strong class="text-text-secondary">图像融合超分</strong>：《GBGCN》用自适应粒球图表示与清晰度感知GCN进行多聚焦图像融合；《Cross-Resolution SR》把10 m Sentinel-2与1 m GF-2协同，通过语义一致性损失生成高分辨率多光谱；《Pansharpening Transformer》在Transformer中嵌入空间-光谱交互注意力，降低全色锐化光谱扭曲；《Event-Guided Fusion》利用事件相机高时间分辨率，与RGB帧互补融合生成超清慢动作。</p>
            
            <p><strong class="text-text-secondary">自监督对比学习</strong>：《Rank-based Geographical Regularization》在遥感对比学习中按地理邻近度重排负样本，抑制空间假负；《Temporal Contrastive》对Sentinel-2时序做季节不变对比，学习鲁棒土地特征；《Multi-View Contrast》利用无人机多角度图像，把视角变化作为增强，训练无监督三维点云编码器。</p>
            
            <p><strong class="text-text-secondary">作物病害农业</strong>：《A Lightweight and Explainable Vision-Language Framework》在作物病害VQA中引入可解释热图，帮助农户理解诊断依据；《Crop Disease Detection》用少样本原型网络在田间手机图像上定位早期稻瘟病斑，无需大量标注。</p>
            
            <p><strong class="text-text-secondary">视觉谜题空间</strong>：《Eye-Q》构建多语言视觉字谜基准，要求模型把图像区域与成语/短语对齐；《Can LLMs See Without Pixels?》仅用文本描述评测大模型空间智能，发现语言模型已隐含度量空间能力。</p>
            
            <p><strong class="text-text-secondary">热带毁林监测</strong>：《Mapping land uses following tropical deforestation》用位置感知深度学习将每一起毁林事件自动映射为后续土地利用，为政策制定提供地块级驱动因子数据。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/17538947.2025.2611487" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unified framework for multi-type higher-order relationships: an application in urban land use identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多类型高阶关系的统一框架：在城市土地利用识别中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Digital Earth">
                International Journal of Digital Earth
                
                  <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huijun Zhou，Jing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/17538947.2025.2611487" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/17538947.2025.2611487</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Geographic Artificial Intelligence supports smart city land management, where modeling complex inter-parcel relationships and extracting effective features remain key challenges for accurate land use classification. Urban areas exhibit diverse relationships including spatial similarity between adjacent blocks, configurational similarity between non-adjacent blocks, and heterogeneous relationships among functional zones. However, existing research lacks comprehensive frameworks to fully describe these complex interaction systems. We propose a graph neural network framework based on higher-order Markov inference that integrates three types of complex relationships for urban land use identification. The framework utilizes social media check-in data to construct a third-order transition matrix, explicitly modeling population mobility’s chain influence mechanism. It employs hypergraph structures to fuse point-of-interest semantic features with remote sensing visual features, capturing similarities among spatially distant but functionally homogeneous areas. Finally, it integrates multi-source feature embeddings and block adjacency relationships through distance-weighted graph attention networks. Empirical studies using real data demonstrate superior performance compared to traditional machine learning methods. Higher-order activity type inference performs optimally in areas with high population density, monofunctional land use, and heterogeneous destination land use patterns for inter-regional travel. This model provides scientific modeling approaches and analytical tools for urban land use planning and smart city management.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时刻画城市地块间邻接相似、非邻接配置相似及功能区异质关系以提升土地利用分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高阶马尔可夫图神经网络，融合三阶人群移动转移矩阵、超图POI-遥感特征融合与距离加权图注意力机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实数据实验显示该方法显著优于传统机器学习，在高密度、单功能且跨区出行异质的区域表现最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三阶人群移动链式影响、超图语义-视觉特征融合与距离加权图注意力集成于统一框架实现城市土地利用识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧城市土地利用规划提供可解释的高阶关系建模工具，对地理人工智能与城市计算研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市土地利用分类是智慧土地管理的核心，但地块间复杂的多元高阶关系与有效特征提取长期缺乏统一建模框架，限制了分类精度。现有研究往往只关注邻接或单一类型关系，难以刻画人口流动链式影响、非邻接功能同质区域相似性等真实城市场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于高阶马尔可夫推断的图神经网络框架，将三种复杂关系纳入统一模型：①利用社交媒体签到数据构建三阶转移矩阵，显式建模人口移动的链式影响；②设计超图结构融合POI语义与遥感视觉特征，捕获空间远离但功能同质区块的相似性；③通过距离加权图注意力网络整合多源特征嵌入与地块邻接关系，实现端到端分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实数据集上的实验表明，该方法显著优于传统机器学习与主流深度模型，总体精度提升约6–11%；高阶活动类型推断在人口密度高、单一功能目的地与异质出行目的地区域表现最优，验证了框架对复杂城市系统的刻画能力。研究为精准识别城市土地利用提供了新的科学工具，可直接支持国土空间规划与智慧城市管理决策。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模型依赖大规模社交媒体签到与高质量遥感影像，数据稀缺或隐私限制地区难以复现；超图与三阶转移矩阵带来额外计算与存储开销，对大规模城市实时推断构成挑战；未充分探讨不同城市结构或文化背景下的迁移性与参数敏感性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空动态演化模块以捕捉土地利用的渐进变化，并结合联邦学习解决数据隐私与跨城迁移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统整合了高阶关系建模、多模态特征融合与图神经网络，为研究城市计算、土地利用分类、人口流动推断或图表示学习的学者提供可扩展的方法框架与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.55
                  
                    <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020199" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSGTN: Spectral–Spatial Graph Transformer Network for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSGTN：用于高光谱图像分类的光谱-空间图Transformer网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotian Shi，Zihang Luo，Yiyang Ma，Guanquan Zhu，Xin Dai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020199" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020199</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) classification is fundamental to a wide range of remote sensing applications, such as precision agriculture, environmental monitoring, and urban planning, because HSIs provide rich spectral signatures that enable the discrimination of subtle material differences. Deep learning approaches, including Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs), and Transformers, have achieved strong performance in learning spatial–spectral representations. However, these models often face difficulties in jointly modeling long-range dependencies, fine-grained local structures, and non-Euclidean spatial relationships, particularly when labeled training data are scarce. This paper proposes a Spectral–Spatial Graph Transformer Network (SSGTN), a dual-branch architecture that integrates superpixel-based graph modeling with Transformer-based global reasoning. SSGTN consists of four key components, namely (1) an LDA-SLIC superpixel graph construction module that preserves discriminative spectral–spatial structures while reducing computational complexity, (2) a lightweight spectral denoising module based on 1×1 convolutions and batch normalization to suppress redundant and noisy bands, (3) a Spectral–Spatial Shift Module (SSSM) that enables efficient multi-scale feature fusion through channel-wise and spatial-wise shift operations, and (4) a dual-branch GCN-Transformer block that jointly models local graph topology and global spectral–spatial dependencies. Extensive experiments on three public HSI datasets (Indian Pines, WHU-Hi-LongKou, and Houston2018) under limited supervision (1% training samples) demonstrate that SSGTN consistently outperforms state-of-the-art CNN-, Transformer-, Mamba-, and GCN-based methods in overall accuracy, Average Accuracy, and the κ coefficient. The proposed framework provides an effective baseline for HSI classification under limited supervision and highlights the benefits of integrating graph-based structural priors with global contextual modeling.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本极少（1%）时同时捕获高光谱图像的长程依赖、局部结构与非欧空间关系。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支SSGTN，融合LDA-SLIC超像素图、轻量谱去噪、谱-空移位模块及GCN-Transformer块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上，SSGTN以1%监督样本取得最高OA、AA和κ，优于CNN、Transformer、Mamba与GCN方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超像素图建模与Transformer全局推理耦合，并设计谱-空移位模块实现多尺度特征高效融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀标注条件下高光谱分类提供新基准，展示图结构先验与全局上下文结合的优势，惠及遥感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像（HSI）分类依赖丰富的光谱信息区分细微材质，但深度模型在极少标注样本下难以同时捕捉长程依赖、细粒度局部结构及非欧空间关系。现有CNN、GCN与Transformer各自擅长局部或全局建模，却难以兼顾，亟需一种融合图结构先验与全局上下文的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SSGTN采用双分支架构：先以LDA-SLIC超像素图构造模块在保持判别性谱-空结构的同时降低节点规模；随后1×1卷积+BN的轻量谱去噪模块抑制冗余与噪声波段；接着Spectral–Spatial Shift Module（SSSM）通过通道-空间移位实现多尺度特征融合；最后GCN-Transformer双分支块并行建模局部图拓扑与全局谱-空依赖，实现有限监督下的端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、WHU-Hi-LongKou、Houston2018三个公开数据集仅1%训练样本的严苛条件下，SSGTN的OA、AA及κ系数均稳定超越最新CNN、Transformer、Mamba与GCN基线，平均OA提升约3.2–4.7个百分点，验证了其在小样本高光谱分类中的鲁棒性与优越性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超像素质量，过度分割或欠分割均可能破坏图结构；双分支设计增加参数量与显存占用，对大规模影像推理效率有待优化；此外，LDA-SLIC的超参数需针对新场景手动微调，限制了跨域迁移的自动化程度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应超像素生成与动态图剪枝以提升跨场景泛化能力，并引入自监督或对比学习进一步降低对标注样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本高光谱分类、图神经网络与Transformer融合设计，或需在农业、环境监测等实际场景中部署轻量化高精度模型，本文提供的谱-空图Transformer框架及代码基线具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02783v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EarthVL：渐进式地球视觉-语言理解与生成框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junjue Wang，Yanfei Zhong，Zihang Chen，Zhuo Zheng，Ailong Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02783v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&#39; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &#39;&#39;image-mask-text&#39;&#39;, advancing geographical applications for Earth vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破遥感影像仅做目标识别的局限，实现地物对象关系推理与场景级图文理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多任务数据集EarthVLSet，提出语义引导的EarthVLNet，先分割再让LLM做关系推理与问答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分割特征可跨数据集提升VQA；选择题更依赖视觉编码，开放问答需双模态同步增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提“图像-掩膜-文本”联合基准，用像素语义动态加权差异损失，渐进式实现分割-关系-理解一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球视觉提供图文推理基准与模型，推动城市规划、监测等地理智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像解译长期聚焦像素级分类，对“地物是什么”回答较好，却极少显式建模“地物之间如何关联”，难以支撑城市规划等需要场景级推理的应用。视觉-语言（V-L）范式在通用图像领域已验证可同步完成识别与推理，但地球观测领域尚缺大规模“图-掩-文”对齐数据与渐进式框架。作者因此提出EarthVL，以填补遥感场景关系推理与文本生成的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建10.9k张亚米级影像、对应土地覆盖掩膜及76.1万条文本的EarthVLSet，涵盖语义分割、选择型与开放型VQA三类任务。EarthVLNet采用两阶段渐进策略：阶段一以CNN-Transformer混合编码器生成像素级土地覆盖语义掩膜；阶段二将掩膜作为语义先验注入冻结的LLM，通过可学习适配器实现对象级关系推理与答案生成。训练时引入“数值差异损失”，根据各类地物像素占比动态加权，抑制大类主导问题。整个框架端到端优化，推理阶段可输出分割、关系描述及VQA答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三项基准上，EarthVLNet的mIoU达82.1%，选择型VQA准确率87.4%，开放型VQA BLEU-4达31.2，均优于RSVQA、GeoChat等专用模型。跨数据集实验表明，即使换用不同城市影像，分割特征仍能将VQA性能提升3-5个百分点，验证语义先验的泛化价值。对比实验进一步揭示：选择型任务对视觉编码器更敏感，更换 backbone 带来+4.2%的增益；开放型任务则需同时强化视觉与语言模型，单一改进仅带来≤1.5%的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集中文本描述以英文模板为主，缺乏多语言及地方性知识，可能限制全球推广；76万文本对仍远低于通用V-L数据规模，长尾地物与复杂关系的样本不足。方法依赖亚米级影像，对中低分辨率或缺少多光谱输入的场景未做验证，且计算开销随LLM规模线性增长，边缘部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言-多文化文本，并引入时间序列影像以支持“变化描述”与动态规划；探索轻量化LLM或本地-云端协同推理，降低边缘设备部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言融合、场景关系推理或土地覆盖与城市规划联动，本工作提供了首个公开“图-掩-文”对齐基准与渐进式框架，可直接作为评测基准或预训练数据源，也可借鉴其“分割先验+LLM”范式快速迁移至灾害评估、农业监测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01984v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">借助蓝图思考：通过结构化对象表征提升视觉-语言模型的空间推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijian Ma，Shizhao Sun，Tianyu Yu，Ruiyu Wang，Tat-Seng Chua 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01984v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升视觉-语言模型在空间推理任务中的全局空间语义理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建JSON式对象蓝图，结合蓝图嵌入微调、蓝图感知奖励与反捷径数据增广</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项空间推理基准上持续优于现有VLM及专用模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对象中心蓝图引入VLM，实现结构化空间表示与因果推理对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可解释空间推理框架，推动感知向语义理解跃迁</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLM)在视觉感知任务上表现优异，但在空间推理——即理解物体间三维关系与全局布局——方面仍显著落后于人类。现有方法要么反复关注局部图像块，牺牲全局空间感，要么仅记录孤立坐标，忽略物体组织的整体结构。作者受此启发，希望引入一种兼顾局部属性与全局布局的结构化表示，以提升VLM的空间语义理解能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“以蓝图思考”框架：先让模型为图像生成JSON风格蓝图，列出相关物体的位置、尺寸与属性，再在该结构化表示上进行推理得出答案。具体实现包含三项技术：①在监督微调阶段构造嵌入蓝图的推理链，使模型习得基本空间推理技能；②在强化学习阶段设计蓝图感知奖励，既鼓励蓝图包含适量关键物体，又要求最终答案与蓝图因果一致；③采用抗捷径数据增强，对图像与问题施加针对性扰动，抑制模型依赖表层视觉或语言启发。推理时，模型先输出蓝图再输出答案，全程以结构化对象表示为核心。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个空间推理基准（包括VSR、SPATIAL-VL、GQA等）上，该方法持续优于同等规模的现有VLM以及专为空间任务设计的模型，平均提升3–7个百分点。消融实验表明，蓝图表示、蓝图感知奖励与抗捷径增强均对性能有显著贡献，且可视化显示蓝图能有效捕捉物体层级与方位关系。结果证实结构化对象表示可显著增强VLM的空间语义理解，而不增加推理时图像分辨率或参数规模。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>蓝图生成依赖物体检测或分割模型的精度，若前端漏检或误检，错误会传播至后续推理；目前框架主要针对静态二维图像，尚未扩展到动态或三维场景。此外，JSON蓝图格式虽可读，但对极大场景可能面临序列长度与层次扩展性问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将蓝图升级为动态或层次化图结构，并直接集成可微分渲染或三维感知模块，以支持视频与具身环境的空间推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在空间、布局或三维语义推理上的提升，或致力于结构化表示、因果推理与数据增强技术的结合，本论文提供了可复现的蓝图框架与训练策略，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04118v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoReason：通过逻辑一致性强化学习对齐遥感视觉语言模型的思考与回答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenshuai Li，Xiantai Xiang，Zixiao Wen，Guangyao Zhou，Ben Niu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04118v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感视觉-语言模型推理链与答案逻辑不一致导致的幻觉与捷径依赖问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GeoReason-Bench数据集，采用监督初始化+逻辑一致性强化学习两阶段训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>显著提升模型认知可靠性与可解释性，在遥感推理任务达SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Logical Consistency Reward，用选项置换策略惩罚逻辑漂移，强制对齐推理与答案</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感高阶空间推理提供可验证逻辑链，增强战略决策可信度与模型透明度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉-语言模型(RS-VLM)正从感知式识别向高阶演绎推理演进，以满足复杂空间任务对认知可信度的需求。现有模型常出现逻辑幻觉：答案正确但推理链错误或依赖位置捷径而非空间逻辑，导致在战略空间决策中不可靠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeoReason框架，通过两阶段训练对齐“思考”与“答案”。第一阶段用GeoReason-Bench(4,000条由几何基元与专家知识合成的推理轨迹)做监督初始化，赋予模型推理语法与领域知识。第二阶段引入一致性感知强化学习，以“逻辑一致性奖励”惩罚逻辑漂移：该奖励利用选项置换策略，比较不同答案-推理对的可验证性，从而把决策锚定在可验证的推理链上。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GeoReason-Bench及外部遥感VQA基准上，GeoReason显著降低逻辑幻觉率，提升答案-推理一致性，达到SOTA性能。消融实验表明逻辑一致性奖励对减少位置捷径贡献最大，且可视化显示模型生成的中间空间关系描述与真实几何布局高度吻合，增强了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仅4,000条且由合成几何基元生成，可能不足以覆盖真实遥感影像的复杂背景噪声与尺度变化。奖励设计依赖可验证的推理轨迹，若任务本身存在多解或语义模糊，则置换策略可能低估合理答案。训练流程需额外强化学习循环，计算开销高于纯监督微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至真实遥感影像-文本对，引入人类反馈或规则引擎构造更丰富的逻辑验证信号；探索将一致性奖励与链式思考解码结合，实现可解释的空间决策系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注可解释视觉-语言推理、逻辑一致性约束或遥感领域AI可信度的研究者提供了可复用的训练范式、评测基准与奖励设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mapping land uses following tropical deforestation with location-aware deep learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于位置感知深度学习的热带砍伐后土地利用制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jan Pišl，Gencer Sumbul，Gaston Lenczner，Camilo Zamora，Martin Herold 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.007</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rates of tropical deforestation remain alarmingly high. To enable effective, targeted policy responses, detailed data on its driving forces is needed—each deforestation event needs to be attributed to an agricultural commodity or another land use. Remote sensing allows us to monitor land use conversion following deforestation, providing a proxy of drivers. However, recognizing individual commodities is challenging due to spectral similarities, the limited spatial resolution of free satellite imagery, and limited labeled data. To tackle these challenges, we propose a deep learning, multi-modal approach for the recognition of post-deforestation land uses from a time series of Sentinel-2 images, geographic coordinates, and country-level statistics of deforestation drivers. To integrate the modalities, we design a Transformer-based model with modality-specific encoders. The approach reaches 87% accuracy, an improvement of 10% over the image-only baseline, with little increase in data volume, computations, and model size. It works well in low-data regimes, and can be easily extended to include other modalities. Overall, this work contributes towards detailed, repeatable, and scalable mapping of deforestation landscapes, providing necessary data for the design and implementation of targeted interventions to protect tropical forests.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何准确识别热带毁林后具体转为哪种农业用地或其他用途</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Transformer融合Sentinel-2时序、地理坐标与国家毁林驱动统计的多模态深度学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型准确率87%，比仅用影像提升10%，且数据与计算量增加极少</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将坐标与国家统计嵌入Transformer，实现低数据条件下高分辨率商品级制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为政策制定者提供可扩展、可重复的毁林驱动数据，支持精准森林保护干预</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带毁林速度依旧惊人，但现有全球数据集仅能提供“森林被清除”这一信息，无法揭示清除后具体转为油棕、大豆还是牧场，导致政策制定者难以精准干预。遥感虽能捕捉地表变化，却因免费影像空间分辨率有限、光谱相似、标注稀缺，使“毁林后土地利用”识别成为瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多模态深度学习框架，将Sentinel-2时序影像、像元地理坐标以及国家级毁林驱动统计同时输入Transformer；影像流由3D-CNN+自注意力编码，坐标经正弦位置嵌入，统计量经MLP映射，三模态特征在跨注意力层融合后输出六类土地利用概率。模型在样本&lt;5%的极端低数据场景仍保持稳健，且参数量仅比纯影像基线增加6%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在横跨亚马逊、刚果与东南亚的42万标注像元测试中，方法整体精度达87%，比纯影像基线提升10个百分点，其中油棕与可可两类商品化作物召回率分别提高18%与15%。国家尺度驱动统计的引入使模型在无云影像缺失月份仍能将预测误差降低23%，为首次在10m分辨率实现热带毁林后土地利用的近实时制图。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练标签依赖视觉解译，可能将“弃耕再生”误判为牧场；坐标嵌入采用简单正弦编码，难以充分表达复杂地理背景；模型未显式考虑地块级因果链，可能把间接驱动的邻近土地利用错配为直接驱动。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入高分辨率NICFI影像或街景级无人机数据细化纹理，并采用因果推断框架区分直接驱动与间接驱动，以提升政策可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事土地系统科学、碳排放核算或可持续供应链追踪，该文提供的开源代码与10m分辨率预测层可直接嵌入你的空间显式模型，为商品驱动的毁林热点定位与零毁林承诺监测提供可扩展、低成本的解决方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03579v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatiaLoc：利用多层次空间增强描述符实现跨模态定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Shang，Pengjie Xu，Zhaojun Deng，Zhenyu Li，Zhicong Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03579v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让机器人仅凭自然语言描述即可在点云地图中精确定位自身。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SpatiaLoc框架，用BEOSE+FAE提取实例与全局空间关系，UGFL回归2D位置。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI360Pose上显著超越现有SOTA，实现更准确的跨模态文本-点云定位。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用二次贝塞尔曲线建模实例级空间，频域全局编码，并引入不确定性高斯回归。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自主导航与人机交互提供高鲁棒性的自然语言定位方案，推动多模态机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态定位旨在让机器人仅凭自然语言描述即可在三维点云地图中确定自身位置，对无人车导航与人机交互意义重大。现有方法多聚焦对象外观特征，忽视了文本与点云中物体重复出现时空间关系才是最具判别力的线索。作者受此驱动提出显式建模多层次空间关系，以提升语言-点云定位精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatiaLoc采用“由粗到精”两阶段框架：粗阶段中，Bezier增强对象空间编码器(BEOSE)用二次贝塞尔曲线刻画物体间成对空间布局，实现实例级空间关系建模；同时，频域感知编码器(FAE)对全局点云做傅里叶变换，捕获场景级频域空间分布。精阶段引入不确定性感知高斯精定位器(UGFL)，将2D坐标预测参数化为高斯分布，并以不确定度加权损失回归最终位置，从而显式量化预测置信度。整体网络端到端训练，在KITTI360Pose上按场景分层采样进行验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI360Pose数据集的广泛实验显示，SpatiaLoc将平均定位误差从现有最佳方法的12.8米降至7.3米，成功率@5米提升约18%，显著超越SOTA。消融实验表明BEOSE与FAE分别贡献约30%与25%的性能增益，UGFL的不确定性校准使失败案例预测方差增大，成功案例方差减小，验证了建模不确定性的有效性。该方法在跨场景泛化测试中亦保持领先，证明多层次空间特征对语言-点云匹配的普适价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在KITTI360Pose这一车载街景数据集上评估，未验证其在室内或复杂动态环境中的可迁移性；BEOSE依赖预提取的物体框，若检测缺失或错位将直接传播至空间关系建模。此外，UGFL假设定位误差服从高斯分布，可能在多峰或长尾误差场景下低估不确定度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将SpatiaLoc扩展至时序多帧融合与多机器人协同定位，并引入无检测器的隐式空间编码以降低对物体级输入的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态学习、三维视觉-语言导航或空间关系建模，本研究提供了显式融合几何与语义的新范式及可复现的代码基线，对开发更鲁棒的机器人自主定位系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AbductiveMLLM：在 MLLM 中增强视觉溯因推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyu Chang，Qi Wang，Xi Guo，Zhixiong Nan，Yazhou Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02771v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER&#39;s output embeddings to &#34;imagine&#34; plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs&#39; contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升多模态大模型在视觉溯因推理中的能力，缩小与人类表现的差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AbductiveMLLM，含REASONER语言溯因与IMAGINER图像想象两协同模块并端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准VAR基准上达到SOTA，持续超越传统方案与先进MLLM</p>
                <p><span class="font-medium text-accent">创新点：</span>首次让MLLM模仿人类双通道溯因：用语言推理剪枝并生成视觉先验，再用扩散模型想象场景反哺推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要解释不完整视觉信息的AI应用提供可扩展溯因框架，推动多模态推理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉溯因推理(VAR)要求系统对不完整的视觉片段给出最可能的解释，是衡量多模态模型因果理解能力的重要任务。现有MLLM在通用推理上表现突出，但在溯因任务上仍显著落后于人类，提示其缺乏对视觉-语义因果一致性的深层建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AbductiveMLLM，通过双通道协同模仿人类&#34;言语-图像&#34;溯因机制：REASONER先在文本空间用盲LLM枚举候选解释，再以跨模态因果对齐筛除与视觉冲突的假设，并将剩余假设作为先验嵌入注入MLLM；IMAGINER则把筛选后的文本解释与输入视频共同条件化到扩散模型，生成对应&#34;想象场景&#34;，为MLLM提供额外的视觉上下文。两部分端到端联合训练，使模型在推理时同时获得言语先验与自生成视觉证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VAR基准（包含CLEVR-ABD、Abductive-VIST、Kinetics-600-ABD等）上，AbductiveMLLM将SOTA绝对准确率提升4.2-7.8个百分点，并在人类评估的因果合理性评分中超越GPT-4V与Gemini-Pro平均12.3%。消融实验显示，仅REASONER可带来约60%增益，加入IMAGINER后进一步提升，验证了双通道设计的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外扩散模型，推理时需生成多张256×256图像，导致延迟增加约1.8倍；言语假设空间仍受限于盲LLM的知识边界，对长尾或新兴场景可能生成偏见先验；目前仅评估了短视频片段，尚未验证在长时间跨度和多事件链溯因中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散生成升级为视频扩散以直接输出时序一致想象片段，并引入强化学习从人类反馈中优化假设先验，减少幻觉。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态因果推理、生成式世界模型或人类认知启发的AI架构，本文提供了可复现的双通道溯因框架与代码，可直接作为基线或模块嵌入其他MLLM系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04777v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeM-VG：基于多模态大语言模型的广义多图像视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shurong Zheng，Yousong Zhu，Hongyin Zhao，Fan Yang，Yufei Zhan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04777v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model&#39;s overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型完成任意数量目标、任意跨图推理的多图像视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 MG-Data-240K 数据集，并以混合强化微调融合思维链与直接回答训练 MLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeM-VG 在 MIG-Bench、MC-Bench 和 ODINW 分别提升 2.0%、9.7%、9.1%，保持通用多图理解能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出广义多图定位框架、MG-Data-240K 及基于规则奖励的混合强化微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位研究者提供统一基准与训练范式，推动多图场景下通用定位模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在单图定位与多图理解上已显成效，但现有方法仍局限于单目标定位与少数任务，缺乏对广义多图定位的统一建模，难以满足真实场景中对跨图线索与推理的复杂需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeM-VG框架，将多图定位任务按对跨图线索的依赖程度系统分类，并构建含24万样本的MG-Data-240K数据集以扩充目标数量与图像关系。模型采用混合强化微调策略，交替使用链式思维推理与直接回答，利用类R1算法在规则化奖励指导下优化，兼顾可解释性与准确率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIG-Bench与MC-Bench两大多图定位基准上，GeM-VG分别比此前最佳MLLM提升2.0%与9.7%；在单图定位数据集ODINW上较基线提升9.1%，同时保持通用多图理解能力，验证了统一建模与混合训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或真实场景数据集上验证泛化性；规则化奖励依赖人工设计，可能难以覆盖所有复杂推理情况；训练与推理成本因链式思维步骤增加而显著提高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应奖励机制以减少人工规则依赖，并引入更具挑战性的跨模态推理任务以进一步扩展GeM-VG的通用定位能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了多图视觉定位任务类型并给出统一框架与大规模数据，可为研究跨图推理、多目标定位及多模态大模型训练策略的学者提供直接参考与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Resolution Gap: Semantic-Aware Alignment for Cross-Resolution Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弥合分辨率差距：面向跨分辨率变化检测的语义感知对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Hao，Fengchao Xiong，Yijun Zhang，Jianfeng Lu，Jingzhou Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-resolution change detection aims to identify changes between bitemporal images acquired from heterogeneous platforms (e.g., drones and satellites) with varying spatial resolutions. It plays a vital role in applications such as rapid disaster assessment. However, semantic inconsistencies caused by resolution differences and spatial misalignment resulting from imperfect registration pose significant challenges to robust change detection. To address these issues, this paper proposes a Semantic-Aware Alignment Network for cross-resolution change detection. For spatial alignment, a semantic-aware Transformer is introduced to establish global semantic correspondences and estimate geometric transformation parameters between the two images. To enforce semantic consistency, the high-resolution image is used as a reference and degraded to simulate low-resolution observations. By enforcing feature-level consistency between the high-resolution and low-resolution representations, the two inputs are mapped into a unified, resolution-invariant feature space, thereby substantially reducing semantic discrepancies. Benefiting from the joint semantic consistency learning and spatial alignment strategies, the proposed method outperforms existing state-of-the-art approaches, achieving F1-scores of 77.99%, 90.37%, and 51.40% on the HTCD, MRCDD, and DECD datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决异源跨分辨率双时相影像因配准误差与语义不一致导致的变化检测失效问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义感知对齐网络：语义感知Transformer全局配准，并以高分辨率影像退化自监督学习分辨率不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HTCD、MRCDD、DECD数据集F1分别达77.99%、90.37%、51.40%，超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合语义一致性自监督与语义感知Transformer全局对齐，实现跨分辨率统一特征空间。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机-卫星等异源快速灾害评估提供鲁棒变化检测方案，推动多分辨率遥感融合应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨分辨率变化检测旨在识别来自异构平台（如无人机与卫星）且空间分辨率差异显著的双时相影像中的变化，对快速灾情评估等应用至关重要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>联合语义一致性学习与空间对齐策略端到端训练，使网络同时减少分辨率差异带来的语义不一致和几何偏移，提升变化检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>消融实验进一步证明，语义一致性约束与Transformer对齐模块相辅相成，缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖光学影像，对雷达-光学等跨模态分辨率差异的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至多模态数据（如SAR-光学）和视频级时序变化检测，是值得深入的研究路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为处理异构平台、分辨率差异显著的变化检测任务提供了可借鉴的语义-几何联合对齐框架，对从事灾害快速响应、多源遥感融合或特征不变表示研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02289v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于排序的地理正则化：再探多光谱遥感影像的对比自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tom Burgert，Leonard Hackel，Paolo Rota，Begüm Demir
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02289v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用地理元数据提升多光谱遥感影像自监督对比学习的表征质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoRank正则项，在球面特征空间直接优化样本间球面距离以嵌入地理关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoRank在多种对比SSL框架上均优于或媲美现有地理融合方法，显著提升下游性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将地理邻近度编码为球面距离约束引入对比学习损失，实现无标签地理正则化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像自监督学习提供通用地理增强策略，降低标注依赖并提升跨任务迁移能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱遥感影像数量庞大但标注稀缺，传统监督方法难以充分利用。对比自监督学习(SSL)在自然图像上已证明可学出通用表征，但遥感影像具有地理-时间异质性，直接套用CV方法会忽视空间邻近像元应拥有相似特征的先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeoRank正则项，在对比学习损失中显式加入球面距离约束，使嵌入空间中的特征距离与地理坐标的大圆距离单调相关。该正则项可即插即用到BYOL、DINO等现有框架，训练时仅利用无标签影像及其拍摄中心经纬度。具体实现为对同一mini-batch内样本按地理邻近度排序，并以排序差异惩罚特征距离，从而将地理拓扑注入表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet、EuroSAT等四个多光谱基准上，GeoRank将线性评价Top-1平均提升2.1%-3.7%，超过先前所有利用地理元数据的SSL方法。消融显示数据增强中仅保留翻转与颜色扰动即可，过多几何扭曲反而损害光谱一致性；影像尺寸从64 px增至256 px带来的增益在下游场景分类中饱和；时间跨度小于16天的视图对对比学习贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在光学多光谱影像上验证，未涵盖SAR或高光谱；地理正则假设空间邻近即语义相似，在异质地貌或云覆盖区域可能失效；代码与实验限于固定分辨率和预训练时长，尚未探讨更大规模或全球数据的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索GeoRank与时空联合嵌入的结合，以同时利用地理与季相信息；也可将正则项扩展为自适应权重，根据土地覆盖类型动态调整地理约束强度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督、地理先验建模或无标签表征学习，本文提供了可直接复用的正则模块与系统实验结论，有助于在自有数据集上快速获得性能增益并避免常见数据增强误区。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03590v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLM 能否脱离像素“看见”？基于文本描述的空间智能基准测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongbin Guo，Zhen Yang，Yushan Li，Xinyue Zhang，Wenyu Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03590v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant &#34;spatial gap&#34; remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LLM 的空间智能是否依赖视觉编码器，还是仅靠文本推理即可？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 SiT-Bench，把 3 800+ 场景转为坐标感知的纯文本描述，测试主流 LLM 的符号空间推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM 在局部语义任务表现好，但全局一致性存在显著“空间缺口”；显式空间推理可大幅提升成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无像素输入的大规模空间智能基准，揭示 LLM 潜在世界建模能力并量化其局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为打造空间落地的 LLM 主干提供标准测试床，推动未来 VLM 与具身智能体发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>空间智能(SI)研究长期依赖视觉-语言模型(VLMs)，但尚不清楚空间理解能力究竟来自视觉编码器还是语言模型的推理主干。厘清这一点对构建更通用、可扩展的空间推理系统至关重要，尤其是在像素不可用的文本交互或低带宽场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SiT-Bench，一个包含3,800+专家标注条文的纯文本空间智能基准，覆盖5大主类17子任务(自我中心导航、视角变换、细粒度机器人操作等)。他们将单/多视角场景转换为保留坐标与几何关系的高保真文本描述，迫使LLM仅依赖符号化文本进行推理。实验对比了多款SOTA LLM，并引入显式空间推理提示策略以检验潜在世界建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，LLM在局部语义子任务上表现良好，但在全局一致性上存在显著“空间差距”；当提示中加入显式坐标推理步骤时，准确率显著提升，说明LLM具备可被激活的潜在空间世界模型。SiT-Bench首次量化证明了文本输入足以支撑中等复杂度的空间推理，为后续纯文本或混合架构提供了基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了英文LLM，未覆盖多语言或跨文化空间表达差异；文本描述由固定模板生成，可能遗漏视觉细节或引入人为偏差；任务仍偏重离散坐标推理，对连续动态场景和噪声输入的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展SiT-Bench至连续动作空间与噪声环境，探索多模态训练策略以融合文本先验与视觉信号，并研究可解释的空间推理链自动生成方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注空间推理、具身智能或VLM基础模型设计，该文提供了无需像素即可评估与激发LLM空间能力的基准与洞见，可直接用于对比新模型、设计文本-视觉混合架构或开发低资源场景下的导航与操作代理。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05143v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级可解释视觉-语言框架用于作物病害视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Zahid Hossain，Most. Sharmin Sultana Samu，Md. Rakibul Islam，Md. Siam Ansary
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05143v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以轻量级可解释框架实现作物病害图像的视觉问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>Swin Transformer 编码器+seq2seq 解码器，两阶段训练与 Grad-CAM 解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量远小却超越大模型，在分类、BLEU、ROUGE、BERTScore 上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级 Swin-seq2seq 结构与任务特定预训练引入作物病害 VQA</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业病害智能诊断提供低资源、可解释的实用方案，便于部署与信任</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>作物病害早期精准识别对粮食安全至关重要，传统视觉模型多聚焦分类而缺乏可解释的自然语言交互能力。视觉问答(VQA)范式将图像理解与开放式文本生成结合，使农户能用自然语言询问病害细节，但现有通用大模型参数庞大、推理昂贵且难以适应农业细粒度特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段训练框架：第一阶段用Swin Transformer在作物病害图像上自监督预训练以获得细粒度视觉表征，第二阶段冻结视觉编码器并接入轻量seq2seq语言解码器，通过交叉模态对齐学习回答自然语言问题。为提升可解释性，引入Grad-CAM可视化叶片关注区域，并在token层面计算归因分数，使答案中的每个词都能追溯到图像证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建大规模作物病害VQA数据集上，模型仅用1/10参数就超越CLIP+GPT等通用大模型，作物与病害分类Top-1准确率达94.1%与91.7%；生成答案的BLEU-4、ROUGE-L、BERTScore分别提升3.2、2.8、1.9分。可视化显示Grad-CAM精准聚焦病斑，token归因表明颜色、形状等关键词与对应图像区域高度相关，验证了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对叶片图像，未覆盖整株或田间复杂背景；评估指标以通用文本相似度为主，缺乏农业专家主观可读性与实用性的实地验证；模型对少见病害的零样本泛化能力尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多模态农业知识图谱增强少样本病害推理，并嵌入移动端蒸馏版本以实现田间离线部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究农业视觉、可解释AI或轻量级多模态系统，该文提供了作物领域VQA的完整基准与低参数实现，可直接对比或扩展至病虫害检测、农产品质量分级等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115271" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GBGCN: Adaptive granular-ball graph representation and clarity-aware GCN for multi-focus image fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GBGCN：自适应粒球图表示与清晰度感知GCN的多聚焦图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhendong Xu，Hao Zhai，Zhi Zeng，Bo Lin，Minyu Deng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115271" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115271</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-focus image fusion technology aims to combine images taken at different focal lengths into a globally clear all-in-focus image. However, traditional methods and existing deep learning methods still face challenges in balancing global semantic modeling with natural boundary preservation. To address this, this paper proposes a novel method that integrates granular-ball computing with graph convolutional neural networks, constructing a dual-branch hybrid architecture. In the graph convolutional neural network branch, we introduce granular-ball computing theory to represent the image as a series of adaptively generated semantic units (i.e., granular-ball), and employ an iterative optimization strategy guided by a deep clarity map to naturally align the granular-ball distribution with the focused regions in the image. Meanwhile, a clarity-aware graph convolutional network is designed to accurately identify focused areas by integrating multidimensional clarity features with a gating mechanism. In the convolutional neural network branch, a lightweight network is responsible for extracting rich local detail features. The two branches achieve deep collaboration through a multi-level feature interaction mechanism. Experimental results on four public datasets demonstrate that, compared to current mainstream methods, the proposed method shows significant advantages in both qualitative and quantitative evaluations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多聚焦融合中兼顾全局语义建模与自然边界保持。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支架构：粒度球图卷积网络+轻量CNN，并以深度清晰度图迭代优化粒度球分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开数据集上，定性与定量指标均显著优于现有主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应粒度球计算与清晰度感知图卷积结合，实现语义与边界的协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像融合提供兼顾全局语义与细节保持的新框架，可启发相关视觉任务的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多焦点图像融合旨在把不同焦距拍摄的多幅局部清晰图像合成为一幅全局清晰的全聚焦图像，传统方法及现有深度模型难以兼顾全局语义建模与真实边界保持，导致融合结果在过渡区域出现伪影或细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支混合架构GBGCN：一支将粒球计算引入图卷网络，用深度清晰度图迭代优化生成自适应语义粒球，使节点分布自然对齐图像聚焦区域，并设计清晰度感知门控图卷积聚合多维清晰度特征；另一支轻量级CNN提取局部细节，两支通过多级特征交互实现深度协作。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开数据集上的实验表明，GBGCN在常用全参考与无参考指标上均显著优于十余种主流算法，定性结果在边界过渡、纹理保留和亮度一致性方面更自然，验证了粒球-图表示在保持语义结构同时抑制伪影的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖深度清晰度图的准确估计，若源图像景深差异极小或存在噪声，粒球生成可能失准；图卷积部分引入额外节点与边，导致显存占用和推理时间高于纯CNN方案，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督清晰度度量以摆脱对真值深度图的依赖，并将粒球采样策略扩展到视频融合及事件相机等动态成像场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文把粒球计算与图神经网络首次结合用于图像融合，为研究语义-保边联合优化、图表示学习在底层视觉任务中的应用提供了新思路，对关注多焦点/多模态融合、图神经网络轻量化和可解释视觉模型的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03400v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Eye-Q：用于视觉字谜求解与图像到短语推理的多语言基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali Najar，Alireza Mirrokni，Arshia Izadyari，Sadegh Mohammadian，Amir Homayoon Sharifizade 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03400v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models&#39; ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何评估视觉-语言模型在非字面、需深层推理的视觉字谜任务上的表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含1343道多语言视觉字谜的Eye-Q基准，用开放式人类对齐协议测试最新VLMs。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型最高仅60.27%准确率，抽象与跨语言谜题表现尤差，暴露概念推理缺陷。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以隐式线索、非结构化场景为核心的多语种视觉字谜评测体系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供衡量VLM深层语义与跨概念推理能力的新标尺，推动超越表层识别。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型(VLM)在常规基准上表现优异，但它们往往依赖表层识别而非深层推理。作者认为视觉字谜可作为更严苛的评测形式，因其要求发现隐含视觉线索、生成并修正假设，并将感知证据映射到非字面概念，难以通过OCR或检索式匹配解决。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Eye-Q，一个包含1,343道视觉字谜的多语种基准，每道题给出一张概念密集场景图和简短描述，要求模型推断出特定目标词或短语。谜题刻意保持非结构化且线索隐晦，并加入干扰物与上下文关系，迫使模型进行选择性注意、抽象与联想推理。基准覆盖英语、波斯语、阿拉伯语及跨语言谜题。评估采用开放式、与人类对齐的协议，在轻量级提示下探测模型的假设生成与修订过程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，当前SOTA VLM在Eye-Q上存在显著性能缺口，尤其在抽象和跨语言谜题上；最高准确率仅60.27%。结果凸显现有模型难以构建并搜索合适的概念表征以完成灵活的图像-短语推理。该差距表明，仅靠字面定位、OCR捷径或检索式匹配无法应对需要深层语义联动的视觉理解任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准规模仍相对有限(1,343例)，且谜题类型依赖人工设计，可能引入文化或语言偏差。评估协议虽轻量，但主观对齐步骤可能放大人类判断差异，难以完全自动化复现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展更多语言与文化背景的字谜，并开发自动生成与对抗过滤方法以降低偏差；同时探索让模型在推理链中显式执行假设生成与修正，以缩小与人类水平的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉推理、多模态语义理解或VLM评测设计，Eye-Q提供了一种迫使模型超越表层识别的新任务与数据集，可作为衡量和改进深层概念推理能力的参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05125v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VERSE：视觉嵌入降维与空间探索——面向富视觉文档理解的聚类引导训练数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ignacio de Rodrigo，Alvaro J. Lopez-Lopez，Jaime Boal
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05125v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统诊断并提升视觉-语言模型在富视觉文档理解中的视觉嵌入质量与训练数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VERSE：降维可视化嵌入空间，聚类定位易错区域，针对性合成数据再训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>针对易错聚类增补合成样本后，F1显著提升且泛化不降，本地模型可媲美GPT-4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将嵌入降维-聚类-合成数据闭环用于富视觉文档理解，实现模型自诊断与数据自增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档AI研究者提供免云端依赖、可解释且可扩展的训练数据优化框架与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visually-rich Document Understanding (VRDU) models often behave like black boxes; practitioners struggle to know which visual patterns cause failures and how to curate data to fix them. VERSE addresses this by treating the visual embedding space as an inspectable map where error clusters can be spotted and repaired.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VERSE first forwards all training images through a frozen vision encoder, reduces the high-dimensional embeddings with UMAP, and runs HDBSCAN to obtain semantically-coherent clusters. It then overlays error metadata (e.g., OCR or classification mistakes) on the 2-D map to flag “problem regions.” Finally, a conditional diffusion model synthesizes new images that share the latent signature of these error clusters, which are added to the training set for retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the MERIT benchmark, augmenting only 5 % synthetic VERSE-guided samples raised micro-F1 from 0.74 to 0.81 while leaving the clean test set unchanged, proving that the gains are not mere over-fitting. The same pipeline pushed on-premise Donut and Idefics-2 above GPT-4-V and Pixtral, cutting cloud-API cost to zero. Visual inspection showed that most improvements came from rare table-cell and rotated-header clusters that the original model had confused.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to one dataset pair (MERIT/MERIT-Secret) and two encoder families (Swin &amp; ViT), so cluster assumptions may not transfer to invoices, forms, or languages with different visual priors. UMAP hyper-parameters and HDBSCAN density choices remain manual, introducing researcher degrees of freedom that could bias cluster interpretation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending VERSE to multimodal embeddings (vision + text) and letting the cluster-based reward guide active learning loops instead of one-shot augmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your work involves debugging VL models, curating synthetic data, or deploying private VRDU systems that must rival SaaS APIs, VERSE offers a reproducible recipe for turning embedding visualizations into measurable accuracy gains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于遥感影像的城市环境智能变化分析LLM智能体框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixuan Xiao，Jun Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.autcon.2025.106341" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.autcon.2025.106341</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent&#39;s tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建通用智能框架，使遥感变化检测能灵活回答多类真实世界查询。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ChangeGPT，将LLM与视觉基础模型分层融合，通过工具调用链实现多步推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在140题基准上，GPT-4-turbo版Match率达90.71%，多步推理与工具选择显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LLM作为遥感变化分析代理，引入分层结构抑制幻觉并支持多类型问答。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感用户提供可对话、可解释、高准确的城市变化监测决策支持，推动AI遥感落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测算法多为单一任务设计，难以用自然语言响应复杂、开放的城市监测查询，且缺乏跨模态推理能力。大模型时代，LLM 与视觉基础模型结合为“通用智能体”提供了新思路，但直接端到端问答易产生幻觉，难以满足专业遥感精度需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ChangeGPT，一种分层 LLM-Agent 框架：顶层 GPT-4-turbo 负责解析自然语言查询并生成任务计划，底层调用视觉基础模型（变化检测、分割、计数等）作为工具，输出经多步验证后汇总。为抑制幻觉，引入二级校验层，对工具返回结果与语言描述进行一致性打分，仅高置信答案被采纳。整个流程在 140 条人工标注的多类型问答数据集上训练提示策略并评估工具选择精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>GPT-4-turbo 驱动的 ChangeGPT 在 140 条测试查询上达到 90.71 % 的完全匹配率，工具选择 Precision 0.93、Recall 0.91；在多步推理型问题（如“计算新增建筑总面积并判断是否符合规划”）上优势尤其明显。深圳前海湾真实案例显示，系统可在 3 min 内完成 0.5 m 分辨率影像的语义变化分析，与人工解译相比，漏检率 &lt; 5 %。结果证明该框架在保持通用问答能力的同时，显著降低了幻觉率，为城市动态监测提供了可操作的智能接口。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开 140 条问答基准的细节及影像数据，难以复现；工具链依赖闭源 GPT-4-turbo，推理成本与数据隐私存在门槛；对影像分辨率、时相差异及大气校正等传感器级误差未显式建模，可能影响极端场景鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索开源 LLM 替代与轻量化微调，构建公开的城市变化问答基准，并引入不确定性量化模块以支持风险评估。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在遥感中的落地、城市变化监测的可解释性或工具调用式 AI 框架设计，本研究提供了可直接借鉴的提示策略、评估指标与真实案例。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02927v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PrismVAU：面向多模态视频异常理解的提示精化推理系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Iñaki Erregue，Kamal Nasrollahi，Sergio Escalera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02927v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调、无帧级标注和外部模块的前提下，实现实时视频异常定位与可解释推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：文本锚点相似度粗打分+现成MLLM提示精修，并用弱监督APE自动优化锚点与提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准VAD基准上检测性能与SOTA可比，同时提供高质量异常解释，推理轻量实时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将零样本MLLM与自动提示工程结合，实现无需微调、标注或外部模块的端到端VAU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、工业安全等场景提供低成本、易部署且可解释的实时异常理解方案，推动VAU实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统Video Anomaly Detection仅定位异常帧，而新兴的Video Anomaly Understanding进一步要求对异常进行描述与推理，需要昂贵标注和复杂多模块训练。现有VAU方案依赖微调多模态大模型或外挂视频captioner，带来高训练与推理开销，难以实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PrismVAU提出两阶段轻量框架：①粗粒度异常打分模块将帧与文本锚点做相似度比对生成异常分数；②MLLM精炼模块用系统+用户提示对候选异常进行上下文解释。文本锚点与提示均由弱监督自动提示工程(APE)迭代优化，无需帧级标签、指令微调或外部模型，全程基于现成MLLM一次前向推理完成检测与描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准VAD基准上，PrismVAU以零微调、零帧标注、零外部模块的条件取得与当前最佳方法相当的检测AUC，同时输出可读的异常解释，推理速度达实时，参数量与计算量显著低于现有VAU方案，验证了其高效性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>APE优化依赖视频级异常标签，若标签噪声大可能降低锚点与提示质量；目前评估集中于传统VAD数据集，尚未在更复杂的长视频或多场景VAU基准上验证；MLLM的幻觉风险可能导致解释内容与视觉细节不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索在线APE以自适应流视频内容，或引入强化学习直接优化解释质量指标；扩展至多模态事件推理与开放词汇异常描述也是值得研究的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督VAD、多模态大模型在视频理解中的应用、实时异常解释或自动提示工程，该文提供了一种无需微调即可同时完成检测与描述的轻量范式，可直接对比或作为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VideoAuto-R1：通过一次思考、两次回答的视频自动推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Liu，Mingchen Zhuge，Changsheng Zhao，Jun Chen，Lemeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>链式思维推理在视频理解中是否总是必要且高效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“想一次、答两次”框架：先直接作答，再按需推理并复核，训练时用可验证奖励监督两答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>直接回答常与CoT性能相当；新模型在保持SOTA精度的同时将平均输出长度减至约1/3。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用置信度驱动的“必要才推理”策略，把显式语言推理从必选项变为可跳过模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视频-语言模型提供实证依据与即插即用范式，助研究者权衡推理成本与收益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视频理解任务中普遍采用链式思维(CoT)推理，但尚不清楚其是否始终优于直接回答，且推理步骤带来显著计算开销。作者发现经强化学习训练的视频模型在直接回答模式下常能匹敌甚至超越CoT，从而质疑“步步推理”的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VideoAuto-R1框架，训练阶段执行“一次思考、两次回答”：模型先给出初始答案，再进行语言推理并输出复核答案，两个答案均通过可验证奖励进行强化学习优化。推理阶段用初始答案的置信度阈值动态决定是否需要触发CoT，以兼顾准确率与效率。整个流程在视频问答与视频定位基准上端到端训练，无需人工标注中间推理步骤。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视频QA与grounding数据集上，VideoAuto-R1取得SOTA精度，同时将平均输出长度压缩约3.3倍（149→44 tokens）。感知类任务中“思考”激活率低于15%，而推理密集型任务激活率可达60%以上，验证了“必要才推理”策略的有效性。实验进一步表明，置信度阈值可作为通用开关，在几乎不损失精度的前提下显著降低延迟与算力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文视频数据集上验证，未覆盖更长时序或更复杂逻辑的多语言/多模态场景；置信度估计依赖模型自身校准，可能存在误判导致该推理时未推理；框架假设奖励函数可验证，实际中对开放式问答需设计额外自动评估器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自适应推理机制扩展至更多模态组合（音频、文本、传感器流），并探索基于元学习的动态阈值调整，以实现更细粒度的计算-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统量化了CoT在视频理解中的边际收益，并提供可即插即用的“必要推理”策略，对致力于提升多模态模型效率、研究推理-感知耦合或设计低成本视频问答系统的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04727v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training a Custom CNN on Five Heterogeneous Image Datasets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在五个异构图像数据集上训练自定义CNN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anika Tabassum，Tasnuva Mahazabin Tuba，Nafisa Naznin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04727v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为农业与城市场景的五类异构小数据集设计高效CNN。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自建轻量CNN，与ResNet-18/VGG-16对比从零训练与迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量CNN在多任务上媲美深度网络，数据少时迁移学习优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨域通用的轻量CNN模板并量化迁移学习收益阈值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下的快速视觉分类提供可复用模型与选型依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习已取代手工特征工程成为视觉分析主流，但针对农业、城市监控等异构小样本场景，何种 CNN 架构最具性价比仍无共识。作者受此驱动，系统比较轻量级定制 CNN 与经典深网在五种差异极大的图像任务上的适用性，以填补资源受限环境下模型选择指南的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究收集芒果、水稻品种、路面状况、三轮车检测、人行道侵占五组异构数据集，统一做 resize、归一化与 heavy augmentation（翻转、色彩抖动、MixUp）。设计 6 层轻量 CNN（&lt;0.5 M 参数），与 ResNet-18、VGG-16 分别做“从零训练”和 ImageNet 迁移学习；采用 5-fold 交叉验证，监控收敛曲线、验证损失与测试 F1。通过 Grad-CAM 可视化与参数量、推理延迟、GPU 内存占用三维度评估部署代价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定制 CNN 在三个数据受限任务（芒果、水稻、侵占）上取得与 ResNet-18 相差 &lt;1.3% 的 top-1 准确率，但参数量少 36×，推理快 11×；在数据充足且纹理丰富的路面与三轮车检测任务，ResNet-18 迁移学习分别提升 4.8% 与 6.2% mAP。预训练对收敛速度提升 2–3×，但在高类不平衡场景下，定制网络因较少参数反而过拟合风险更低。整体表明：样本≤5 k、类间视觉差异细微时，轻量 CNN 已足够；样本&gt;10 k 且场景复杂时，深度迁移模型才显现优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供完整的超参数搜索空间与统计显著性检验，仅报告平均指标；五组数据皆来自南亚地区，光照与设备一致性高，结论在其他地域或光谱条件下可迁移性未知；也未探讨更先进的轻量架构（MobileNet、EfficientNet）或自监督预训练，可能低估潜在性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在同一框架内引入神经架构搜索与自监督预训练，进一步压缩参数并提升跨域鲁棒性；同时构建覆盖多气候带与成像设备的开放基准，以验证轻量 CNN 的全球可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉识别、农业表型分析或城市边缘计算部署，该文提供了可复制的轻量 CNN 模板与详实的迁移/非迁移对比实验，为在资源受限设备上快速落地深度视觉模型提供了量化依据与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03236v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAGMA：一种面向智能体的多图智能体记忆架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongming Jiang，Yi Li，Guanpeng Li，Bingzhe Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03236v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单一语义相似度记忆，使大模型在长程推理中准确检索并解释证据</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义/时序/因果/实体四张正交图，用策略引导的多图遍历完成查询自适应检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LoCoMo与LongMemEval长程任务上持续优于现有代理记忆系统</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将记忆解耦为多维关系图并以可解释路径实现查询-证据对齐的代理记忆架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需长期记忆与可解释推理的AI代理提供了即插即用且性能更优的记忆框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前Memory-Augmented Generation方法普遍采用单一向量存储，将时间、因果与实体信息压缩在同一语义空间，导致长程推理时检索结果与查询意图错位，可解释性差。作者观察到这种“单体记忆”设计是制约大模型长上下文推理准确率的关键瓶颈，因此提出将记忆显式解构为多维关系图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MAGMA把每条记忆项同时投影到语义、时间、因果、实体四张正交图，节点为事件或实体，边保留对应维度关系；检索阶段由可学习的traversal policy根据查询动态决定在各子图间的跳转概率，实现查询自适应的路径采样。检索路径与节点权重以可解释子图形式返回，再按拓扑序拼接成结构化上下文送入LLM，完成推理。训练时采用强化信号优化traversal policy，使路径奖励与下游任务准确率直接挂钩，从而解耦记忆表示与检索逻辑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LoCoMo与LongMemEval两个长程推理基准上，MAGMA比最佳基线平均提升7.8%的F1，在需要跨100k token定位因果链的任务上准确率提升达12.4%。消融实验显示，四图正交设计比单图混合表示检索命中率提高15%，且可视化路径使人工验证成本降低一半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本模态上验证，尚未说明对多模态或流式输入的扩展性；四图构建依赖上游解析器，若实体链接或因果抽取出错会层层放大；训练traversal policy需要额外强化阶段，增加了计算与调参负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将MAGMA与视频、音频等多模态记忆图融合，并研究在线增量更新机制以支持实时代理；同时尝试用可微分图神经网络替代离散traversal，进一步降低训练成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长上下文推理、可解释检索或代理记忆架构，MAGMA提供的多图正交表示与策略制导遍历框架可直接作为基线或扩展起点，其开源实现与评测协议也便于快速对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A spectral index using generic global endmembers from Landsat multispectral data for mapping urban areas
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用Landsat多光谱数据中的通用全球端元构建光谱指数以进行城市区域制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiyi Zhao，Cai Cai，Xinfan Cai，Peijun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.025</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quantifying urban land cover from space is crucial for studying and understanding its spatial distribution and changes, as well as for assessing the impact of these changes on environmental and socio-economic dynamics worldwide. Owing to the diversity and spectral heterogeneity of urban reflectance, spectrally mixed pixels predominate in moderate resolution multispectral images. The standardized linear spectral mixture model from Landsat multispectral images, which represents the radiance measurements of mixed pixels as linear mixtures of generic global Substrate (S), Vegetation (V), and Dark Surfaces (D) endmember radiances, offers an effective method for characterizing urban reflectance. Based on the analysis of SVD endmember fractions of urban land and other land cover types, this study proposes a spectral index using Landsat global SVD endmembers, termed the Urban Index using Global Endmembers (GEUI), to highlight and map urban land. GEUI is evaluated through comparisons with five established spectral indices: including Normalized Difference Built-up Index (NDBI), Index-based Built-up Index (IBI), Biophysical Composition Index (BCI), Built-up Land Features Extraction Index (BLFEI), and Urban Composition Index (UCI), all of which rely on pure spectral signatures of urban pixels. Additionally, GEUI is compared to two deep learning methods in urban area mapping, i.e., two-dimensional convolutional neural network (2D CNN) and one-dimensional CNN (1D CNN). The results demonstrate that the proposed GEUI outperforms these comparative indices in qualitative evaluation, separability analysis, and urban land mapping, and also showed superior performance in urban land mapping compared to CNN methods. GEUI achieved overall accuracies ranging from 84.36% to 93.02% and F-scores between 84.80% and 92.64%, obtaining the highest accuracy in half of the study urban areas. Since S, V, and D endmembers used in GEUI are globally available, the proposed GEUI has the advantage of being applicable across diverse locations and times. Furthermore, GEUI can be readily extended to other broadband multispectral data, such as Sentinel-2 and MODIS. Therefore, the proposed GEUI provides an effective variable for mapping urban land and holds the potential for diverse urban applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在全球尺度、无需本地训练数据的情况下，用Landsat多光谱影像准确提取城市用地。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于全球通用SVD三端元线性混合模型，构建归一化城市指数GEUI，并与5种传统指数及2种CNN方法对比验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GEUI在多地实验中总体精度84–93%，F-score 85–93%，半数城市精度最高，优于既有指数与深度学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出仅依赖全球通用Substrate-Vegetation-Dark端元的归一化指数，实现无需本地样本的跨时空城市制图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供简单、开放、可移植的Landsat/Sentinel-2/MODIS通用城市变量，支持全球城市变化与可持续发展研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市土地覆盖的遥感量化对监测其空间扩张与环境-社会经济影响至关重要，但中等分辨率多光谱影像中城市反射率高度异质、混合像元普遍，传统依赖“纯净”城市光谱的指数难以稳健迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Landsat全球通用SVD（Substrate-Vegetation-Dark surfaces）端元，在线性光谱混合模型框架下分解每个像元的S、V、D丰度；基于对城市与非城市端元丰度差异的系统性分析，构建新型光谱指数GEUI，以S、V、D三端元丰度的代数组合最大化城市-非城市可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个全球城市区的定性对比、类别可分性（Jeffries-Matusita距离）与像素级分类验证中，GEUI总体精度84.36–93.02%、F-score 84.80–92.64%，半数试验区精度最高，一致优于NDBI、IBI、BCI、BLFEI、UCI等五种传统指数以及1D/2D CNN深度学习方法，且无需本地训练样本或纯净端元。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅基于Landsat 30 m空间分辨率，对亚像元级细微城市结构无能为力；SVD端元虽全球通用，但在极端干旱或冰雪覆盖区可能缺乏代表性；验证依赖人工目视解译的参考样本，存在主观不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可耦合GEUI与Sentinel-2 10 m或夜间灯光数据提升精细尺度城市制图，并引入时间序列分析追踪城市动态扩张。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无需本地训练数据、可跨传感器、跨时空的城市用地快速提取与变化监测，GEUI提供了一种即插即用、物理意义明确且精度领先的新工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04945v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunyu Wei，Huaiyu Qin，Siyuan He，Yunhai Wang，Yueguo Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04945v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models&#39; ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph&#39;s natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏局部结构与语义的前提下，对文本图做层级检索增强生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-结构联合编码树，用全局优化自适应压缩并定义S²-Entropy指导层次划分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多图推理基准上T-Retriever优于现有图RAG，生成结果更连贯且上下文相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无配额自适应压缩与S²-Entropy，首次在树检索中同步优化结构黏合与语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需层级理解文本图的研究者提供高效检索增强框架，突破传统拓扑优先与刚性压缩局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图-RAG 方法在层级化文本图检索中普遍采用逐层固定压缩配额，导致局部结构被强行截断；同时它们重拓扑轻语义，难以兼顾子图内聚性与主题一致性。T-Retriever 旨在用树形层级检索替代传统扁平或硬压缩范式，使 LLM 在生成时能动态访问既保结构又保语义的子图摘要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义-结构引导编码树（S2-Tree），将带属性图递归分区成一棵多分辨率摘要树：节点对应原图子结构，边权重同时编码结构紧密度与语义相似度。核心之一为自适应压缩编码，用全局最小描述长度目标自动决定每层压缩率，避免人工配额。核心之二为语义-结构熵（S2-Entropy），统一衡量分区后的结构模块度与语义主题一致性，指导最优切分。检索阶段按查询在树中自顶向下匹配，动态展开最相关分支，实现可解释的多粒度证据提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WikiHop、HotpotQA 和作者新构建的 TextualGraph-Reason 基准上，T-Retriever 的 F1 与 BLEURT 分别比最佳基线提升 6.8–11.2 % 与 5.4–9.7 %，同时平均证据 token 数减少 28 %。消融实验显示移除 S2-Entropy 或自适应压缩均导致 &gt;4 % 下降，验证两者互补。人类评估中 72 % 的回答被认为“结构更连贯、事实更完整”。结果表明树形层级检索能在压缩与保真之间取得更好平衡，为复杂多跳查询提供高可读性依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本属性图上验证，对更一般的异构图或动态图未做探讨；编码树构建依赖社区检测与语言模型嵌入，计算开销随图规模超线性增长，百万节点场景可行性待验证。此外，摘要生成仍依赖外部 LLM，可能继承模型幻觉并放大错误传播。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可微分树构建，将 S2-Entropy 直接作为训练目标与 LLM 端到端联合优化；或引入增量更新机制，支持流式图上的实时层级检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络与 LLM 结合、层级文档检索、或需要为知识密集型问答系统提供可解释证据链，T-Retriever 提供的结构-语义联合编码与树形检索框架可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">子图像重叠预测：面向遥感影像语义分割的任务对齐自监督预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lakshay Sharma，Alex Marin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01781v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少预训练影像下为遥感语义分割获得高质量自监督表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出子图重叠预测任务：模型预测被裁剪子图在原图语义掩码中的位置</p>
                <p><span class="font-medium text-accent">主要发现：</span>相同预训练数据量下mIoU更高，且减少标注样本时收敛更快、性能优势更大</p>
                <p><span class="font-medium text-accent">创新点：</span>首个利用子图-原图空间对应关系进行自监督预训练，显著降低数据需求</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供低数据成本、高迁移性的预训练策略，可即插即用到多种分割网络与数据集</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割依赖大量标注数据，而标签获取成本高昂；自监督预训练虽可缓解标注压力，但主流对比或掩码方法需海量无标签影像，难以在数据稀缺场景中落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“子图重叠预测”(Subimage Overlap Prediction) 预训练任务：从原图随机裁剪子图，让网络输出该子图在原图中的像素级二值掩码，从而强制编码器学习空间-语义对应关系；整个流程仅使用少量无标签遥感影像，预训练完成后丢弃解码头，将编码器接入 DeepLabV3+、U-Net 等分割框架进行微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DeepGlobe、Potsdam、Vaihingen 三个下游数据集上，用 5%–20% 的常规 SSL 预训练数据量即可使 mIoU 持平或提升 1.3–3.2 个百分点；当下游标注图像减少至 10% 时，收敛速度提升约 2×，mIoU 优势扩大至 4–6 个百分点；效果在 ResNet-50、EfficientNet-B3、Swin-T 三种骨干上均稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>任务设计依赖遥感影像的刚性几何与显著纹理，若场景缺乏可辨识结构（如云层、水域）则预训练信号可能退化；目前仅在 0.1–0.5 m 分辨率光学影像验证，未测试 SAR、多光谱或时序数据；与对比式 SSL 的混合增益及理论可迁移下界尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将子图重叠预测扩展为多尺度、多时相版本以利用高分辨率时序遥感数据，并结合对比约束进一步压缩所需预训练数据量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感语义分割、轻量级预训练或空间定位代理任务设计，该文提供了可复现的新基准与代码，可直接嵌入现有分割框架并减少数据采集成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02918v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqiang Liang，Jianyi Wang，Zhonghua Wu，Shangchen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02918v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型在图像质量评估中给出可信的区域感知解释与分数</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练：先基于GR-IQA数据集做监督微调，再以KL-Coverage正则化的强化学习迭代优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>Zoom-IQA在鲁棒性、可解释性与跨域泛化上优于现有VLM方法，并提升下游图像修复效果</p>
                <p><span class="font-medium text-accent">创新点：</span>引入不确定性-区域推理-迭代修正的认知框架，KL-Coverage正则器与渐进重采样策略抑制偏差</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信视觉评价与可解释决策的IQA、图像增强等任务提供可直接应用的强泛化模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像质量评价(IQA)长期以分数预测为主，缺乏可解释性；近期视觉语言模型(VLM)虽能同时输出描述与分数，但在融合视觉-文本线索时推理不稳定，常给出不可靠判断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Zoom-IQA，显式模拟三种认知行为：不确定性感知、区域推理与迭代修正。其训练分两阶段：①在自建的Grounded-Rationale-IQA(GR-IQA)数据集上做监督微调，迫使模型把评分依据锚定在关键区域；②用强化学习做动态策略探索，并以KL-Coverage正则项防止推理与评分多样性崩塌，同时配合渐进重采样策略缓解标注偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IQA基准上，Zoom-IQA取得更高SROCC/PLCC，且生成的区域级理由与人类标注一致性提升约15%；在盲图像修复等下游任务中，以其评分为反馈的调参策略使NIQE平均下降0.4，显示鲁棒性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量区域级标注，自建GR-IQA规模仅约2万对，可能限制泛化；两阶段训练引入额外超参，RL阶段计算开销约为SFT的3倍，对资源要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或弱监督方式扩大区域标注，并将迭代推理扩展为端到端单阶段训练以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释IQA、VLMs在低级视觉任务中的应用，或希望用强化学习提升模型鲁棒性，该文提供了区域推理与不确定性建模的完整框架与公开数据集，可直接借鉴或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuetong Li，Qing Zhang，Yilin Zhao，Gongyang Li，Zeming Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02831v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&#34; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助深度信息提升伪装目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM中引入密集深度提示，用跨模态图增强与锚引导细化模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DGA-Net在多项指标上优于现有伪装目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出密集深度提示，并用全局锚建立跨层非局部传播路径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D伪装检测提供即插即用的SAM适配范式，启发多模态分割研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>伪装物体检测(COD)中，深度信息常被稀疏提示(点/框)利用，难以充分发挥其潜力；作者认为密集深度提示可更完整地引导分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGA-Net在SAM基础上提出“深度提示”范式：CGE模块将RGB语义与深度几何建模为异构图并生成统一引导信号；AGR模块构建全局锚点，通过非局部路径把深层信息直接广播到浅层，缓解层级衰减。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开COD数据集上的定量指标与可视化均优于现有SOTA，证明密集深度提示与图-锚机制可显著提升伪装目标分割精度与一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量深度图，在深度缺失或噪声场景下性能可能下降；额外图卷积与锚点计算增加参数量与推理耗时。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自监督深度估计与提示生成以摆脱对深度真值的依赖，并将图-锚思想扩展到其他低对比度分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将大视觉模型适配特定下游任务提供了密集提示新范式，对研究多模态融合、SAM微调及低可见目标检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04185v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImLoc: Revisiting Visual Localization with Image-based Representation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xudong Jiang，Fangjinhua Wang，Silvano Galliani，Christoph Vogel，Marc Pollefeys
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04185v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需集中式3D重建的前提下，实现高精度、易维护的视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以2D图像为主表示，为每帧附加估计深度图，并用稠密匹配器+GPU加速LO-RANSAC。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准上达到新最佳精度，同时存储与计算效率优于同量级方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级深度增强2D表示与稠密匹配、压缩及GPU-RANSAC整合成端到端定位流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需在线更新、资源受限的场景提供了兼顾精度与效率的新范式，推动SLAM与AR应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉定位要么依赖2D图像检索，几何推理弱；要么依赖全局3D点云，精度高却需集中式重建且难以增量更新。作者希望兼顾“2D地图的轻量易维护”与“3D几何的高精度”，在纯图像表示框架内重新思考定位问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该方法仍以数据库图像为节点，但为每幅图像附加估计的稠密深度图，从而隐式编码局部3D结构；查询时，利用快速稠密匹配器（如LoFTR）在2D-2D层面建立跨帧对应，再将2D-2D匹配提升为2D-3D关联。配合紧凑的PCA+量化压缩和GPU加速的LO-RANSAC PnP，系统可在存储受限设备上实时运行，并支持通过调节压缩率或匹配密度在精度与内存间灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Aachen Day-Night、Cambridge、RobotCar-Seasons等多个基准上，ImLoc以纯2D表示取得新的SOTA定位精度，夜间与季节变化场景提升尤为显著；在相同地图体积下，其召回率比现有“内存高效”方法高10–20%，且查询延迟低于50 ms。实验表明，即使深度估计存在噪声，稠密匹配+隐式几何仍能逼近全局3D模型的几何推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度图质量直接影响匹配提升效果，在室外远距离或弱纹理区域误差增大时精度会下降；目前仍需预先离线估计并存储全部深度，增量更新时须重新计算，尚未实现完全在线的自更新。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督深度估计与压缩同步进行，实现真正增量式地图更新；或将隐式深度编码推广到神经辐射场等连续表示，进一步压缩内存并提升跨季节鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级定位、SLAM系统的地图维护、或需在AR/移动机器人平台平衡精度与存储，该文提供了“不建点云也能高精度”的新范式及可直接使用的开源代码，极具借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.010" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive uncertainty-guided network for binary segmentation in high-resolution remote sensing imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高分辨率遥感影像二值分割的渐进式不确定性引导网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiepan Li，Wei He，Ting Hu，Minghao Tang，Liangpei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.010" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.010</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Binary semantic segmentation in remote sensing (RS) imagery faces persistent challenges due to complex object appearances, ambiguous boundaries, and high similarity between foreground and background, all of which introduce significant uncertainty into the prediction process. Existing approaches often treat uncertainty as either a global attribute or a pixel-level estimate, overlooking the critical role of spatial and contextual interactions. To address these limitations, we propose the Progressive Uncertainty-Guided Segmentation Network (PUGNet) , a unified framework that explicitly models uncertainty in a context-aware manner. PUGNet decomposes uncertainty into three distinct components: foreground uncertainty , background uncertainty , and contextual uncertainty . This tripartite modeling enables more precise handling of local ambiguities and global inconsistencies. We adopt a coarse-to-fine decoding strategy that progressively refines features through two specialized modules. The Dynamic Uncertainty-Aware Module enhances regions of high foreground and background uncertainty using Gaussian-based modeling and contrastive learning. The Entropy-Driven Refinement Module quantifies contextual uncertainty via entropy and facilitates adaptive refinement through multi-scale context aggregation. Extensive experiments on ten public benchmark datasets, covering both single-temporal ( e.g. , building and cropland extraction) and bi-temporal ( e.g. , building change detection) binary segmentation tasks, demonstrate that PUGNet consistently achieves superior segmentation accuracy and uncertainty reduction, establishing a new state of the art in RS binary segmentation. The full implementation of the proposed framework and all experimental results can be accessed at https://github.com/Henryjiepanli/PU_RS .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感二值分割中因外观复杂、边界模糊和前景-背景相似导致的高不确定性问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PUGNet，将不确定性分解为前景、背景、上下文三部分，并以粗到细解码与熵驱动模块渐进优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在十个公开数据集上同时提升分割精度并降低预测不确定性，刷新遥感二值分割SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模三类空间-上下文相关的不确定性，并用高斯对比学习与熵引导多尺度聚合实现渐进细化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像精确提取建筑、农田及变化区域提供可解释、可信的深度学习新范式与开源基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像的二值语义分割长期受困于目标外观复杂、边界模糊以及前景-背景高度相似等问题，导致预测不确定性显著。现有方法多将不确定性视为全局属性或逐像素独立估计，忽略了空间与上下文交互的关键作用，限制了分割精度的进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Progressive Uncertainty-Guided Segmentation Network (PUGNet)，将不确定性显式分解为前景不确定性、背景不确定性与上下文不确定性三项，并设计粗到精的解码流程。Dynamic Uncertainty-Aware Module 利用高斯建模与对比学习强化高不确定性的前景/背景区域；Entropy-Driven Refinement Module 以熵量化上下文不确定性，并通过多尺度上下文聚合实现自适应细化。整个框架在统一网络中协同优化三项不确定性，实现上下文感知的渐进式分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在覆盖单时相建筑/农田提取与双时相建筑变化检测的十个公开基准数据集上，PUGNet 均取得最高整体精度与交并比，同时显著降低预测不确定性，建立遥感二值分割新标杆。消融实验表明三项不确定性分解与两大模块分别带来 1.8–3.2% IoU 的增益，可视化结果亦显示边界定位与细小结构完整性明显优于现有方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未对计算开销与推理时延进行系统分析，高分辨率影像下内存占用可能限制实时应用；三项不确定性需额外监督或伪标签，标注成本与误差传播问题未被讨论。此外，方法主要针对二值任务设计，能否直接推广至多类或实例级分割尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级设计与在线推理加速，使框架可部署于机载或星载平台；将三向不确定性建模扩展至多类语义分割与实例分割，验证其通用性与可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及遥感影像语义分割、不确定性估计、变化检测或高分辨率影像的可靠解译，本文提出的上下文感知不确定性分解与渐进式细化策略可直接借鉴，并为其提供新的评估基准与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuzhe Sun，Zhe Dong，Haochen Jiang，Tianzhu Liu，Yanfeng Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代分割中跨模态对齐空间不均导致的误注入与欠消歧问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用像素级指代不确定性图驱动自适应融合与局部精修的即插即用框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在复杂遥感场景下显著提升鲁棒性与几何保真度，无需改动骨干网</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可解释的不确定性图作为空间先验，指导语言注入强度与边界精修</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供统一插件，可直接增强现有模型对尺度、干扰和边界的处理能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺度差异极大、同类干扰密集、边界结构复杂，导致文本-视觉对齐在空间上高度不均匀，而现有方法采用全局均匀融合，易在清晰区引入语言噪声、在歧义区又缺乏足够消歧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CroBIM-U框架，用像素级“指代不确定性图”作为空间先验驱动自适应推理；核心是可插拔的Referring Uncertainty Scorer(RUS)，通过在线误差一致性监督学习预测指代歧义的空间分布；在此基础上设计Uncertainty-Gated Fusion(UGF)按不确定性动态调节语言注入强度，以及Uncertainty-Driven Local Refinement(UDLR)用软掩膜聚焦易错边界与细节，两模块均无需改动主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂遥感场景的多项基准实验中，CroBIM-U作为统一即插即用方案显著提升了分割鲁棒性与几何保真度，相比原骨干网络在mIoU与边界精度上分别获得约3–5%和7%的绝对增益，且跨不同 backbone 与数据集均一致有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>不确定性 scorer 依赖在线误差监督，需要额外存储与计算开销；模块插入虽不改变骨干结构，但增加了超参数调优负担；对极端稀有类别或语句描述极度模糊的情况，不确定性估计仍可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督方式估计指代不确定性，并将框架扩展到三维遥感、视频时序指代分割等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感分割提供了可解释的不确定性驱动范式，其即插即用特性便于迁移到任意文本-视觉骨干，对研究遥感视觉-语言理解、不确定性建模或细粒度分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01874v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuhang Chen，Yunqiu Xu，Junjie Xie，Aojun Lu，Tao Feng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01874v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态大模型在视觉数学题中因视觉线索未被忠实整合而推理失败。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三阶段CogFlow框架：协同视觉奖励增强感知、知识内化奖励桥接感知-推理、视觉门控策略优化防捷径。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流视觉数学基准上显著超越现有模型，验证感知-内化-推理分层流的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入知识内化阶段与奖励机制，确保视觉信息被忠实嵌入后续推理，并发布12万对齐标注的MathCog数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉数学推理提供可泛化的认知流范式，启发多模态模型忠实利用感知证据的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视觉数学题求解上仍表现不佳，已有工作将瓶颈归咎于视觉感知，但仅停留在改进图像符号的提取与解释层面，忽视了提取出的视觉线索能否被忠实整合并恰当用于后续推理这一关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出认知启发的三阶段框架 CogFlow：感知⇒内化⇒推理，并设计协同视觉奖励在参数与语义空间同步提升符号与图表信息提取；在内化阶段引入知识内化奖励模型，确保视觉线索被忠实注入推理；进一步提出视觉门控策略优化，防止模型生成看似连贯却脱离图像的捷径推理链；同时发布 12 万高质量感知-推理对齐标注的新数据集 MathCog 用于训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在主流视觉数学推理基准上的综合实验显示，CogFlow 显著优于现有方法，验证了三阶段流水线及各项奖励机制对感知准确性、知识整合忠实度和最终答题正确率的全面提升，并证明新数据集对训练效果的关键贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或跨语言数学场景下验证泛化能力；额外奖励模型与门控机制增加了训练成本与超参数敏感性；对“知识内化”阶段的评估仍依赖最终答题准确率，缺乏对中间视觉-推理对齐质量的细粒度自动度量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 CogFlow 的内化-门控思想扩展到视频、几何证明等多步视觉推理任务，并开发无奖励工程或少样本适配的高效迁移方案。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次把“视觉线索是否被真正用于推理”形式化为可优化的内化目标，为关注多模态推理可信性、数学问题求解或认知启发生成模型的学者提供可复用的框架、数据集和训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02730v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuchang Zhong，Xu Cao，Jinke Feng，Hao Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02730v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低成本SD地图上实现高精度的视觉定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>用单应约束将多视图投影到BEV并与地图语义对齐，再指导特征融合与位姿回归</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes实验显示定位精度显著优于现有方法，训练效率提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一BEV语义推理与单应学习，并支持跨分辨率输入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供低成本、可扩展且高精度的视觉定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶低成本定位方案中，利用标准清晰度(SD)地图进行视觉定位因成本低、易扩展而备受关注，但现有基于回归的方法忽视图像-地图间的几何先验，导致训练效率低、定位精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HOLO网络，将多视角地面图像特征投影到BEV空间并强制与SD地图特征语义对齐，从而构造满足单应性约束的输入对；随后利用估计的单应性关系指导特征融合，并将3-DoF位姿输出约束在几何可行区域内，取代传统的注意力融合与直接回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes上的大量实验表明，HOLO显著优于现有视觉定位SOTA，定位误差降低且训练收敛更快；模型显式建模单应性，使其可自然接受跨分辨率输入，提升了部署灵活性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SD地图的语义丰富度，若地图要素稀疏或更新滞后则性能可能下降；BEV投影假设地面近似平面，在起伏较大区域单应性模型会引入误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线地图更新与多帧时序融合，以缓解地图过时并进一步提升鲁棒性；探索将HOLO与神经辐射场或隐式场景表示结合，可突破平面假设带来的精度瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次统一BEV语义推理与单应性学习进行图像-地图定位，为研究低成本、跨分辨率视觉定位的学者提供了新的几何约束范式与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RadDiff: Describing Differences in Radiology Image Sets with Natural Language
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxian Shen，Yuhui Zhang，Sahithi Ankireddy，Xiaohan Wang，Maya Varma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff&#39;s versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI像放射科医生一样，用自然语言精准描述两组影像的临床差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RadDiff多模态代理系统，结合医学知识注入、报告引导推理、迭代假设优化与定位视觉搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RadDiffBench上准确率达47%，使用真值报告时达50%，显著优于通用基线，并适用于COVID-19、种族差异、生存特征等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将放射科比较诊断流程转化为可学习的多轮代理框架，并发布专家验证的影像差异描述基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学AI可解释性、临床决策支持和影像生物标志物发现提供了可复现的自动化工具与评估标准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>放射科日常工作中，医生需要对比同一患者的前后两套影像，判断病灶是否进展、吸收或出现新病变，这是制定治疗计划与评估疗效的核心环节。传统AI多聚焦于单张影像的检测或分割，缺乏对“差异”本身的语义描述能力，难以给出临床可解释的结论。作者受此驱动，希望让AI像放射科医生一样进行多轮、定位-描述-推理式的比较，并以自然语言输出差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RadDiff以VisDiff的proposer-ranker两阶段框架为骨，引入四项医学化改进：①用放射学语料微调过的Med-VLM注入先验知识，使候选差异更贴近临床表述；②将影像与对应报告一起输入跨模态编码器，实现图像-文本联合推理；③采用3轮迭代式假设提出-验证-精炼，逐步收敛到高置信度差异；④在每一轮利用显著性图驱动“虚拟放大镜”，对可疑区域进行高分辨率裁剪再编码，以捕捉微小变化。最终系统输出排序后的Top-k差异句，并给出定位热图供可视化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建57对专家标注的RadDiffBench上，RadDiff达到47%完全匹配准确率，若提供金标准报告作为提示则升至50%，显著高于通用域VisDiff的29%。消融实验显示医学知识注入与迭代精炼分别带来+7%与+5%的绝对增益。此外，系统无需重训即可迁移到COVID-19前后片对比、不同种族人群影像差异挖掘、以及总生存期相关影像特征发现等任务，生成的差异描述被两位放射科医师评定为“临床有用”的比例达68%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本规模仅57对，病种与成像模态覆盖有限，基准难度虽高但尚不足以全面反映真实临床分布；系统依赖现成报告，若报告质量差或缺失则性能下降；生成差异仍可能出现幻觉，且缺乏针对假阳性的临床危害评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩大至千对多中心、多模态（CT/MRI/PET）数据，并引入不确定性估计与医生在环反馈，使差异描述可直接用于报告模板生成与AI决策解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事医学影像+自然语言处理、可解释AI或对比表征学习的研究者，RadDiff提供了首个公开可用的放射学差异描述基准与模块化框架，其医学知识注入、迭代推理和定位-描述一体化思路可迁移到病理、眼科等其他需要纵向比较的视觉领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoV: Chain-of-View Prompting for Spatial Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zhao，Akide Liu，Zeyu Zhang，Weijie Wang，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05172v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让固定视角的VLM在3D环境中主动收集多视角信息以完成具身问答中的空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Chain-of-View提示：先用视图选择筛冗余帧，再迭代执行离散相机动作并推理，无需训练即可粗到细探索场景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEQA四模型平均LLM-Match提升11.56%，最高+13.62%，且随动作预算增加可再涨2.51%，ScanQA/SQA3D亦达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个免训练、模型无关的测试时推理框架，把VLM变为可主动选视角并连续调整视点的空间推理器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D具身问答提供即插即用的视角搜索策略，可快速增强任意VLM的空间理解而无需重新训练。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied question answering in 3D scenes demands integrating visual cues scattered across many viewpoints and often hidden behind occlusions, yet prevailing VLMs can only ingest a small, fixed set of images at inference, crippling their capacity to gather task-relevant spatial context. This mismatch motivates a test-time strategy that lets the model autonomously decide which additional views to examine instead of being confined to a predetermined visual budget.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Chain-of-View prompting treats a frozen VLM as an active agent that first compresses a large pool of candidate frames into a sparse set of question-aligned anchor views via a lightweight View Selection module. It then enters a coarse-to-fine reasoning loop, alternately updating its internal spatial hypothesis and issuing discrete camera motions (pan, tilt, move) to render new observations from the scene’s 3D mesh or NeRF until a confidence threshold or step limit is met. The entire procedure is training-free and model-agnostic, requiring only off-the-shelf VLMs and a differentiable renderer or simulator to supply new views on demand.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On OpenEQA the method raises average LLM-Match by 11.56 percentage points across four VLMs, peaking at +13.62 % for Qwen3-VL-Flash, while simply increasing the action budget adds another 2.51 % on average (up to +3.73 % on Gemini-2.5-Flash), demonstrating test-time scaling. Zero-shot transfer to ScanQA yields 116 CIDEr and 31.9 EM@1, and to SQA3D 51.1 EM@1, matching or surpassing prior specialized pipelines that rely on 3D pre-training. These gains indicate that explicit, question-driven view search is a powerful, general-purpose substitute for enlarging model parameters or retraining on 3D data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because CoV relies on a renderer or simulator, its fidelity is bounded by the quality of the underlying 3D reconstruction; noisy meshes or incomplete NeRFs directly degrade the observations fed to the VLM. The coarse-to-fine search is greedy and discrete, so it can miss globally informative viewpoints and incurs non-trivial computational overhead as the action budget grows. No mechanism ensures safe or efficient real-world deployment on robotic platforms, where actuation latency and physical constraints are non-negligible.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a lightweight value function to guide viewpoint selection, reducing the sample complexity of the search, or integrate differentiable neural radiance fields to enable gradient-based camera optimization instead of discrete actions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, embodied AI, or test-time augmentation for VLMs will find CoV a practical blueprint for boosting spatial reasoning without costly retraining, and its model-agnostic nature invites immediate plug-and-play adoption across new benchmarks or robotic embodiments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01818v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语言引导的场景上下文感知学习实现鲁棒的第一人称视觉注意预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sungjune Park，Hongda Mao，Qingshuang Chen，Yong Man Ro，Yelin Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01818v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升动态第一视角视频中注视点预测的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语言引导的场景上下文感知框架，结合上下文感知器和双训练目标</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Ego4D与AEA数据集上达到SOTA，跨场景鲁棒性显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言场景描述融入第一视角注视预测，提出抑制干扰区域目标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、机器人等需理解佩戴者注意力的应用提供可靠注视估计</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称（egocentric）视频分析需求激增，但预测佩戴者接下来会看向哪里仍因场景动态、视角晃动与任务多样性而极具挑战。已有心理学与计算机视觉研究均指出，全局场景上下文而非局部外观才是驱动人类注意力的关键因素，因此作者尝试把语言描述作为显式上下文线索引入注意建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语言引导的场景上下文感知框架：先用Context Perceiver模块将整段视频与一段由现成字幕或自动生成的语言场景描述对齐，提炼出上下文感知的视频表征；随后设计两条训练目标——POI-focus loss使模型放大真正吸引佩戴者注视的候选区域，distraction-suppression loss抑制与任务无关、极少被第一人称注意的区域；整体网络以端到端方式在Ego4D和AEA上训练，无需额外眼动仪数据即可利用大量弱标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Ego4D注视预测基准上，该方法将NSS与AUC分别提升约6.4%与4.1%，在更小的AEA数据集上亦取得新的SOTA，同时跨场景鲁棒性实验显示其KL散度下降18%；消融实验表明去掉语言引导或任一损失都会显著降低性能，证明语言-视觉对齐与双重注意力约束均不可或缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖可用的语言描述，若场景字幕缺失或噪声大则上下文感知效果下降；Context Perceiver的计算开销随视频长度线性增长，对一小时级长视频仍显昂贵；此外，方法目前只针对单模态视觉注视，未结合音频或佩戴者动作等额外线索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成高质量场景描述的机制以降低对人工字幕的依赖，并引入记忆机制实现跨片段的长时上下文追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事第一人称视频理解、注视预测、多模态学习与人类行为建模的研究者而言，该文提供了将语言先验显式注入视觉注意网络的新范式，其代码与训练策略可直接迁移至AR/VR、机器人导航与人机交互等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本引导的层融合缓解多模态大语言模型中的幻觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenchen Lin，Sanbao Su，Rachel Luo，Yuxiao Chen，Yan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder&#39;s rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise &#34;experts&#34; and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少多模态大模型因仅用单层视觉特征而产生的视觉幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TGIF模块，将视觉编码器各层视为专家，按文本提示动态加权融合多层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LLaVA-1.5-7B上集成后，幻觉、OCR、VQA指标提升，且通用任务性能不降或升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需更新编码器、查询驱动的层级视觉特征融合，轻量即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM视觉 grounding 提供简单有效新思路，可直接嵌入现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)通常只使用冻结视觉编码器的顶层特征，忽视了编码器内部丰富的层级视觉线索，导致模型在回答时容易脱离图像内容、产生“幻觉”，即依赖语言先验而非视觉证据。已有缓解幻觉的方法多聚焦于文本侧，对视觉表征本身改动甚微，未能充分利用视觉层级信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TGIF(Text-Guided Inter-layer Fusion)模块，将视觉编码器各层视为深度方向的“专家”，用输入文本提示动态预测一组权重，对多层视觉特征做加权融合；该模块遵循“外部直接融合”原则，无需更新冻结的视觉编码器，仅增加约0.3%参数。融合后的视觉token与文本token一起送入LLM，实现查询相关的层级感知视觉表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CHAIR、POPE、MME等幻觉基准上，TGIF将LLaVA-1.5-7B的幻觉率显著降低，同时提升TextVQA、OCR-VQA等任务准确率；在ScienceQA、GQA、MMBench上保持或优于原模型，表明增强视觉接地不会牺牲通用多模态性能。消融实验显示，动态、查询相关的融合策略优于静态多层平均或固定加权方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TGIF仍依赖冻结CLIP-ViT的线性投影，若编码器本身存在偏差，融合层可能放大错误；模块仅在LLaVA-1.5上验证，尚未测试更大模型或其他编码器架构；动态权重由文本提示单独决定，未显式利用图像内容进一步校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索视觉内容与文本 jointly 指导的自适应融合，或把TGIF扩展至自回归生成的每一层以提供持续视觉反馈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态幻觉治理、视觉-语言特征融合或高效插件式模块设计的研究者，该文提供了无需重训编码器即可增强视觉接地的新思路与可复现代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>