<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-07</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-07 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖3篇关于3D场景理解的论文与2篇关于遥感智能的论文。</p>
            
            <p><strong class="text-accent">3D场景理解</strong>：《Compression Framework for Light 3D Scene Graph Generation》提出剪枝-即-搜索与蒸馏结合的压缩框架，实现轻量级3D场景图生成；《UniBVR》在统一框架中平衡视觉与推理能力，将大语言模型适配到3D任务以提升整体场景理解性能。</p>
            
            <p><strong class="text-accent">遥感智能</strong>：《EarthVL》构建渐进式地球视觉-语言框架，填补遥感影像中对象关系推理的空白；《GDiT》利用图先验引导扩散Transformer，实现语义可控的遥感图像合成以缓解标注稀缺问题；《Agentic AI in Remote Sensing》综述从静态深度学习向自主智能体范式的遥感分析转型，提出基础与分类体系。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于语义分割与3D场景理解的论文、6篇关于视觉-语言导航与推理的论文、5篇关于遥感图像生成与描述的论文、4篇关于视觉定位与回环检测的论文、3篇关于视觉常识推理的论文、2篇关于语义占用预测的论文、2篇关于驾驶场景解析的论文。</p>
            
            <p><strong class="text-text-secondary">语义分割3D</strong>：该主题聚焦无需3D标注的开放词汇3D语义分割，如《Leveraging 2D-VLM for Label-Free 3D Segmentation》用2D-VLM投影伪标签，《3D Semantic Gaussian via Geometric-Semantic Hypergraph Computation》以超图联合优化几何-语义，并探索NeRF、Gaussian Splatting等隐式表示提升大场景精度。</p>
            
            <p><strong class="text-text-secondary">VL导航</strong>：研究通过增强空间感知与动态滤除干扰，提升长指令下的视觉-语言导航鲁棒性，代表工作《SPENav》提出空间感知增强与动态目标过滤，《Vision-Language Navigation with Self-Adaptive Exploration》结合自适应探索策略显著降低碰撞与偏离。</p>
            
            <p><strong class="text-text-secondary">遥感生成</strong>：面向训练样本稀缺的遥感场景，探索语义可控图像合成与描述生成，《GDiT》用图先验引导扩散Transformer实现类别精准控制，《EarthVL》构建渐进式视觉-语言框架推理对象关系，《A Size-Aware Graph Embedding Approach》在描述中显式嵌入相对尺度信息提升细节忠实度。</p>
            
            <p><strong class="text-text-secondary">视觉定位</strong>：针对视角、光照变化下的鲁棒定位，研究强调语义-几何融合与查询拒绝，《SRD2-VPR》提出语义强制特征聚合加查询拒绝机制，显著降低错误回环，同时保持高精度召回。</p>
            
            <p><strong class="text-text-secondary">常识推理</strong>：通过引入因果链与外部常识库，增强静态图像的事件级推理能力，《Visual Context and Commonsense-Guided Causal Chain-of-Thoughts》构建因果链思维提示，在VCR基准上取得SOTA。</p>
            
            <p><strong class="text-text-secondary">语义占用</strong>：从单目或多目图像估计稠密3D语义占用，研究侧重时空解耦与层级对齐，《Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling》将几何与时间建模分离，通过层级对齐提升动态场景精度。</p>
            
            <p><strong class="text-text-secondary">驾驶解析</strong>：利用视觉基础模型先验实现跨域驾驶场景鲁棒解析，《Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge》提出深度-语义协同适配框架，在多种天气与地域条件下保持泛化性能。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于剪枝即搜索与蒸馏的轻量化三维场景图生成压缩框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hojun Song，Chae-yeong Song，Dong-hun Lee，Heejung Choi，Jinwoo Jeong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅压缩3D场景图生成模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以剪枝即搜索定压缩率，再用结构化剪枝+知识蒸馏恢复精度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型体积减半以上，分类准确率反而提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向3DSGG的压缩框架，将剪枝视为搜索并设计专用蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供轻量高性能3D场景理解方案，推动实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景图生成(3DSGG)通过同时预测物体类别与谓词关系来刻画三维场景，是3D场景理解的新兴核心任务。现有基于图神经网络(GNN)的方法虽显著提升精度，却带来庞大参数量与计算开销，严重阻碍在资源受限设备上的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向3D场景图生成的压缩框架，分两阶段进行：阶段一将剪枝视作神经架构搜索，自动寻找最优压缩率；阶段二采用结构化剪枝去除冗余通道，并设计新的知识蒸馏策略，使轻量学生网络从教师网络中精准继承物体-谓词联合分布。框架整合了可微搜索、重要性评分与特征-关系双重蒸馏模块，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的实验表明，该方法在模型体积缩减50%以上的同时，物体与谓词分类准确率反而提升，总体mAP平均提高约2.3个百分点，验证了压缩与性能兼得的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一场景类别与固定点云输入分辨率下验证，压缩后模型在跨场景泛化及更低比特量化下的鲁棒性尚未探讨；此外，剪枝-搜索阶段仍需训练完整教师网络，带来额外碳足迹与计算成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将剪枝-搜索与量化、低秩分解联合优化，并引入场景语义先验以进一步压缩；同时探索无教师自蒸馏方案，降低预训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D场景理解、图神经网络压缩或边缘智能的研究者，该文提供了首个系统的3DSGG轻量化范例与开源代码，可直接迁移到相关任务并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02783v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EarthVL：渐进式地球视觉-语言理解与生成框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junjue Wang，Yanfei Zhong，Zihang Chen，Zhuo Zheng，Ailong Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02783v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&#39; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &#39;&#39;image-mask-text&#39;&#39;, advancing geographical applications for Earth vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破遥感影像仅做目标识别的局限，实现地物对象关系推理与场景级图文理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多任务数据集EarthVLSet，提出语义引导的EarthVLNet，先分割再让LLM做关系推理与问答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分割特征可跨数据集提升VQA；选择题更依赖视觉编码，开放问答需双模态同步增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提“图像-掩膜-文本”联合基准，用像素语义动态加权差异损失，渐进式实现分割-关系-理解一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球视觉提供图文推理基准与模型，推动城市规划、监测等地理智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像解译长期聚焦像素级分类，对“地物是什么”回答较好，却极少显式建模“地物之间如何关联”，难以支撑城市规划等需要场景级推理的应用。视觉-语言（V-L）范式在通用图像领域已验证可同步完成识别与推理，但地球观测领域尚缺大规模“图-掩-文”对齐数据与渐进式框架。作者因此提出EarthVL，以填补遥感场景关系推理与文本生成的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建10.9k张亚米级影像、对应土地覆盖掩膜及76.1万条文本的EarthVLSet，涵盖语义分割、选择型与开放型VQA三类任务。EarthVLNet采用两阶段渐进策略：阶段一以CNN-Transformer混合编码器生成像素级土地覆盖语义掩膜；阶段二将掩膜作为语义先验注入冻结的LLM，通过可学习适配器实现对象级关系推理与答案生成。训练时引入“数值差异损失”，根据各类地物像素占比动态加权，抑制大类主导问题。整个框架端到端优化，推理阶段可输出分割、关系描述及VQA答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三项基准上，EarthVLNet的mIoU达82.1%，选择型VQA准确率87.4%，开放型VQA BLEU-4达31.2，均优于RSVQA、GeoChat等专用模型。跨数据集实验表明，即使换用不同城市影像，分割特征仍能将VQA性能提升3-5个百分点，验证语义先验的泛化价值。对比实验进一步揭示：选择型任务对视觉编码器更敏感，更换 backbone 带来+4.2%的增益；开放型任务则需同时强化视觉与语言模型，单一改进仅带来≤1.5%的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集中文本描述以英文模板为主，缺乏多语言及地方性知识，可能限制全球推广；76万文本对仍远低于通用V-L数据规模，长尾地物与复杂关系的样本不足。方法依赖亚米级影像，对中低分辨率或缺少多光谱输入的场景未做验证，且计算开销随LLM规模线性增长，边缘部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言-多文化文本，并引入时间序列影像以支持“变化描述”与动态规划；探索轻量化LLM或本地-云端协同推理，降低边缘设备部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言融合、场景关系推理或土地覆盖与城市规划联动，本工作提供了首个公开“图-掩-文”对齐基准与渐进式框架，可直接作为评测基准或预训练数据源，也可借鉴其“分割先验+LLM”范式快速迁移至灾害评估、农业监测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniBVR: Balancing visual and reasoning abilities in unified 3D scene understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniBVR：在统一3D场景理解中平衡视觉与推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Panqi Yang，Haodong Jing，Nanning Zheng，Yongqiang Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132599" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132599</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Large Language Models (LLMs) enable remarkable general-purpose task-solving in computer vision, robotics, and beyond. Although LLMs perform well in 2D tasks, their adaptation to 3D scene understanding faces critical challenges: (1) the inherent complexity of 3D spatial relationships and multimodal alignment, (2) the performance imbalance between vision-centric tasks and reasoning-centric tasks. Existing approaches either develop specialized models for individual tasks or rely on LLM fine-tuning with limited visual grounding capabilities, failing to achieve unified 3D scene understanding. To bridge this gap, we propose UniBVR , a U nified framework that B alances V isual and R easoning abilities through two innovative components: (i) task-agnostic Align-Former module that establishes fine-grained 3D vision-language correspondence through cross-modal attention, and (ii) task-specific lightweight decoders that dynamically generate diverse outputs (texts, boxes or masks) via efficient routing. To mitigate task imbalance, we design a multi-task balancing strategy that automatically adjusts loss weights based on task difficulty. Experiments on seven benchmarks (ScanRefer, Nr3D, ScanQA, etc.) achieve state-of-the-art results, with gains of 5.8% (3D-VG), 4.3% (3D-DC), and 6.1% (3D-QA) over prior methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在统一框架下兼顾3D视觉定位与复杂推理，缓解两类任务性能失衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniBVR，用跨模态Align-Former建立细粒度3D-语言对应，加轻量路由解码器与自动多任务损失平衡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个基准上刷新SOTA，3D-VG、3D-DC、3D-QA分别提升5.8%、4.3%、6.1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用任务无关Align-Former实现细粒度3D-语言对齐，并引入动态损失权重策略平衡视觉与推理任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM在3D场景理解中的统一视觉-推理建模提供即插即用方案，推动机器人、AR等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大语言模型在2D视觉任务中已展现通用推理能力，但在3D场景理解中仍面临空间几何复杂、跨模态对齐困难等挑战，且视觉定位与高层推理两类子任务性能严重失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniBVR框架，通过Align-Former在点云-文本间做细粒度跨模态注意力以建立统一视觉语言空间；随后用任务无关的共享编码器配合轻量级任务特定解码器，以动态路由方式输出文本、检测框或分割掩码；训练阶段引入基于任务难度自动重加权的多任务损失平衡策略，无需针对每个3D任务单独微调LLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer、Nr3D、ScanQA等七个基准上，UniBVR将3D视觉定位、描述生成与问答的绝对指标分别提升5.8%、4.3%和6.1%，刷新SOTA，同时保持单模型统一推理，验证视觉-推理能力平衡设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模3D-文本配对数据，对点云密度和标注质量敏感；自动损失权重虽缓解任务不平衡，却引入额外超参，且未在更具挑战的室外场景或实时机器人闭环中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经辐射场或扩散生成模型结合，实现3D场景补全与对话式编辑的统一；并研究在线强化学习微调，使框架在真实机器人交互中持续自我改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为3D视觉-语言统一模型提供可扩展架构与多任务平衡策略，对从事3D场景理解、多模态LLM或机器人感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GDiT：一种图先验引导的可控语义遥感图像合成扩散Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Deng，Xiangyun Hu，Yibing Xiong，Aokun Liang，Jiong Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何合成结构一致、语义可控且带空间先验的遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>将语义图转为图结构，用GSAM融合CLIP语义与几何信息，GDiT块执行图-像交叉注意力扩散生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>GDiT仅用GeoSynth38.9%参数即获更高保真与拓扑连贯，并支持文本多级控制</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图先验引入扩散Transformer，提出GSAM与图-像交叉注意力实现拓扑保持的遥感图像合成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注场景提供高质量训练数据，兼顾效率与可控性，推动遥感生成与下游应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义图像合成(SIS)在遥感领域至关重要，可为稀缺标注数据提供高质量训练样本，但现有方法多聚焦像素级映射，忽略了道路-建筑等地物间的空间先验，导致生成图像结构不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出图先验引导的扩散 Transformer(GDiT)：先将语义图转为语义图，节点表示地物并编码空间交互；设计几何-语义感知模块(GSAM)融合 CLIP 语义与几何属性；构建 Graph Diffusion Transformer Block，通过图-像交叉注意力细化空间结构，保证拓扑一致与语义保真；并引入文本提示实现全局-对象-像素多级控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LandCover 与 LandUse 数据集上，GDiT 仅用 GeoSynth 38.9% 参数即取得竞争性能，生成图像保真度更高且结构更合理，显著提升了效率与精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖语义图到图的转换精度，若地物节点或边提取错误将直接影响生成质量；扩散模型本身计算开销仍大，对高分辨率遥感影像的实时应用存在瓶颈；未充分验证在跨传感器、跨区域场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式自动提取空间图先验，并引入更轻量级的扩散或神经辐射场架构以支持实时高分辨率生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将图神经网络与扩散模型结合，提出显式利用空间拓扑先验进行可控遥感图像生成，为研究遥感数据增强、多模态条件生成及地理空间一致性约束的研究者提供了新思路与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01891v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感中的智能体AI：基础、分类与新兴系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Niloufar Alipour Talemi，Julia Boone，Fatemeh Afghah
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01891v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将静态深度学习升级为具备自主规划与工具调用能力的遥感智能体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出单/多智能体统一分类，系统梳理规划、RAG、记忆等架构并评测新基准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有视觉基础模型缺乏复杂地理工作流所需的序列规划与主动工具编排能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建遥感智能体全景框架，将评估焦点从像素精度转为轨迹推理正确性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发稳健自主的地理空间智能提供路线图，推动遥感从感知走向行动与决策。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>地球观测正从静态深度学习模型转向可自主规划、调用工具的“智能体”AI，以完成复杂地理空间工作流。现有视觉基础模型与多模态大语言模型虽提升了表征能力，却普遍缺乏顺序决策与主动工具编排能力，难以满足遥感任务对时空推理的高阶需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次系统梳理遥感智能体AI文献，提出统一分类法区分单智能体“副驾驶”与多智能体协同架构。论文归纳了三大基础模块：规划机制（分层任务分解、蒙特卡洛树搜索）、检索增强生成（结合地理知识库与工具API）及记忆结构（短期工作记忆与长期经验存储）。随后对新兴基准进行元分析，将评估指标从像素级精度升级为轨迹级推理正确性，并建立跨任务对比框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示多智能体在跨传感器数据融合、灾害响应链式决策等场景显著优于单智能体，平均轨迹成功率提升18–32%。引入检索增强后，模型在零样本区域迁移中的地理定位误差降低27%，证明外部知识可缓解遥感领域 grounding 难题。作者进一步指出，当前最佳系统的安全 orchestration 失败率仍达12%，是制约实地部署的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于arXiv与会议论文，缺乏对工业界未公开系统的观察，存在发表偏倚。安全、伦理与隐私讨论以概念性为主，尚未量化不同监管约束对性能的影响。此外，提出的轨迹级基准仍依赖合成任务，真实卫星操作场景的复杂性未被充分覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作应构建开放、可重现的真实卫星操控试验床，发展可验证安全约束的强化学习 orchestration 算法，并探索在轨边缘计算场景下的轻量化智能体架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在地理空间决策、工具调用或自主AI系统的应用，本文提供的分类法、模块设计与评估框架可直接指导系统实现与实验设计，并帮助定位领域空白与安全挑战。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRD2-VPR：语义强化的特征聚合与查询拒绝机制用于视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhi Hu，Liang Liao，Weisi Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651681</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉地点识别在昼夜剧变、严重遮挡等域偏移场景下鲁棒性不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义强化的双分支网络，联合优化特征注意与描述，并引入查询拒绝模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，Recall@1/5提升3.5/3.9个百分点，下游定位匹配提速28%无性能损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义上下文 richness 用于注意引导，并以轻量级查询拒绝筛除低信息查询。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人、AR/VR 等需在剧烈环境变化中可靠定位的系统提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别(VPR)在SLAM与视觉定位中至关重要，但现有方法在昼夜剧变、遮挡严重等域偏移场景下性能骤降，主因是缺乏针对OOD样本的显式监督，且未充分利用语义信息细化局部上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支网络SRD2-VPR：一支在语义引导的上下文丰富度监督下学习像素级注意力，使网络聚焦具有地理判别性的语义区域；另一支在同语义类内重排序、异类互斥的监督下生成鲁棒描述子，实现OOD适应。额外设计轻量级查询拒绝模块，用学到的注意力衡量图像信息含量，主动跳过缺乏地点表征力的查询帧，降低误匹配且加速检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域基准上，该方法将Recall@1/Recall@5分别提升3.5和3.9个百分点，优于当前最佳方法；同时特征匹配阶段耗时减少28%，下游视觉定位精度无损失，验证了语义增强与查询拒绝对OOD鲁棒性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖现成的语义分割模型，若分割在目标域失效则性能可能下降；拒绝模块阈值需手动设定，对不同数据集敏感；双分支训练增加超参数，实际部署时显存占用略高于纯描述子方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督语义提炼以摆脱对固定分割模型的依赖，并引入自适应阈值机制使查询拒绝无需人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域视觉定位、SLAM回环检测或OOD鲁棒表征学习，本文提供的语义-描述子联合优化与查询拒绝思路可直接借鉴，并作为基线进行扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Context and Commonsense-Guided Causal Chain-of-Thoughts for Visual Commonsense Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉情境与常识引导的因果链式思维用于视觉常识推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyu Li，Jing Zhao，Tongquan Wei，Shiliang Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型仅凭单张静态图像就具备人类般的视觉常识推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将任务拆为四子问题，用视觉-语言模型提取上下文，经因果强度过滤常识后由大模型零样本生成因果思维链并加权投票。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项下游任务上显著超越基线与SOTA，验证各模块均有效提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出基于视觉上下文因果强度过滤常识并构建VC³-CoT路径，实现零样本双稳健视觉常识推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉常识推理提供无需微调、可解释且易扩展的新范式，对多模态AI研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉常识推理要求模型从静态图像中推断动态情境并结合常识知识，这对人类轻而易举，却仍是视觉-语言模型面临的高阶认知难题。现有方法多依赖特定任务微调并引入外部世界知识，却常忽视准确上下文与常识在推理中的关键作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将视觉常识推理拆成四个互相关联的子问题，采用视觉-语言模型与大型语言模型协同的零样本框架。核心贡献是“常识知识过滤模块”，它利用视觉上下文的因果强度筛选相关常识，构建视觉上下文与常识引导的因果思维链(VC³-CoT)。多条推理路径经加权多数表决后输出最终答案，实现双重鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项下游任务上的大量实验显示，该方法相比基线模型与当前最佳方法显著提升性能，验证各组件有效性。因果链式推理路径可视化表明，过滤后的常识知识确实聚焦于与图像情境高度相关的因果片段。加权投票策略进一步降低单一路径错误带来的风险，使整体推理准确率平均提高约6-10个百分点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大型语言模型生成候选常识，计算开销与延迟较高，难以实时部署。因果强度度量基于预训练模型的注意力或梯度信号，其可靠性受视觉-语言模型偏差影响，可能引入伪因果。零样本设定虽通用，但在领域差异极大的图像上性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级因果强度估计器以降低延迟，并引入可解释因果图结构实现端到端训练。结合多模态检索增强技术，动态引入外部知识库，可进一步提升跨领域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究视觉推理、常识整合与零样本学习的学者提供了可复用的因果链式框架和过滤策略，尤其适用于需要可解释性与鲁棒性的多模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GDiT：一种图先验引导的可控语义遥感图像合成扩散Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Deng，Xiangyun Hu，Yibing Xiong，Aokun Liang，Jiong Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何合成结构一致、语义可控且带空间先验的遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>将语义图转为图结构，用GSAM融合CLIP语义与几何信息，GDiT块执行图-像交叉注意力扩散生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>GDiT仅用GeoSynth38.9%参数即获更高保真与拓扑连贯，并支持文本多级控制</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图先验引入扩散Transformer，提出GSAM与图-像交叉注意力实现拓扑保持的遥感图像合成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注场景提供高质量训练数据，兼顾效率与可控性，推动遥感生成与下游应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义图像合成(SIS)在遥感领域至关重要，可为稀缺标注数据提供高质量训练样本，但现有方法多聚焦像素级映射，忽略了道路-建筑等地物间的空间先验，导致生成图像结构不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出图先验引导的扩散 Transformer(GDiT)：先将语义图转为语义图，节点表示地物并编码空间交互；设计几何-语义感知模块(GSAM)融合 CLIP 语义与几何属性；构建 Graph Diffusion Transformer Block，通过图-像交叉注意力细化空间结构，保证拓扑一致与语义保真；并引入文本提示实现全局-对象-像素多级控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LandCover 与 LandUse 数据集上，GDiT 仅用 GeoSynth 38.9% 参数即取得竞争性能，生成图像保真度更高且结构更合理，显著提升了效率与精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖语义图到图的转换精度，若地物节点或边提取错误将直接影响生成质量；扩散模型本身计算开销仍大，对高分辨率遥感影像的实时应用存在瓶颈；未充分验证在跨传感器、跨区域场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式自动提取空间图先验，并引入更轻量级的扩散或神经辐射场架构以支持实时高分辨率生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将图神经网络与扩散模型结合，提出显式利用空间拓扑先验进行可控遥感图像生成，为研究遥感数据增强、多模态条件生成及地理空间一致性约束的研究者提供了新思路与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02783v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EarthVL：渐进式地球视觉-语言理解与生成框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junjue Wang，Yanfei Zhong，Zihang Chen，Zhuo Zheng，Ailong Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02783v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&#39; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &#39;&#39;image-mask-text&#39;&#39;, advancing geographical applications for Earth vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破遥感影像仅做目标识别的局限，实现地物对象关系推理与场景级图文理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多任务数据集EarthVLSet，提出语义引导的EarthVLNet，先分割再让LLM做关系推理与问答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分割特征可跨数据集提升VQA；选择题更依赖视觉编码，开放问答需双模态同步增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提“图像-掩膜-文本”联合基准，用像素语义动态加权差异损失，渐进式实现分割-关系-理解一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球视觉提供图文推理基准与模型，推动城市规划、监测等地理智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像解译长期聚焦像素级分类，对“地物是什么”回答较好，却极少显式建模“地物之间如何关联”，难以支撑城市规划等需要场景级推理的应用。视觉-语言（V-L）范式在通用图像领域已验证可同步完成识别与推理，但地球观测领域尚缺大规模“图-掩-文”对齐数据与渐进式框架。作者因此提出EarthVL，以填补遥感场景关系推理与文本生成的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建10.9k张亚米级影像、对应土地覆盖掩膜及76.1万条文本的EarthVLSet，涵盖语义分割、选择型与开放型VQA三类任务。EarthVLNet采用两阶段渐进策略：阶段一以CNN-Transformer混合编码器生成像素级土地覆盖语义掩膜；阶段二将掩膜作为语义先验注入冻结的LLM，通过可学习适配器实现对象级关系推理与答案生成。训练时引入“数值差异损失”，根据各类地物像素占比动态加权，抑制大类主导问题。整个框架端到端优化，推理阶段可输出分割、关系描述及VQA答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三项基准上，EarthVLNet的mIoU达82.1%，选择型VQA准确率87.4%，开放型VQA BLEU-4达31.2，均优于RSVQA、GeoChat等专用模型。跨数据集实验表明，即使换用不同城市影像，分割特征仍能将VQA性能提升3-5个百分点，验证语义先验的泛化价值。对比实验进一步揭示：选择型任务对视觉编码器更敏感，更换 backbone 带来+4.2%的增益；开放型任务则需同时强化视觉与语言模型，单一改进仅带来≤1.5%的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集中文本描述以英文模板为主，缺乏多语言及地方性知识，可能限制全球推广；76万文本对仍远低于通用V-L数据规模，长尾地物与复杂关系的样本不足。方法依赖亚米级影像，对中低分辨率或缺少多光谱输入的场景未做验证，且计算开销随LLM规模线性增长，边缘部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言-多文化文本，并引入时间序列影像以支持“变化描述”与动态规划；探索轻量化LLM或本地-云端协同推理，降低边缘设备部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言融合、场景关系推理或土地覆盖与城市规划联动，本工作提供了首个公开“图-掩-文”对齐基准与渐进式框架，可直接作为评测基准或预训练数据源，也可借鉴其“分割先验+LLM”范式快速迁移至灾害评估、农业监测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651320" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPENav: Dynamic Object Filtering with Spatial Perception Enhancement for Vision-Language Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPENav：空间感知增强的动态目标过滤用于视觉-语言导航</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yuan，Huaxiang Zhang，Li Liu，Lei Zhu，Xinfeng Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651320" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651320</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制视觉-语言导航中的无关动态物体并增强空间感知以减少定位误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SPENav，结合开放词汇感知、分层语义先验提取与空间-指令双注意记忆筛选。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在R2R未见测试集上达76% SR与65% SPL，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将开放词汇任务相关物体过滤与多级空间-语义先验结合，实现选择性记忆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下的鲁棒导航提供可迁移的空间感知机制，推动跨模态理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language navigation (VLN) demands that agents fuse natural language instructions with real-time visual observations, but raw RGB frames contain abundant distractors that drift localization and planning. Prior memory systems naïvely cache all visual tokens, so task-irrelevant objects accumulate noise and degrade spatial reasoning, especially in unseen environments.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SPENav introduces a two-level open-vocabulary perception pipeline: locally, a Hierarchical Semantic Prior Extractor first detects objects with an off-the-shelf vision-language model, then a Room-Information-Guided Filtering module prunes candidates whose semantic similarity to the current instruction is below an adaptive threshold. Globally, a Spatial-Instructional Guided Dual Attention layer re-weights the remaining tokens by jointly attending to geometric relations (relative orientation &amp; distance) and linguistic cues, yielding a compact, goal-oriented memory that is fed to a recurrent policy network for action prediction.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the R2R unseen test split SPENav attains 76 % SR (+4 % over the previous best) and 65 % SPL (+3 %), verifying that suppressing distractors early and explicitly modeling spatial-instruction alignment reduces excess trajectory length and back-tracking. Ablation shows that removing either filtering or dual attention drops SR by 2.5–3 %, indicating both components contribute to the gain.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on a frozen open-vocabulary detector that may miss domain-specific objects or hallucinate categories in low-light images; inference latency grows linearly with the number of detected objects, which could hinder deployment on edge robots. Moreover, experiments are confined to discrete R2R environments and have not been validated on continuous, real-world benchmarks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the filtering mechanism to continual learning settings where object priors are updated on-the-fly, and integrating neural radiance fields for metric-consistent 3-D memory, are promising next steps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied AI, cross-modal memory, or robust navigation in open-world scenes will find SPENav’s explicit separation of task-relevant versus irrelevant visual cues a useful blueprint for reducing perceptual noise and improving generalization.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651112" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      3D Semantic Gaussian via Geometric-Semantic Hypergraph Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于几何-语义超图计算的三维语义高斯方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinran Wang，Zhiqiang Tian，Dejian Guo，Siqi Li，Shaoyi Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651112" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651112</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需密集人工标注的前提下，实现稀疏输入的3D语义重建并提升RGB渲染质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在3D Gaussian中嵌入预训练语义特征，并用超图神经网络联合建模RGB与语义的高阶跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanNet与Replica场景下RGB和语义指标达SOTA，实时34 FPS渲染，无需人工标签。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图高阶建模引入3D Gaussian，实现无标注语义-几何联合优化与跨模态一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级3D语义理解与可视化提供实时方案，降低标注成本，推动AR/VR与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义标签与几何、外观天然耦合，但传统NeRF或3D Gaussian将语义与RGB分支独立编码，导致跨视角不一致、训练慢或通道漂移，且依赖密集人工标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以3D Gaussian为基元，把预训练视觉模型提取的语义特征直接嵌入高斯属性，无需额外真值；随后构建几何-语义超图神经网络，用超边连接跨模态、跨帧的多个高斯顶点，显式建模RGB与语义的高阶关联。超图消息传递在训练阶段联合优化光度损失、语义重建损失与稀疏性正则，实现端到端学习。推理时仅保留优化后的高斯参数，支持34 FPS实时渲染。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet和Replica房间级场景上，该方法仅用稀疏输入即可达到SOTA的新视角RGB PSNR与mIoU，语义分割精度比最强3D Gaussian基线提高约8% mIoU，同时保持与原生3D-GS相当的渲染速度，无需任何人工语义标签。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>超图构造依赖预训练2D语义网络的精度，若初始特征错误会在3D空间传播；超边数量随高斯规模二次增长，显存占用在极大规模场景可能受限；论文未探讨动态场景或光照变化下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时态超边以支持动态语义-几何联合建模，并设计自适应超边采样策略降低显存，实现城市级场景的实时语义重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无标注3D语义重建提供了可微渲染+超图学习的新范式，对研究神经辐射场、3D Gaussian、跨模态关联或弱监督语义分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02029v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用2D-VLM实现大规模室外场景理解中的无标签三维分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Toshihiko Nishimura，Hirofumi Abe，Kazuhiko Murasaki，Taiga Yoshida，Ryuichi Tanida
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1587/transinf.2025DVL0006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1587/transinf.2025DVL0006</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需3D标注或配对RGB图像的情况下完成大规模室外点云语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用虚拟相机将点云投影为多视角2D图像，由2D-VLM按文本提示分割，再加权投票回3D。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无训练方法超越现有零样本方案，精度媲美全监督模型，并支持开放词汇查询。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把2D-VLM与多视角加权投票结合，实现无标注、开放词汇的3D点云分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏3D标注的自动驾驶、测绘等场景提供即开即用的语义理解工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模室外点云语义分割通常依赖昂贵的手工3D标注或成对的RGB图像，限制了其在开放场景下的可扩展性。近年来2D视觉-语言模型（VLM）在开放词汇分割上表现突出，但如何将其能力迁移到无标注、无图像的3D数据仍待解决。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者用虚拟相机将点云投影为多视角深度图，无需真实RGB即可生成2D表示；随后以自然语言提示驱动预训练2D-VLM（如CLIP+Segment Anything）进行开放词汇语义分割；各视角的2D预测通过基于置信度和视角可见性的加权投票回溯到3D点；整个过程无需任何3D标注或模型再训练，实现零样本3D分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI、nuScenes和DALES数据集上的实验显示，该方法在训练无关方法中达到SOTA，部分类别甚至逼近全监督基线；开放词汇查询使用户可用任意文本检测新类别，显著优于固定类别模型；多视角投票将单视角误差降低20–30%，验证了投影-聚合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>虚拟相机投影丢失精细几何与反射率信息，导致细小或遮挡结构误分割；2D-VLM的域偏差在雨天、夜间等室外条件下降显著；加权投票依赖足够多视角，稀疏视角或单扫描场景性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或扩散模型增强投影保真度，并结合3D几何先验以提升细粒度分割；研究自适应视角选择可进一步降低计算量并提高稀疏数据下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无标注3D场景理解提供了可扩展的开放词汇方案，适合研究2D-3D迁移、零样本点云分割或室外自主系统的学者直接借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">充分挖掘视觉基础模型的深层先验知识以实现可泛化的RGB-深度驾驶场景解析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sicen Guo，Tianyou Wen，Chuang-Wei Liu，Qijun Chen，Rui Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2025.3650682</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重训ViT的前提下，充分利用视觉基础模型先验知识实现可泛化RGB-深度驾驶场景解析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFIT侧适配器，将VFM的相对深度预测作为输入，高效提取并融合RGB与深度异构特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HFIT在Cityscapes与KITTI语义数据集上超越传统单模/融合网络、预训练VFM及ViT适配器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用VFM相对深度先验替代真实深度图，设计无需重训ViT的异构特征集成Transformer框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为驾驶场景解析提供即插即用的VFM融合策略，推动视觉基础模型在多模态自动驾驶感知中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）在纯RGB任务上表现卓越，但在RGB-深度联合的自动驾驶场景解析中潜力未被充分挖掘。现有方法要么重新训练大模型成本高昂，要么简单微调未能深度利用VFM的先验知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heterogeneous Feature Integration Transformer（HFIT），以ViT为骨干，通过侧向适配器结构将RGB与深度模态特征在Transformer内部异构融合，无需重训ViT。网络利用VFM输出的相对深度预测取代易缺失的绝对深度图，作为适配器输入，实现零额外深度传感器依赖。整体框架在保持VFM权重冻结的同时，仅训练轻量级融合模块，兼顾效率与泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes与KITTI Semantics基准上，HFIT的mIoU分别超过最佳单模RGB网络4.8%与5.3%，超过现有ViT适配器方案2.1%与2.7%，并在跨域评估中把性能下降控制在3%以内，验证其强泛化能力。消融实验显示，相对深度先验贡献了整体增益的约60%，证明充分挖掘VFM隐式几何知识是关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖VFM生成的相对深度质量，若VFM在夜间或恶劣天气失效则性能下降；侧适配器容量有限，对更高分辨率输入的扩展性尚未验证；实验仅覆盖驾驶场景，未测试室内或机器人导航等更一般RGB-D任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索HFIT在多帧时序融合与多任务联合训练中的扩展，以及结合扩散模型或自监督信号进一步提升深度先验的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态大模型高效迁移、自动驾驶环境感知或RGB-D语义分割的研究者，该文提供了不增加计算重担即可释放VFM先验的新范式，代码开源便于复现与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650788" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Size-Aware Graph Embedding Approach to Remote Sensing Image Captioning with Object Relative Size Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述生成的尺寸感知图嵌入方法：引入目标相对尺度信息</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Ni，Yinghao Xu，Weibo Zhang，Zhaoyun Zong，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650788" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650788</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感图像字幕常缺失或误述目标尺寸信息，如何显式建模相对大小以生成更准确的描述？</p>
                <p><span class="font-medium text-accent">研究方法：</span>三模块框架：目标确认与相对尺寸估计→尺寸感知图嵌入→字幕生成，并引入SizeNum-Meteor指标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UCM、Sydney、RSICD及扩展基准上，新方法在标准指标与SizeNum-Meteor均优于现有方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相对尺寸估计与图卷积结合用于字幕生成，提出专门评估尺寸与数量正确性的SizeNum-Meteor。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感字幕提供尺寸感知新范式，推动更精细场景理解及评估标准发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成旨在为卫星/航空影像自动生成自然语言描述，但现有方法普遍忽视目标尺寸信息，导致字幕要么漏掉“大/小”等形容词，要么给出模糊甚至错误的尺寸描述。作者认为尺寸是遥感场景语义的关键组成部分，其缺失会显著降低字幕的定量可理解性与实用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三模块框架：首先用检测网络确认目标类别并估计其相对尺寸（以场景最大目标为参照，输出1–5离散等级）；随后将目标作为节点、尺寸差异作为边权构建尺寸感知图，并用图卷积网络传播尺寸上下文；最后把增强后的节点特征送入Transformer字幕生成器，在解码端额外引入尺寸嵌入向量以显式预测“large/small”等形容词。训练阶段采用交叉熵+尺寸等级回归联合损失，并在推理时强制尺寸词束搜索约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM、Sydney、RSICD三个公开数据集及作者扩展的带尺寸标注版本（-N-S）上，新框架在BLEU-4、CIDEr等标准指标上平均提升2.3–4.1分；新指标SizeNum-Meteor（同时惩罚数量与尺寸错误）比最佳基线高出8.7分，人类评估显示尺寸描述准确率从62%提升到84%。消融实验表明相对尺寸估计模块贡献最大，单独移除即导致SizeNum-Meteor下降5.2分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预先训练的检测器，若检测漏检或尺寸估计错误会级联放大到字幕；相对尺寸仅用五级离散码，无法表达细粒度或绝对尺度；SizeNum-Meteor目前只支持英语且需人工规则匹配，跨语言或跨传感器通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入亚米级分辨率/多光谱信号直接回归目标物理长度，并探索连续向量尺寸表示以支持更细腻的形容词生成；同时扩展SizeNum-Meteor至少数民族语言及多源卫星数据，实现跨域尺寸一致性评估。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及遥感视觉-语言任务、目标属性描述或细粒度语义评价，该文提供的尺寸感知图嵌入范式、尺寸回归损失及SizeNum-Meteor代码可为实验基线与评价工具提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语义占位预测的分层上下文对齐：解耦几何与时序建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bohan Li，Jiajun Deng，Yasheng Sun，Xiaofeng Wang，Xin Jin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650478</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多帧特征聚合时同位置语义不一致导致的上下文错位问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分层对齐：先解耦几何与时间分支分别对齐，再按语义一致性全局融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、NuScenes-Occupancy与NuScenes LiDAR分割上超越SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何与时间上下文解耦并引入置信度-位姿先验的分层对齐框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为基于相机的3D语义占用预测提供更鲁棒的时空特征融合范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目相机只能给出 2D 观测，而自动驾驶等应用需要稠密 3D 语义占用预测；现有方法把多帧上下文直接拼在一起，却常因遮挡、动态物体或位姿误差导致同一空间位置在不同时刻的特征语义不一致，从而削弱融合效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-SOP 将多帧图像特征先解耦成“几何体积”和“时序体积”两条并行流：几何流利用深度置信度做局部邻域匹配，时序流利用相机位姿做帧间匹配，各自完成初步对齐；随后把两组对齐后的 4D 体积按语义一致性进行全局重对齐，并以可学习权重合成最终语义占用体积，实现层级式上下文校准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SemanticKITTI 与 NuScenes-Occupancy 语义场景补全任务上，Hi-SOP 将 mIoU 分别提升 2.8 和 3.4 个百分点，达到新的 SOTA；在 NuScenes LiDAR 语义分割任务上亦取得 1.6 mIoU 的增益，验证了几何-时序解耦对齐策略的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖准确的深度置信度和相机位姿，在剧烈颠簸或传感器失效场景下对齐质量可能下降；层级融合引入额外计算与显存开销，实时性仍逊于单帧方法；论文未探讨与基于激光雷达或毫米波雷达的融合潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自标定与不确定性估计，使对齐模块对位姿/深度误差更鲁棒；或把几何-时序解耦思想扩展到多模态输入，实现视觉-激光雷达统一占用预测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 语义感知、多帧融合或占用网格表示，本文提出的“解耦-对齐-合成”范式可直接迁移到其他传感器配置，也可作为特征一致性正则化策略嵌入现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651449" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SURFNet: A Surface-aware UAV-Satellite Geolocation Framework via Feature Aggregation and Dual Positional Encoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SURFNet：一种通过特征聚合与双位置编码实现的表面感知无人机-卫星地理定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Liu，Wensheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651449" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651449</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机-卫星跨视角地理定位在域偏移下因背景捷径与布局错位导致的脆弱性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SURFNet两阶段框架，结合SSAA数据增强、双位置编码DPE与地表感知聚合块GAB。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在University-1652与SUES-200上获得竞争性精度，并实现U→S零样本跨数据集体位最优泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模地表结构，用SSAA保留显著面要素并合成结构保持伪背景，配合旋转感知编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GNSS拒止环境下无人机导航提供准确且可迁移的视觉定位特征，推动CVGL实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位(CVGL)在GNSS失效环境中对无人机导航至关重要，但背景捷径与布局错位导致模型在域偏移下极易失效。现有方法多依赖外观匹配，缺乏对地面结构显式建模，难以在不同数据集或飞行高度间泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SURFNet采用两阶段框架：首先提出卫星语义增广算法(SSAA)，在保留显著地表要素的同时生成结构保持的伪背景，扩充卫星训练集；其次设计双位置编码(DPE)将可学习2D嵌入与旋转感知编码结合，并通过地面感知聚合块(GAB)动态融合局部-全局线索，实现显式地面结构建模。整体流程以表面感知增强与结构引导聚合为核心，输出对场景类型鲁棒的特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在University-1652与SUES-200上，SURFNet不仅取得有竞争力的域内精度，还在University-1652→SUES-200的零样本跨高度迁移中刷新SOTA；仅用50类子集冻结主干微调轻量头，即可把University-1652→DenseUAV的巨大风格/覆盖差距显著缩小。子集实验表明，模型利用广义表面显著性而非仅限于SSAA四类，即可在水体、自然景观等场景稳健泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSAA依赖卫星影像可获取的语义分割模型，若分割失败会引入伪影；零-shot迁移仍受源域高度分布与场景丰富度限制，如SUES-200→University-1652因源域多样性不足出现性能不对称；实时部署时两阶段推理与多分支聚合带来额外计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或自监督语义估计以摆脱对精确分割模型的依赖，并研究自适应位置编码以在线调整不同飞行高度与视角。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为CVGL领域首提“表面感知”显式建模的工作，其增广-编码-聚合三环节可迁移至其他跨视角匹配任务，为研究域泛化、GNSS拒止导航或遥感影像匹配的学者提供可直接对比的新基线与开源思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01984v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">借助蓝图思考：通过结构化对象表征提升视觉-语言模型的空间推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijian Ma，Shizhao Sun，Tianyu Yu，Ruiyu Wang，Tat-Seng Chua 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01984v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升视觉-语言模型在空间推理任务中的全局空间语义理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建JSON式对象蓝图，结合蓝图嵌入微调、蓝图感知奖励与反捷径数据增广</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项空间推理基准上持续优于现有VLM及专用模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对象中心蓝图引入VLM，实现结构化空间表示与因果推理对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可解释空间推理框架，推动感知向语义理解跃迁</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLM)在视觉感知任务上表现优异，但在空间推理——即理解物体间三维关系与全局布局——方面仍显著落后于人类。现有方法要么反复关注局部图像块，牺牲全局空间感，要么仅记录孤立坐标，忽略物体组织的整体结构。作者受此启发，希望引入一种兼顾局部属性与全局布局的结构化表示，以提升VLM的空间语义理解能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“以蓝图思考”框架：先让模型为图像生成JSON风格蓝图，列出相关物体的位置、尺寸与属性，再在该结构化表示上进行推理得出答案。具体实现包含三项技术：①在监督微调阶段构造嵌入蓝图的推理链，使模型习得基本空间推理技能；②在强化学习阶段设计蓝图感知奖励，既鼓励蓝图包含适量关键物体，又要求最终答案与蓝图因果一致；③采用抗捷径数据增强，对图像与问题施加针对性扰动，抑制模型依赖表层视觉或语言启发。推理时，模型先输出蓝图再输出答案，全程以结构化对象表示为核心。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个空间推理基准（包括VSR、SPATIAL-VL、GQA等）上，该方法持续优于同等规模的现有VLM以及专为空间任务设计的模型，平均提升3–7个百分点。消融实验表明，蓝图表示、蓝图感知奖励与抗捷径增强均对性能有显著贡献，且可视化显示蓝图能有效捕捉物体层级与方位关系。结果证实结构化对象表示可显著增强VLM的空间语义理解，而不增加推理时图像分辨率或参数规模。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>蓝图生成依赖物体检测或分割模型的精度，若前端漏检或误检，错误会传播至后续推理；目前框架主要针对静态二维图像，尚未扩展到动态或三维场景。此外，JSON蓝图格式虽可读，但对极大场景可能面临序列长度与层次扩展性问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将蓝图升级为动态或层次化图结构，并直接集成可微分渲染或三维感知模块，以支持视频与具身环境的空间推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在空间、布局或三维语义推理上的提升，或致力于结构化表示、因果推理与数据增强技术的结合，本论文提供了可复现的蓝图框架与训练策略，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的鲁棒多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声干扰下实现多源遥感影像的高效半稠密匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MARSNet，用Mamba捕获长程依赖、冻结DINOv2抗噪、自适应融合并改进线性注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在含噪与无噪多源遥感数据及大规模三维重建任务上，匹配精度与鲁棒性均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba结构引入遥感匹配，结合冻结DINOv2与自适应Mamba式线性注意力实现高效抗噪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像配准、三维重建提供兼顾效率与鲁棒性的新框架，推动噪声场景下多源数据融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和成像机理不一致的共同作用下，传统稀疏或半稠密匹配方法精度骤降，而无检测器方法在大视角与强噪声并存时效率低、鲁棒性差，已成为制约后续三维重建与变化检测的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MARSNet，将 Mamba 线性扫描结构嵌入 Transformer，以线性复杂度捕获长程依赖；冻结的 DINOv2 作为强噪声不变特征提取器，避免重新训练带来的过拟合；提出自适应跨模态融合模块，动态加权光学与 SAR/红外特征；最后用类 Mamba 线性注意力替代传统自注意力，在 1K×1K 影像上实现 O(N) 复杂度的半稠密匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的噪声-free、随机高斯噪声、周期性条带噪声三种多源遥感测试集上，MARSNet 的匹配召回率平均提升 8.7%，重投影误差降低 23%；在 1.2 M 张通用多视角数据集做位姿误差评估，SOTA 方法 0.67 px 的中误差被压缩到 0.48 px，验证了三维重建任务的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与多源遥感噪声数据集，难以复现；DINOv2 冻结导致网络对极端辐射差异的适应性仍受预训练分布限制；Mamba 扫描顺序在旋转大於 45° 时可能丢失跨行上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索扫描顺序的自学习策略，并将网络扩展到视频级多源时序匹配，以服务于动态目标监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究跨模态配准、三维重建或噪声鲁棒特征学习，该文提供了线性复杂度下兼顾精度与效率的新范式，可直接借鉴其 Mamba-Attention 混合架构与冻结基础模型的组合思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01526v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BARE：迈向偏见感知与推理增强的单塔视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbing Li，Linhui Xiao，Zihan Zhao，Qi Shen，Yixiang Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01526v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制单塔视觉定位中的模态偏差并强化指代推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>保留模态专属特征，引入语言显著调制、视觉偏差校正与指代关系增强三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上实现新SOTA，同时计算效率优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在单塔框架内联合显式去偏与指代语义推理，提出模块化三组件设计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效且可解释的多模态指代理解提供了去偏新思路与即插即用模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Grounding 要求模型仅依据自然语言表达式在图像中精确定位目标区域，是多模态理解的核心任务。近期单塔(one-tower)架构通过共享编码器融合图文特征，在零样本与迁移场景下取得进展，却暴露出模态偏差被过度放大、指代推理不足两大痛点，制约了鲁棒性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BARE 框架，在单塔结构内显式保持模态私有特征，通过三个协同模块进行偏差抑制与推理增强：i) Language Salience Modulator 先提取词汇显著性再动态重标定文本特征，抑制非关键词的干扰；ii) Visual Bias Corrector 利用对比学习校准视觉区域分布，削弱数据集固有的共现偏好；iii) Referential Relationship Enhancement 构建跨模态异构图并执行消息传递，迭代推理对象-属性-关系三元组以强化指代语义。整体采用端到端训练，仅增加轻量投影与图网络，保持单塔的高效推理优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO、RefCOCO+、RefCOCOg、Flickr30k Entities 与 Visual Genome 五项基准上，BARE 将平均 top-1 精度提升 2.1-4.8 个百分点，达到新 SOTA，同时推理速度比双塔方法快 1.6×，参数量减少 23%。消融实验显示各模块对偏差缓解与推理增益贡献互补，可视化表明激活区域与真实指代目标重叠率提高 9.3%，验证了偏差抑制与语义细化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英语指代表达与公开基准上验证，跨语言或复杂长句场景下的泛化能力尚不明确；偏差校正模块依赖统计先验，若测试分布与训练差异显著，可能引入新的估计误差。此外，指代图构建采用固定启发式规则，对隐含关系或多重指代的建模仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将 BARE 的偏差校正思想扩展至视频指代定位及视觉对话，实现时空一致的多轮推理；或引入可学习的关系挖掘替代手工图结构，进一步提升复杂指代场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合中的偏差治理、单塔架构的效率-性能权衡，或希望将语义推理模块迁移到 VQA、图像字幕等任务，BARE 提供了一套即插即用且开源的解决方案与详尽实验分析，可直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651125" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-view and Multi-step Interaction for Change Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨视角多步交互的变化描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tiantao Xian，Zhiheng Zhou，Wenlve Zhou，Delu Zeng，Bo Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651125" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651125</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视角偏移干扰下为图像对生成准确、稳定的真实变化语义表示并生成自然语言描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨视角多步交互网络：跨视角交互编码器增强单图内部关系，多步变化感知器由粗到精捕捉变化，融合模块生成细粒度表示，并辅以自监督反向重建模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开数据集上达到SOTA，显著抑制伪变化干扰，生成更精准的变化描述。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨视角交互与多步渐进式变化感知结合，并引入自监督反向重建提升变化语义捕获能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为变化描述、遥感监测、自动驾驶等需跨视角变化理解的领域提供鲁棒特征提取与描述生成新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change captioning 需要同时理解两张图像并精确定位语义差异，但现有方法在视角变化带来的伪变化干扰下容易生成不稳定或错误的描述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨视角多步交互网络：先用跨视角交互编码器在单图内部做交叉参照以强化特征，再用多步变化感知器由粗到细地抽取变化语义，随后通过融合模块动态整合成细粒度变化表示；另外设计反向表示重建模块，以自监督方式约束模型聚焦真实语义变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开数据集上达到 SOTA，显著降低视角偏移导致的伪变化误判，生成的变化描述在准确性与稳定性方面均优于现有方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对图像的精确对齐，对严重几何畸变或光照突变场景鲁棒性未验证；多步交互引入额外计算开销，实时性受限；反向重建损失的超参数需针对数据集仔细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无对齐假设的宽松匹配策略，并引入视觉-语言预训练以提升零样本泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决视角变化下的伪变化干扰，为跨视角差异检测、图像对比描述及自监督表示学习提供可复用框架与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01416v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AirSpatialBot：具备空间感知的空中智能体用于细粒度车辆属性识别与检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Zhou，Ran Ding，Xue Yang，Xue Jiang，Xingzhao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01416v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升遥感视觉-语言模型对无人机车辆影像的空间理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含3DBB的AirSpatial数据集，采用图像预训练+空间微调两阶段策略训练VLM</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提AirSpatialBot显著优于现有VLMs，暴露其空间推理缺陷</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入3DBB的遥感空间定位与问答数据集及可执行细粒度检索的空中智能体</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供大规模空间指令数据与空间感知训练范式，推动无人机应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉-语言模型(VLM)已在跨模态理解上取得显著进展，但在空间定位与推理方面仍明显落后于自然场景模型，制约了其在精细交通监控、智能巡检等真实任务中的可用性。无人机影像中车辆目标小、视角高、密集排列，对模型的细粒度属性识别与空间关系理解提出了更高要求，亟需专门的数据集与算法框架来填补空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了一个含20.6万条指令的无人机车辆影像数据集AirSpatial，首次引入3D边框(3DBB)并设计了两个新任务：空间定位(Spatial Grounding)与空间问答(Spatial QA)。训练采用两阶段策略：先在通用图文对上执行图像理解预训练，再在AirSpatial上进行空间理解微调，从而将已有VLM知识迁移到遥感空间域。基于微调后的空间感知VLM，作者开发了空中智能体AirSpatialBot，它通过动态任务规划、图像理解、空间推理与执行模块的闭环协作，实现任意自然语言查询下的细粒度车辆属性识别与检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，现有主流VLM在空间定位与问答任务上显著落后于AirSpatialBot，验证了对遥感空间理解进行专门训练的必要性。AirSpatialBot在细粒度属性识别与多约束检索准确率上均取得SOTA，且可解释地输出3D边框与推理链，为后续决策提供直观依据。消融实验显示，3DBB与两阶段训练分别带来约8%和12%的性能增益，证明空间先验与渐进式迁移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖无人机视角下的车辆类别，尚未扩展到多类遥感目标或卫星影像，限制了模型的通用性。3D边框依赖激光雷达或多视角重建，在纯RGB单图场景下无法获取，可能影响实际部署的可行性。此外，20万条指令虽规模可观，但语言模板仍可能带来隐性偏差，影响模型在开放词汇查询上的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入卫星与无人机多源影像，构建跨视角、跨分辨率的空间理解基准，并探索无3D监督下的单目深度估计与空间推理。结合链式思维(CoT)与大规模语言模型，实现更复杂的遥感时空问答与决策规划。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将3D空间定位与问答引入遥感VLM，为从事细粒度目标识别、跨模态检索、无人机智能体或遥感基础模型的研究者提供了公开数据集、训练策略和完整代码，可直接作为后续算法对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01513v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FastV-RAG：面向快速且细粒度的视频问答之检索增强生成方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gen Li，Peiyu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01513v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在视频问答中既快速又准确地利用外部知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VideoSpeculateRAG，用轻量草稿模型生成候选答案再由重模型验证，并引入相似度过滤修正实体</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持或提升准确率的同时将推理速度提高约2倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将投机解码与RAG结合用于视频QA，并用简单相似度过滤解决实体识别错误</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时响应的知识密集型多模态任务提供了高效可靠的RAG范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown strong visual reasoning skills but still lag when required to fuse external knowledge for video question answering. Existing RAG pipelines for video QA are computationally heavy and often degrade answer quality because retrieved knowledge is noisy and entity recognition is brittle.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose VideoSpeculateRAG, a two-stage speculative decoding framework: a lightweight draft VLM rapidly proposes multiple answer candidates, and a heavier verifier VLM re-scores and refines only the promising ones, cutting overall inference time. To curb entity mismatch errors in retrieved text, they add a cosine-similarity filter that aligns named entities in the passage with those in the question before the passage is fed to the model. All components are trained end-to-end with a contrastive objective that rewards both answer correctness and retrieval precision.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MSRVTT-QA, ActivityNet-QA and NExT-QA the system matches or exceeds standard RAG baselines while running ~2× faster, with absolute gains of +1.8–3.4 % in accuracy and a 46 % reduction in entity linking errors. Ablation shows speculative decoding contributes 70 % of the speed-up and the entity filter contributes 60 % of the accuracy improvement, verifying that both ideas are complementary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to short-form videos (&lt;120 s) and English captions; longer videos or multilingual settings may expose memory bottlenecks. The draft and verifier models share the same architecture family, so the approach may generalize poorly when the two models are structurally incompatible. Retrieval still relies on CLIP-style features, which can miss fine-grained temporal moments.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend speculative decoding to hierarchical video representations and integrate reinforcement learning to train the draft model to propose higher-recall candidates. Explore joint training of retrieval and generation to further tighten the knowledge gap.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal reasoning, speculative decoding, or knowledge-enhanced VLMs can borrow the two-stage verification and entity-filtering ideas to boost both speed and robustness in their own RAG pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651028" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causality-Inspired Graph Neural Networks for Cross-Modal Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨模态检索的因果启发的图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Li，Zhixin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651028" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651028</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除跨模态检索中GNN因对象语义冲突与分布漂移导致的不可信匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨模态结构因果模型，用因果干预GNN提取潜在因果表示，并辅以模态对比学习与软排序损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，消融实验验证各模块均有效提升检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果干预引入GNN进行跨模态检索，提出软排序损失实现细粒度相似度排序。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升GNN在跨模态任务中的可解释性与鲁棒性提供了因果视角的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态检索(CMR)需要同时理解图像、文本等多模态语义，现有GNN方法虽能建模复杂关联，却易受细粒度分类器输出的冲突语义影响，导致对分布外样本敏感且解释性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出因果启发的图神经网络(CIGNN)：先构建跨模态结构因果模型，用do-calculus干预剥离混淆因子，学习潜在因果表示而非表层对象语义；再设计模态对比损失，在公共空间中显式对齐 intra-/inter-modal 样本；最后引入软排序损失，用连续相似度替代0/1标签实现细粒度排序优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30k和MSR-VTT上的mAP、R@K指标均优于现有最佳，平均提升3.2-4.7个百分点；消融实验显示因果干预、对比损失与软排序分别贡献约40%、35%、25%的性能增益，可视化表明因果表示对背景扰动鲁棒且可解释。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>因果图依赖预定义模态节点与边，若真实因果结构未知或动态变化可能引入新偏差；干预采样在大规模图上的计算开销显著；软排序超参数需针对数据集单独调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的因果图发现机制，减少人工结构假设；将干预过程蒸馏为轻量级模块，实现端到端高效训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态理解、可解释GNN或因果机器学习的学者，该文提供了将因果推理与图网络结合的新范式，可直接迁移到视频-文本、音频-图像等其它异构检索任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMformer: A Single-Stage CNN-Transformer Hybrid for Weakly Supervised Semantic Segmentation in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMformer：面向航空影像弱监督语义分割的单阶段 CNN-Transformer 混合模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruixue Zhou，Jihao Li，Wenkai Zhang，Shuoke Li，Jialiang Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651514" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651514</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用图像级标签实现航拍影像弱监督语义分割，克服CNN感受野小、激活不全及背景混淆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单阶段CNN-Transformer混合框架CAMformer，辅以类间解耦、不确定度加权优化与上下文干预样本增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID和Vaihingen数据集上mIoU达39.5%与46.4%，领先现有单阶段方法，并在VOC2012展现跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单阶段CNN-Transformer协同、类间解耦、不确定度加权与上下文干预集成，显著提升伪标签质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模遥感影像提供高效低成本精准分割方案，推动CNN-Transformer混合与上下文学习在RS WSSS中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像弱监督语义分割（WSSS）仅用图像级标签即可训练，极大降低大规模场景标注成本，但纯CNN方法受限于局部感受野，易出现激活不完整与背景混淆。Transformer虽能建模长程依赖，却多为多阶段流程、计算冗余，且对遥感特有的类别噪声与上下文偏差缺乏专门处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段CNN-Transformer混合框架CAMformer，以并行分支同时提取局部细节与全局语义并融合生成可靠伪标签。针对遥感类别冲突，设计类间解耦策略ICD，在特征空间分离易混类别；引入不确定性加权优化UWO，用像素级不确定性抑制噪声激活；提出上下文干预样本增强CISE，对训练图像进行依赖关系扰动，削弱场景上下文偏差。整体端到端训练，无需额外多阶段后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在iSAID与ISPRS Vaihingen两大挑战性数据集上，CAMformer分别取得39.5%与46.4% mIoU，优于现有单阶段方法，并在PASCAL VOC2012跨域验证中展现一定泛化能力。消融实验表明ICD、UWO、CISE各模块分别带来2-4% mIoU提升，验证了混合架构与上下文感知学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开遥感数据集及一个自然影像集上测试，尚未验证在其他分辨率、传感器或更细粒度类别下的稳健性。方法引入三个新模块，整体参数量与训练显存高于纯CNN基线，对资源受限场景部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级CAMformer变体以适应机载或边缘设备，并将ICD/UWO思想扩展到点云、多光谱等跨模态遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感弱监督分割、CNN-Transformer混合设计或噪声鲁棒学习，本文提供的单阶段框架与上下文干预策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010161" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Injected Discriminative Model for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感视觉定位的文本注入判别模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minhan Hu，Keke Yang，Jing Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010161" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010161</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感视觉定位中同时提升视觉-语言语义聚焦与相似目标区分能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将文本线索轻量级注入视觉骨干，并引入框外注意力正则化损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上取得领先性能，验证简单融合与正则化策略有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需复杂融合模块的文本注入骨干及针对遥感场景的框外注意力正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态细粒度定位提供高效新范式，可直接嵌入现有检测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉定位(RSVG)任务要求模型根据自然语言描述在高分辨率遥感图像中精确标出目标边界框。传统两阶段方法先独立提取视觉与文本特征再进行融合，导致视觉编码阶段缺乏对描述中关键语义的关注，难以突出待定位对象。遥感影像中同类地物外观高度相似、背景复杂，进一步加剧了精确定位的难度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种“文本注入式判别模型”：在视觉骨干网络(如ResNet)的每个残差块末端，仅用1×1卷积将语言特征向量映射到视觉特征通道维度并逐通道相加，实现零参数增加的语言引导视觉增强。为抑制混淆背景，设计Attention Regularization Loss：在预测框外采样K个难分辨负样本区域，强制其注意力权重低于框内目标权重，从而放大目标-背景差异。整体训练仍沿用DETR风格的集合预测损失，仅额外加入λ·L_attn正则项。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个RSVG基准(NWPU VHR-RSVG、RSVG-1k、DIOR-RSVG)上，该方法以≤2%的参数量增幅将平均Top-1定位准确率提升3.1-4.7个百分点，达到新的SOTA；可视化显示注入文本后热力图显著聚焦于描述关键词对应的地物，且正则化项使框外相似目标的响应降低约18%。消融实验表明语言注入与注意力正则分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单源光学影像上验证，未涉及多光谱、SAR或时序数据；注意力正则依赖额外的难负采样启发式规则，对超参数K敏感；方法仍基于离线提取的文本特征，尚未探索端到端的大语言模型协同。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究将多模态大模型作为统一编码器，实现遥感影像-文本-地理知识联合推理；并引入自监督预训练以利用海量未标注遥感-文本对，提升模型对新颖地物描述的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感理解、轻量化融合设计或细粒度目标定位，该文提供的“即插即用”文本注入单元与背景-区分正则化策略可直接迁移至遥感字幕生成、变化描述检测等任务，减少重新设计复杂融合模块的成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PVF-DectNet++：基于透视体素的自适应多模态融合三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ke Wang，Weilin Gao，Kai Chen，Tianyi Shao，Liyang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3650671</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机融合中深度缺失与固定权重导致的语义提取不足、适应性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>透视体素投影对齐特征，RGB-I 稠密插值提取全局多级图像特征，可学习融合模块自适应加权</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI 上 mAP 提升 3.56%，nuScenes mAP/NDS 增 3.8%/2.6%，Waymo 行人/骑行者 AP 显著提高</p>
                <p><span class="font-medium text-accent">创新点：</span>提出透视体素对齐与 RGB-I 多通道表示，并设计可学习通道自适应融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶 3D 检测提供更精准的多模态融合方案，尤其改善小目标与复杂环境表现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶对3D目标检测的精度要求极高，单一LiDAR或相机模态均难以同时提供稠密几何与丰富语义。现有LiDAR-相机融合方法普遍依赖图像深度估计误差大且融合权重固定，导致在行人、骑行者等小目标上召回率不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PVF-DectNet++提出透视体素投影(Perspective Voxel Projection)，将点云与图像对齐到统一视角空间，避免显式深度估计误差传播。其自适应图像语义提取模块把RGB与点云反射强度插值为稠密RGB-I多通道张量，通过全局上下文网络捕获多级语义。融合阶段引入可学习通道-空间权重网络，根据外观、颜色及环境变化动态调整LiDAR与图像特征的贡献比例。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI基准上，PVF-DectNet++对行人、骑行者、车辆的AP分别达到66.3%、78.8%、86.8%，mAP较基线PVF-DectNet提升3.56%。nuScenes上mAP与NDS分别再涨3.8%和2.6%，Waymo上小目标AP提升更为显著，证明其对多数据集、多天气、多密度的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>透视体素投影依赖高精度外参标定，标定误差会直接扭曲跨模态对齐；动态权重模块增加参数量与显存，对车载嵌入式GPU的实时性提出更高要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定或自标定的柔性对齐机制，并将可学习融合压缩为轻量化二进制掩码以提升边缘端推理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态3D感知、小目标检测或动态融合策略，本文提供的透视体素对齐与自适应权重思路可直接迁移到其它LiDAR-视觉框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Excluding the Interference for Open-Vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">开放词汇语义分割中的干扰排除研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Shao，Shiyuan Zhao，Rui Xu，Yan Wang，Baodi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650803</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 在开放词汇语义分割中易将新类误判为常见类，需排除干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>EXIST-Net 两阶段框架：MPN 生成掩码，MFC 与 MRC 分别计算包含/排除概率，PCor 校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上显著提升新类分割精度，MRC 块模型无关且资源占用低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“排除干扰”反向分类器 MRC，用排除概率修正识别，实现两阶段精细分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇语义分割提供即插即用低耗模块，助力 CLIP 类模型抑制类别偏差。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)需在测试时识别训练阶段从未见过的类别，而主流CLIP-based方法因类别不平衡与训练数据仅含已知类，易将新类误判为常见类，产生“干扰项”问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EXIST-Net，将单步识别拆为两阶段：先用Mask Proposal Network生成类无关掩码，再由Mask Forward Classifier给出掩码属于某类的包含概率；核心为Mask Reverse Classifier，并行计算掩码“不属于”每类的排除概率，最后Probability Corrector用排除概率修正包含概率，实现“先排除干扰、再精细分类”。MRC模块仅增加轻量反向分类头，可与任何CLIP-based分割框架即插即用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个主流OVSS数据集上，EXIST-Net显著提升了新类识别精度并降低干扰误判，平均mIoU优于ELSE-Net及最新方法约2–4个百分点；消融实验表明仅引入MRC即可为现有方法带来稳定增益，且显存与推理时间增幅&lt;5%，验证其模型无关性与低资源消耗。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在RGB图像基准上验证，未探讨复杂场景(强光照、跨域)下排除概率的稳定性；MRC依赖预定义类别名，若开放词汇极度扩展，反向分类头计算量线性增长，可能削弱效率优势。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言蒸馏将MRC压缩为动态提示，或利用不确定性估计自适应决定是否启用排除机制，以进一步扩展词汇规模并提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇/零样本分割、CLIP偏差修正或即插即用模块设计，本文提供的“排除-再确认”范式与轻量MRC块可直接迁移至相关框架，加速新类识别性能的提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AbductiveMLLM：在 MLLM 中增强视觉溯因推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyu Chang，Qi Wang，Xi Guo，Zhixiong Nan，Yazhou Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02771v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER&#39;s output embeddings to &#34;imagine&#34; plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs&#39; contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升多模态大模型在视觉溯因推理中的能力，缩小与人类表现的差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AbductiveMLLM，含REASONER语言溯因与IMAGINER图像想象两协同模块并端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准VAR基准上达到SOTA，持续超越传统方案与先进MLLM</p>
                <p><span class="font-medium text-accent">创新点：</span>首次让MLLM模仿人类双通道溯因：用语言推理剪枝并生成视觉先验，再用扩散模型想象场景反哺推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要解释不完整视觉信息的AI应用提供可扩展溯因框架，推动多模态推理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉溯因推理(VAR)要求系统对不完整的视觉片段给出最可能的解释，是衡量多模态模型因果理解能力的重要任务。现有MLLM在通用推理上表现突出，但在溯因任务上仍显著落后于人类，提示其缺乏对视觉-语义因果一致性的深层建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AbductiveMLLM，通过双通道协同模仿人类&#34;言语-图像&#34;溯因机制：REASONER先在文本空间用盲LLM枚举候选解释，再以跨模态因果对齐筛除与视觉冲突的假设，并将剩余假设作为先验嵌入注入MLLM；IMAGINER则把筛选后的文本解释与输入视频共同条件化到扩散模型，生成对应&#34;想象场景&#34;，为MLLM提供额外的视觉上下文。两部分端到端联合训练，使模型在推理时同时获得言语先验与自生成视觉证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VAR基准（包含CLEVR-ABD、Abductive-VIST、Kinetics-600-ABD等）上，AbductiveMLLM将SOTA绝对准确率提升4.2-7.8个百分点，并在人类评估的因果合理性评分中超越GPT-4V与Gemini-Pro平均12.3%。消融实验显示，仅REASONER可带来约60%增益，加入IMAGINER后进一步提升，验证了双通道设计的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外扩散模型，推理时需生成多张256×256图像，导致延迟增加约1.8倍；言语假设空间仍受限于盲LLM的知识边界，对长尾或新兴场景可能生成偏见先验；目前仅评估了短视频片段，尚未验证在长时间跨度和多事件链溯因中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散生成升级为视频扩散以直接输出时序一致想象片段，并引入强化学习从人类反馈中优化假设先验，减少幻觉。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态因果推理、生成式世界模型或人类认知启发的AI架构，本文提供了可复现的双通道溯因框架与代码，可直接作为基线或模块嵌入其他MLLM系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于剪枝即搜索与蒸馏的轻量化三维场景图生成压缩框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hojun Song，Chae-yeong Song，Dong-hun Lee，Heejung Choi，Jinwoo Jeong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅压缩3D场景图生成模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以剪枝即搜索定压缩率，再用结构化剪枝+知识蒸馏恢复精度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型体积减半以上，分类准确率反而提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向3DSGG的压缩框架，将剪枝视为搜索并设计专用蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供轻量高性能3D场景理解方案，推动实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景图生成(3DSGG)通过同时预测物体类别与谓词关系来刻画三维场景，是3D场景理解的新兴核心任务。现有基于图神经网络(GNN)的方法虽显著提升精度，却带来庞大参数量与计算开销，严重阻碍在资源受限设备上的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向3D场景图生成的压缩框架，分两阶段进行：阶段一将剪枝视作神经架构搜索，自动寻找最优压缩率；阶段二采用结构化剪枝去除冗余通道，并设计新的知识蒸馏策略，使轻量学生网络从教师网络中精准继承物体-谓词联合分布。框架整合了可微搜索、重要性评分与特征-关系双重蒸馏模块，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的实验表明，该方法在模型体积缩减50%以上的同时，物体与谓词分类准确率反而提升，总体mAP平均提高约2.3个百分点，验证了压缩与性能兼得的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一场景类别与固定点云输入分辨率下验证，压缩后模型在跨场景泛化及更低比特量化下的鲁棒性尚未探讨；此外，剪枝-搜索阶段仍需训练完整教师网络，带来额外碳足迹与计算成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将剪枝-搜索与量化、低秩分解联合优化，并引入场景语义先验以进一步压缩；同时探索无教师自蒸馏方案，降低预训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D场景理解、图神经网络压缩或边缘智能的研究者，该文提供了首个系统的3DSGG轻量化范例与开源代码，可直接迁移到相关任务并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02422v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">看得更广，想得更深：面向复杂视觉推理的协同跨模态思维链</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenting Lu，Didi Zhu，Tao Shen，Donglin Zhu，Ayong Ye 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02422v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有跨模态思维链过度依赖单一大区域且推理步骤语义割裂，阻碍复杂视觉推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CoCoT框架：动态多区域定位选最相关图像区，关系感知推理迭代对齐多区视觉线索形成连贯链。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六基准上LLaVA-1.5平均提升15.4%，Qwen2-VL提升4.0%，并发布74k样本CoCoT-70K数据集。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态多区域协作与关系感知链式推理结合，构建带多区标注与结构化链的大规模视觉推理数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可扩展的细粒度跨模态推理范式，推动复杂问答与认知AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言推理方法在跨模态链式思考(CoT)中常把整幅图当作单一粗粒度区域，导致关键细节被淹没，且各步推理之间缺乏语义连贯，出现碎片化。作者观察到这两点是制约复杂视觉推理性能的核心瓶颈，因而提出让模型“看得更宽、想得更深”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CoCoT框架包含两大模块：Dynamic Multi-Region Grounding先用可变形注意力根据当前问题动态检测并裁剪多个细粒度区域，再为每个区域生成视觉token；Relation-Aware Reasoning则在每步推理中通过跨模态注意力迭代对齐不同区域的视觉线索，使它们协同更新隐藏状态，从而构建一条逻辑连贯的跨模态思维链。训练数据方面，作者基于现有VQA与图像字幕语料，利用自动管道标注多区域边界与逐步推理链，构建出74k样本的CoCoT-70K。整个框架以指令微调方式嵌入到LLaVA-1.5和Qwen2-VL，无需修改底层LLM结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMBench、MMMU、MathVista等六项挑战性基准上，CoCoT将LLaVA-1.5的平均准确率提升15.4%，给Qwen2-VL带来4.0%的绝对增益，尤其在需要多步计数、属性比较与时空推理的问题中改进幅度最大。消融实验显示，仅添加多区域 grounding 或仅添加关系推理分别提升约7%和5%，二者组合产生互补效应。人类评估表明，CoCoT生成的推理链在逻辑连贯性与区域引用准确率上优于传统CoT模板。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>动态区域选择依赖预训练检测器，若场景物体密集或罕见则可能出现漏检与误对齐；推理链仍由LLM自回归生成，存在幻觉累积风险，且缺乏形式化逻辑验证。此外，CoCoT-70K的自动标注管道对复杂几何或抽象概念覆盖不足，可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可微分逻辑模块对推理链进行约束验证，并探索与视频时序区域追踪结合，实现长程动态事件推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态推理、链式思考或细粒度视觉定位的研究者，该文提供了系统性的多区域协同思路与大规模带推理链数据集，可直接用于微调或作为基线比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越LLaVA-HD：深入探索高分辨率多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YiFan Zhang，Qingsong Wen，Chaoyou Fu，Kun Wang，Xue Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650761</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不暴增计算量的前提下，让多模态大语言模型真正利用高分辨率图像进行精细视觉推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全局-局部双分支：混合适配器提炼全局语义，可学习查询压缩局部块并用相似度筛选关键令牌，交替训练优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>更少但更信息丰富的局部令牌即可提升性能，交替训练平衡全局-局部学习，仅用200万数据达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合适配器全局专家、查询式局部令牌压缩与相似度选择、交替训练策略集成于高分辨率MLLM框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高分辨率视觉-语言理解提供新范式，显著降低计算与数据成本，对多模态模型研究与部署具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视觉输入对多模态大语言模型(MLLM)的视觉感知与推理至关重要，但现有LLaVA-HD类方法简单地把切片局部图块放大到与全局分支同分辨率，导致随分辨率提升计算量激增，且局部token过多会淹没全局上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SliME框架：全局分支用混合适配器(Mixture of Adapters)挖掘不同任务擅长的专家特征；局部分支引入可学习查询嵌入先压缩图块token，再用基于相似度的选择器保留与问题最相关的少数token；训练上采用全局挖掘块与局部压缩块交替训练而非同时端到端，以平衡两者学习；并构建了一个对细节要求极高的新数据集强化局部压缩层训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验在仅200万训练数据下取得多个基准领先成绩，验证“少即是多”现象：用更少但高信息量的局部token反而提升性能；交替训练策略显著优于联合端到端训练；新数据集有效增强模型对细粒度细节的捕捉能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖手工设计的适配器组合与查询数量，可能难以泛化到所有视觉任务；相似度选择器基于余弦相似度，可能忽略低相似但关键的细节token；整体流程增加训练与推理的工程复杂度，实际部署成本需进一步评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应适配器网络结构搜索与动态查询数预测，并研究无相似度选择器的可微分token稀疏化机制以实现端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统剖析了高分辨率MLLM的效率瓶颈，提出可插拔的局部token压缩与全局专家混合策略，为研究高分辨率视觉-语言模型、高效多模态推理或视觉token选择的研究者提供可直接对比与扩展的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02289v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于排序的地理正则化：再探多光谱遥感影像的对比自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tom Burgert，Leonard Hackel，Paolo Rota，Begüm Demir
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02289v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用地理元数据提升多光谱遥感影像自监督对比学习的表征质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoRank正则项，在球面特征空间直接优化样本间球面距离以嵌入地理关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoRank在多种对比SSL框架上均优于或媲美现有地理融合方法，显著提升下游性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将地理邻近度编码为球面距离约束引入对比学习损失，实现无标签地理正则化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像自监督学习提供通用地理增强策略，降低标注依赖并提升跨任务迁移能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱遥感影像数量庞大但标注稀缺，传统监督方法难以充分利用。对比自监督学习(SSL)在自然图像上已证明可学出通用表征，但遥感影像具有地理-时间异质性，直接套用CV方法会忽视空间邻近像元应拥有相似特征的先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeoRank正则项，在对比学习损失中显式加入球面距离约束，使嵌入空间中的特征距离与地理坐标的大圆距离单调相关。该正则项可即插即用到BYOL、DINO等现有框架，训练时仅利用无标签影像及其拍摄中心经纬度。具体实现为对同一mini-batch内样本按地理邻近度排序，并以排序差异惩罚特征距离，从而将地理拓扑注入表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet、EuroSAT等四个多光谱基准上，GeoRank将线性评价Top-1平均提升2.1%-3.7%，超过先前所有利用地理元数据的SSL方法。消融显示数据增强中仅保留翻转与颜色扰动即可，过多几何扭曲反而损害光谱一致性；影像尺寸从64 px增至256 px带来的增益在下游场景分类中饱和；时间跨度小于16天的视图对对比学习贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在光学多光谱影像上验证，未涵盖SAR或高光谱；地理正则假设空间邻近即语义相似，在异质地貌或云覆盖区域可能失效；代码与实验限于固定分辨率和预训练时长，尚未探讨更大规模或全球数据的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索GeoRank与时空联合嵌入的结合，以同时利用地理与季相信息；也可将正则项扩展为自适应权重，根据土地覆盖类型动态调整地理约束强度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督、地理先验建模或无标签表征学习，本文提供了可直接复用的正则模块与系统实验结论，有助于在自有数据集上快速获得性能增益并避免常见数据增强误区。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A radiometrically and spatially consistent super-resolution framework for Sentinel-2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 Sentinel-2 的辐射与空间一致超分辨率框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cesar Aybar，Julio Contreras，Simon Donike，Enrique Portalés-Julià，Gonzalo Mateo-García 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除深度学习超分在Sentinel-2影像中产生的伪影、辐射失真与几何错位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>合成数据谱-空域适配+低频硬约束网络，CNN/Mamba/Swin架构对比并用xAI解释。</p>
                <p><span class="font-medium text-accent">主要发现：</span>10m→2.5m超分PSNR领先，反射率偏差≈0，几何偏移最小，下游光谱任务保真。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域适配合成训练与强制低频保真的SEN2SR框架，公开模型与代码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测提供高可信2.5m Sentinel-2影像，支持精准地表监测与光谱应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Sentinel-2 10 m/20 m 多光谱数据在精细尺度地表监测中空间分辨率不足，而现有深度学习超分模型易产生虚假结构、辐射畸变与几何偏移，阻碍定量遥感应用。完全合成数据训练虽能控制退化过程，却常因域差异在真实影像上失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SEN2SR 框架：首先在合成数据阶段按 Sentinel-2 实际 MTF、光谱响应与成像条件进行退化并做光谱-空间调和，以缩小域差距；随后构建多架构超分网络（CNN、Mamba、Swin Transformer），在最后一层加入可微分低通硬约束层，强制输出保留原图低频频谱，实现辐射与几何自洽；最终 2.5 m 产品同时提升 RGB、NIR(10 m) 与红边、SWIR(20 m) 波段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在严格保持近零反射率偏差(&lt;0.3%) 与亚像素级空间偏移(&lt;0.1 px) 的同时，SEN2SR 取得最高 PSNR/SSIM，显著优于 ESRGAN、SwinIR 等标杆；下游土地覆盖分类与 LAI 反演误差降低 8-15%，xAI 激活图显示性能与高频细节重建区域高度相关；定性上在冰雪、城市、湿地等分布外场景未出现伪纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖合成训练数据，对真实 Sentinel-2 中未建模的复杂大气、BRDF 与传感器噪声敏感；低通约束虽保证辐射一致，却可能抑制本应存在的真实高频信号；此外，2.5 m 输出与现有 2.9 m 商业数据存在尺度错配，限制融合应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或物理-感知域适应，以真实影像微调并学习未建模退化；同时耦合大气校正与 BRDF 模型，实现端到端地表反射率超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事光学卫星超分、多光谱融合、定量遥感或深度学习域适应，该文提供的调和合成数据流程、低频约束层设计及开源代码可为算法改进与基准测试提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651537" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EyeSim-VQA：基于自由能量引导的眼动仿真视频质量评价框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaoyang Wang，Wen Lu，Jie Li，Lihuo He，Maoguo Gong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651537" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651537</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视频质量评估中模拟人眼自由能驱动的自修复与扫视感知机制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支网络：美学全局分支+技术局部分支，嵌入自由能自修复模块与扫视融合预测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开VQA数据集上达到或超越SOTA，兼具可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自由能自修复、全局-局部双路径与扫视动态融合引入VQA框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生物启发的VQA提供可解释新范式，推动视觉感知建模与质量评价交叉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视频质量评估(VQA)的核心挑战是如何让模型预测与人类主观感受对齐。尽管基于自由能的自修复机制在图像质量评估中已显成效，但将其迁移到时序视频场景仍非易事；同时，受生物学启发的一体化感知、局部分析与注视扫描范式在高层视觉任务中表现突出，却在VQA领域鲜有系统探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EyeSim-VQA采用双分支结构：美学分支对整帧图像做全局感知评价，技术分支对图像块做细粒度结构与语义分析；两分支分别输入缩放全帧与随机块，并嵌入专用增强模块以模拟自由能驱动的自适应修复。框架在保持原始骨干网络不变的前提下，通过残差式融合注入高层视觉特征。最后，受扫视动力学启发设计的预测头将全局与局部表征按虚拟注视轨迹加权融合，输出质量分数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开VQA数据集上，EyeSim-VQA与十余种最新方法相比取得SOTA或次优结果，平均SRCC提升2.3%-4.1%，同时可视化实验表明其双分支响应与主观注视热图高度一致。消融实验证实自由能自修复模块贡献约35%的性能增益，且高层特征注入策略在额外参数量&lt;1%的情况下带来显著收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大量patch采样，推理时显存占用约为同类方法的1.8倍；自由能先验目前基于静态图像统计，对快速运动或场景切换的视频帧适应性下降。此外，主观实验规模有限，仅覆盖1080P内容，对4K/8K超高清视频的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序自由能先验以刻画运动预测误差，并结合强化学习优化注视路径，实现真正的动态自修复；同时构建跨分辨率主观数据库，验证模型在超高清场景的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究视觉质量评价、生物启发视觉模型或自由能原理在深度学习中的应用，该文提供了可扩展的双分支架构、注入高层特征的即插即用策略以及开源代码，可直接作为基准或二次开发平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniSparseBEV：面向自动驾驶的统一稀疏查询多任务学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhou，Yi Zhang，Honggang Qi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651369</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不构建稠密BEV的情况下实现高效的多任务自动驾驶感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用跨任务共享稀疏查询与Z-DCA直接从图像特征采样，辅以2D监督训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NuScenes上3D检测与BEV分割均优于单任务方法，且计算显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将统一稀疏查询与Z轴可变形交叉注意力引入多任务BEV框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉多任务自动驾驶提供轻量、强基线，可缓解稠密BEV计算瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉中心的多任务学习已成为自动驾驶感知的主流范式，但现有方法普遍依赖稠密BEV特征图，导致网络结构复杂、显存与计算开销大，难以在车载芯片实时运行。作者观察到不同感知任务在三维空间存在天然关联，却缺乏一种统一且稀疏的表征来同时完成检测与分割，因此提出用稀疏查询替代稠密BEV。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架核心是一组跨任务共享的可学习稀疏查询，通过统一的Transformer解码器与任务特定头实现多任务预测；提出Z-DCA模块，让BEV分割查询沿Z轴采样多高度图像特征，直接跳过显式BEV构建，降低计算量；引入2D语义分割辅助分支，在网络浅层即提供稠密监督，加速收敛并提升深度估计一致性；整体采用端到端训练，检测与分割损失联合优化，无需后处理融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NuScenes上，UniSparseBEV以1/4计算量超越CenterPoint、BEVFormer等单任务SOTA，mAP提升1.2，NDS提升1.0，BEV分割mIoU提升2.4；鲁棒性实验表明在图像丢帧、恶劣天气、相机外参扰动下性能下降幅度小于基线30%，验证稀疏查询的鲁棒性；可视化显示共享查询自动学会任务间共享的静态/动态物体特征，减少冗余计算。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全基于视觉，未融合激光雷达或毫米波，在夜间或强逆光场景下精度仍显著下降；稀疏查询数量与空间分布需人工设定，缺乏自适应机制，可能在密集交通场景遗漏小目标；Z-DCA仅沿Z轴采样，对曲率较大的非平坦路面几何建模能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可学习查询扩展为场景自适应的动态数量与位置，并结合时序信息构建稀疏4D表征；探索与激光雷达点云的跨模态稀疏对齐，实现全天候高鲁棒感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究车载多任务感知、稀疏表征或轻量级BEV，该文提供了不依赖稠密栅格的统一范式与开源训练细节，可直接作为强基线或嵌入现有框架降低延迟。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>