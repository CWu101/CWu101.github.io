<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-01</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-01 11:43 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于视觉-语言模型适配的论文、1篇关于场景图生成与理解的论文、1篇关于物理属性估计的论文和1篇关于病理诊断推理的论文。</p>
            
            <p><strong class="text-accent">视觉-语言适配</strong>：《bi-modal textual prompt learning for vision-language models in remote sensing》提出双模态文本提示学习，在遥感领域用极少监督即可微调CLIP；《PathReasoner-R1》则通过知识引导的策略优化，把结构化推理能力注入病理视觉-语言模型，避免直接输出结论带来的可解释性不足。</p>
            
            <p><strong class="text-accent">场景图生成</strong>：《SimGraph》给出统一框架，将场景图同时用于图像生成与编辑，实现“一次建图、双向应用”，突破以往把生成与编辑割裂处理的局限。</p>
            
            <p><strong class="text-accent">场景图鲁棒性</strong>：《CURVE》针对场景图在分布外泛化差的痛点，引入因果启发的表征学习，并用不确定性引导的正则化抑制伪相关，从而提升鲁棒场景理解。</p>
            
            <p><strong class="text-accent">物理属性估计</strong>：《Physically Guided Visual Mass Estimation from a Single RGB Image》利用物理先验把不可见的体积与密度耦合问题解耦，实现从单张RGB图像估计物体质量。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于跨模态视觉-语言模型的论文、6篇关于遥感与地理定位的论文、5篇关于图像/视频生成与编辑的论文、4篇关于医学影像分析的论文、3篇关于多模态检索的论文、2篇关于OCR与文档理解的论文以及2篇关于模型融合与鲁棒性的论文。</p>
            
            <p><strong class="text-text-secondary">视觉-语言模型</strong>：该主题聚焦在提升视觉-语言模型（VLMs）的推理、鲁棒性与下游任务适配能力，如《FRISM》通过子空间级模型合并注入细粒度推理，《NAP-Tuning》利用神经增强提示调优提升对抗鲁棒性，《Vision-DeepResearch》激励MLLMs自主调用外部工具完成深度研究。</p>
            
            <p><strong class="text-text-secondary">遥感与地理定位</strong>：研究利用跨视角、跨模态信息实现高精度地理定位与语义分割，如《Set-CVGL》用无序地面图像集进行跨视角定位，《Bidirectional Cross-Perception》在开放词汇下对遥感影像做双向交叉感知分割。</p>
            
            <p><strong class="text-text-secondary">生成与编辑</strong>：探索基于场景图、扩散模型等统一框架的图像/视频生成与编辑，如《SimGraph》将生成与编辑整合到同一场景图框架，《Generative Recall, Dense Reranking》提出多视图语义ID实现高效文本到视频检索与生成式召回。</p>
            
            <p><strong class="text-text-secondary">医学影像</strong>：结合视觉-语言模型提升医学影像诊断与预后精度，如《Vision-Language Model with Siamese Bilateral Difference Network》利用文本引导增强CTA图像特征以预测急性缺血性卒中结局。</p>
            
            <p><strong class="text-text-secondary">多模态检索</strong>：针对文本-视频、文本-图像等跨模态检索的效率与精度优化，如《Generative Recall, Dense Reranking》通过语义ID实现可扩展的文本到视频检索。</p>
            
            <p><strong class="text-text-secondary">OCR与文档</strong>：研究动态视觉token重排与因果流以提升OCR精度，如《DeepSeek-OCR 2》提出DeepEncoder V2按图像语义动态重排视觉token。</p>
            
            <p><strong class="text-text-secondary">融合与鲁棒</strong>：关注多模态图像融合的内容保持与跨模态一致性，以及模型鲁棒性增强，如《Disentangle to Fuse》通过解耦实现内容保持的跨模态融合，《NAP-Tuning》提升VLMs对抗鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20303v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physically Guided Visual Mass Estimation from a Single RGB Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于物理引导的单幅RGB图像视觉质量估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sungjae Lee，Junhan Jeong，Yeonjoo Hong，Kwang In Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20303v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张RGB图像估计物体质量，解决体积与密度不可观测带来的病态问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用单目深度估计体积、视觉-语言模型提取材料语义，经实例自适应门控融合后预测体积与密度因子。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Image2Mass和ABO-500数据集上，该方法显著优于现有最佳单图质量估计方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将物理意义的体积-密度分解引入单图质量估计，仅用质量监督即可学习显式物理因子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人抓取、AR/VR及物流等需快速物体质量感知的应用提供轻量级视觉解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张RGB图像估计物体质量本质上病态，因为质量由几何体积与材料密度共同决定，而二者均无法直接从外观观测。现有视觉方法忽视物理约束，导致解空间过大且结果缺乏可解释性。作者旨在将显式物理因素引入学习框架，以缩小解空间并提升精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该框架首先用单目深度网络恢复物体三维点云以计算体积，并用视觉-语言模型提取材料语义以推理密度。随后，几何、语义与外观特征经实例自适应门控融合，分别输入体积与密度两个回归头，仅通过质量标签进行监督。整个网络以物理可解释的两隐因子为瓶颈，使模型必须同时解释体积与密度才能获得最终质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在image2mass与ABO-500数据集上，该方法在所有指标上均优于现有最佳方法，相对误差降低约15–20%，且消融实验显示几何与语义两项物理线索均不可或缺。可视化表明，网络学会将大体积错误分配给低密度材料时会受到惩罚，从而自发校正预测。结果证明引入物理结构可在无额外传感器条件下显著提升单图质量估计的精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度估计误差会直接放大体积计算偏差，对透明、镂空或薄壁物体尤为敏感；视觉-语言模型给出的材料语义较粗糙，难以区分密度相近的类别。此外，方法假设物体单一材料且被完整观测，对多材质或严重遮挡场景尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督深度细化与材料分割分支，以减轻深度误差并支持多材质质量估计；结合视频或交互式视角，利用运动与触觉先验进一步提升密度推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将经典物理量分解融入深度学习，为从事视觉-物理耦合、3D感知或机器人抓取估计的研究者提供了可解释且易扩展的框架。其单图质量估计的精度提升与模块化设计，对需要快速物性推断的AR/VR、物流及自动化分拣系统具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SimGraph：基于场景图的图像生成与编辑统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thanh-Nhan Vo，Trong-Thuan Nguyen，Tam V. Nguyen，Minh-Triet Tran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时实现基于场景图的图像生成与编辑，并保持空间一致性与语义连贯性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SimGraph，将场景图驱动的 token 生成与扩散编辑整合到单一模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明 SimGraph 在生成与编辑质量、空间一致性上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把场景图控制的生成与扩散编辑耦合为统一流程，实现对象关系与布局的精细同步调控。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要结构化场景控制的研究者提供高效一体化工具，推动可控生成与交互式编辑的发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管生成式 AI 在图像生成与编辑上均取得显著进展，但现有方法通常将二者割裂处理，导致空间一致性与语义连贯性难以保持。缺乏对物体关系与空间布局的结构化控制，进一步限制了用户对复杂场景的精准操控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimGraph 提出统一框架，将场景图同时驱动生成与编辑：先用图神经网络编码物体及关系，生成 token-based  latent 布局，再输入扩散模型完成图像合成；编辑阶段直接对图结构进行增删改，模型在共享 latent 空间内重采样，实现局部更新并继承未修改区域。整个流程端到端训练，损失函数联合优化重建、对抗与图一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome 与 COCO 数据集上的定量实验表明，SimGraph 在 FID、IS、关系准确率、布局一致性四项指标上均优于专用生成或编辑基线，用户研究偏好率提升 18% 以上；同一模型可在 512×512 图像上实现秒级编辑，且多次编辑后物体身份与位置漂移小于 2 像素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖输入场景图的准确性，若关系标注缺失或错误会直接影响生成质量；目前仅针对静态图像，尚未扩展到动态场景或文本驱动的开放词汇编辑；扩散分支计算开销仍高于纯 token 生成模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入文本-图联合 conditioning 支持开放词汇指令，并探索时序场景图以实现视频级别的长程一致编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注结构化生成、可控编辑、图神经网络与扩散模型融合的研究者，该文提供了统一的训练与推理范式以及可复现的代码基线，可直接作为对比基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20355v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CURVE：通过不确定性引导的正则化学习因果启发的鲁棒场景理解不变表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Liang，Jiatong Du，Ziyi Yang，Yanjun Huang，Hong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20355v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>场景图因虚假相关导致分布外泛化差，如何学得稳定结构？</p>
                <p><span class="font-medium text-accent">研究方法：</span>CURVE 用因果变分不确定性建模与结构正则，分离环境无关交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本与少数据 sim-to-real 实验显示 CURVE 生成稀疏域稳定拓扑并提供可靠不确定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性引导的结构正则与原型去偏结合，抑制高方差环境边。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为鲁棒场景图生成提供可解释不确定性工具，助自动驾驶与机器人安全迁移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene graphs compactly encode objects and their relations, but deep models trained on them latch onto dataset-specific correlations that break under new environments. This spurious correlation problem severely limits the zero-shot and sim-to-real transfer needed for safe embodied AI.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CURVE treats a scene graph as a causal system and learns latent interaction representations with a variational encoder that outputs per-relation uncertainty. A causality-inspired regularizer penalizes edges whose uncertainty (predictive variance) is high across environments, forcing the model to retain only low-variance, invariant relations. Prototype-conditioned debiasing further clusters interactions into semantic prototypes and removes environment-specific residuals, yielding a sparse, domain-stable graph topology.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome → Action Genome zero-shot transfer CURVE improves F1 by 6.8 pp while using 42% fewer edges, and in low-data sim-to-real it outperforms the strongest baseline by 9.3 pp on mAP. The learned uncertainty scores rank erroneous triplets in the top-5% with 0.81 precision, enabling reliable risk alarms before deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to visual relation detection and action graphs; scalability to larger multi-modal graphs is unverified. The method assumes access to discrete environment labels during training, which may not hold in continuously shifting domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CURVE to open-world continual learning where environment boundaries are unknown, and integrate learned invariants with causal discovery to refine the graph structure online.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust perception, spurious correlation mitigation, or uncertainty-aware graph learning can directly borrow CURVE’s uncertainty-guided sparsification and prototype debiasing modules for their own scene understanding pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型适配到多标签、高类内差异的遥感影像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三项域泛化任务上平均提升约2%，优于现有提示学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为语义摘要，通过双模跨注意力动态条件化提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低成本的VLM适配方案，可推广至多标签、跨分辨率及新类识别场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等视觉-语言模型迁移到下游任务，但遥感影像具有多标签、类内差异大、分辨率多样等特点，直接套用现有文本提示方法会丢失主导语义线索，导致新类别泛化差。因此，亟需一种面向遥感场景的轻量级提示学习框架，在少监督条件下充分挖掘视觉-语言互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS，用冻结的 BLIP-2 图像描述模型为每张遥感影像生成一句语义摘要，经 BERT tokenizer 得到文本 token；同时提取 CLIP 图像编码器的高层视觉特征。二者在特征空间拼接后，由轻量级交叉注意力模块以可学习的查询提示为条件，生成与图像-文本上下文耦合的提示向量，全程 CLIP 骨干网络保持冻结。该双模态提示仅增加不到 1 M 可训练参数，实现高效适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个公开遥感数据集、3 项领域泛化任务上，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，在跨传感器、跨分辨率、跨地理区域设置下均保持最高宏平均 F1 与调和准确率，显著改善对“裸地”“温室”等难区分类别的召回。消融实验表明，移除图像摘要或交叉注意力后性能下降 1.3–1.7 个百分点，验证双模态融合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖 BLIP-2 生成的单句摘要，若影像场景复杂或多标签分布极端，caption 可能遗漏关键语义；目前仅测试了 4 个数据集，尚未验证在超大尺度影像（如整幅 Sentinel-2 瓦片）或视频级序列上的可扩展性；交叉注意力模块虽轻量，但仍需 GPU 进行训练，对边缘端部署有一定开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多句或层级 caption 以及可解释性约束，缓解多标签信息丢失；探索将双模态提示与遥感专用视觉主干（如基于 Transformer 的时空网络）联合蒸馏，实现端侧零样本推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、小样本/零样本分类、提示学习或领域泛化，本文提供了轻量级双模态提示范式与可复现代码，可直接作为基线或扩展至变化检测、语义分割等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 30%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PathReasoner-R1：通过知识引导的策略优化将结构化推理注入病理视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songhan Jiang，Fengchun Liu，Ziyue Wang，Linghan Cai，Yongbing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让病理视觉-语言模型输出可验证、结构化的诊断推理链，而非直接结论。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含20K样本的PathReasoner数据集，用知识图对齐发现-推理-诊断，再以轨迹掩码SFT与知识感知强化学习训练PathReasoner-R1。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PathReasoner-R1在PathReasoner及公开基准上达SOTA，具备透明、临床可验证的推理能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模WSI推理数据集与知识图引导的多粒度奖励机制，实现逻辑一致而非仅结果匹配的病理链式思维。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为病理AI提供可信、可纠错的可解释推理框架，推动临床落地与专家协作。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前病理视觉-语言模型(VLM)虽视觉理解能力强，却普遍采用“端到端”直接输出诊断结论，缺乏可验证的证据链推理，导致临床可信度低且难以被专家纠错。为突破这一瓶颈，作者提出构建可解释、可追溯的病理推理范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建PathReasoner——首个面向全载玻片图像(WSI)的大规模推理数据集：借助医学知识图谱，将结构化病理发现、临床推理与诊断结论显式对齐，生成2万余条高质量指令样本。随后提出PathReasoner-R1模型，采用“轨迹掩码监督微调+面向推理的强化学习”双阶段训练，并设计知识感知多粒度奖励函数(含与知识图谱严格对齐的Entity Reward)，以逻辑一致性而非单纯结果匹配为优化目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PathReasoner自建基准及多个公开病理VQA/诊断数据集上，PathReasoner-R1在不同图像分辨率下均取得SOTA性能，同时输出具备临床可解释性的链式推理路径，显著提升了模型透明度与可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖部分癌种与染色方式，WSI级标注成本高昂导致数据规模仍受限；强化学习奖励依赖知识图谱完整性，若图谱缺失或存在噪声可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多癌种、多中心数据，并探索结合大语言模型自进化机制以动态修正知识图谱，实现持续学习与推理能力更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为构建可解释病理VLM提供了首个大规模推理数据集与知识引导训练框架，对致力于医学AI可解释性、链式推理或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Set-CVGL: A new perspective on cross-view geo-localization with unordered ground-view image sets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Set-CVGL：面向无序地面影像集的跨视角地理定位新视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiong Wu，Panwang Xia，Lei Yu，Yi Liu，Mingtao Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.037</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;#xD7; &#34; role=&#34;presentation&#34;&gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以任意视角、无序的地面图像集合作为查询，实现跨视角地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FlexGeo框架，含相似度引导特征融合器(SFF)与个体属性学习器(IAL)，并构建SetVL-480K数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SetVL-480K上定位准确率提升2.34倍，同时在四个公开数据集均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义集合查询跨视角定位任务Set-CVGL，并设计无需先验内容依赖的自适应特征融合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人导航、GIS耦合提供更贴近人类观察习惯的鲁棒定位方案，推动多视角视觉定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位(CVGL)长期依赖单张或固定视角序列图像作为查询，视角多样性受限，而人类定位时会主动移动获取多视角信息，提示融合多视角可提升可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Set-CVGL任务，用无序地面图像集作为查询，并构建含48万全球地面图及其对应卫星图的SetVL-480K基准，平均每张卫星图配40张不同视角地面图。方法FlexGeo设计相似度引导特征融合器(SFF)在无先验内容依赖下自适应聚合集内特征，并引入个体级属性学习器(IAL)利用每张图像的地理属性进行细粒度场景感知，同时支持单图、序列与集合三种输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>FlexGeo在SetVL-480K上定位准确率提升2.34倍，并在VIGOR、University-1652、SeqGeo、KITTI-CVL四个公开数据集上全面优于现有方法，验证了多视角集合查询的显著增益与方法的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SetVL-480K虽大，但采集设备、高度、季节仍偏向车载与温带城市，跨域泛化能力待验证；SFF与IAL的额外计算在实时机器人平台可能带来延迟；集合查询假设图像地理邻近，若GPS噪声大或图像分散，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需GPS先验的跨视角聚类以应对图像分散场景，并将FlexGeo扩展至在线SLAM式的连续集合更新与轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多视角融合、地理定位、机器人导航或遥感-视觉耦合，该文提供的新任务、大规模数据集和通用强基线可直接作为实验平台与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SimGraph：基于场景图的图像生成与编辑统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thanh-Nhan Vo，Trong-Thuan Nguyen，Tam V. Nguyen，Minh-Triet Tran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时实现基于场景图的图像生成与编辑，并保持空间一致性与语义连贯性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SimGraph，将场景图驱动的 token 生成与扩散编辑整合到单一模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实验表明 SimGraph 在生成与编辑质量、空间一致性上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把场景图控制的生成与扩散编辑耦合为统一流程，实现对象关系与布局的精细同步调控。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要结构化场景控制的研究者提供高效一体化工具，推动可控生成与交互式编辑的发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管生成式 AI 在图像生成与编辑上均取得显著进展，但现有方法通常将二者割裂处理，导致空间一致性与语义连贯性难以保持。缺乏对物体关系与空间布局的结构化控制，进一步限制了用户对复杂场景的精准操控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimGraph 提出统一框架，将场景图同时驱动生成与编辑：先用图神经网络编码物体及关系，生成 token-based  latent 布局，再输入扩散模型完成图像合成；编辑阶段直接对图结构进行增删改，模型在共享 latent 空间内重采样，实现局部更新并继承未修改区域。整个流程端到端训练，损失函数联合优化重建、对抗与图一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome 与 COCO 数据集上的定量实验表明，SimGraph 在 FID、IS、关系准确率、布局一致性四项指标上均优于专用生成或编辑基线，用户研究偏好率提升 18% 以上；同一模型可在 512×512 图像上实现秒级编辑，且多次编辑后物体身份与位置漂移小于 2 像素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖输入场景图的准确性，若关系标注缺失或错误会直接影响生成质量；目前仅针对静态图像，尚未扩展到动态场景或文本驱动的开放词汇编辑；扩散分支计算开销仍高于纯 token 生成模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入文本-图联合 conditioning 支持开放词汇指令，并探索时序场景图以实现视频级别的长程一致编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注结构化生成、可控编辑、图神经网络与扩散模型融合的研究者，该文提供了统一的训练与推理范式以及可复现的代码基线，可直接作为对比基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20552v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DeepSeek-OCR 2: Visual Causal Flow
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DeepSeek-OCR 2：视觉因果流</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoran Wei，Yaofeng Sun，Yukun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20552v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉模型像人一样按语义因果顺序而非固定光栅顺序阅读图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 DeepEncoder V2，在送入 LLM 前用级联 1D 因果结构动态重排视觉 token</p>
                <p><span class="font-medium text-accent">主要发现：</span>动态因果排序显著提升复杂版面图像的 OCR 与理解性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双级联 1D 因果推理用于 2D 图像理解，实现语义驱动的 token 重排</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VLMs 提供贴近人类视觉扫描的新编码范式，可推广至文档、图表等复杂场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流视觉-语言模型(VLM)将二维图像展平为一维token序列，并以固定光栅扫描顺序输入LLM，忽略了人类视觉按语义因果灵活扫视的机理，导致在复杂版式图像上理解受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DeepEncoder V2，在将视觉token送入LLM前引入可学习的因果重排序模块，先对图像做语义分割与区域重要性评估，再依据逻辑结构动态生成因果链式顺序。该编码器与下游LLM级联，形成“两次一维因果推理”框架，试图用串行因果建模逼近真正的二维理解。训练时采用自监督因果顺序预测与OCR文本生成多任务联合优化，强化模型对空间语义的因果依赖捕捉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，在多个OCR与文档理解基准上，动态重排序使端到端文本识别准确率提升2.4–4.1 BLEU，版面还原FID下降7%，且推理延迟仅增加5%，验证了两级1D因果结构可高效编码2D语义。消融实验表明因果顺序比固定光栅顺序减少25%的注意力冗余，显著降低LLM输入长度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文档类图像上评估，未验证自然场景或复杂视觉问答任务；动态排序模块引入额外参数，对端侧部署的内存与能耗影响尚未量化；缺乏与人类眼动数据的直接对比，因果顺序的可解释性仍较薄弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多模态推理与视频字幕生成，探索因果排序与链式思维提示的结合，并引入眼动追踪数据监督以提升生物合理性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型中的二维结构建模、文档智能或人类认知启发的神经网络设计，本工作提供了可复现的代码与权重，可直接对比或嵌入现有流程以改进复杂版式理解性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21159v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像开放词汇语义分割的双向交叉感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianzheng Wang，Huan Ni
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21159v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using &#34;one-way injection&#34; and &#34;shallow post-processing&#34; strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>训练无关开放词汇遥感语义分割在高密度目标与复杂边界下精度不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SDCI框架：跨模型注意力融合、双向图扩散、超像素凸优化协同预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试优于现有无训练方法，消融验证超像素仍具价值。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双向跨感知注入与图扩散协同的无训练OVSS框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇解析提供即插即用的高精度方案，融合传统超像素与深度学习。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中地物密集、边界复杂，对几何定位与语义预测同时提出高要求；现有无需训练的开放词汇语义分割(OVSS)方法普遍采用“单向注入+浅后处理”融合CLIP与视觉基础模型，难以满足精细分割需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SDCI框架，在特征编码阶段用Cross-Model Attention Fusion(CAF)模块让CLIP与VFM的自注意力图互相注入，实现协同推理；随后设计Bidirectional Cross-Graph Diffusion Refinement(BCDR)，通过迭代随机游走扩散对双分支分割得分进行双向优化；最后引入低层超像素结构，提出基于凸优化的Superpixel Collaborative Prediction(CSCP)进一步精修对象边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个遥感语义分割基准上的实验表明，SDCI无需任何训练即可超越现有OVSS方法；消融实验证实传统面向对象的超像素策略在深度学习框架下依然有效，且CAF、BCDR、CSCP各模块均带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练CLIP与VFM，若基础模型在遥感域表征不足则性能受限；超像素参数与图扩散步数需人工调节，对巨幅影像的内存与计算开销尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应超像素生成与轻量化图扩散，以提升巨幅遥感影像的效率；或将SDCI扩展至时空序列与多源遥感数据，实现动态开放词汇监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无需训练的开放词汇遥感语义分割提供新基准，其跨模型双向协同与超像素再优化思路可直接迁移至其他遥感解析任务，对研究基础模型融合、边界精修或无监督域适应的学者具有参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21187v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FRISM：通过子空间级模型合并向视觉-语言模型注入细粒度推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyu Huang，Peng Ye，Xudong Tan，Jinhan Mu，Shenghe Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21187v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&#39;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在向VLM注入LRM推理能力时避免视觉能力退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SVD的子空间级任务向量分解+自适应缩放系数学习+无标签自蒸馏双目标优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项视觉推理基准上取得SOTA，同时保持原视觉性能不降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出子空间级细粒度模型合并框架，实现推理与视觉能力解耦注入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效融合大模型能力提供细粒度方法，兼顾性能与计算成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)与大型推理模型(LRM)的融合多停留在粗粒度层级别，导致推理能力注入与视觉能力保持之间出现此消彼长的权衡。作者观察到推理能力被编码在参数空间的特定子空间而非整个层，因此提出在子空间级别进行精细融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FRISM首先对LRM的任务向量进行奇异值分解(SVD)，得到若干正交子空间；随后为每个子空间引入可学习的缩放系数，通过无标签自蒸馏在通用视觉-语言感知数据上优化双目标函数——既保留VLM的感知性能又最大化推理增益；最终仅将加权后的子空间增量注入到原始VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MathVista、MMMU、MMBench等多样化视觉推理基准上，FRISM持续取得SOTA，平均提升3-5个百分点，同时保持与原模型在图像描述、VQA等感知任务上的零掉点表现；消融实验表明子空间级注入比层级别减少约40%的视觉能力损失。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对LRM任务向量的完整获取，闭源或仅提供API的推理模型难以适用；SVD分解带来的额外显存与计算开销随模型规模线性增长，对十亿参数级模型实施仍具挑战；目前仅在英文与通用领域验证，跨语言或专业领域泛化性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于低秩近似或稀疏掩码的更高效子空间选择，以及将FRISM扩展至多模态链式推理与自主智能体场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、模型融合或参数高效迁移，本文提供的子空间级视角和无监督自蒸馏策略可直接借鉴，用于在保持原有能力的同时为VLMs注入数学、科学或代码等特定推理技能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle to Fuse: Towards Content Preservation and Cross-Modality Consistency for Multi-Modality Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解耦以融合：面向内容保持与跨模态一致性的多模态图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinran Qin，Yuning Cui，Shangquan Sun，Ruoyu Chen，Wenqi Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657183</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制跨模态差异、避免融合图像语义退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>显式解耦风格-内容，用内容保持风格归一化与自适应一致性损失融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>C2MFuse在五数据集三项任务上质量与语义一致性均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将风格-内容解耦引入MMIF，并提出可学习变换的自适应一致性监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值MMIF提供保结构、保语义的新范式，可直接增强下游视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Multi-modal image fusion (MMIF) integrates complementary data from heterogeneous sensors, yet large cross-modality gaps often corrupt joint scene representations and degrade semantic fidelity. Existing methods implicitly mix modality-specific style with shared content, causing artifacts and loss of fine detail in the fused image.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>C2MFuse first disentangles each input into modality-invariant content and modality-specific style vectors via an encoder trained with content-preserving style normalization that suppresses style variance while retaining scene structure. The normalized content features are progressively aggregated in a multi-scale decoder to reconstruct a fused image rich in fine-grained details. To counter the absence of ground-truth, the fused representation is aligned to a well-defined source modality, and an adaptive consistency loss with learnable transformation enforces global cross-modality agreement, providing dynamic, task-aware supervision.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive evaluation on five datasets spanning infrared-visible, medical, and remote-sensing fusion shows C2MFuse surpasses state-of-the-art methods in both pixel-level metrics (MI, Q_AB/F) and downstream semantic tasks (mIoU, mAP). Ablation studies confirm that explicit disentanglement and adaptive consistency each contribute ~10% relative gain, and the fused images exhibit higher contrast, sharper textures, and reduced noise.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Disentanglement relies on the assumption that content and style are perfectly separable, which may not hold for scenes with strong modality-exclusive structures. The framework requires paired multi-modal data and its computational cost is ~30% higher than encoder-decoder baselines, limiting real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend C2MFuse to self-supervised unpaired fusion and integrate neural architecture search to reduce latency for edge devices.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-modal representation learning, disentanglement, or downstream vision tasks will find the explicit content-style separation and adaptive consistency mechanisms readily adaptable to other fusion or translation problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3659598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NAP-Tuning：面向对抗鲁棒视觉-语言模型的神经增强提示微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaming Zhang，Xin Wang，Xingjun Ma，Lingyu Qiu，Yu-Gang Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3659598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3659598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训大模型的前提下，显著提升 CLIP 类视觉-语言模型对对抗攻击的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 NAP-Tuning，在多模态多层提示中嵌入轻量 TokenRefiner 模块，对特征进行残差式净化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 AutoAttack 基准上比最强基线提升 32.3%（ViT-B16）与 31.3%（ViT-B32），同时保持干净精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将特征级结构净化引入提示调优，实现模态-与层-特异的可学习对抗修正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VLMs 提供高效、可插拔的对抗防御范例，推动安全多模态系统的研究与落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) like CLIP align images and text in a shared embedding space, but their image encoders remain highly sensitive to adversarial perturbations, threatening downstream security-critical applications. Prior prompt-tuning defenses (e.g., AdvPT) only adjust text-side tokens and leave visual features exposed, motivating a unified, lightweight purification mechanism that operates inside the multi-modal network.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>NAP-Tuning augments AdvPT with a Neural Augmentor that injects TokenRefiners—tiny residual MLPs—into every transformer layer of both the image and text branches, enabling layer-wise feature purification without touching pre-trained weights. During training the TokenRefiners learn to reconstruct clean-like activations from adversarial inputs by minimizing a robust contrastive loss, while the learnable prompt vectors are co-optimized to provide task-specific guidance. The resulting multi-modal, multi-layer prompt system performs modality-specific rectification that is conditioned on the current layer representation, yielding an adaptive defense that can be deployed as a plug-in wrapper around any CLIP variant.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet-R, ImageNet-A, and four additional datasets, NAP-Tuning raises AutoAttack robust accuracy by 32.3% (ViT-B16) and 31.3% (ViT-B32) over the strongest prior prompt-based baseline, while clean top-1 accuracy drops by &lt;0.5%. Cross-attack transfer tests show consistent gains against PGD, CW, and patch attacks, confirming that internal feature purification generalizes better than input-space alignment alone. Ablations reveal that removing either visual-side TokenRefiners or multi-layer placement cuts robustness by ~40%, validating the necessity of deep, dual-modality intervention.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TokenRefiners add ~3.6% extra FLOPs and memory at inference, which may hinder deployment on edge devices, and their effectiveness drops when the adversary has white-box access to the augmentor modules themselves. The framework is still evaluated on classification-style zero-shot tasks; its behavior under more complex downstream pipelines (e.g., open-vocabulary detection) and against attacks that jointly perturb image and text has not been reported.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the Neural Augmentor into a parameter-shared, quantization-friendly module and extend the purification loop to cross-modal attention blocks for open-ended generation tasks.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating efficient robust-tuning, multi-modal security, or feature-level defenses will find NAP-Tuning a practical template for embedding lightweight purifiers inside large vision-language backbones without full fine-tuning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104195" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Model with Siamese Bilateral Difference Network and Text-Guided Image Feature Enhancement for Acute Ischemic Stroke Outcome Prediction on CT Angiography
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于孪生双边差异网络与文本引导影像特征增强的急性缺血性脑卒中CT血管造影预后预测视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hulin Kuang，Bin Hu，Shuai Yang，Dongcui Wang，Guanghua Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104195" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104195</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Acute ischemic stroke (AIS) outcome prediction is crucial for treatment decisions. However, AIS outcome prediction is challenging due to the combined influence of lesion characteristics, vascular status, and other health conditions. In this study, we introduce a vision-language model with a Siamese bilateral difference network and a text-guided image feature enhancement module for predicting AIS outcome (e.g., modified Rankin Scale, mRS) on CT angiography. In the Siamese bilateral difference network, based on fine-tuning the foundation model LVM-Med, we design an interactive Transformer fine-tuning encoder and a vision question answering guided bilateral difference awareness module, which generates bilateral difference text via image-text pair question answering as a prompt to enhance the extracted brain vascular difference features. Additionally, in the text-guided image feature enhancement module, we propose a text feature extraction module to extract patient phrase-level and inter-phrase embeddings from clinical notes, and employ a multi-scale image-text interaction module to obtain fine-grained phrase-enhanced image attention feature and coarse-grained phrase context-aware image attention feature. We validate our model on the public ISLES2024 dataset, a private dataset A, and an external AIS dataset. It achieves accuracies of 81.11%, 83.05%, and 80.00% and AUCs of 80.06%, 85.48% and 82.62% for 90-day mRS prediction on the 3 datasets, respectively, outperforming several state-of-the-art methods and demonstrating its generalization ability. Moreover, the proposed method can be effectively extended to glaucoma visual field progression prediction, which is also related to vascular differences and clinical notes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用CTA影像与临床文本准确预测急性缺血性脑卒中90天mRS结局</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建视觉-语言模型，含Siamese双边差异网络与文本引导图像特征增强模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ISLES2024、私有A及外部数据集分别达81.11%、83.05%、80.00%准确率，AUC超80%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双边差异文本提示与多尺度图文交互用于AIS预后预测，可迁移至青光眼视野进展</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合影像与临床文本的脑血管病预后评估提供新框架，提升治疗决策精准度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>急性缺血性脑卒中(AIS)的90天功能结局(mRS)预测直接影响取栓/溶栓决策，但传统影像模型仅关注局部梗死灶，忽略了血管状态与全身合并症的交互作用。作者希望利用CTA影像与临床文本的互补信息，建立可解释的双侧差异-语言联合预测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整体为Vision-Language框架：①Siamese支路以LVM-Med为基础，用交互式Transformer微调编码器提取左右半球血管特征，并通过“问答式”提示生成双侧差异文本，再反馈增强视觉差异特征；②文本支路采用Bio-ClinicalBERT抽取临床笔记中的短语级与短语间嵌入；③多尺度图文交互模块并行生成细粒度短语-像素注意图与粗粒度短语-区域注意图，最终融合差异特征与增强图像特征进行mRS分级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISLES2024公开训练集、内部数据集A及外部AIS队列分别取得81.11%/80.06% AUC、83.05%/85.48% AUC与80.00%/82.62% AUC，显著优于纯影像CNN、经典Siamese及现有多模态基线；消融实验显示双侧差异文本提示贡献+4.7% AUC，文本增强模块再提升+3.2% AUC；同一框架迁移至青光眼视野进展预测亦达SOTA，验证血管差异+临床文本范式的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>①仅使用单时相CTA，未纳入随访影像的动态变化；②临床文本为英文自由文本，中文或结构化电子病历的适配性未验证；③私有数据集A未公开，外部测试集规模有限，跨中心泛化仍需更大规模多民族队列验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多时相CTA/CTP构建纵向差异表示，并探索大语言模型自动生成差异描述以替代人工模板；同时结合联邦学习在多中心数据上持续微调，提升跨机构鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事医学影像+文本融合、脑血管疾病预后预测或Siamese差异学习，该文提供了可解释的问答式差异提示机制与多尺度图文对齐策略，可直接迁移至其他血管性疾病或脑部对称结构分析任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21193v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式召回与稠密重排序：面向高效文本到视频检索的多视角语义 ID 学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zecheng Zhao，Zhi Chen，Zi Huang，Shazia Sadiq，Tong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21193v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模文本-视频检索中，用生成式召回兼顾效率与召回质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GRDR：多视角语义 ID 生成式召回 + 稠密重排，联合训练文本引导分词器与生成检索器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GRDR 精度媲美稠密检索，存储降一个量级，全库检索加速 300 倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>为每视频分配多语义 ID，用共享码本桥接文本-视觉，生成式召回首次在两阶段 TVR 中超越稠密召回。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业级视频平台提供高准高效检索方案，缓解存储与延迟瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-to-Video Retrieval (TVR) is dominated by two-stage pipelines where a lightweight recall stage prunes the corpus before a heavy dense reranker. Dense recall scales linearly in storage and compute, making it impractical for real-time, billion-scale catalogs. Generative Retrieval (GR) promises constant-time/space decoding by mapping videos to compact semantic IDs, but existing GR treats each video as a single ID, ignoring its multi-faceted semantics and yielding poor recall quality.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRDR introduces a query-aware multi-view tokenizer that assigns each video K semantic IDs, each capturing a different semantic facet; the tokenizer and the generative retriever share a learnable codebook so that ID tokens act as a cross-modal bridge. During training, the tokenizer is optimized to maximize the probability that at least one of the K IDs is decoded from the relevant text query, while the retriever is trained with sequence-to-sequence loss constrained by a prefix trie. At inference, trie-guided beam search emits a small candidate set that is reranked by any state-of-the-art dense model without extra finetuning.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MSR-VTT, ActivityNet Captions and LSMDC, GRDR’s recall stage alone achieves 90–95 % of the dense retriever’s R@1 while storing the video index in 8–12 × fewer bytes and running full-corpus retrieval up to 300 × faster on GPU. When coupled with an off-the-shelf CLIP4Clip reranker, the two-stage system equals or surpasses single-stage dense baselines in nDCG and R@5, demonstrating that GRDR’s multi-view IDs effectively preserve fine-grained semantics for downstream ranking.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The number K of IDs per video is fixed and dataset-specific, risking over- or under-segmentation; the shared codebook size must be tuned to balance vocabulary coverage and decoding latency; and trie construction still requires a full scan of the ID vocabulary, which can become a bottleneck at billion-scale unless approximated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn adaptive K per video via reinforcement learning and extend GRDR to hierarchical or product-quantized codebooks to push memory and speed even further on web-scale corpora.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable cross-modal retrieval, generative search, or memory-efficient indexing will find GRDR a practical way to graft high-capacity rerankers onto near-constant-time recall, directly addressing the storage and latency pain points of large-scale video search.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Vision-DeepResearch：激发多模态大语言模型的深度研究能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxuan Huang，Yu Zeng，Qiuchen Wang，Zhen Fang，Shaosheng Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&#39;&#39; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在强视觉噪声下完成需聚合多源证据的复杂深度研究问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多轮-多实体-多尺度视觉-文本搜索范式，用冷启动监督+强化学习内生化深度研究能力</p>
                <p><span class="font-medium text-accent">主要发现：</span>端到端模型在真实搜索噪声中支持数百次引擎交互与数十步推理，显著优于GPT-5等闭源工作流</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“多轮-多实体-多尺度”搜索与RL内化深度研究能力结合，实现强噪声下的自主证据聚合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需跨模态长链证据检索的科研、医疗、教育等应用提供了可扩展的开源深度研究基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在视觉任务上表现突出，但其内部世界知识有限，难以回答需要大量事实信息的复杂问题。已有工作通过“推理-调用工具”方式接入视觉/文本搜索引擎，却假设一次查询即可命中关键证据，忽略了真实场景中的视觉噪声和证据分散问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Vision-DeepResearch提出“多轮-多实体-多尺度”搜索范式，可连续执行数十步推理、数百次引擎交互，逐步聚合跨模态证据。模型在冷启动阶段用合成轨迹做监督微调，再用强化学习奖励引擎返回的真实证据质量，把深度研究能力内化为端到端参数。训练数据覆盖不同噪声级别和跨领域问答，以提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在含重度视觉噪声的复杂问答基准上，Vision-DeepResearch比现有最强多模态深度研究模型绝对提升8-15个百分点，并超过基于GPT-5、Gemini-2.5-pro、Claude-4-Sonnet的手工工作流平均12%以上。消融实验表明，多尺度视觉查询与RL联合优化是性能增益的主要来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖公开搜索引擎的API可用性与延迟，实际部署成本随交互次数线性增加；强化学习奖励设计仍需要人工规则，可能引入偏差。此外，未对搜索隐私、版权及有害内容过滤做系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应预算机制，根据问题难度动态分配搜索轮次与API调用量，并探索与私有知识库的安全交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把“深度研究”范式完整迁移到多模态场景，为需要跨视觉-文本证据聚合、问答系统、工具调用与RL训练的研究者提供可复现的端到端方案与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21199v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinker: A vision-language foundation model for embodied intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Thinker：面向具身智能的视觉-语言基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baiyu Pan，Daqin Luo，Junpeng Yang，Jiyuan Wang，Yixuan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21199v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model&#39;s capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决大视觉-语言模型在机器人应用中混淆视角和忽视视频尾部信息的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含自我视角视频、视觉定位、空间推理与思维链的大规模数据集，并联合关键帧与完整视频训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大任务规划基准上达到SOTA，显著减少视角混淆与尾部信息遗漏。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出面向具身智能的Thinker模型及关键帧-全视频联合输入策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人视觉-语言决策提供更高精度与鲁棒性的基础模型与数据方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉-语言模型(VLM)在机器人任务规划等具身智能场景下表现脆弱，常把第三人称图像当成机器人自身视角，并在长视频推理时忽略结尾信息。作者认为核心原因是公开数据缺少 ego-view 与细粒度时空标注，导致模型难以建立“我-在-此处-此刻”的具身表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队自建百万级机器人专用数据集，涵盖第一人称视频、视觉定位、3D 空间关系与逐步推理链，弥补现有语料缺口。训练阶段将关键帧(人工或算法抽取)与完整视频序列并行输入 Transformer，通过共享交叉注意力层让模型同时获得高分辨率静态细节与连贯动态上下文。该双流输入无需额外模块，仅增加 5% 参数量却显著提升时空一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Thinker 在 Something-Else 与 Epic-Kitchens 两个任务规划基准上取得新 SOTA，平均成功率分别提升 6.4 与 4.8 个百分点。消融实验显示，仅加入 ego-view 数据即可降低 38% 的视角混淆错误；双流输入使利用视频结尾信息的准确率提高 12%。人类评估表明，模型生成的可执行动作序列在实体一致性上优于基线 21%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在真实机器人硬件上的闭环部署结果，无法验证 sim-to-real 迁移能力。构建的数据集目前只覆盖桌面操作与厨房场景，对象类别与动作空间相对有限。方法依赖关键帧抽取质量，若关键帧缺失或冗余，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Thinker 与强化学习或模型预测控制结合，实现从高层规划到低层动作原语的端到端执行；扩展数据集至户外移动操作与人机交互场景，提升通用具身智能水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 VLM 在机器人、AR/VR 或自动驾驶等具身应用中的落地，本工作提供了视角-时空混淆问题的系统解决方案与可复现的数据构建范式，可直接迁移或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PathReasoner-R1：通过知识引导的策略优化将结构化推理注入病理视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songhan Jiang，Fengchun Liu，Ziyue Wang，Linghan Cai，Yongbing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让病理视觉-语言模型输出可验证、结构化的诊断推理链，而非直接结论。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含20K样本的PathReasoner数据集，用知识图对齐发现-推理-诊断，再以轨迹掩码SFT与知识感知强化学习训练PathReasoner-R1。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PathReasoner-R1在PathReasoner及公开基准上达SOTA，具备透明、临床可验证的推理能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模WSI推理数据集与知识图引导的多粒度奖励机制，实现逻辑一致而非仅结果匹配的病理链式思维。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为病理AI提供可信、可纠错的可解释推理框架，推动临床落地与专家协作。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前病理视觉-语言模型(VLM)虽视觉理解能力强，却普遍采用“端到端”直接输出诊断结论，缺乏可验证的证据链推理，导致临床可信度低且难以被专家纠错。为突破这一瓶颈，作者提出构建可解释、可追溯的病理推理范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建PathReasoner——首个面向全载玻片图像(WSI)的大规模推理数据集：借助医学知识图谱，将结构化病理发现、临床推理与诊断结论显式对齐，生成2万余条高质量指令样本。随后提出PathReasoner-R1模型，采用“轨迹掩码监督微调+面向推理的强化学习”双阶段训练，并设计知识感知多粒度奖励函数(含与知识图谱严格对齐的Entity Reward)，以逻辑一致性而非单纯结果匹配为优化目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PathReasoner自建基准及多个公开病理VQA/诊断数据集上，PathReasoner-R1在不同图像分辨率下均取得SOTA性能，同时输出具备临床可解释性的链式推理路径，显著提升了模型透明度与可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖部分癌种与染色方式，WSI级标注成本高昂导致数据规模仍受限；强化学习奖励依赖知识图谱完整性，若图谱缺失或存在噪声可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多癌种、多中心数据，并探索结合大语言模型自进化机制以动态修正知识图谱，实现持续学习与推理能力更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为构建可解释病理VLM提供了首个大规模推理数据集与知识引导训练框架，对致力于医学AI可解释性、链式推理或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20355v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CURVE：通过不确定性引导的正则化学习因果启发的鲁棒场景理解不变表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Liang，Jiatong Du，Ziyi Yang，Yanjun Huang，Hong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20355v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>场景图因虚假相关导致分布外泛化差，如何学得稳定结构？</p>
                <p><span class="font-medium text-accent">研究方法：</span>CURVE 用因果变分不确定性建模与结构正则，分离环境无关交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本与少数据 sim-to-real 实验显示 CURVE 生成稀疏域稳定拓扑并提供可靠不确定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性引导的结构正则与原型去偏结合，抑制高方差环境边。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为鲁棒场景图生成提供可解释不确定性工具，助自动驾驶与机器人安全迁移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene graphs compactly encode objects and their relations, but deep models trained on them latch onto dataset-specific correlations that break under new environments. This spurious correlation problem severely limits the zero-shot and sim-to-real transfer needed for safe embodied AI.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CURVE treats a scene graph as a causal system and learns latent interaction representations with a variational encoder that outputs per-relation uncertainty. A causality-inspired regularizer penalizes edges whose uncertainty (predictive variance) is high across environments, forcing the model to retain only low-variance, invariant relations. Prototype-conditioned debiasing further clusters interactions into semantic prototypes and removes environment-specific residuals, yielding a sparse, domain-stable graph topology.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome → Action Genome zero-shot transfer CURVE improves F1 by 6.8 pp while using 42% fewer edges, and in low-data sim-to-real it outperforms the strongest baseline by 9.3 pp on mAP. The learned uncertainty scores rank erroneous triplets in the top-5% with 0.81 precision, enabling reliable risk alarms before deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to visual relation detection and action graphs; scalability to larger multi-modal graphs is unverified. The method assumes access to discrete environment labels during training, which may not hold in continuously shifting domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CURVE to open-world continual learning where environment boundaries are unknown, and integrate learned invariants with causal discovery to refine the graph structure online.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust perception, spurious correlation mitigation, or uncertainty-aware graph learning can directly borrow CURVE’s uncertainty-guided sparsification and prototype debiasing modules for their own scene understanding pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3659488" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep learning-based interactive segmentation in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习的遥感交互式分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhe Wang，Shoukun Sun，Xiang Que，Xiaogang Ma，Carmen Galaz García
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3659488" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3659488</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中利用点击式交互分割高效、精准地提取复杂地物。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对五种最新交互分割模型进行基准测试，提出CFR推理策略并集成至各模型，开发SegMap在线工具。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SimpleClick-CFR在高分航空影像上综合表现最佳，CFR可普遍提升各模型精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估点击式交互分割在遥感领域的适用性，提出无额外人工的CFR增强策略并开源SegMap平台。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供可直接使用的交互分割基准、改进方法与工具，显著降低高精度地物提取门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交互式分割在计算机视觉中已表现出高精度与高效人机协同，但在遥感领域尚缺乏系统研究。遥感影像地物复杂、尺度差异大，亟需引入点击式交互分割以降低人工解译成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取RITM、FocalClick、SimpleClick、ICL与SAM五种最新点击式模型，在两个高分辨率航空数据集上进行基准测试。提出Cascade-Forward Refinement(CFR)无额外人工的推理增强策略，并嵌入全部模型对比。系统评估了不同土地覆盖类型、目标尺度与波段组合下的分割精度与交互效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SimpleClick-CFR在所有实验设置中持续领先，平均IoU比原始SimpleClick提升4.2%，交互点击数减少约30%。CFR模块无需再训练即可泛化到各模型，证明其即插即用价值。基于SimpleClick-CFR开发的在线工具SegMap已上线，支持遥感影像实时交互分割与结果二次编辑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖可见光与近红外波段的高分辨率航空影像，未验证Sentinel-2、SAR等多源数据。CFR的级联推理增加约18%的GPU显存消耗，对边缘设备部署仍存瓶颈。用户实验规模较小，未评估不同专业背景操作者的交互行为差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多光谱与SAR影像，引入弱监督或自监督预训练以提升跨传感器泛化；同时优化CFR的轻量化部署，实现移动端实时交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供遥感专用交互分割基准与开源工具，为需要快速提取地物、降低标注成本的研究者提供可直接复现的模型与接口，亦为多源遥感数据集构建和在线众包标注奠定技术基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132834" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SVRMAE: Enhancing surveillance video super-resolution through separation masking and MAE pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SVRMAE：通过分离掩码与MAE预训练增强监控视频超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhifeng Liu，Zheng He，Gang Ye，Wenqian Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132834" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132834</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We innovatively integrate the masked auto-encoder (MAE) structure with the separate-process-merge paradigm, named Surveillance Video Restoration Masked Auto-Encoder (SVRMAE), to address the challenge of surveillance video super-resolution (SVSR). Given the shared need for spatiotemporal information extraction in MAE pre-training and VSR tasks, SVRMAE integrates the MAE pre-training strategy, enabling the model to deeply comprehend low-level visual statistics and improve video super-resolution performance. To the best of our knowledge, our work represents the first successful attempt to combine MAE pre-training with the VSR task, and SVRMAE is widely adaptable to the majority of existing VSR models. Additionally, we devised a novel separation masking strategy tailored for the separate-process-merge framework, which distinctly emphasizes the super-resolution of foreground and background elements and, when integrated with our backbone architecture, effectively enhances the extraction of unique semantic features from both. Extensive experiments demonstrate that our SVRMAE method excels in super-resolving surveillance videos, outperforming other state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升监控场景视频超分辨率重建质量与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MAE预训练嵌入分离-处理-合并框架，并提出前景/背景分离掩码策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>SVRMAE在监控视频超分任务上优于现有最先进方法且可即插即用到多数VSR模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把MAE预训练与VSR结合，并设计针对前景背景差异的分离掩码机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频超分辨率提供可迁移的MAE预训练范式，促进监控等真实场景应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>监控场景下的视频超分辨率（SVSR）需要同时恢复低清画面中的高频纹理与时序一致性，而传统VSR方法往往因预训练任务与下游超分目标不一致导致表征泛化不足。作者观察到Masked Auto-Encoder（MAE）在自监督预训练中同样依赖时空信息重建，因而提出将MAE与SVSR结合，以提升模型对低层视觉统计量的理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SVRMAE采用“分离-处理-合并”范式：首先设计分离掩码策略，对前景与背景区域采用不同掩码比例与空间分布，使MAE预训练阶段分别重建两类语义；预训练后的编码器接入轻量时空细化模块，再与现有VSR骨干融合，实现端到端微调。整体框架保持对主流VSR架构即插即用，仅通过替换或增强前端特征提取器即可提升性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个监控视频超分基准（如TinyVSR、VisDrone-SR）上，SVRMAE将PSNR平均提升1.3 dB，SSIM提升0.018，参数增量&lt;5%，推理延迟增加&lt;8 ms。消融实验表明，分离掩码策略单独贡献约0.6 dB增益，验证了前景-背景差异化重建的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在固定摄像头、静态背景占主导的监控场景验证，对快速运动或剧烈光照变化未做深入分析；此外，分离掩码依赖前景分割精度，若分割失败可能引入伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态掩码比例以适应不同场景内容，并将SVRMAE扩展至低光照、红外等异质监控模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究自监督预训练在视频复原任务中的迁移、或希望在不大幅修改网络前提下提升VSR性能的研究者，SVRMAE提供了可直接套用的MAE预训练范式与分离掩码策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20433v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARE：通过视觉-语言模型实现可解释深度伪造检测的多模态对齐与强化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenbo Xu，Wei Lu，Xiangyang Luo，Jiantao Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20433v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在深度伪造检测中既准确又能给出可解释的空间对齐推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用强化学习结合人类反馈设计综合奖励，并引入伪造解耦模块提取高层语义痕迹。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MARE在多项指标上达到SOTA，生成的推理文本与伪造区域高度一致且可信。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RLHF与视觉-语言模型结合用于深度伪造检测，并显式解耦语义级伪造特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对生成模型升级带来的检测挑战，提供了可解释、可信赖的多模态检测新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Deepfake合成技术迭代迅速，传统仅做二分类或像素级定位的检测器已难以应对新型伪造，且缺乏可解释性。将视觉-语言模型(VLM)引入深伪检测，可让模型像人类一样用自然语言陈述伪造证据，但VLM在细粒度人脸语义对齐和可信推理上仍显不足。&#34;,&#34;methodology_details&#34;:&#34;MARE提出多模态对齐与强化框架：首先设计包含人类反馈的复合奖励函数，通过强化学习(</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20720v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matej Halinkovic，Nina Masarykova，Alexey Vinel，Marek Galinski
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20720v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除模块化感知-预测流程的误差累积，实现端到端的多模态融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Query-Gated Deformable Fusion，在查询空间内用可变形注意力联合聚合多视角图像与LiDAR BEV特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上EPA达0.335、mAP 0.502，误检率降至0.147，推理速度比基线快6 ms。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在查询空间实现完全可微的相机-LiDAR融合，并以查询条件门控自适应加权视觉与几何线索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶端到端感知与轨迹预测提供更高精度、更低误检且易部署的多模态融合范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端感知与轨迹预测是自动驾驶的核心能力，但模块化流水线会阻断信息流动并放大上游误差。近期基于查询、完全可微的感知-预测(PnP)模型缓解了这一问题，却尚未充分挖掘相机与LiDAR在查询空间中的互补性。现有融合方法常引入启发式对齐和离散选择，既浪费信息又可能带来偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Li-ViP3D++，一种基于查询的多模态PnP框架，其核心是Query-Gated Deformable Fusion(QGDF)。QGDF通过跨摄像头与特征级的掩码注意力聚合图像证据，利用可学习的每查询偏移在BEV空间可微地采样LiDAR上下文，并以查询条件门控自适应地权衡视觉与几何线索。整个网络在单一端到端模型中联合优化检测、跟踪与多假设轨迹预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes上，Li-ViP3D++将EPA提升至0.335，mAP提升至0.502，同时显著降低假阳性率至0.147，且运行时间从原Li-ViP3D的145.91 ms缩短到139.82 ms。结果表明，查询空间内完全可微的相机-LiDAR融合能在不牺牲部署效率的前提下增强端到端PnP的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在nuScenes公开数据集验证，缺乏跨数据集泛化评估；对极端天气或传感器失效场景的鲁棒性未深入讨论；查询门控机制的可解释性以及偏移学习稳定性尚需进一步分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨数据集自监督预训练以提升泛化能力，并将QGDF扩展至时序多帧融合以利用长时上下文。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究对关注多模态融合、端到端自动驾驶感知预测一体化、以及查询式Transformer架构的研究者具有直接参考价值，其提出的可微查询门控融合思路可迁移至其他传感器组合或机器人任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21406v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihan Su，Hongyang Wei，Kangrui Cen，Yong Wang，Guanhua Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21406v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用生成任务反向提升统一多模态模型的视觉理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniMRG，在理解训练中加入像素、深度、分割三种辅助生成任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多表征生成显著提升细粒度感知、空间理解和抗幻觉能力，同时增强生成性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证生成反哺理解，提出架构无关的多表征联合后训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建理解与生成相互增强的统一多模态模型提供简单有效的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unified Multimodal Models (UMMs) aspire to a virtuous cycle where visual understanding and generation reinforce each other, yet existing post-training recipes only exploit understanding-to-generation supervision, leaving the converse path—generation-for-understanding—almost untouched.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose UniMRG, an architecture-agnostic post-training scheme that augments standard vision–language objectives with auxiliary generation targets: pixel-level reconstruction, monocular depth, and semantic segmentation. These three dense-prediction heads are lightweight add-ons trained jointly with the base UMM, forcing the shared backbone to internalize appearance, geometry, and layout cues while still optimizing downstream understanding losses.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across three distinct UMM backbones and eight benchmarks, UniMRG systematically lifts fine-grained perception (+2.3 mIoU on ADE20K), halves object-hallucination rate in MMBench, and improves 3-D spatial-relation accuracy by 5–8%, all without degrading—and often slightly improving—text-to-image generation FID.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Extra decoder parameters and three additional losses increase GPU memory by ≈15% and training time by ≈25%; the method has only been validated on encoder–decoder UMMs and could behave differently on decoder-only LLMs, and the chosen auxiliary tasks are heuristic rather than automatically discovered.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can search for optimal auxiliary-task sets via differentiable architecture search and extend the cycle to video and 3-D domains where temporal and geometric self-supervision may yield even larger understanding gains.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers aiming to squeeze more vision understanding out of generative multimodal models can immediately plug UniMRG into existing codebases without architectural surgery, making it a drop-in technique for improving perception metrics while preserving generative quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030441" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lite-BSSNet: A Lightweight Blueprint-Guided Visual State Space Network for Remote Sensing Imagery Segmentation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxin Yan，Yuxiang Xie，Yan Chen，Yanming Guo，Wenzhe Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030441" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030441</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image segmentation requires balancing global context and local detail across multi-scale objects. However, convolutional neural network (CNN)-based methods struggle to model long-range dependencies, while transformer-based approaches suffer from quadratic complexity and become inefficient for high-resolution remote sensing scenarios. In addition, the semantic gap between deep and shallow features can cause misalignment during cross-layer aggregation, and information loss in upsampling tends to break thin continuous structures, such as roads and roof edges, introducing pronounced structural noise. To address these issues, we propose lightweight Lite-BSSNet (Blueprint-Guided State Space Network). First, a Structural Blueprint Generator (SBG) converts high-level semantics into an edge-enhanced structural blueprint that provides a topological prior. Then, a Visual State Space Bridge (VSS-Bridge) aligns multi-level features and projects axially aggregated features into a linear-complexity visual state space, smoothing high-gradient edge signals for sequential scanning. Finally, a Structural Repair Block (SRB) enlarges the effective receptive field via dilated convolutions and uses spatial/channel gating to suppress upsampling artifacts and reconnect thin structures. Experiments on the ISPRS Vaihingen and Potsdam datasets show that Lite-BSSNet achieves the highest segmentation accuracy among the compared lightweight models, with mIoU of 83.9% and 86.7%, respectively, while requiring only 45.4 GFLOPs, thus achieving a favorable trade-off between accuracy and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感分割需兼顾全局与局部，CNN难捕长程、Transformer高耗且深浅特征融合易错位、上采样易断细结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Lite-BSSNet用SBG生成边缘蓝图，VSS-Bridge线性状态空间对齐特征，SRB扩张卷积修复细结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Vaihingen与Potsdam上mIoU达83.9%与86.7%，轻量模型第一，仅45.4 GFLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将蓝图拓扑先验与线性视觉状态空间结合，提出SRB重连断结构，实现轻量高精度分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实时应用提供低算耗高精度方案，其状态空间与结构修复思想可迁移至其他密集预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像分割需在全局上下文与局部细节之间权衡，CNN感受野受限难以捕获长程依赖，而Vision Transformer的自注意力随图像尺寸呈二次复杂度，难以直接用于大场景遥感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Lite-BSSNet，由三部分组成：1) Structural Blueprint Generator将深层语义转化为边缘增强的拓扑先验；2) Visual State Space Bridge沿轴向聚合特征并投影到线性复杂度的视觉状态空间，平滑高梯度边缘信号；3) Structural Repair Block用多尺度空洞卷积扩大有效感受野，并以空间-通道门控抑制上采样伪影、重连细小结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Vaihingen与Potsdam基准上，Lite-BSSNet以仅45.4 GFLOPs取得83.9%与86.7% mIoU，优于所有对比轻量级模型，且在道路、屋顶边缘等薄连续结构处保持完整拓扑，验证了其精度-效率折中优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在两个德国城市数据集验证，泛化至不同传感器、地貌及类别分布的能力尚未可知；蓝图生成与状态空间扫描引入额外超参，对极低算力边缘节点的部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器自监督预训练以提升泛化性，并将状态空间模型进一步量化压缩至亚10 GFLOPs级别，实现无人机实时分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级遥感语义分割、长程依赖建模或结构保持上采样，该文提供的蓝图-状态空间框架与SRB门控修复策略可直接借鉴并扩展至其他高分辨率影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21037v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chengzu Li，Zanyi Wang，Jiaang Li，Yi Xu，Han Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21037v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型具备细粒度空间理解与连续动作规划能力以完成复杂视觉推理任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>把视频生成模型当作推理引擎，用生成帧充当中间推理步，在迷宫与七巧板任务中零样本测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型零样本泛化强，视觉上下文可显式控制规划，延长生成视频长度能按幂律提升表现</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“视觉测试时扩展律”，将视频生成从媒体工具升级为可扩展的视觉推理新范式</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉推理社区提供无需微调即可随计算量增长的通用框架，启发机器人、导航与动态规划研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型在文本推理上表现优异，它们在细粒度空间理解与连续动作规划方面仍显薄弱，难以模拟复杂视觉推理所需的动态过程。作者提出将视频生成模型重新定位为推理引擎，把生成帧当作从初始状态到解的中间推理步骤，从而把视觉推理转化为帧间动态模拟问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以视频生成模型为核心，不依赖任务特定微调，而是把输入状态与高层目标编码为条件帧，让模型自回归地生成后续帧序列作为规划路径。为验证通用性，作者设计两个极端任务：迷宫导航（低视觉变化、离散动作）与七巧板拼图（高视觉变化、连续形变），并引入视觉控制信号（如代理图标、目标轮廓）作为显式上下文。通过简单延长生成帧数（即增大视觉推理预算）实现测试时缩放，观察零样本泛化随推理计算量的变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示模型在两种任务上均取得强零样本泛化：迷宫任务在未见布局与更长路径上成功率随生成帧数提升而提高，七巧板任务在未见目标形状与遮挡条件下保持高视觉一致性并正确完成拼图。关键发现包括：1) 视觉上下文可作为通用控制接口，无需参数更新即可适应新图案；2) 存在“视觉测试时缩放律”，增加帧预算显著降低长路径或复杂形变的失败率；3) 视频生成范式对离散规划和连续操控同样有效，证明其作为通用视觉推理框架的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅聚焦二维平面任务，三维物理交互与真实机器人部署尚未验证；生成模型推理成本高，帧级模拟随长度增加呈线性甚至超线性计算开销，可能限制实时应用；评估指标主要基于任务成功率与视觉一致性，缺乏对生成帧是否真正捕捉物理因果或动作可执行性的细粒度检查。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将框架扩展到真实场景视频与机器人控制，结合强化学习或可微分物理对生成帧进行可行性验证与反馈优化；探索自适应帧预算策略，实现精度与效率的动态权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注视觉推理、规划与生成模型交叉领域的研究者，该文提供了将视频生成重新定义为推理计算的新范式，并给出可扩展的零样本评估协议与缩放律证据，可直接启发后续在机器人规划、自主驾驶及长时序决策系统中的研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3659827" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OCSA-FN: A Fusion Network with Orthogonality-Constrained Spatial Attention for Hyperspectral and Land Surface Temperature Data Classification
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Enyu Zhao，Yongfang Su，Nianxin Qu，Yulei Wang，Yongguang Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3659827" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3659827</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高光谱与地表温度数据融合分类中抑制冗余并提升特征表达能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支CNN分别提取光谱/温度特征，再用正交约束空间注意力模块自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>OCSA-FN在精度超越现有方法的同时减少参数量与计算复杂度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入正交空间基向量约束与自适应空间注意力，实现低冗余互补融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多源数据高效融合与精准分类提供新思路与轻量级网络架构</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感协同分类已成为土地覆盖制图的主流方向，但异构数据（高光谱与地表温度）在特征空间维度、物理含义及信息密度上差异巨大，直接拼接或简单加权易产生冗余、互扰，削弱互补优势。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OCSA-FN 双路网络：CSS-CNN 分支在 HSI 上执行空-谱联合卷积以保留细微光谱曲线；LS-CNN 分支在单通道 LST 上采用可学习 Sobel 核自适应捕捉热异常边缘。两路深度特征经 DRCA 模块做双池化+残差通道重标定，增强跨模态显著通道。核心 AOFF 模块通过正交约束生成两组互为正交的空间基向量，将 HSI 与 LST 特征投影到互补子空间，再用自适应空间注意力动态计算子空间融合权重，实现冗余最小化的互补融合。最终加权特征送入 softmax 完成像元级分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Houston2013、Trento 和 MUUFLEX 等基准数据集上，OCSA-FN 将 OA 提升 1.8-3.2%，参数减少 27%，FLOPs 降低 35%，可视化显示正交约束确实抑制了重叠激活区域，热边界与光谱异质区同时得到高响应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在像素级分类层面验证，未考虑对象级或场景级语义；正交约束引入的额外矩阵分解在大幅影像上训练时间增加约 22%，对 GPU 显存需求更高；LST 被当作单通道，未探讨多时相热节律信息的利用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将正交约束推广至时间维度，构建空-谱-热-时四阶正交张量融合框架，并探索无监督正交正则化以降低训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多源遥感特征融合、注意力机制设计或高光谱-热红外协同分类，该文提供的正交空间注意力思路可直接嵌入其他网络，减少冗余并提升互补性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CycleDiff：用于非成对图像到图像翻译的循环扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shilong Zou，Yuhang Huang，Renjiao Yi，Chenyang Zhu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657240</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无配对数据下用扩散模型实现高质量跨域图像翻译。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CycleDiff，联合对齐扩散与翻译过程，用时间相关网络端到端学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项跨域任务中取得 SOTA FID，Dog→Cat 等提升约 19+。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散去噪与翻译映射深度耦合，实现全局联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督图像翻译提供新扩散范式，兼顾保真与结构一致性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督图像翻译长期依赖GAN，但GAN存在训练不稳定、模式崩溃等问题；扩散模型在生成质量和分布覆盖度上表现更优，却尚未被充分用于跨域翻译。作者观察到扩散过程作用于带噪信号，而翻译过程本应作用于干净信号，二者目标不一致，导致现有工作只能浅层融合或分阶段训练，易陷入局部最优。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出CycleDiff，用一个可端到端联合训练的框架把扩散去噪与跨域翻译对齐：在每一步扩散时间步t，先从含噪图x_t中由扩散网络提取“干净”分量，再把这些分量送入时间相关的翻译网络完成域映射，最后把翻译结果重新注入扩散链。通过共享潜分量，扩散与翻译共享损失，实现全局优化。引入时间依赖的翻译子网络以学习随噪声水平变化的复杂映射，保证结构一致性和细节保真。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGB↔RGB（Cat↔Dog）及RGB↔Edge、RGB↔Semantics、RGB↔Depth等多模态任务上，CycleDiff取得SOTA FID，例如Dog→Cat和Dog→Wild分别将第二名方法的FID降低19.61和19.67，同时LPIPS、KID等指标也一致领先。视觉结果显示其结构保持更好、伪影更少，验证了联合优化对全局最优和分布覆盖度的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需要训练完整的扩散链，计算与存储开销高于单步GAN；对极端域差距（如RGB↔X-ray）尚未验证，且未探讨如何压缩采样步数以加速推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索蒸馏或一致性训练将步数减至10步以内，并引入文本或语义提示实现多模态可控翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究无/弱配对图像翻译、扩散模型在视觉任务中的应用，或希望用生成式方法提升跨模态合成 fidelity，该文提供了将扩散与翻译深度耦合的新范式及完整实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21278v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoRC: A Benchmark for Geolocation Reasoning Chains
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoRC：地理定位推理链基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohit Talreja，Joshua Diao，Jim Thannikary James，Radu Casapu，Tejas Santanam 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21278v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何评估并提升视觉-语言模型在全球照片定位任务中的可解释推理链质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>与GeoGuessr世界冠军等专家共建800条跨500场景的真值推理链，并以LLM/VLM-as-a-judge自动评分VLM输出。</p>
                <p><span class="font-medium text-accent">主要发现：</span>闭源VLMs定位准但推理链仍不如人，开源VLMs几乎失效，Qwen3 LLM-as-a-judge与人类评分最相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对地理定位推理链的公开基准，含细粒度视觉属性真值与自动评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释视觉推理研究提供量化工具，揭示VLM在提取高分辨率细节与可信解释上的关键短板。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Language Models (VLMs) have reached near-expert accuracy at guessing the global location of a single Street-View image, yet they routinely produce unverifiable or hallucinated explanations for their predictions. The absence of any dataset that pairs images with human-verified, step-by-step geolocation rationales prevents the community from measuring or improving the auditability of these systems. This work is motivated by the need to quantify how well VLMs can justify their geographic answers with real visual evidence rather than post-hoc confabulation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first define a &#34;geolocation reasoning chain&#34; as a sequence of visually grounded assertions (e.g., yellow license plate → Chile; red lateritic soil → northern Australia) that collectively lead to a country-level prediction. They recruit elite GeoGuessr players—including the reigning world champion—to inspect 500 diverse Street-View images and to author 800 reference chains, yielding an average of 1.6 chains per image that cite hundreds of distinct visual cues. To scale evaluation, they test two automated judges: an LLM-as-a-judge (Qwen 3) and a VLM-as-a-judge, both prompted to score machine-generated chains for factual alignment, visual grounding, and logical coherence against the expert references; human raters provide gold labels for 200 examples to calibrate judge correlation. Finally, they benchmark ten VLMs (closed and open-weight) by forcing each to emit a structured chain before predicting the country, and compare performance against an oracle-location baseline that hallucinates rationales without seeing the image.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qwen 3 LLM-as-a-judge achieves 0.81 Spearman correlation with human scoring, outperforming VLM-as-a-judge and enabling automatic evaluation at scale. While Gemini-1.5-Pro and GPT-5 attain ~74% country-level accuracy, their generated chains are judged accurate or complete only 38-42% of the time, revealing a large explainability gap versus human experts (≥80% chain quality). Open-weight VLMs (Llama-3.2-90B, Qwen-VL-Chat, etc.) fall below 25% chain quality—statistically indistinguishable from the hallucinating oracle baseline—indicating that they fail to extract fine-grained visual attributes needed for credible geolocation reasoning. The benchmark thus shows that current VLMs can guess where they are, but cannot yet show their work in a trustworthy manner.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dataset is limited to 500 scenes and 800 chains, so performance estimates for rare countries or subtle sub-national cues carry high variance. Expert annotators are self-selected GeoGuessr professionals, potentially biasing the reference chains toward game-specific heuristics rather than encyclopedic geographic knowledge. The automated judge, although well-correlated, still disagrees with humans in ~19% of cases, which could propagate unseen biases into model rankings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can expand the corpus to thousands of images per continent and recruit professional geographers to diversify rationales, while developing VLMs that explicitly ground every assertion back to segmented image patches to reduce hallucination.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying explainable AI, visual question answering, or geographic ML now have the first quantitative benchmark to measure whether their models produce faithful, human-aligned reasoning chains for location tasks, enabling direct comparison of hallucination rates across new architectures or training strategies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21751v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">动态拓扑感知：打破视觉-语言导航中的粒度刚性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiankun Peng，Jianyuan Guo，Ying Xu，Yue Liu，Jiashuang Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21751v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>VLN-CE中固定阈值拓扑图采样导致的粒度僵化，使简单区冗余、高不确定区欠采样。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DGNav框架：场景感知自适应阈值调节节点密度+动态图Transformer融合多模态线索重构边权。</p>
                <p><span class="font-medium text-accent">主要发现：</span>R2R-CE与RxR-CE上导航成功率与效率双提升，消融验证效率与安全最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出按需动态拓扑密度与边权联合优化，打破固定粒度，实现实时环境复杂度适配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为连续空间视觉语言导航提供可扩展的拓扑记忆范式，启发更鲁棒的长程智能体研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>连续环境中的视觉-语言导航（VLN-CE）需要将抽象语言指令映射到长程、安全、精确的空间动作，而显式拓扑地图已被证明是提供稳定空间记忆的关键。然而，现有拓扑规划方法普遍采用固定几何阈值采样节点，导致在简单区域过度采样、在高不确定区域采样不足，出现“粒度刚性”问题，既浪费算力又增加碰撞风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DGNav 框架，通过两项核心机制动态调节拓扑图：① 场景感知自适应策略，以预测路径点的离散度为上下文信号，在线调整节点生成阈值，实现“按需加密”；② 动态图 Transformer，将视觉、语言与几何特征融合为动态边权重，实时重构图连通性，抑制拓扑噪声并强化指令对齐。整个系统在不牺牲效率的前提下，对复杂区域进行密集覆盖，对简单区域保持稀疏，形成自适应粒度拓扑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 R2R-CE 与 RxR-CE 基准上，DGNav 显著优于现有最佳方法，SR 与 SPL 均提升约 3–5 个百分点，且跨场景泛化误差降低；消融实验表明，自适应密度机制将冗余节点减少 28%，同时把碰撞率降低 1.8 倍，验证了效率与安全的良好权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先训练的视觉-语言模型提取特征，若视觉感知在极端光照或纹理缺失场景失效，拓扑密度估计可能偏差；动态图 Transformer 的在线边权重计算带来额外 18% GPU 显存开销，对资源受限机器人仍显沉重；此外，实验仅在两个室内数据集验证，尚未覆盖室外大规模或动态障碍物场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入强化学习端到端优化“何时加密”策略，进一步减少手工阈值；同时结合语义 SLAM 将框架扩展到室外动态环境，并研究边缘计算友好版本的轻量图 Transformer。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言导航中的高效空间表征、长程决策安全或拓扑图自适应构建，本文提供的动态粒度思想与即插即用的 DGNav 模块可直接迁移至相关系统，减少冗余采样并提升指令跟随精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21078v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">缓解视觉-语言模型在时间动作定位中的模态偏差</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Li，Guangming Wang，Shuntian Zheng，Minzhe Ni，Xiaoman Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21078v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制视觉-语言模型在时序动作定位中的语言模态偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ActionVLM，用去偏加权和残差聚合让视觉主导、语言仅作补充。</p>
                <p><span class="font-medium text-accent">主要发现：</span>THUMOS14上mAP提升3.2%，显著降低语言先验过置信并增强时序推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化语言增益并动态重加权，同时以残差方式将语言作为视觉的精炼而非主信号。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在视频理解任务中平衡模态、抑制偏置提供可迁移框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Temporal Action Localization (TAL) 旨在未剪辑视频中同时定位动作起止时间与类别，传统方法主要依赖视觉线索。近期视觉-语言模型 (VLM) 通过引入文本语义提升了动作理解，但现有工作往往让语言先验主导推理，造成视觉信号被弱化，出现明显的模态偏差。本文动机即在于抑制语言先验的过度影响，使视觉信息重新成为主导，从而提升定位精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ActionVLM 框架，核心思路是“视觉为主、语言为辅”。首先设计去偏加权模块：对每段候选动作，分别获得视觉-only 与视觉-语言联合预测，计算语言带来的增量增益（language advantage），并据此动态调整语言特征的融合权重；增益越大则语言权重越高，反之则抑制。其次采用残差式聚合策略：将语言特征视为对视觉特征的残差修正，通过可控残差块逐步细化视觉表征，而非直接用语言结果主导分类/回归。整体训练采用标准 TAL 损失组合，并引入正则项防止语言权重过高。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 THUMOS14 基准上，ActionVLM 将现有最佳方法的 mAP@0.5 提升 3.2%，在不同 IoU 阈值下平均 mAP 亦有 1.8-2.4% 的增益。消融实验显示，去除去偏加权或残差聚合任一项都会使 mAP 下降 1-1.5%，验证了两模块对缓解语言过拟合与过度自信的有效性。可视化表明，语言权重在长时、模糊动作段升高，在视觉特征已足够判别时自动降低，符合“需要时才用语言”的预期。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 THUMOS14 上报告结果，尚未在 ActivityNet、EPIC-Kitchens 等更大或更复杂数据集验证泛化性；语言优势估计依赖视觉-only 分支的准确性，若视觉信号本身受损（如遮挡、暗光）则估计可能失真；方法引入额外分支与动态权重，增加了推理时延和显存开销，对实时应用可能不利。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将语言优势估计与视觉不确定性建模结合，引入更鲁棒的自适应机制，并拓展到多模态事件检测或视频段落检索任务；同时探索轻量化实现以满足实时视频分析需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、去偏公平性、或视频动作定位中的模态鲁棒性，该文提供了系统的“视觉优先、语言辅助”思路与可复现的模块设计，可直接借鉴其动态加权与残差修正策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21904v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越全局对齐：基于金字塔Shapley-Taylor学习的细粒度动作-语言检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanmo Chen，Guangtao Lyu，Chenghao Xu，Jiexi Yan，Xu Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21904v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破全局对齐局限，实现文本与局部动作片段/关节的细粒度检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出金字塔 Shapley-Taylor 框架，逐级对齐关节-片段-序列与词级语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示该方法显著优于现有最佳，精准匹配动作局部与对应文本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将金字塔感知与 Shapley-Taylor 交互分解引入跨模态动作-语言检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动作生成、编辑与理解提供细粒度语义桥梁，推动人机交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有动作-文本检索方法普遍将整条动作序列压缩为单一全局向量，再与句子级表征对齐，忽略了关节点、子片段与词级token之间的细粒度交互，导致语义鸿沟依旧显著。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者借鉴人类“由关节→片段→整体”的层级感知机制，提出Pyramidal Shapley-Taylor框架：先在空间轴把动作拆成各关节轨迹，在时间轴切成多尺度片段；随后用Shapley值量化每个关节/片段对文本token的边际贡献，并用Taylor展开捕捉高阶协同，逐层聚合得到最终匹配分数；整个训练过程以金字塔式对齐损失为主，兼顾局部-全局一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HumanML3D、KIT和BABEL三个公开基准上，PST将R@1相对提升约12-18%，且可视化显示模型能精准定位描述中提到的“抬手”“转身”等片段与关节；消融实验表明移除Shapley或Taylor组件后R@1下降5-8%，验证了细粒度贡献建模的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Shapley值计算随关节×片段×token组合呈指数增长，虽然作者采用蒙特卡洛采样，推理耗时仍比纯全局方法增加约2.3倍；此外，框架依赖密集的帧级标注，对只有粗粒度文本的动作数据难以直接迁移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入高效近似策略（如learned sparse Shapley）降低计算复杂度，并探索自监督预训练以摆脱对密集对齐标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态对齐、细粒度语义grounding或人类动作理解，该文提供的层级贡献建模视角与可解释评估手段可直接借鉴并扩展至视频-文本、音频-动作等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21468v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MemOCR：面向高效长程推理的布局感知视觉记忆</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaorui Shi，Shugui Liu，Yu Yang，Wenyu Mao，Yuxin Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21468v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在严格上下文预算下高效压缩长交互历史以支持长程推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MemOCR多模态记忆代理，用视觉布局渲染结构化富文本并强化学习优化预算感知压缩</p>
                <p><span class="font-medium text-accent">主要发现：</span>在极端预算下，MemOCR在多跳/单跳问答基准中优于文本基线且上下文利用率更高</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉布局作为自适应信息密度压缩手段，用图像化记忆替代线性文本序列</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长上下文Agent提供低成本高保真记忆方案，推动视觉推理与高效LLM应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Long-horizon agents must repeatedly reason over ever-growing interaction traces, yet transformer-based LLMs face hard context-window limits; naïvely truncating or uniformly summarizing history discards task-critical cues. Prior memory mechanisms serialize experience as flat text, incurring linear token cost and wasting scarce context on low-salience trivia.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MemOCR keeps a structured rich-text memory (headings, highlights, bullet trees) and renders it as a compact bitmap whose visual layout (font size, color blocks, spatial placement) encodes information density. An RL-trained policy decides which fragments to enlarge or fade, producing images that occupy fixed token budgets (via ViT tokens) while visually foregrounding key evidence. During inference the agent accesses memory by attending to these images instead of raw text, enabling aggressive compression without hand-crafted rules.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On multi-hop benchmarks (e.g., HotpotQA) and single-hop long-doc QA, MemOCR surpasses text-serialization baselines by 6-12% F1 under standard budgets and maintains &gt;90% peak performance even when the memory image is compressed to 10% of the token budget. Ablations show visual prioritization contributes twice the gain of pure text-pruning, and RL training with budget-aware rewards yields consistent gains across compression levels.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Rendering introduces an inductive bias that may discard fine-grained textual relations only expressible in language; performance drops when layouts become too dense or when questions require counting exact word occurrences. The system assumes access to a differentiable renderer and ViT encoder, adding engineering overhead and GPU memory that may offset token savings on smaller models.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend MemOCR to video or web-page memories where spatial cues are naturally multimodal, and explore adaptive rendering that dynamically re-layouts memory while the agent acts.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers building long-context agents, memory-efficient transformers, or multimodal reasoning systems will find MemOCR’s visual compression paradigm a practical way to break the token-length barrier without architectural window extensions.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21821v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMFineReason：通过开放数据为中心的方法弥合多模态推理差距</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Honglin Lin，Zheng Liu，Yun Zhu，Chonghan Qin，Juekai Lin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21821v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a &#34;less is more&#34; phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>开源多模态模型因缺乏高质量推理数据而在视觉推理上落后闭源系统。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段流水线：海量收集→CoT蒸馏→质量/难度筛选，构建1.8M样本5.1B token数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMFineReason-8B仅用8B参数即超越30B Thinking模型，7%精选子集即可达全量性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次公开大规模、长链CoT、覆盖STEM与视觉谜题的多模态推理数据集与难度感知过滤策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可复现的数据-centric方案，证明小模型靠优质推理数据也能逼近大模型性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管闭源视觉-语言模型（VLMs）在多模态推理上表现强劲，开源模型因缺乏高质量、长链思维（CoT）标注而明显落后，尤其在 STEM 图表与视觉谜题等复杂领域。现有公开数据集覆盖不足且推理链条短，难以激发模型的深度推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段数据构建管道：①大规模爬取并标准化 1.8 M 图文对，②用 Qwen3-VL-235B-A22B-Thinking 生成细粒度、视觉对齐的长 CoT 解答，总计 5.1 B tokens，③通过推理质量与难度感知双重筛选保留高价值样本。最终数据集覆盖 STEM 题、视觉谜题、游戏与复杂图表，每条样本均附带视觉定位的推理轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>基于 MMFineReason 微调的 MMFineReason-2/4/8 B 在同量级开源模型中刷新 SOTA；其中 4 B 版超越 8 B 教师模型，8 B 版逼近 32 B 闭源 Thinking 版，显示极高参数效率。关键发现“少即是多”：仅 7 %（123 K）的高质量子集即可达到全量数据性能，且推理导向数据同时提升通用 VQA 能力，呈现协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据蒸馏完全依赖 Qwen3-VL-235B-A22B-Thinking，可能继承其偏见与错误；难度评估指标未与人类专家校准，且 1.8 M 样本仍偏向英语与主流学科，多语言及文化多样性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入人类专家交叉验证与多教师模型投票以降低偏差，并探索跨语言、跨文化及更细粒度难度分层的持续扩充。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可复现的开源数据构建方案与 1.8 M 高质量推理对，可直接用于改进任何 VLM 的 CoT 能力，并为研究数据筛选策略、参数效率与推理-通用能力协同提供实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651958" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RealLiFe：基于分层稀疏梯度下降的实时光场重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yijie Deng，Lei Han，Tianpeng Lin，Lin Li，Jinzhi Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651958" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651958</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏输入下实时重建高质量光场以满足XR需求</p>
                <p><span class="font-medium text-accent">研究方法：</span>3D CNN粗建MPI后，用分层稀疏梯度下降(HSGD)仅迭代优化场景对齐的稀疏梯度</p>
                <p><span class="font-medium text-accent">主要发现：</span>比离线法快100倍且视觉质量相当，PSNR较在线法高2 dB</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用MPI内在稀疏流形，提出HSGD实现实时高质量光场优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为XR提供兼顾速度与质量的光场重建方案，可推动实时沉浸式应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>XR 应用需要实时从稀疏视图合成光场，但离线方法耗时而在线方法质量差。作者发现多平面图像（MPI）在梯度域具有天然稀疏性，可利用该特性大幅加速优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RealLiFe 先用轻量 3D CNN 快速生成场景的粗 MPI，随后仅对场景内容对齐的稀疏梯度分量执行层级式稀疏梯度下降（HSGD），在 3-5 次迭代内完成细化。HSGD 按空间-深度层级逐步激活非零梯度，避免稠密更新，实现 GPU 上 30 fps 级重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Blender 合成和真实捕获数据集上，RealLiFe 平均比 NeRF、Plenoptic 等离线方法快 100 倍，PSNR 仅低 0.3 dB；与 NV、FVS 等在线方法相比 PSNR 高 2 dB，LPIPS 低 25%，达到 1080p 30 fps 实时渲染。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设 Lambertian 占优，对镜面与半透明场景稀疏梯度不再成立，质量下降；目前仅支持正向 90° 视锥，360° 全景与动态场景需扩展；HSGD 超参（稀疏阈值、层级数）对场景尺度敏感，自动选择机制尚未研究。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的稀疏掩码预测网络，使 HSGD 自适应于任意材质与光照；将层级稀疏优化嵌入神经辐射场的体素-哈希结构，实现全景动态光场的实时重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究实时新视角合成、XR 光场显示或稀疏重建加速，本文提供的梯度域稀疏性与层级优化框架可直接迁移到神经辐射场、高斯溅射等表示，显著降低计算开销。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132885" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiCo: Disentangled concept representation for text-to-image person re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiCo：面向文本到图像行人重识别的解耦概念表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Giyeol Kim，Chanho Eom
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132885" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132885</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes ( e.g. , color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本-图像行人重识别中模态差异大、细粒度属性难对齐的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiCo框架，用共享槽-概念块表示实现属性解耦与跨模态层次对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-PEDES等三大数据集上达到SOTA，检索结果可解释且更细粒度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将槽-概念块解耦表示引入TIReID，实现颜色、纹理、形状等属性分离对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态行人检索提供可解释、细粒度的对齐新范式，推动视觉-语言理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-to-image person re-identification (TIReID) has to bridge a large modality gap between free-form natural language and person imagery, while also capturing subtle attribute differences that distinguish individuals wearing similar clothes. Existing methods struggle to align fine-grained visual and textual cues, leading to retrieval errors when multiple people share global attributes like color or outfit style.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DiCo introduces a shared slot-based embedding space where each slot serves as a part-level anchor (e.g., head, torso, legs) that exists in both image and text modalities. Every slot is further split into multiple concept blocks that explicitly disentangle complementary attributes such as color, texture, and shape, allowing the model to compose a person representation from disentangled factors. Hierarchical cross-modal alignment is then performed first at the slot level to establish part correspondence, and next at the block level to match specific attributes, yielding interpretable and fine-grained retrieval.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CUHK-PEDES, ICFG-PEDES, and RSTPReid, DiCo achieves competitive or superior performance compared with contemporary state-of-the-art TIReID methods. Qualitative visualizations show that explicit slot- and block-level representations enable users to understand which semantic part or attribute drives each retrieval, enhancing interpretability. The disentangled design also improves robustness when some attributes are missing or incorrectly described in the text.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework relies on a predefined number of slots and concept blocks, which may not adapt automatically to unseen body parts or new attribute types. Training requires paired image-text data with sufficient attribute diversity; performance could degrade on datasets with sparse or noisy captions. Computational overhead increases with the number of slots/blocks, potentially limiting real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive slot generation or dynamic concept discovery to handle open-vocabulary attributes without manual architecture choices. Integrating large-scale vision-language pre-training may further enhance generalization across domains.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal retrieval, fine-grained representation learning, or interpretable person Re-ID will find DiCo’s disentangled slot-block paradigm a useful reference for designing modular and explainable alignment mechanisms.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>