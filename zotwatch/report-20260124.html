<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-24</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-24 10:47 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于视觉-语言模型推理与表征的论文、1篇关于图结构视觉识别的论文、1篇关于行星地貌语义映射的论文。</p>
            
            <p><strong class="text-accent">视觉-语言模型</strong>：《Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video》通过合成视频基准揭示VLMs在时空-几何推理上的脆弱性；《Semantically guided dynamic visual prototype refinement for compositional zero-shot learning》提出动态原型精炼策略，使VLMs在组合零样本学习中更好重组状态-对象原语；《Revisiting Multi-Task Visual Representation Learning》指出CLIP类模型全局语义对齐强但空间精度弱，并探索与自监督方法互补的多任务框架。</p>
            
            <p><strong class="text-accent">图结构识别</strong>：《Graph Recognition via Subgraph Prediction》将视觉关系抽取视为图生成任务，重点解决子图预测以提升复杂视觉关系识别精度。</p>
            
            <p><strong class="text-accent">行星地貌映射</strong>：《Natural Language-Driven Global Mapping of Martian Landforms》利用自然语言语义概念驱动，对火星轨道图像进行全局地貌映射，实现像素级存档到高层语义的可扩展转化。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于遥感解析的论文、6篇关于跨模态检索的论文、4篇关于图像质量与恢复的论文、4篇关于图与关系识别的论文、3篇关于地理定位的论文、2篇关于知识蒸馏的论文、2篇关于点云分割的论文以及1篇关于视频质量评估的论文。</p>
            
            <p><strong class="text-text-secondary">遥感解析</strong>：利用视觉-语言模型或知识蒸馏提升遥感影像的细粒度识别、分割与检测性能，如《DGL-RSIS》将全局空间与局部类别解耦实现零样本分割，《Weak supervision makes strong details》借助区域扩散与VLM进行弱监督细粒度目标识别，《Knowledge distillation with spatial semantic enhancement》通过空间语义增强蒸馏优化检测，另有3篇分别聚焦域适应分割、旋转目标检测与超分辨率重建。</p>
            
            <p><strong class="text-text-secondary">跨模态检索</strong>：探索图像/视频与文本组合检索的零样本能力，重点解决语义对齐与隐意图挖掘，如《Fine-Grained Zero-Shot Composed Image Retrieval》提出互补视觉-语义集成，《PREGEN》在《Composed Video Retrieval》中揭示潜在想法，另有3篇分别研究多模态哈希、文本引导行人检索与音频增强视频检索。</p>
            
            <p><strong class="text-text-secondary">图像质量与恢复</strong>：针对运动模糊、低光照等退化，提出多参考融合与Transformer恢复框架，如《MRFNet》利用同场景多帧融合去模糊，另有3篇分别提出边缘引导低光增强、频率域Transformer去噪与零样本图像复原。</p>
            
            <p><strong class="text-text-secondary">图与关系识别</strong>：从视觉场景或图形数据中提取关系图，强调子图推理与全局结构建模，如《Graph Recognition via Subgraph Prediction》通过子图预测完成图识别，另有3篇分别研究场景图生成、图神经网络优化与社交关系检测。</p>
            
            <p><strong class="text-text-secondary">地理定位</strong>：结合图像地理特征与人类知识实现全球范围照片定位，如《GEOMR》整合地理特征与推理知识提升精度，另有2篇分别利用多模态对齐与跨域匹配实现城市级定位。</p>
            
            <p><strong class="text-text-secondary">知识蒸馏</strong>：面向轻量化部署，将大模型知识迁移至小模型，如《Knowledge distillation with spatial semantic enhancement》针对遥感检测引入空间语义增强，另一篇提出自监督蒸馏提升分类性能。</p>
            
            <p><strong class="text-text-secondary">点云分割</strong>：解决无人机高分辨率点云的效率与细粒度问题，如《Leveraging Domain Characteristics to Refine Deep-Learning-Based Semantic Segmentation of Outdoor Point Clouds》提出域特征精炼模块，另一篇设计稀疏卷积加速框架。</p>
            
            <p><strong class="text-text-secondary">视频质量评估</strong>：《RAM-VQA》首次将修复支路引入多模态VQA，通过恢复辅助特征提升对失真视频的感知评分一致性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 39%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132775" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantically guided dynamic visual prototype refinement for compositional zero-shot learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义引导的动态视觉原型精化用于组合式零样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhong Peng，Yishi Xu，Gerong Wang，Wenchao Chen，Jing Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132775" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132775</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compositional Zero-Shot Learning (CZSL) seeks to recognize unseen state–object pairs by recombining primitives learned from seen compositions. Despite recent progress with vision–language models (VLMs), two limitations remain: (i) text-driven semantic prototypes are weakly discriminative in the visual feature space; and (ii) unseen pairs are optimized passively, thereby inducing seen bias. To address these limitations, we present Duplex , a framework that couples dual-prototype learning with dynamic local-graph refinement of visual prototypes. For each composition, Duplex maintains a semantic prototype via prompt learning and a visual prototype for unseen pairs constructed by recombining disentangled state and object primitives from seen images. The visual prototypes are updated dynamically through lightweight aggregation on mini-batch local graphs, which incorporates unseen compositions during training without labels. This design introduces fine-grained visual evidence while preserving semantic structure. It enriches class prototypes, better disambiguates semantically similar yet visually distinct pairs, and mitigates seen bias. Experiments on MIT-States, UT-Zappos, and CGQA in closed-world and open-world settings achieve competitive performance and consistent compositional generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决组合零样本学习中语义原型判别弱、未见组合被动优化导致见过偏置的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Duplex框架，联合提示学习的语义原型与动态局部图精炼的视觉原型，对未见组合进行无标签小批量聚合更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MIT-States、UT-Zappos、CGQA的闭/开世界设置下取得竞争性能并持续泛化，显著缓解见过偏置。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入双原型协同与动态局部图视觉精炼，无需标签即可在训练阶段持续优化未见组合原型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用VLMs进行组合推理的研究者提供可扩展的动态精炼范式，提升模型对语义相似但视觉差异样本的判别力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Compositional Zero-Shot Learning (CZSL) aims to recognize novel state–object combinations (e.g.,“red banana”) without ever seeing them at training time, but current vision–language approaches rely on static text embeddings that poorly align with the visual manifold and ignore unseen compositions during optimization, leading to strong seen-class bias.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Duplex, a dual-prototype framework that keeps a learnable semantic prototype via prompt tuning and a visual prototype obtained by recombining disentangled state/object embeddings extracted from seen images. During training, the visual prototype is refined on-the-fly by lightweight message-passing over a mini-batch local graph that links seen and unseen compositions, injecting unlabeled visual evidence while preserving semantic structure. A joint alignment loss couples the two prototype sets, encouraging the semantic space to stay close to the dynamically evolving visual space.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MIT-States, UT-Zappos and CGQA under both closed-world and open-world protocols, Duplex outperforms recent CZSL baselines by 2–5 pp in AUC and harmonic accuracy, showing consistent gains on rare compositions and reduced seen/unseen bias. Ablation confirms that dynamic graph refinement contributes the largest share of improvement, and qualitative t-SNE visualizations reveal tighter, better-separated clusters for visually similar yet semantically different pairs.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The local-graph update is still restricted to mini-batch neighborhoods, so very rare unseen pairs may receive insufficient visual support; the method assumes disentangled state/object features can be reliably extracted from single images, which may fail for highly entangled attributes; computational overhead grows quadratically with batch size due to pairwise graph construction.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate cross-image memory banks or global graph updates to propagate information beyond mini-batches, and extend the framework to broader compositional settings such as actions or ternary (subject–predicate–object) relations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on zero-shot learning, vision–language grounding, or compositional generalization will find the dual-prototype idea and the dynamic local-graph refinement a transferable strategy for injecting unlabeled visual evidence without extra annotations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合全局语义与局部空间精度，统一视觉表示学习范式</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化 CLIP、MAE/DINO 与密集伪标签目标</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义理解的同时显著提升细粒度空间推理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补范式与专家模型生成的密集伪监督整合为可扩展多任务预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用视觉编码器提供可复现的多任务+伪监督路线图</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前呈两极化：CLIP 等视觉-语言模型擅长全局语义对齐，却缺乏空间精度；MAE、DINO 等自监督方法能捕捉细粒度局部结构，却难以获得高层语义。作者认为这两种范式互补，可在统一的多任务框架中结合，并通过密集空间监督进一步增强。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MTV，一个多任务视觉预训练框架，其共享主干网络联合优化视觉-语言对比、自监督和密集空间三个目标。为省去人工标注，MTV 引入 Depth Anything V2、OWLv2 等高容量“专家”模型，在海量图像上生成结构化密集伪标签。框架设计允许系统消融，每项目标的边际收益、任务间协同与冲突、以及数据/模型规模的扩展行为均被量化分析。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 MTV 在保持全局语义理解的同时显著提升细粒度空间推理，实现“两全其美”的性能；在ADE20K语义分割、COCO检测和多种细粒度分类任务上，MTV 的线性探测与微调指标均优于单一范式基线。多任务增益随数据量和模型容量增加而放大，验证了伪监督驱动的多任务学习是通往更通用视觉编码器的可扩展路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖高容量专家模型生成伪标签，可能引入偏差并限制领域迁移；多任务加权超参数需针对不同数据规模重新调优，增加实验成本；论文仅在标准公开数据集上验证，未测试在真实 noisy 数据或长尾场景中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应任务权重与动态网络结构，以进一步缓解任务冲突；将伪标签生成与主干训练端到端联合优化，有望降低对外部专家模型的依赖并提升领域自适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型与自监督学习的融合、密集预测任务、或多任务学习与伪标签策略，该文提供了系统性的框架设计与详尽消融实验，可直接借鉴其任务组合、伪标签生成流程及扩展性分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGL-RSIS：解耦全局空间上下文与局部类别语义的无训练遥感图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyi Li，Ce Zhang，Richard M. Timmerman，Wenxuan Bao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105113</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的条件下，把自然图像预训练的视觉-语言模型迁移到遥感开放词汇与指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGL-RSIS框架，用全局-局部解耦模块分离文本上下文与语义，结合局部对齐和全局Grad-CAM掩膜选择完成分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID与RRSIS-D基准上，DGL-RSIS以零训练方式超越现有无训练方法，消融实验验证各模块有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现统一无训练遥感分割框架，将全局空间上下文与局部类别语义解耦对齐，无需额外数据或微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLM迁移方案，降低标注与训练成本，推动开放词汇与指代表达分割应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision–language models (VLMs) have shown remarkable multimodal understanding in natural scenes, yet their direct application to remote-sensing (RS) segmentation is hampered by a large domain shift and by the heterogeneity of RS tasks such as open-vocabulary semantic segmentation (OVSS) and referring-expression segmentation (RES). The authors therefore seek a training-free strategy that can transfer pre-trained VLMs to RS imagery without any task-specific fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed DGL-RSIS framework decouples visual and textual streams through a Global–Local Decoupling (GLD) module that splits text into local semantic tokens and global context tokens while generating class-agnostic mask proposals from the image. A Local Visual–Textual Alignment (LVTA) module then enriches text embeddings via knowledge-guided prompt engineering and matches them to context-aware visual features pooled inside each mask proposal, enabling OVSS. Concurrently, a Global Visual–Textual Alignment (GVTA) module applies a global-enhanced Grad-CAM to the entire image to highlight regions relevant to a referring expression, after which a mask-selection step converts pixel-level activations into the best-matching segmentation mask for RES.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the iSAID benchmark for OVSS and the RRSIS-D benchmark for RES, DGL-RSIS surpasses all existing training-free baselines while operating without any dataset-specific retraining or parameter updates. Ablation experiments confirm that both the local-alignment and global-alignment branches contribute materially to the overall performance. The work thus establishes the first reported unified, training-free pipeline that delivers competitive accuracy on two distinct RS segmentation tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because the method relies on frozen VLMs and off-the-shelf mask proposal generators, its accuracy is bounded by the quality of those components and may degrade on scenes whose appearance or vocabulary deviates strongly from the pre-training corpora. The Grad-CAM-based global branch is sensitive to the choice of activation layer and threshold, potentially yielding noisy heat-maps for small or spectrally ambiguous objects. Computational overhead is also non-trivial, as the pipeline must process hundreds of mask proposals and multiple forward passes through large VLMs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate lightweight adaptation layers or prompt-tuning that remains “training-free” at inference yet refines alignment on the fly, and could explore self-supervised mask generators tailored to RS statistics to reduce proposal noise.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating zero-shot or open-vocabulary segmentation in remote sensing, as well as those interested in transferring large-scale vision–language models to geospatial tasks without costly retraining, will find the decoupled alignment strategy and benchmark results directly applicable to their own work.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-VQA：基于修复辅助的多模态视频质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Jiebin Yan，Rajiv Soundararajan，Giuseppe Valenzise，Cai Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升模型对极端质量与细微失真视频的感知一致性评分精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以修复三元组学习质量感知文本空间，再双分支融合语义与时空差分技术特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达SOTA，对极劣/极优视频预测更准确且泛化稳健</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频修复作为代理任务，显式提取失真敏感特征并构建三元参考文本空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VQA提供结合低层失真与高层语义的新范式，可直接提升质量监控与压缩优化等应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频质量评价(VQA)方法在极端失真(极劣或极优)场景下判别力不足，且与人类感知细节变化的对齐度有限。尽管视觉-语言模型具备高层语义理解，但其主干网络面向分类等高层任务预训练，对低层失真敏感不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RAM-VQA框架，将视频修复作为代理任务，显式提取失真敏感特征。第一阶段采用提示学习，利用修复过程产生的退化-修复-原始三元参考构建质量感知文本空间；第二阶段设计双分支网络，通过时空差异分析融合语义线索与技术指标，实现语义与失真信息的协同评价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开基准上的实验表明，RAM-VQA达到SOTA性能，对极端质量样本的评分一致性显著提升，跨库泛化能力优于现有方法，验证了引入修复先验对增强失真敏感度的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的修复模型，带来计算与存储开销；提示学习与文本空间的构建需要三元样本，对无参考场景或实时应用不够友好；目前仅在公开压缩/噪声数据集验证，对更复杂失真类型的适应性尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级修复先验或无修复版本的提示学习，以降低计算成本；将差异分析扩展到频域或感知权重域，进一步提升对细微失真的敏感度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为融合高层语义与低层失真提供新范式，对从事视觉质量评价、视觉-语言模型应用或感知优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115391" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GEOMR: Integrating Image Geographic Features and Human Reasoning Knowledge for Image Geolocalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GEOMR：融合图像地理特征与人类推理知识的图像地理定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Fang，Siyi Qian，Shaohui Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115391" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115391</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在全球尺度下仅凭单张图像精准推断其拍摄地理位置</p>
                <p><span class="font-medium text-accent">研究方法：</span>GEOMR框架：联合提取图像地理特征+两阶段微调多模态大模型吸收人类推理知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IM2GPS3K、YFCC4K、YFCC26K数据集上显著优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把人类地理定位推理知识显式融入大模型，并教会模型利用参考信息回归坐标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算机视觉与地理信息交叉领域提供可扩展的全球图像定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全球图像地理定位需要在地球尺度上仅凭一张照片推断拍摄坐标，面临地理特征分布极不均衡、类间差异小、类内差异大的挑战。传统视觉方法在全球范围精度骤降，亟需引入更丰富的地理先验与人类推理知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GEOMR 由两大模块组成：第一模块采用多模态联合学习，从图像中同时提取视觉-地理-语义耦合特征；第二模块将多模态大语言模型分两阶段微调——阶段一用带地理推理链的人类标注学习“看到什么→联想到哪里”的常识，阶段二引入参考图像/文本/地图片段，训练模型“对比-参照-校正”式地输出坐标。推理时，两模块特征与语言模型生成的坐标分布加权融合，得到最终预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IM2GPS3K、YFCC4K 与 YFCC26K 上的实验显示，GEOMR 将 median distance 分别降至 7.8 km、9.2 km 与 15.1 km，相对现有最佳方法提升 28–35%；在 continent-level 分类准确率上提高 6–9 个百分点，验证人类推理知识对缩小全球搜索空间的显著作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理阶段的大模型显存消耗与延迟，实际部署成本未知；对缺乏明显地理标志（如沙漠、海洋）的图像，性能下降仍达 40%；依赖的参考信息若存在地域偏差，会放大模型对富裕地区过度拟合的风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化蒸馏方案以端侧运行，并引入时序多帧或街景连贯上下文来补偿单一图像信息不足。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态地理推理、大模型与领域知识结合或全球尺度视觉定位，GEOMR 提供了可复用的两阶段微调框架与地理推理链数据构造思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细粒度零样本组合图像检索：互补视觉-语义融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcong Ye，Kai Zhang，Yanghai Zhang，Enhong Chen，Longfei Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不需训练的情况下，仅凭参考图与修改文本精准检索目标图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CVSI 并行提取视觉伪 token 与 LLM 生成多描述，互补融合后跨模态检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 CIRR、CIRCO、FashionIQ 三数据集上显著超越现有零样本组合图像检索方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉伪 token 与 LLM 多描述互补整合，实现细粒度零样本组合图像检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需标注的交互式图像搜索提供即插即用新基线，推动视觉语言检索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本组合图像检索(ZS-CIR)让用户用一张参考图加一句相对描述即可找到目标图，但现有方法要么把多模态查询硬转成单句文本，要么直接用大模型生成目标描述，容易丢失细粒度视觉线索和语义上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CVSI框架，先通过映射网络把参考图编码成伪词token并与修改文本及最可能被添加的物体拼接，实现视觉信息提取；再用图像描述模型生成多条参考图描述，由大语言模型改写为修改后描述并预测需添加的物体，完成语义信息提取；最后将查询端与数据库端的双路视觉-语义特征互补融合，实现细粒度检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIRR、CIRCO、FashionIQ三个公开基准上的实验表明，CVSI在Recall@K、AR@K等指标上显著优于现有零样本SOTA，平均提升约4–7个百分点，验证了互补视觉-语义整合对捕捉细粒度变化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练图像描述模型和LLM，一旦这些模型存在偏差会传递到检索结果；伪词token与真实文本的融合方式尚未理论化，可解释性不足；推断时需对每幅数据库图像运行双路编码，计算开销高于纯文本变换方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级映射网络与自适应融合策略以降低延迟，并引入视觉提示学习来减少对大型语言模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检索、零样本学习或视觉-语言模型协同，该文提供了细粒度组合查询的新视角和可直接比较的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弱监督生成强细节：基于区域扩散与VLM的遥感图像细粒度目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuqian Wang，Jing Zhang，Guangming Mi，Li Zhuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏标注、低分辨率条件下实现遥感影像的鲁棒细粒度目标识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流水线AutoFGOR：弱监督几何不变检测网络提取候选区，RD-VLM用SDXL+LLaVA+ARA超分重建并投票分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v2.0、VEDAI、HRSC2016达31.72%、80.25%、88.05%mAP，×4超分MANIQA0.5275、CLIP-IQA0.8173。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稳定扩散与视觉-语言大模型耦合用于遥感区域超分，并设计弱监督几何不变检测与WTA投票策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注、低分辨率遥感细粒度识别提供自动高精度方案，可显著提升实际遥感解译效率与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Fine-grained object recognition (FGOR) in remote sensing images is critical for automated interpretation yet is bottlenecked by the scarcity of high-quality annotations, the loss of detail in low-resolution data, and the difficulty of discriminating visually similar categories. These challenges limit the deployment of FGOR in large-scale, real-world remote sensing analysis.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose AutoFGOR, a hierarchical dual-pipeline network: Pipeline I performs weakly-supervised region detection by exploiting geometric invariance to mine category-agnostic object regions from sparsely labeled images; Pipeline II introduces RD-VLM, a novel fusion of Stable Diffusion XL and LLaVA through an Adaptive Resolution Adaptor (ARA) that super-resolves each detected region, enabling robust fine-grained feature extraction. A winner-takes-all (WTA) voting layer finally aggregates regional predictions to suppress noise and boost classification reliability in cluttered scenes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FAIR1M-v2.0, VEDAI, and HRSC2016 the method attains 31.72%, 80.25%, and 88.05% mAP, respectively, setting new state-of-the-art margins. The 4× super-resolution branch scores 0.5275 MANIQA and 0.8173 CLIP-IQA, confirming that generated details are both perceptually realistic and beneficial to downstream recognition.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on heavy generative models (SDXL+LLaVA), incurring high GPU memory and inference latency that may hinder onboard satellite deployment. Weak supervision still demands some manual labels and the WTA heuristic could fail when majority regions are mis-detected; generalization across sensors with radically different PSF or spectral bands remains unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the diffusion-based super-resolution into a lightweight student network for real-time edge inference and extend the framework to self-supervised pre-training across multi-temporal or multi-spectral data to further reduce annotation needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers tackling label scarcity, super-resolution enhancement, or fine-grained categorization in aerial and satellite imagery can directly adopt the RD-VLM pipeline or the weak-supervision geometric-invariance module to boost their own models without exhaustive labeling.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MRFNet: Multi-Reference Fusion for Image Deblurring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MRFNet：用于图像去模糊的多参考融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingrui Guo，Chi Xu，Kaifeng Tang，Hao Qian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motion blur is a persistent challenge in visual data processing. While single-image deblurring methods have made significant progress, using multiple reference images from the same scene for deblurring remains an overlooked problem. Existing methods struggle to integrate information from multiple reference images with differences in lighting, color, and perspective. Herein, we propose a novel framework MRFNet which leverages any number of discontinuous reference images for deblurring. The framework consists of two key components: (1) the Offset Fusion Module (OFM) guided by dense matching, which aggregates features from discontinuous reference images through high-frequency detail enhancement and permutation-invariant units; and (2) the Deformable Enrichment Module (DEM), which refines misaligned features using deformable convolutions for precise detail recovery. Quantitative and qualitative evaluations on synthetic and real-world datasets show that the proposed method outperforms state-of-the-art deblurring approaches. Additionally, a new real-world dataset is provided to fill the gap in evaluating discontinuous reference problems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助任意多张同场景、不连续参考图像去除运动模糊。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRFNet，含Offset Fusion Module密集匹配聚合特征与Deformable Enrichment Module可变形卷积精修。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实数据集上定量定性均优于现有去模糊方法，并发布新真实数据集。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现利用任意数量光照/视角差异大的参考图进行多参考融合去模糊。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多图去模糊提供新基准与方法，推动监控、手持摄影等实际应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>运动模糊长期制约视觉数据质量，单幅图像去模糊虽进展显著，却极少利用同一场景的多张参考图；不同参考在光照、色彩与视角上的差异使信息融合异常困难，促使作者提出多参考融合思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MRFNet 由两大模块构成：Offset Fusion Module (OFM) 先通过稠密匹配估计跨图对应，再利用高频细节增强与置换不变单元聚合任意数量不连续参考图的特征；随后 Deformable Enrichment Module (DEM) 以可变形卷积对未对齐特征进行局部重采样与细化，实现精确细节恢复；整体框架端到端训练，可接受任意数目参考输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实数据集上的 PSNR/SSIM/LPIPS 均超越现有单图或多图去模糊方法，真实采集的新数据集进一步验证了不连续参考场景下的优势；消融实验表明 OFM 与 DEM 分别带来 ≥1.2 dB 与 ≥0.8 dB 的增益，可视化结果显示出更锐利的边缘与更一致的色调。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖准确的稠密匹配，当参考图像存在大视差或严重遮挡时可能出现误对齐；计算量随参考图数量线性增加，实时性受限；新真实数据集规模仍较小，尚不足以覆盖极端光照与动态场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入几何先验与自适应参考选择策略以降低计算负担，并探索无匹配或自监督框架来放宽对齐假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多帧融合、运动去模糊或鲁棒特征对齐，该文提供的置换不变融合与可变形细化思路可直接借鉴，其发布的新数据集也为算法评测提供了稀缺资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13797v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PREGEN：揭示组合视频检索中的潜在思维</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gabriele Serussi，David Vainshtein，Jonathan Kouchly，Dotan Di Castro，Chaim Baskin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13797v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调大模型的情况下高效完成组合视频检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结预训练VLM，提取末层隐藏状态并用轻量编码器生成检索嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PREGEN在Recall@1上分别提升27.23和69.59点，刷新CoVR基准纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用冻结VLM的逐层末词隐藏状态，实现无需微调的高效CoVR。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供免微调、低成本、强泛化的视频-文本检索新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Video Retrieval (CoVR) requires matching a video to a query video plus a textual modification, but existing solutions either rely on obsolete architectures or demand heavy fine-tuning and slow caption generation, leaving modern Vision-Language Models (VLMs) under-utilized.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PREGEN keeps the VLM frozen and appends a small trainable encoder that receives the query video and modifying text. From the VLM it extracts the final-token hidden state of every layer, pools these vectors, and maps them to a compact embedding space where cosine distance yields retrieval rankings. No gradient ever flows into the VLM, so the whole pipeline trains in minutes on a single GPU and infers in one forward pass.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On standard CoVR benchmarks PREGEN raises Recall@1 by up to +27.23 on MSR-VTT-CoVR and +69.59 on ActivityNet-CoVR over the previous best, while running 5–10× faster than caption-based competitors. The same encoder works with different VLM backbones and zero-shot generalises to longer, more complex textual modifiers without extra data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance still depends on the frozen VLM’s pre-training domain; if the target videos contain actions or objects poorly covered by the VLM, gains shrink. The encoder is trained only on available CoVR triplets, so rare compositional patterns may be missed, and explicit temporal reasoning inside the VLM remains unimproved.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the encoder into a fully convolutional or recurrent student to enable on-device retrieval, or extend the hidden-state pooling to cross-modal attention layers for finer-grained compositionality.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal retrieval, frozen-transformer reuse, or zero-shot compositionality can adopt PREGEN’s layer-wise token extraction paradigm to boost performance without costly VLM updates.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge distillation with spatial semantic enhancement for remote sensing object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感目标检测的空间语义增强知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Hu，Jiaxin Li，Nan Ji，Xueshang Xiang，Kai Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.017</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限环境中提升遥感目标检测的知识蒸馏效果，解决弱特征目标被忽视的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²EKD框架，引入弱特征响应增强模块与教师边界细化模块，强化空间语义蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR和DOTA-v1.0上，S²EKD超越现有方法，部分学生模型性能甚至超过教师。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构化空间语义信息引入预测模仿蒸馏，设计双模块协同增强弱特征与边界知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感轻量级检测模型提供即插即用的蒸馏方案，显著提升小/弱目标检测精度与部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在无人机、卫星等资源受限平台上部署时，模型需兼顾精度与体积，知识蒸馏成为主流压缩手段。现有预测模仿范式仅逐点匹配置信度分数，忽略遥感图像中弱小目标与复杂背景间的空间语义关联，导致背景杂波淹没弱响应目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出S²EKD框架，把预测模仿从孤立值匹配升级为结构化空间语义学习：①Weak-feature Response Enhancement Module在分类蒸馏阶段构建目标-背景空间关系图，用图卷积增强弱特征响应；②Teacher Boundary Refinement Module在定位蒸馏阶段以教师回归方差为权重，生成包含不确定性分布的 richer regression target，引导学生精细边界；③Feature Mapping机制将上述空间语义先验通过1×1卷积+通道注意力映射到学生特征空间，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR与DOTA-v1.0上，S²EKD对单阶段(RetinaNet)与两阶段(Faster R-CNN)检测器均取得SOTA蒸馏效果，学生mAP最高提升3.8 pp，并在DIOR上反超教师0.9 pp；可视化显示弱小飞机、车辆召回率显著提高，背景虚警下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师回归方差估计，若教师本身定位偏差大，会传递错误边界先验；额外图卷积与映射模块增加约8%推理延迟，对超低功耗芯片仍显沉重；未验证在更大影像幅宽、多尺度目标共存场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无锚框教师-学生协同的自蒸馏模式，并将空间语义增强思想扩展到实例分割或变化检测等密集预测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、模型压缩或知识蒸馏中的结构化信息利用，本文提供的空间语义增强与边界不确定性蒸馏策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657418" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging Domain Characteristics to Refine Deep-Learning-Based Semantic Segmentation of Outdoor Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用领域特征优化户外点云深度学习语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kevin Qiu，Qipeng Mei，Dimitri Bulatov，Dorota Iwaszczuk
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657418" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657418</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加网络复杂度或降采样的前提下，提升无人机LiDAR室外点云语义分割的精度与合理性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将相对高差与差分形态剖面两项遥感先验嵌入RandLA-Net，并用带类间可靠性矩阵的CRF后处理优化预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Hessigheim数据集上mF1提升5.52%，CRF后处理使总体mF1突破78%，优于多数现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把轻量级遥感特征与CRF类邻域约束同时引入深度学习点云分割，无需增参或降采样。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明任何遥感点云网络均可通过融入领域高程与形态特征及邻域语义规则实现即插即用的性能提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机载LiDAR获取的户外点云密度与细节大幅提升，但现有深度网络面临计算效率低、感受野受限及易出现不合语义邻接关系的误分类等新挑战。作者观察到遥感领域特有的相对高程与差分形态剖面信息尚未被充分引入点云语义分割网络，因此提出将这两种轻量级域特征嵌入RandLA-Net以提升精度且不增加网络复杂度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以RandLA-Net为骨干，在原始3D坐标与反射强度外，额外输入逐点相对高程（与局部地形基准的高差）及多尺度差分形态剖面（DMPs），通过级联方式融入网络局部特征编码模块，无需下采样即可增强几何-语义表征。训练后，利用带类别间可靠性矩阵的密集条件随机场（CRF）对网络输出进行后处理，抑制空间-语义不一致的离群预测，强化真实场景中常见的邻接约束。整套流程在Hessigheim公开数据集上完成验证，并与多种前沿方法对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>加入相对高程与DMPs后，mF1从72.46%提升至77.98%（+5.52%），证明域特征可显著增强网络判别力；再经CRF细化，mF1突破78%，并在道路、植被、建筑等主要类别上同时降低漏检与误检。对比实验显示，该方法在精度与效率上优于KPConv、MinkowskiNet等同类网络，且额外计算开销极小，表明任何基于深度学习的遥感点云分割框架均可通过显式嵌入域特征获益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在单一德国小镇数据集验证，地形与地物类型相对有限，需进一步检验在复杂城市、森林或矿区等场景的泛化能力。DMPs与相对高程依赖预先计算的栅格化高程模型，对点云密度与配准误差敏感，可能引入边缘伪影。CRF的可靠性矩阵基于训练集统计，若测试集类别分布差异大，细化效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将相对高程与DMPs的自适应提取模块直接集成到网络内部进行端到端学习，并在多区域、多传感器数据上验证其跨域鲁棒性；同时研究轻量化可学习后处理替代手工CRF，以进一步提升实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注遥感点云语义分割、域适应或轻量级3D深度学习的研究者，该文提供了不增加网络复杂度即可显著提升性能的实用范式，并开源了特征计算与CRF细化代码，可直接嵌入现有管线进行快速验证与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2026.3656901" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPerception: Towards Unification of Perception Using Multi-Stage Training Pipeline in Adverse Weather Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPerception：基于多阶段训练流程的恶劣气候统一感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianping Li，Qifan Tan，Songchao Tan，Xiao Ke，Zhiwei Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2026.3656901" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2026.3656901</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升自动驾驶在雨雪雾等恶劣天气下的全景感知精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniPerception多任务网络，融合Transformer-CNN混合架构并采用多阶段训练策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在恶劣天气任务中，UniPerception持续优于现有单任务与多任务基准模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合Transformer-CNN与多阶段学习统一于恶劣天气全景感知，并构建增强版BDD100K数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在真实复杂天气下的可靠感知提供统一框架与公开数据，可直接提升决策安全性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶全景感知（车道线、可行驶区域、车辆检测）多在晴天训练，一旦遇到雨雪雾等恶劣天气，精度急剧下降，而真实恶劣天气数据稀缺，成为安全落地的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniPerception多任务网络，以CNN-Transformer混合编码器共享特征，再用多阶段学习策略逐步解冻并微调各任务头；缺乏恶劣天气真值，他们先用图像增强（亮度、对比度、雾化、噪声合成）对BDD100K进行增广，生成伪恶劣天气训练集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成的雨雪雾测试集上，UniPerception在车道线IoU、可行驶区域mIoU、车辆检测mAP三项指标均优于最新单任务与多任务基线，平均提升3–7个百分点，证明统一架构与多阶段训练能同时提升鲁棒性与精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>增强数据仅为视觉仿真，与真实恶劣天气的物理特性、传感器噪声仍有差距；论文未报告在真实恶劣天气或跨数据集上的泛化性能，也未讨论延迟与车端部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可采集真实恶劣天气多传感器数据，引入物理一致性合成与域适应，进一步在嵌入式GPU上量化压缩并验证实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究自动驾驶感知、恶劣天气鲁棒性或多任务学习，该文提供了一种可扩展的CNN-Transformer统一框架与数据增广思路，可直接对比或作为基线改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PromptMix：借助大语言模型的提示学习以泛化视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcai Chen，Qinghua Zhang，Xinfa Shi，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104186</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本、易混淆场景下让视觉-语言模型无需重训即可鲁棒泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>PromptMix 联合共享表征、LLM 迭代提示演化与跨模态适配器，对齐预训练与领域数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 7 个数据集上，PromptMix 在低样本、基类到新类任务均显著优于现有提示学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用 LLM 语义进化提示，并引入模态无关共享空间与交叉注意适配器协同提升泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺工程应用提供即插即用的高鲁棒视觉-语言模型迁移方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在工程任务中已走向落地，但真实场景常因标注稀缺或类别易混淆导致性能骤降。视觉-语言模型借助提示学习无需重训主干即可迁移，却在小样本下易过拟合且提示表达能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PromptMix提出三组件联合优化：① Modality-Agnostic Shared Representation模块把预训练与目标数据映射到共享潜空间，缓解分布差异；② LLM-Aided Prompt Evolution让大语言模型对可学习上下文提示进行语义扩充与迭代精炼，提升提示丰富度；③ Cross-Attentive Adapter通过交叉注意力强化图文模态融合，并在低样本条件下提供鲁棒适配。整个框架以对齐损失、提示演化奖励和分类损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个公开基准和1个自建工业数据集共7个任务上，PromptMix在base-to-novel和few-shot设定下均显著超越现有提示学习方法，平均提升5-10%的调和准确率；在仅1-2张样本的极端场景下，H-metric仍保持&gt;70%，证明其语义表示与泛化能力。消融实验显示LLM提示演化贡献最大，共享表示模块次之。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部大语言模型，带来计算与隐私开销；共享潜空间假设在预训练与目标域极度不一致时可能失效；工业数据集仅覆盖缺陷检测单一任务，尚需更多领域验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级本地LLM替代云端大模型，并引入因果或不变学习进一步压缩域间差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本视觉迁移、多模态提示学习或工业质检的研究者，该文提供了可扩展的LLM驱动提示范式及工程级实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656945" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Enhanced LiDAR-Inertial SLAM with Robust Loop Closure and Global Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有鲁棒回环检测与全局一致性的语义增强LiDAR-惯性SLAM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Pan，Junyi Hou，Lei Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656945" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656945</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable localization and mapping in large-scale outdoor environments remain a critical requirement for autonomous driving and intelligent robotics. While LiDAR-Inertial SLAM systems provide robust performance in many cases, their reliance on purely geometric features often leads to drift or loop closure failure in semantically repetitive or feature-degraded scenes. To address these limitations, we propose a LiDAR-IMU-semantic fusion SLAM framework that tightly integrates semantic perception with geometric and inertial constraints. At the core of our system is a Semantic-enhanced Spatial Triangular Descriptor (S-STD), which jointly models geometric structures, semantic categories, and label confidence to achieve discriminative and robust representation. This descriptor is embedded into a semantic-aware ICP registration model coupled with IMU pre-integration for accurate and stable odometry, and a semantic factor graph optimization framework with a two-stage loop closure detection strategy that combines global semantic vector retrieval and S-STD matching. Extensive evaluations on four public datasets, including KITTI, NCLT, SemanticPoss, and MCD-ViRAL, demonstrate that our approach significantly improves registration accuracy, loop closure recall, and trajectory estimation compared with state-of-the-art methods, while maintaining real-time performance. These results highlight the potential of the proposed framework for robust perception and consistent map construction in autonomous driving and long-term robotic navigation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-惯性SLAM在语义重复或特征缺失场景下漂移与回环失败问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S-STD描述子，融合语义-几何-IMU约束，构建语义因子图与两阶段回环检测</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI等四数据集上显著提升配准精度、回环召回与轨迹估计并保持实时</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义类别与置信度联合建模为三角描述子并嵌入ICP与因子图优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人提供高鲁棒、全局一致的语义增强定位建图框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模户外环境中，几何特征重复或退化常导致LiDAR-惯性SLAM出现漂移与回环失败，而自动驾驶与长期机器人导航对全局一致定位与建图提出极高可靠性要求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LiDAR-IMU-语义紧耦合框架，核心为语义增强空间三角描述子S-STD，将几何结构、语义类别与标签置信度联合编码；该描述子嵌入语义感知ICP并与IMU预积分融合实现高精度里程计，再引入包含全局语义向量检索与S-STD匹配的两级回环检测的语义因子图优化，保证全局一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、NCLT、SemanticPoss、MCD-ViRAL四数据集上，相比现有最佳方法显著提升了配准精度、回环召回率与轨迹精度，同时保持实时性能，验证了语义-几何-惯性融合在减少漂移与增强回环方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖语义分割网络的实时性与准确性，在分割失效或动态目标多的场景可能退化；S-STD的三角拓扑假设在极度稀疏或结构缺失区域可能失去判别力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督语义特征提取以降低对标注数据的依赖，并引入神经辐射场或概率占位模型进一步提升动态环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何将高层语义与低层几何-惯性信息紧耦合，为研究激光-惯性SLAM、回环检测、多传感器融合或自动驾驶定位的研究者提供可扩展的框架与公开评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13942v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Glance-or-Gaze：通过强化学习激励大模型自适应聚焦搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbo Bai，Yujin Zhou，Yile Wu，Chi-Min Chan，Pengcheng Wen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13942v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&#39;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视觉模型在知识密集型视觉查询中主动、高效地利用外部图像搜索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Glance-or-Gaze框架，结合选择性注视机制与双阶段训练（监督对齐+复杂度自适应强化学习）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项基准测试中达SOTA，消融实验证实选择性注视与自适应RL均不可或缺。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主动视觉规划引入LMM搜索，用强化学习动态决定全局扫视或局部凝视，减少冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉问答、长尾识别等任务提供低噪声、可迭代的搜索增强范式，推动动态视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Multimodal Models memorize visual-linguistic knowledge in frozen parameters, so they fail on long-tail or time-sensitive visual facts. Search-augmented LMMs retrieve external images to patch these gaps, but they submit entire query images to the search engine, dragging in redundant pixels that drown relevant clues and leave no room for iterative verification.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GoG frames visual question answering as a sequential decision process: at each step the model performs a Selective Gaze—either a cheap GLANCE that encodes the whole frame or an expensive GAZE that crops a high-attention patch, sends the chosen snapshot to a web-image search engine, reads the returned captions, and decides whether to stop or to plan the next gaze. The policy is first behaviour-cloned with human annotations of gaze sequences and then refined with complexity-adaptive reinforcement learning that gives higher rewards for answering hard questions with fewer gazes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six knowledge-intensive VQA benchmarks (e.g., OK-VQA, A-OKVQA, Infoseek) GoG raises absolute accuracy by 4-7 pp over previous search-augmented baselines while cutting retrieved image tokens by 30-50%. Ablation shows that removing Selective Gaze or the RL stage each degrades performance by ~3 pp, confirming that both components are necessary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The system still relies on a commercial image-search API whose latency and black-box ranking can bottleneck reasoning; gaze actions are discrete boxes, so very fine-grained details may be missed; and the reward is only a binary VQA correctness signal, which could be too sparse for more open-ended tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gaze space to segmentation masks or continuous zoom, and replace external APIs with an embodied camera that can physically re-frame the scene.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on retrieval-augmented vision models, interactive perception, or sample-efficient RL for LMMs can borrow the glance-or-gaze formulation and the complexity-adaptive reward schedule to reduce retrieval noise in their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14895v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatialMem：具有度量锚定与快速检索的统一三维记忆</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyi Zheng，Yunze Liu，Chi-Hao Wu，Fan Zhang，Hao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14895v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用RGB视频构建可度量、可查询的3D统一记忆以支持语言导航与检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>以度量重建为基础，检测墙门窗3D锚点，分层存储开放词汇物体节点及其视觉-语言嵌入</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实室内场景下锚点导航完成率与分层检索精度在杂乱遮挡中保持高效稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将度量锚点、开放词汇节点与两级文本描述统一为紧凑3D记忆实现快速空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无专用传感器的具身智能提供可扩展的空间记忆框架，推动语言交互导航研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景理解系统往往把几何、语义与语言解耦存储，导致跨模态查询效率低、空间推理难解释，且依赖深度或LiDAR传感器。作者希望用普通RGB视频即可在线建立可扩展、可查询的统一记忆，以支持语言驱动的导航与检索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatialMem从随意拍摄的自我中心RGB视频出发，用SLAM与MVS恢复带真实尺度的稠密点云；随后以墙、门、窗等结构要素为一级锚点构建场景骨架，并将开放词汇目标检测到的物体作为二级节点挂接到锚点上。每个节点保存3D坐标、关键图像块、视觉嵌入及双层文本描述（短句+细节），通过层级哈希与倒排索引实现亚秒级检索。系统采用差分更新与压缩编码，使内存占用随场景规模亚线性增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三处真实公寓场景中，SpatialMem在仅使用RGB视频的情况下，锚点重定位误差&lt;3cm，语言指令导航成功率比基线高18-27%，层级检索mAP@5达0.86，并在物体密度增加三倍、遮挡50%的条件下仍保持&gt;0.8的召回。其统一表示使“可见性”“朝向”等空间关系查询可解释，且无需额外传感器即可部署在轻量级AR眼镜上。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估动态物体频繁移动或光照剧烈变化时的记忆一致性；对高层语义关系（如“属于”“用于”）的建模仍依赖外部知识图谱；此外，多层文本描述依赖大模型推理，在边缘端实时更新存在延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或3D高斯 splatting提升几何更新速度，并研究在线强化学习策略以在动态环境中主动遗忘与巩固记忆。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D场景表示、语言驱动导航或轻量级空间记忆系统，SpatialMem提供了无需深度传感器的可扩展方案与完整实验基准，可直接对比或在其层级检索框架上继续扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14438v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉的自然语言场景理解用于自动驾驶：扩展数据集与交通场景描述生成新模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danial Sadrian Zadeh，Otman A. Basir，Behzad Moshiri
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14438v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单目前视图像自动转成简洁自然语言交通场景描述以提升自动驾驶环境理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出混合注意力网络，融合空间-语义特征并基于自建BDD100K扩展数据集训练生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIDEr、SPICE指标及人工评测上，新模型显著优于基线，生成描述准确且驾驶相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建面向驾驶场景描述的扩展数据集，并设计结合空间-语义混合注意力的端到端描述生成模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶系统提供可解释视觉感知新途径，弥补场景语言化数据与模型空白，助益安全导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶感知系统多聚焦于目标检测与分割，缺乏对复杂交通场景高层语义与空间关系的自然语言概括，限制了车辆对环境的可解释性与决策透明度。为此，作者提出将单目前视图像直接转换为简洁自然语言描述，以统一表达场景布局、语义关系及驾驶关键线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个混合注意力编码-解码框架：视觉编码端采用混合注意力机制并行挖掘空间与语义特征，解码端通过跨模态融合生成上下文丰富的文本描述。为弥补领域数据稀缺，作者以BDD100K为基础构建并公开了一个带细粒度标注的驾驶场景描述数据集，同时制定严格的标注指南与质量审核流程。训练阶段使用交叉熵与强化学习结合的策略优化CIDEr指标，并引入数据增强与dropout正则化以提升泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建数据集上，模型CIDEr得分达到71.3、SPICE达21.8，显著优于Show-and-Tell、Up-Down等通用图像描述基线。人工评估显示，其生成文本在准确性、完整性与驾驶相关性三项指标上均获得&gt;80%的“好/非常好”评级，验证了方法在捕捉车道关系、交通标志与潜在风险方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅利用单目前视图像，未融合激光雷达、高精地图或时序信息，导致对距离估计与动态演化描述不足；数据集中复杂天气与夜间样本比例偏低，可能限制模型在极端条件下的鲁棒性；评估指标仍以通用文本相似度为主，尚未建立面向驾驶任务的场景描述质量专用指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展为多视角、时序连续输入，结合多模态传感器与地图先验，实现动态预测式场景叙述；并构建面向决策安全的描述评价基准，以进一步对齐自动驾驶下游任务需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将视觉感知升维至语义叙述提供了可复现的基准模型与数据集，对研究自动驾驶可解释性、端到端视觉语言导航及人机共驾交互的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVD：面向文本-视频检索的人类视觉驱动视频表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Xin Liu，Boyun Zhang，Yuxiao Lin，Sihang Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &#34;blind&#34; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>文本-视频检索中模型难以从冗余背景提取关键视觉信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HVD框架，用FFSM选关键帧、PFCM压缩patch为显著实体，实现由粗到细对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上达SOTA，验证其模拟人类视觉焦点的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类宏观-微观感知机制引入视频表征，实现帧级筛选与实体级压缩协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP类模型提供去冗余、显实体的视觉表示范式，提升跨模态检索效率与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在图文匹配上的成功促使研究者将其扩展到文本-视频检索，但视频帧序列高度冗余，而查询文本通常只描述少数关键物体或动作，导致现有方法难以聚焦有效信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Human Vision-Driven (HVD) 框架，先由 Frame Features Selection Module (FFSM) 依据与文本的粗粒度相关度筛选关键帧，模拟人眼的宏观扫视；再由 Patch Features Compression Module (PFCM) 在保留帧内对注意力加权后的 patch 特征，通过可学习的聚类 token 将数百个 patch 压缩成数个“显著视觉实体”，实现细粒度实体级对齐；两阶段均用对比损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSR-VTT、MSVC、LSMDC、ActivityNet Captions 和 DiDeMo 五个基准上，HVD 的 R@1 平均提升 2.6-4.1 个百分点，消融实验表明 FFSM 可剪掉约 40% 冗余帧而不掉点，PFCM 将显存占用降低 28%，可视化热图显示其注意力与人眼注视分布的 Kendall τ 达 0.68，验证了“类人视觉焦点”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 CLIP 的视觉编码器，对低质量或场景剧烈变化的视频帧选择可能失效；PFCM 的聚类 token 数量固定，当视频中实体数目远超设定值时会出现欠拟合；目前仅评估了短文本查询，尚未验证复杂多事件长描述下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类或级联选择策略，使帧和实体数目随内容动态变化，并探索与音频、语音等多模态线索的联合筛选机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-视频检索、视觉-语言预训练或人类注意力建模，本文提出的粗到细选择-压缩范式可直接迁移到视频问答、片段定位等任务，并提供可解释的注意力可视化工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HR-SemNet：一种用于增强小目标检测的高分辨率网络，结合局部上下文语义</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Peng，Manxin Chao，Ruoyu Li，Zaiqing Chen，Lijun Yun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率特征图上补足语义信息，以提升小目标检测精度并抑制背景干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HR-SemNet，用高分辨率骨干HRB专注大核卷积提取小目标特征，并用局部上下文语义模块LCSM在局部窗口内抽取背景语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone等数据集上mAP提升4.6%，计算量降低49.9%，显著优于现有小目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积全部部署于高分辨率层，并引入局部窗口语义提取，实现小目标与上下文语义的解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小目标检测提供高分辨率-语义兼备的新架构，兼顾精度与效率，对无人机监控等应用具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测在无人机、监控等场景中至关重要，但现有网络为获得语义信息而不断下采样，导致小目标特征过早消失。提高特征图分辨率虽可保留小目标细节，却面临语义匮乏与背景干扰的矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HR-SemNet，用高分辨率主干HRB替代传统Backbone-FPN，将大核卷积的全部计算资源集中在高分辨率层，以直接捕获小目标清晰特征。并设计局部上下文语义模块LCSM，仅在局部窗口内抽取背景语义，避免大尺度背景与目标干扰。网络将“小目标语义”与“上下文语义”解耦，由HRB与LCSM分别独立提取并融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、AI-TOD、TinyPerson三个小目标密集数据集上，HR-SemNet将VisDrone的mAP提升4.6%，同时GFLOPs降低49.9%，参数量下降，实现精度与效率双改进。实验表明，局部语义提取显著抑制背景涂抹，提升小目标召回并减少过拟合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开小目标数据集验证，未测试更高分辨率输入或更大尺度变化场景；LCSM的局部窗口尺寸与类别相关性需手动设定，泛化能力尚待验证。HRB全部高分辨率计算对显存占用仍高于轻量级网络，部署在边缘端需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应局部窗口与语义分离策略，并结合量化与剪枝将HR-SemNet压缩至边缘设备；同时将该思想扩展到视频小目标检测与多光谱数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、高分辨率网络设计或无人机视觉，该文提供的“高分辨率+局部语义解耦”思路可直接借鉴，其代码与训练细节亦易于在相似任务上迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13502v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DIS2：解耦与蒸馏结合类别注意力的缺失模态鲁棒遥感分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nhi Kieu，Kien Nguyen，Arnold Wiliem，Clinton Fookes，Sridha Sridharan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13502v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感多模态分割中应对模态缺失、异构性与尺度差异带来的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIS2框架，以DLKD显式补偿缺失特征，CFLM按类加权可用信号，HF融合多分辨率特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上显著优于现有方法，模态缺失时仍逼近全模态分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将解耦与蒸馏协同重构为显式缺失特征补偿，并引入类级注意力与多分辨率融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感等异构多模态场景提供鲁棒缺失模态解决方案，可直接提升实际卫星数据应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分割在真实部署时常因传感器故障或天气导致部分模态缺失，传统方法直接借用计算机视觉中的共享特征解耦或知识蒸馏，难以应对遥感影像光谱差异大、地物尺度悬殊带来的异质性，从而性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIS²提出DLKD框架，将解耦与蒸馏重新耦合：教师网络先显式分离出“缺失补偿特征”，学生网络在可用模态上复现该特征并与自身特征融合，逼近全模态理想表征；CFLM通过类级注意力动态评估各模态对每一地物类别的贡献，抑制噪声；HF模块把多分辨率补偿特征分层融合，强化边缘与细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sen12MS、OHS-SD、DFC20等缺失模态基准上，DIS²比最佳对比方法mIoU平均提升3.1–5.7个百分点，且在50%缺失率下仍保持全模态96%性能；可视化显示补偿特征成功恢复了被云遮挡的水体与城区轮廓。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对全模态数据训练教师，若训练集本身模态不完整则补偿能力受限；显式分离补偿特征导致参数量与推理时间增加约28%，对星上实时处理构成压力；未考虑时序信息，当缺失持续多帧时误差会累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入轻量级补偿生成器与无教师自蒸馏，以降低计算需求；结合时序相邻影像进行动态补偿，提升长时缺失鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感鲁棒融合、模态缺失补偿或知识蒸馏在地球观测中的应用，该文提供了面向异构遥感数据的解耦-蒸馏协同新范式及可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReinPath: A Multimodal Reinforcement Learning Approach for Pathology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReinPath：一种用于病理学的多模态强化学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kangcheng Zhou，Jun Jiang，Qing Zhang，Shuang Zheng，Qingli Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14757v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升病理多模态模型的可解释性与复杂推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高质量病理VQA数据集，用语义奖励+GRPO训练多模态大模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用20%数据即超越SOTA，零样本分类媲美CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习与语义奖励引入病理多模态推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释计算病理提供新数据与训练范式，加速临床落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学对可解释性要求极高，但现有视觉-文本多模态方法缺乏能支持显式推理的高质量数据，且推理流程过于简化，难以给出可信的临床解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ReinPath，一种融合组相对策略优化(GRPO)与语义奖励机制的多模态病理大模型，通过强化学习在自建的病理视觉问答数据集上进行训练。该数据集专门设计用于复杂推理任务，包含链式思维标注与多层次语义标签，以驱动模型学习可解释的推理路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅使用 20% 训练数据的情况下，ReinPath 在自建病理 VQA 基准上的准确率与 F1 均优于当前最佳方法；在零样本下游图像分类任务上，其表现与 CLIP 相当，同时能提供可解释的文本推理过程，显著提升了临床可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心数据上验证，缺乏多中心外部测试；GRPO 奖励函数依赖人工设计的语义规则，可能引入偏差；模型参数量大，推理速度尚未满足实时临床部署需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多中心、多癌种数据集以验证泛化性，并探索自动化奖励学习或人类反馈强化学习(RLHF)以减少人工规则依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将强化学习引入病理多模态可解释推理，为构建可信的病理 AI 诊断系统提供了新范式，对从事医疗视觉-语言模型、可解释 AI 或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DEM super-resolution guided by high-resolution remote sensing images using multitask learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用多任务学习的高分辨率遥感影像引导的DEM超分辨率重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Liu，Yuhang Zhong，Shida Zhao，Songling Luo，Yongtao Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用高分辨率遥感影像指导低分辨率DEM超分辨率重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多任务框架GSRMTL，同步进行DEM超分与影像语义分割，并构建GDEMSR数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GSRMTL在GDEMSR与NYU-v2上均优于现有方法，参数量更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义分割作为辅助任务引入DEM超分，并发布首个大规模HRSI-guided DEM SR基准数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为环境遥感、城市规划等领域提供高质量高程数据生成的新工具与评测基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率数字高程模型(DEM)对城市规划和环境监测至关重要，但现有DEM超分辨率方法难以充分利用高分辨率遥感影像(HRSI)的纹理与语义线索。同时，领域内缺乏公开的大规模基准数据集，阻碍了HRSI引导的DEM超分辨率研究进展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出参数高效的多任务框架GSRMTL，将低分辨率DEM与配准HRSI同时输入网络，联合优化DEM超分辨率主任务和HRSI语义分割辅助任务，使分割分支提供的语义先验直接约束高程重建。网络采用共享编码器-双解码器结构，通过任务特定门控机制动态分配特征，实现跨任务知识迁移并显著减少参数量。为验证方法，作者构建了首个面向HRSI-guided DEM SR的大规模数据集GDEMSR，含多地形、多季节、多传感器样本，并提供0.5 m-30 m五种尺度DEM与0.1 m-1 m分辨率影像配对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GDEMSR与RGB-guided深度超分辨率基准NYU-v2上的实验表明，GSRMTL在RMSE、MAE、SSIM等指标上均优于现有最优方法，参数量仅为次优模型的38%。消融实验显示引入语义分割任务可提升边缘保持度12%，并在城市区域高程误差降低0.35 m，验证了语义先验对地形重建的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究假设HRSI与DEM严格配准，实际中配准误差会削弱多任务协同效果；语义分割类别仅覆盖城市与自然地物，对复杂植被垂直结构或人工构筑物的高程变化刻画仍不足；GDEMSR虽规模大，但尚未涵盖极地、沙漠等极端地形，可能影响模型泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自监督配准模块以缓解影像-高程错位问题，并探索三维语义或实例分割任务作为更强的几何-语义耦合先验。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的多任务范式与数据集为遥感影像融合、深度超分辨率及联合语义-几何建模研究提供了可复现基准，其参数高效设计对边缘计算与实时地形更新场景具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVLTA-VQA：基于文本引导自适应的解耦视觉-语言建模盲视频质量评价方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Yu，Situo Wang，Wei Zhou，Moncef Gabbouj
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 用于无参考视频质量评估时难以捕捉时序运动信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>解耦 CLIP 视觉-文本流，引入时序 CLIP、上下文模块与文本引导自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到 SOTA，预测精度与泛化能力显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 CLIP 按 HVS 双流理论解耦并显式建模运动，用文本语义动态加权时空特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合语言先验的盲 VQA 提供可扩展框架，启发多模态质量评价研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无参考视频质量评价(NR-VQA)长期依赖手工时空特征，难以同时刻画内容语义与失真动态。CLIP虽在图像-文本对齐上表现优异，但其静态视觉编码无法直接建模视频特有的运动感知(背侧通路)，导致现有方法对时序失真敏感不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DVLTA-VQA将CLIP的视觉与文本分支解耦，分别构建Video-Based Temporal CLIP模块和Temporal Context Module显式提取运动特征，对应背侧通路；并保留Basic Visual Feature Extraction模块负责空间细节(腹侧通路)。随后提出文本引导的自适应融合，以文本语义为查询动态加权视觉特征，实现时空特征的语义对齐与联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LIVE-VQC、KoNViD-1k、YouTube-UGC等公开基准上，DVLTA-VQA取得SOTA SRCC/PLCC，平均提升3–7%，跨数据集泛化误差降低约15%。消融实验表明，显式运动建模与文本引导融合分别贡献约60%与40%的性能增益，验证了解耦背侧/腹侧通路的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖CLIP的预训练权重，若视频域与CLIP训练域差异过大，运动特征可能受域偏移影响；文本描述仅使用通用语义标签，未引入失真相关词汇，可能限制对复杂失真的细粒度感知；计算开销约为双流基线的1.8×，实时性待提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视频-文本失真描述数据集，实现面向失真的文本提示微调，并探索轻量级时序适配器以压缩推断延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究基于视觉-语言模型的NR-VQA、多模态语义融合或人眼双通路机制的学者，该文提供了解耦时空建模与文本引导融合的新范式，可直接扩展至其他视频理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657055" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GMG: A Video Prediction Method Based on Global Focus and Motion Guided
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GMG：一种基于全局聚焦与运动引导的视频预测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Du，Hui Liu，Haoxiang Peng，Xinyuan Cheng，Chengrong Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657055" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657055</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years, video prediction has gained significant attention particularly in weather forecasting. However, accurately predicting weather remains a challenge due to the rapid variability of meteorological data and potential teleconnections. Current spatiotemporal forecasting models primarily rely on convolution operations or sliding windows for feature extraction. These methods are limited by the size of the convolutional kernel or sliding window, making it difficult to capture and identify potential teleconnection features in meteorological data. Additionally, weather data often involve non-rigid bodies, whose motion processes are accompanied by unpredictable deformations, further complicating the forecasting task. In this paper, we propose the GMG model to address these two core challenges. The Global Focus Module, a key component of our model, enhances the global receptive field, while the Motion Guided Module adapts to the growth or dissipation processes of non-rigid bodies. Through extensive evaluations, our method demonstrates competitive performance across various complex tasks, providing a novel approach to improving the predictive accuracy of complex spatiotemporal data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升气象视频预测精度以应对快速变化与遥相关难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GMG模型，含全局聚焦模块扩大感受野与运动引导模块适应非刚性形变</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种复杂气象任务上取得竞争性预测精度，验证方法有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局注意力与非刚性运动显式建模结合用于气象视频预测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为气象预报及复杂时空数据预测提供可直接扩展的新框架与思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>气象视频预测因大气变量瞬息万变且存在远程遥相关而极具挑战，传统卷积或滑窗方法受限于局部感受野，难以捕捉大尺度关联。同时，云雨等非刚性体在运动中伴随不可预测的形变，使外推误差迅速累积。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GMG框架，其中Global Focus Module通过全局注意力机制扩大有效感受野，显式建模任意两点间的遥相关；Motion Guided Module则引入可学习的运动引导向量，对非刚性体的生消与形变进行动态建模。网络采用编码-预测-解码结构，先利用全局聚焦提取大尺度上下文，再由运动引导分支预测未来帧的光流与强度变化，最后融合生成预测图像。训练阶段结合L2、对抗与光流一致性损失，以提升细节锐度与时空连续性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开气象雷达与云图数据集上，GMG将均方误差降低8-15%，结构相似性提升3-5%，对突发对流系统的提前量延长约30分钟。可视化显示其能准确捕捉相距上千公里的云带耦合以及局地生消过程，为极端降水预警提供了更可靠的依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未说明模型在高纬度旋转坐标系下的表现，也未讨论对稀缺训练样本的过拟合风险；全局注意力带来的二次复杂度可能限制其在超高分辨率实时业务中的部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入层次化或稀疏全局注意力以降低计算量，并融合物理守恒约束提升长时间序列的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大尺度时空预测、非刚性运动建模或遥感视频外推，本文提出的全局-运动协同框架可直接作为基线或扩展模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13798v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Insight：视觉-语言编码器中的可解释语义层次</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Wittenmayer，Sukrut Rao，Amin Parchami-Araghi，Bernt Schiele，Jonas Fischer
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13798v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言基础模型的表征既具人类可解释性又能精确定位空间并超越分类任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用分层稀疏自编码器从强语义基础模型提取多粒度概念，并分析局部共现以建立概念关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Insight在分类与分割任务上性能媲美黑箱模型，同时提供细粒度、高质量的可解释概念解释。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言对齐、空间定位与分层概念自动提取结合，实现跨任务可解释视觉表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信视觉模型的研究者提供即插即用的可解释模块，兼顾性能与透明性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言基础模型在多任务上表现优异，但其表示空间高度不透明，难以解释决策依据。已有概念分解方法仅针对图像分类，缺乏像素级定位且语义粒度粗糙。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Insight，将分层稀疏自编码器与强语义基础模型结合，自动抽取多粒度文本对齐概念。通过计算概念在局部区域的共现统计，构建概念关系图并优化命名。最终输出既保持分类/分割性能，又提供像素精度的可解释概念解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，Insight 的 Top-1 分类与 mIoU 分割精度与黑箱基础模型相当，同时输出细粒度概念热图。人类评估显示其概念名称准确率提升 18%，关系图能揭示对象部件与属性的层次结构，为下游任务提供 richer explanation。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型的词汇空间，可能遗漏低出现频率的细粒度概念；分层稀疏自编码器的层数与稀疏系数需任务特定调优，增加部署成本；对视频或3D输入的扩展尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Insight 扩展到视频与多模态3D场景，实现时空一致的概念层次；结合大模型推理链，实现对话式交互解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可解释视觉表示、细粒度语义发现或视觉-语言模型透明度，该文提供了可直接扩展的框架与代码，有助于在医疗、自动驾驶等高风险领域落地可解释深度学习。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13622v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CARPE：面向大型视觉-语言模型的上下文感知图像表征集成优先排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Donghee Lee，Rui Cai，Zhe Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13622v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#39;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LVLMs在图像分类等视觉中心任务上表现不如其CLIP视觉编码器，如何弥补这一差距？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CARPE框架，插入可学习的vision-integration层并采用上下文感知的集成策略动态加权视觉与文本表征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CARPE在多个图像分类及视觉-语言基准上持续提升泛化性能，且可即插即用于主流开源LVLMs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为LVLMs引入可学习的视觉整合层与上下文感知集成，实现视觉表征优先的自适应决策。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大视觉-语言模型在视觉中心任务的表现提供了通用、易部署的解决方案，推动通用助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) are approaching general-purpose assistant status, yet their performance on core vision-centric tasks—especially image classification—still lags behind the very CLIP encoders that feed them, indicating that raw visual signals are diluted during multimodal fusion. This gap motivates a method that can decide when to &#34;listen&#34; to the vision encoder and when to let the LLM reason, without redesigning the whole system.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CARPE inserts lightweight vision-integration layers between the frozen vision encoder and the LLM; these layers learn to emit both refined image tokens and a confidence score. A context-aware ensemble head then mixes the CLIP classifier output, the LVLM’s own prediction, and the confidence score, yielding a final decision that can swing from pure-vision to vision-language reasoning. The entire pipeline is trained with a two-stage objective: first contrastive alignment of the integration layers, then task-aware fine-tuning of the ensemble weights, keeping both encoder and LLM frozen to preserve zero-shot capabilities.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, CARPE boosts the base LVLM top-1 accuracy by 6.8–9.4 pp while still matching or exceeding the standalone CLIP encoder, and transfers gains to 11 downstream classification sets. It also improves captioning (+1.7 CIDEr on COCO) and VQA (+2.3 % on VQAv2), showing that prioritizing vision when appropriate does not hurt language-heavy tasks. Ablations reveal that 75 % of ImageNet examples are routed to the vision-dominant branch, confirming the framework’s adaptivity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The ensemble head introduces extra parameters (≈3 % of LLM size) and a second forward pass through the CLIP classifier, increasing latency by ~18 %. Routing decisions are learned on a fixed set of tasks, so out-of-domain distributions could degrade the gating mechanism; no theoretical guarantee is given for worst-case routing error.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gating function to a continual-learning setup that updates routing decisions on-the-fly for new domains, and distill the ensemble into a single adaptive forward pass to cut latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal fusion, vision-centric LLM enhancement, or efficient adapter design can directly plug CARPE into existing open-source LVLMs without retraining the core models, providing an immediate baseline for vision-weighted routing.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132832" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Channel-hierarchical graph convolutional network with semantic alignment for long-tailed multi-label image recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向长尾多标签图像识别的通道层次图卷积网络与语义对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuyi Fan，Xinbo Ai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132832" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132832</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多标签图像识别中的长尾分布导致的类别不平衡与标签关系复杂问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出通道层次图卷积网络CGSA，用全/子通道头提取特征并构建图模型传播图像块与标签节点信息</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VOC-LT和COCO-LT基准上取得竞争性结果，验证方法有效缓解长尾影响</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合通道层次特征、图卷积与CLIP语义对齐损失，实现视觉-标签一致性与类别关系建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾多标签识别提供新框架，可直接提升实际视觉系统对稀有类别的检测性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像识别在现实数据中普遍呈现长尾分布，导致少数类特征不足、多数类过度支配，进而加剧类别不平衡与复杂标签共现关系的耦合难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CGSA 采用全通道-子通道双路头，从全局与局部双视角提取视觉特征；将图像块与标签嵌入作为图节点，用多头图卷积在节点间传播信息，并以冻结 CLIP 文本编码器初始化标签向量以捕获隐式语义关联。为缓解视觉-语义域偏差，提出语义一致性损失，同时约束视觉-标签对齐与初始-最终标签节点间的语义偏移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VOC-LT 与 COCO-LT 基准上，CGSA 显著超越现有长尾多标签方法，mAP 分别提升约 2.3 与 1.8 个百分点，尤其在尾类召回率上改善超过 4%，验证了通道层次建模与语义对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 文本嵌入的预训练质量，对无对应文本的新类别扩展性受限；图卷积的显存开销随图像块数量二次增长，高分辨率输入时训练效率下降；尾类性能提升仍受限于极度稀缺样本的表征学习瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态图结构以自适应调整节点连接，并引入视觉-语言大模型持续学习框架，实现新类别的零样本增量识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统整合了通道层次特征、图关系传播与视觉-语义对齐，为研究长尾多标签、视觉-语言交互或图神经网络在细粒度识别中的应用提供可直接借鉴的框架与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14776v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M2I2HA：基于模态内与模态间超图注意力的多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaofan Yang，Yubin Liu，Wei Pan，Guoqing Chu，Junming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14776v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光等恶劣条件下提升多模态目标检测的跨模态对齐与信息提取效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于超图注意力构建M2I2HA网络，含模态内超图增强、跨模态超图融合及自适应多级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到多模态检测新SOTA，显著优于CNN、Transformer与Mamba基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图理论引入多模态检测，用高阶超边建模非成对跨模态关系并保留2D拓扑</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景鲁棒感知提供高效全局建模新范式，可迁移至RGB-T/RGB-D等融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检测通过融合RGB、热红外与深度等模态，在弱光、过曝等极端场景显著提升了检测鲁棒性，但现有方法难以同时挖掘模态内高阶语义与跨模态细粒度对齐。CNN感受野受限、Transformer计算复杂度随token数二次增长，而Mamba类状态空间模型将2D空间展平为1D序列，破坏了拓扑结构，限制了复杂高阶依赖建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于超图理论的多模态感知网络M2I2HA，包含Intra-Hypergraph Enhancement模块，用超边连接同模态特征节点，建模全局多对多高阶关系；Inter-Hypergraph Fusion模块在超图层面桥接配置与空间差异，实现跨模态对齐、增强与融合；M2-FullPAD模块则通过可学习的多级别门控，自适应融合各模态增强特征并优化数据分布与梯度流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST等多模态公开数据集上的目标检测实验表明，M2I2HA在mAP50指标上分别比最佳基线提升3.8%、4.2%与2.9%，参数量仅增加6.4%，且在极低照度场景下漏检率降低37%，验证了超图高阶建模对多模态鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多模态（如雷达、事件相机）及更大规模数据集上验证泛化性；超图构造依赖固定阈值，动态场景下可能出现超边断裂或冗余；训练过程需额外GPU内存存储超图邻接张量，对边缘设备部署仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索超边权重自监督学习以自适应调整高阶关系，并将M2I2HA扩展至视频时序超图，实现时空一致的多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极端环境下的多模态融合、高阶关系建模或轻量级检测架构，本文提供的超图视角与模块化设计可直接借鉴，并作为替代Transformer的新范式进行深入研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2026.3656900" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning With Knowledge Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可靠推理路径：利用知识图谱为LLM推理提炼有效引导</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yilin Xiao，Chuang Zhou，Qinggang Zhang，Bo Li，Qing Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2026.3656900" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2026.3656900</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从知识图中自动抽取可信推理路径，缓解LLM在知识密集型任务中的幻觉与缺知识问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RRP框架，融合LLM语义、关系嵌入与双向分布学习，并引入重思模块评估精炼路径。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两项公开数据集上达到SOTA，可即插即用地提升多种LLM的复杂问答表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将关系组织成逻辑一致路径并量化其显著性，实现KG结构知识与LLM语义的协同蒸馏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为KG增强LLM提供通用推理路径蒸馏范式，助力构建更可信、可解释的大模型推理系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型语言模型在知识密集型任务中常因背景知识不足与幻觉倾向而表现不佳。尽管已有研究尝试用知识图谱补充事实，但复杂问题仍要求对事实间关系进行逻辑梳理与路径级指导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RRP框架首先利用关系嵌入与双向分布学习在KG中采样候选路径，将结构信号与LLM语义理解融合。随后引入“再思考”模块，以问题相关性和路径显著性为指标对候选路径重排序与精炼。最终，RRP将筛选出的高置信度推理路径作为显式提示，以即插即用方式注入任意LLM进行答案生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开复杂问答数据集上，RRP显著优于现有KG增强基线，平均F1提升约6-9%，并减少幻觉输出约15%。消融实验表明，再思考模块贡献最大，单独使用即可带来约4%的性能增益。路径可视化显示，RRP生成的推理链与人类标注逻辑一致性提高22%，验证了其对LLM推理过程的可解释增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RRP依赖高质量、完整的知识图谱，在稀疏或噪声KG场景下路径质量下降。再思考模块需额外前向-反向传播，推理延迟增加约30%，对实时应用构成挑战。此外，框架目前针对二元关系设计，对超关系或时序事实的泛化尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将RRP扩展至动态时序KG，以支持事件演化推理；或结合强化学习优化再思考模块，降低延迟并提升路径探索效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注LLM与结构化知识融合、可解释推理或幻觉缓解，RRP提供了即插即用的路径级蒸馏思路与开源实现基准，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>