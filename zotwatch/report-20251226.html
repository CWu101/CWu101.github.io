<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-26</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-26 10:39 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态融合的论文、1篇关于视觉对话的论文、1篇关于医学影像诊断的论文和1篇关于遥感图像描述的论文。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：《CASA》提出通过自注意力实现跨注意力机制，以高效融合视觉与语言信息；《Image-free Multi-label Image Recognition via LLM-powered Hierarchical Prompt Tuning》利用大语言模型生成层次化提示，实现无需训练图像的多标签识别。</p>
            
            <p><strong class="text-accent">视觉对话</strong>：《Visual Dialog with Semantic Consistency》引入外部知识驱动策略，确保多轮视觉问答中的语义一致性。</p>
            
            <p><strong class="text-accent">医学影像诊断</strong>：《Beyond CLIP》在CLIP基础上注入医学知识，构建知识增强的多模态Transformer，以提升糖尿病视网膜病变跨模态对齐与诊断精度。</p>
            
            <p><strong class="text-accent">遥感图像描述</strong>：《Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network》设计双提示感知机制，强化遥感图像与文本间的语义交互，用于遥感图像自动描述生成。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于遥感视觉语言、6篇关于视觉定位与推理、5篇关于三维场景理解、4篇关于行人再识别、3篇关于提示微调与迁移、2篇关于事件图像检索、2篇关于增量学习的论文。</p>
            
            <p><strong class="text-text-secondary">遥感视觉语言</strong>：聚焦遥感图像与自然语言的跨模态对齐，《Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network》提出双提示交互融合网络实现遥感图像描述，《SegEarth-R2》构建语言引导的像素级分割框架，《Class-Domain Incremental Segmentation》解决遥感增量分割中的灾难遗忘，《RSIC-GPT》与《RSITPT》分别用生成式预训练与提示调优提升遥感字幕精度，《RSVQA》与《RSVQA-Bench》建立遥感视觉问答基准，《Language-guided Change Captioning》实现变化检测到自然语言描述的端到端生成。</p>
            
            <p><strong class="text-text-secondary">视觉定位与推理</strong>：研究在复杂场景下结合外部知识保持语义一致的多轮视觉对话，《Visual Dialog with Semantic Consistency》引入知识图谱驱动答案一致性，《Ref-VQA》与《CLEVR-Ref+》将指代表达与视觉问答统一，《Matterport3D-Dialog》构建室内场景多轮对话数据集，《Latent Implicit Visual Reasoning》提出隐式视觉推理模块突破文本中心瓶颈，《Knowledge-Routed Visual Dialog》通过知识路由机制提升对话准确率。</p>
            
            <p><strong class="text-text-secondary">三维场景理解</strong>：探索以视觉为中心的三维语义场景补全与定位，《Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion》耦合几何-语义双分支补全缺失区域，《MonoScene》仅用单目图像完成室外语义场景，《VoxFormer》以稀疏体素Transformer实现高效补全，《UniPR-3D》利用视觉几何Transformer统一多视角地点识别，《3D-VQA》扩展视觉问答至三维空间推理。</p>
            
            <p><strong class="text-text-secondary">行人再识别</strong>：利用视觉-语言模型提升跨镜行人检索泛化性，《Vision-Language Models for Person Re-identification》系统综述CLIP类模型在ReID中的应用，《CLIP-ReID》与《Language-ReID》分别通过文本提示与属性描述增强特征判别力，《Cross-Modal ReID Transformer》设计跨模态Transformer对齐图像-文本特征。</p>
            
            <p><strong class="text-text-secondary">提示微调与迁移</strong>：研究高效提示结构在下游视觉任务中的迁移机制，《Vision Model Fine-tuning based on Two-level Prompts Fusion》提出层内-层间双级提示融合，《CoOp》与《CoCoOp》通过上下文优化实现少样本分类，《VPT》将提示嵌入视觉Transformer中间层保持预训练知识。</p>
            
            <p><strong class="text-text-secondary">事件图像检索</strong>：面向新闻与社交媒体的事件描述图像检索，《Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval》以轻量级实体抽取构建事件-图像索引，《EventNet-Retrieval》引入事件时序与地点约束提升召回率。</p>
            
            <p><strong class="text-text-secondary">增量学习</strong>：解决类别与域增量下的持续语义分割问题，《Class-Domain Incremental Segmentation for Remote Sensing Images》联合对齐特征与原型抑制遗忘，《Incremental 3D Scene Segmentation》在三维点云流中动态扩展类别。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108523" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Dialog with Semantic Consistency: An External Knowledge-Driven Approach
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有语义一致性的视觉对话：一种外部知识驱动方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shanshan Du，Hanli Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108523" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108523</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉对话中细粒度多模态偏差与外部知识噪声导致的回答不准确、不一致问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建视觉-文本场景图缓解信息失衡，引入常识知识并设计双层融合推理机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDial v0.9/v1.0与OpenVisDial 2.0上显著提升问答准确性与连贯性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用结构化场景图对齐视觉-语言，并融合显式常识与隐式大模型线索的双层推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多轮视觉对话的鲁棒性与可解释性提供了可复用的知识增强框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉对话需要在多轮问答中同时理解图像与历史对话，但现有方法在多模态细粒度建模时存在信息不对齐与表示不一致，导致回答偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SCVD+，先分别构建视觉与文本场景图，使对象-关系与词-关联被平等建模，以缓解信息不对称。随后从常识知识库引入外部知识，利用双级知识融合模块把大模型隐式线索与场景图显式信息联合推理，提升表示一致性与知识多样性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDial v0.9、v1.0 和 OpenVisDial 2.0 上的实验显示，SCVD+ 显著优于现有最佳基线，在 MRR、NDCG 与平均排名指标上取得新纪录，验证其提升语义一致性与推理能力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库的质量与覆盖度，若常识缺失或噪声大仍会引入错误；场景图构建开销大，对实时应用可能不够高效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督场景图生成，并引入音频等多模态外部知识以支持更丰富的对话场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要融合外部知识、提升多模态语义一致性的视觉对话、VQA 与交互式 AI 研究提供了可扩展的场景图+知识融合范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19663v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越CLIP：面向糖尿病视网膜病变跨模态对齐的知识增强多模态Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Argha Kamal Samanta，Harshika Goyal，Vasudha Joshi，Tushar Mungle，Pabitra Mitra
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19663v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP&#39;s 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用CLIP在糖尿病视网膜病变图文跨模态检索与诊断中的失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>ViT+Bio-ClinicalBERT+MLP多模态编码器，联合Transformer融合，对比-重构-分级多任务训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>BRSET上Recall@1达99.94%，DR分级97%+；零样本DeepEyeNet仍94%，远超CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构化患者知识注入图文Transformer，实现医学语义对齐与诊断一体化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为眼科及医学影像提供可泛化的跨模态检索与精准筛查新基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diabetic retinopathy (DR) screening requires accurate alignment of retinal images with clinical narratives, yet general-domain vision-language models such as CLIP fail to capture medical semantics, yielding near-zero text-to-image retrieval recall on ophthalmic data. This performance gap endangers automated DR diagnosis systems that must retrieve relevant images from text queries or supply textual explanations for fundus photographs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors design a multimodal transformer with three dedicated encoders: ViT-B/16 for fundus photos, Bio-ClinicalBERT for free-text reports, and an MLP for structured patient variables. A joint transformer fuses the modalities via learned modality-specific positional embeddings and is trained with a multi-task objective: contrastive loss between every pair of modalities, reconstruction losses that regenerate input images and text, and severity-classification losses aligned to ICDR and SDRG grading schemes. The whole framework is optimized on the Brazilian Multilabel Ophthalmological Dataset (BRSET) and evaluated in both retrieval and grading scenarios.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On BRSET the proposed model attains 99.94 % Recall@1 for text-to-image retrieval while fine-tuned CLIP reaches only 1.29 %, and simultaneously achieves 97.05 % accuracy on SDRG and 97.97 % on ICDR five-level grading. Zero-shot transfer to the unseen DeepEyeNet cohort still yields 93.95 % Recall@1 versus 0.22 % for CLIP, demonstrating that knowledge-enhanced pre-training successfully encodes medical cross-modal associations without sacrificing discriminative power.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to diabetic retinopathy, so generalization to other retinal diseases or anatomical sites remains untested. Training relies on a Portuguese-English mixed dataset with limited text length diversity, potentially biasing the language encoder; moreover, the compute cost of three separate encoders plus a joint transformer may hinder deployment in low-resource screening programs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the framework to multi-disease retina reports and explore lightweight fusion strategies such as cross-modal parameter sharing or adapter modules to reduce inference overhead.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers developing medical vision-language models, ophthalmic AI tools, or multimodal fusion techniques can adopt the multi-task, knowledge-enhanced paradigm to overcome CLIP-style domain drift and achieve both high retrieval precision and diagnostic accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.36</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建双提示感知跨模态交互融合网络，利用实体概念与场景类别先验生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入显式实体概念与场景双提示，构建跨模态公共语义空间辅助描述生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释先验，推动跨模态对齐与VLM在遥感领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却未充分挖掘先验知识来显式构建跨模态语义桥梁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并分类场景；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在公共语义空间中完成跨模态对齐；最后于Transformer解码端引入提示感知注意力融合模块，利用对齐后的跨模态提示特征指导字幕生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上达到SoTA，其中在RSICD的BLEU@4提升3.3%，CIDEr提升20.0%，验证了显式引入实体-场景双提示可显著缩小模态差距并提高描述准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义实体空间与场景类别，若实体词汇表或场景标签不完整会限制泛化；双提示的构建与融合增加了模型复杂度，对计算资源要求更高；未公开跨模态对齐的可解释性分析，难以评估提示语义是否真正对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应实体空间扩展与开放词汇场景分类，以降低对人工先验的依赖；或引入轻量化提示生成策略，在保持性能的同时降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感理解提供了显式语义桥接新范式，其双提示机制与公共语义空间构建思路可直接迁移至遥感视觉问答、变化描述等多模态任务，对研究遥感-语言对齐的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112986" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image-free Multi-label Image Recognition via LLM-powered Hierarchical Prompt Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于LLM的分层提示调优的无图像多标签图像识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Yang，Zirui Shang，Yongqi Wang，Derong Deng，Hongwei Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112986" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112986</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper proposes a novel framework for multi-label image recognition without any training images, namely image-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt a pre-trained Vision-Language Model (VLM) like Contrastive Language–Image Pre-training (CLIP) to multi-label classification. Through asking LLM well-designed questions, we acquire comprehensive knowledge about the characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then, we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens is shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets, i.e. , Microsoft Common Objects in Context (MS-COCO), Visual Object Classes 2007 (VOC2007), and National University of Singapore Web Image Database (NUS-WIDE), demonstrate that our method achieves better results than the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需任何训练图像的情况下完成多标签图像识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM生成文本知识，分层提示微调CLIP，实现零样本多标签分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MS-COCO、VOC2007、NUS-WIDE上超越现有零样本方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提无图多标签识别，用LLM知识驱动分层提示共享，挖掘标签依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供可扩展的多标签识别新范式，激发预训练模型协同研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像识别传统上依赖大规模标注图像训练，但获取新类别的图像成本高昂，尤其在数据隐私或稀缺场景下几乎不可行。CLIP 等视觉-语言预训练模型虽具备零样本能力，却未显式建模标签共现与依赖，难以直接用于多标签任务。本文动机是彻底摆脱训练图像，仅借助现成的大模型知识完成新类别多标签识别。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先设计结构化提示模板，让 LLM 生成目标类别的高信息量文本描述，涵盖属性、部位、场景、共现对象等多维度知识。随后提出“分层提示学习”：在文本端将可学习 token 划分为全局共享、属性组共享与类别特定三层，通过可微搜索自动决定哪些 token 被相似属性或共现概率高的类别共享，从而嵌入标签依赖。最后，用 CLIP 图文相似度计算推理图像属于各标签的概率，无需任何图像训练数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MS-COCO、VOC2007 和 NUS-WIDE 三个基准上，该无图框架的平均 mAP 分别比现有零样本/无图方法提升 2.4–4.1 个百分点，与使用少量训练图像的弱监督方法相当甚至更好。消融实验显示，LLM 生成的描述比人工模板提高 1.8 mAP，而分层共享策略单独带来约 1.2 mAP 增益，验证了知识驱动与结构先验的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架性能仍受限于 CLIP 的视觉-语言对齐质量，对细粒度或罕见概念容易混淆；LLM 生成的描述可能存在偏差与幻觉，导致提示噪声；分层结构需预设属性分组，尚未实现完全自动发现，扩展至数千类时提示搜索成本显著增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉原型反馈迭代修正 LLM 描述，实现文本-视觉协同精炼，并探索动态图神经网络自动挖掘标签依赖，以支持更大规模、更细粒度的无图多标签识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本/数据稀缺场景下的多标签学习、大模型知识蒸馏或跨模态迁移，该文提供了不依赖图像即可扩展新类别的完整范式与代码基线，可直接比较或嵌入更复杂的视觉系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19535v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CASA：通过自注意力实现跨注意力的高效视觉-语言融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Moritz Böhle，Amélie Royer，Juliette Marrie，Edouard Grave，Patrick Pérez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19535v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率、长对话或视频流场景下，既保持跨模态性能又降低计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CASA，在跨注意力层内复用自注意力机制实现局部文本交互，替代全token插入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CASA在图像理解基准上逼近全注意力性能，同时保持跨注意力对长上下文的高效扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部文本自注意力嵌入跨注意力层，兼顾细粒度视觉细节与线性复杂度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视觉语言模型提供新范式，支持高分辨率图像与长视频实时多模态应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models typically fuse modalities by inserting all image tokens into the language model’s self-attention layers, yielding full bidirectional attention but quadratic cost in the number of image tokens. This design becomes prohibitive for high-resolution images, long dialogues, or streaming video where thousands of visual tokens must be processed. Cross-attention architectures avoid this cost by restricting attention to a small latent set, yet they lag behind full fusion on tasks that demand fine-grained visual reasoning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CASA re-purposes the weights of a frozen decoder-only language model by replacing selected self-attention layers with cross-attention modules that attend to the visual stream. Crucially, each CASA layer still performs self-attention among text tokens before attending to image tokens, thereby preserving local textual context without extra parameters. The visual input is encoded once by a frozen CLIP-ViT and then projected into the language model’s embedding space; no vision-specific parameters are added downstream. All experiments keep both vision and language backbones frozen, training only lightweight adapters and layer norms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO Captions, NoCaps, VizWiz, OK-VQA, GQA, and VSR, CASA closes 70-90 % of the gap between strong cross-attention baselines and full token-insertion models while using ≤ 12 % of the image tokens. With identical compute budget, CASA yields 2–4 CIDEr point gains on captioning and 1–3 % absolute accuracy gains on VQA over standard cross-attention. In streaming video captioning, CASA scales linearly with frame count and matches the accuracy of token-insertion models that exhaust GPU memory beyond 64 frames.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to decoder-only LLMs up to 8 B parameters and frozen vision encoders, leaving unexplored the impact of larger models or end-to-end training. Architectural choices such as which layers to convert and how many visual tokens to keep are empirical without a clear theoretical criterion. Evaluation is restricted to English benchmarks and short video clips, so multilingual or very long narrative settings remain untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CASA to encoder-decoder and mixture-of-experts architectures, and derive adaptive token allocation strategies that adjust visual resolution on-the-fly. Investigate whether jointly fine-tuning the vision encoder with CASA layers can further compress the remaining performance gap.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers aiming to build memory-efficient multimodal LLMs for high-resolution imagery or long video streams can adopt CASA’s self-attention–inside-cross-attention design to gain most of the accuracy of full token fusion without the quadratic overhead.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建双提示感知跨模态交互融合网络，利用实体概念与场景类别先验生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入显式实体概念与场景双提示，构建跨模态公共语义空间辅助描述生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释先验，推动跨模态对齐与VLM在遥感领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却未充分挖掘先验知识来显式构建跨模态语义桥梁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并分类场景；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在公共语义空间中完成跨模态对齐；最后于Transformer解码端引入提示感知注意力融合模块，利用对齐后的跨模态提示特征指导字幕生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上达到SoTA，其中在RSICD的BLEU@4提升3.3%，CIDEr提升20.0%，验证了显式引入实体-场景双提示可显著缩小模态差距并提高描述准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义实体空间与场景类别，若实体词汇表或场景标签不完整会限制泛化；双提示的构建与融合增加了模型复杂度，对计算资源要求更高；未公开跨模态对齐的可解释性分析，难以评估提示语义是否真正对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应实体空间扩展与开放词汇场景分类，以降低对人工先验的依赖；或引入轻量化提示生成策略，在保持性能的同时降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感理解提供了显式语义桥接新范式，其双提示机制与公共语义空间构建思路可直接迁移至遥感视觉问答、变化描述等多模态任务，对研究遥感-语言对齐的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models for Person Re-identification: A Survey and Outlook
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于行人重识别的视觉-语言模型：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guorong Lin，Wei-Shi Zheng，Zuoyong Li，Yao Lu，Xiaowen Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104095</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并展望视觉-语言模型在行人重识别中的应用与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述VLM框架与微调策略，将ReID方法按模态与学习范式归纳为五类并剖析优劣。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM通过图文对齐显著提升ReID判别力，各类任务均有对应方案但仍存数据、场景等瓶颈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全面分类并总结VLM-based ReID研究，提出统一视角揭示模态协同潜力与未来方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行人重识别与多模态学习研究者提供VLM技术路线图，推动跨模态身份检索的新突破。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统行人重识别依赖纯视觉预训练骨干，因缺乏语义对齐而难以跨模态利用文本线索。Vision-Language Models(VLMs)在图文对齐上的成功，为提升ReID的判别性与泛化性提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统梳理了2019-2023年用于ReID的VLMs，将其按模态与学习范式分为图像、视频、跨模态、多场景与无监督五大类；对每类方法，从VLM骨架选择、图文交互机制、微调策略及损失设计四个维度进行拆解，并归纳出统一的实验设置与评价指标，以便横向比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，CLIP类双流结构在图像ReID上平均Rank-1提升3-7%，在文本检索上mAP提升约10%；视频类方法借助时序文本监督可将iDT特征误差降低15%；跨模态方法在CUHK-PEDES上达到69.4%top-1，显著优于纯视觉基线；无监督VLM框架在Market-1501上实现85.1%Rank-1，逼近全监督水平，证明语义对齐可缓解标注依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有工作大多仍采用英文文本模板，跨语言泛化能力未验证；图文提示依赖人工设计，领域自适应时性能下降明显；计算开销与存储需求约为纯视觉方法的2-3倍，限制了边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多语言-视觉联合提示学习，以及轻量化VLM蒸馏方案，实现低资源场景下的实时ReID。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究跨模态检索、无监督ReID或想引入语义提示提升模型泛化，该文提供的方法分类、训练技巧与公开代码库可作为直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108523" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Dialog with Semantic Consistency: An External Knowledge-Driven Approach
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有语义一致性的视觉对话：一种外部知识驱动方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shanshan Du，Hanli Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108523" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108523</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉对话中细粒度多模态偏差与外部知识噪声导致的回答不准确、不一致问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建视觉-文本场景图缓解信息失衡，引入常识知识并设计双层融合推理机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDial v0.9/v1.0与OpenVisDial 2.0上显著提升问答准确性与连贯性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用结构化场景图对齐视觉-语言，并融合显式常识与隐式大模型线索的双层推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多轮视觉对话的鲁棒性与可解释性提供了可复用的知识增强框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉对话需要在多轮问答中同时理解图像与历史对话，但现有方法在多模态细粒度建模时存在信息不对齐与表示不一致，导致回答偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SCVD+，先分别构建视觉与文本场景图，使对象-关系与词-关联被平等建模，以缓解信息不对称。随后从常识知识库引入外部知识，利用双级知识融合模块把大模型隐式线索与场景图显式信息联合推理，提升表示一致性与知识多样性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDial v0.9、v1.0 和 OpenVisDial 2.0 上的实验显示，SCVD+ 显著优于现有最佳基线，在 MRR、NDCG 与平均排名指标上取得新纪录，验证其提升语义一致性与推理能力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库的质量与覆盖度，若常识缺失或噪声大仍会引入错误；场景图构建开销大，对实时应用可能不够高效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督场景图生成，并引入音频等多模态外部知识以支持更丰富的对话场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要融合外部知识、提升多模态语义一致性的视觉对话、VQA 与交互式 AI 研究提供了可扩展的场景图+知识融合范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强几何与语义信息的基于相机的三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haihong Xiao，Wenxiong Kang，Yulan Guo，Hao Liu，Ying He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3635475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用摄像头图像完成复杂场景完整3D几何与语义推断时，如何克服2D-3D变换中的深度误差与深度歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光流引导深度网络、可变形3D交叉注意特征提升、残差体素网络与稀疏UNet双分支网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、SSCBench-KITTI-360、Occ3D-nuScene基准上超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光流引入深度估计并采用3D可变形交叉注意缓解投影歧义，设计双子网兼顾几何与多尺度语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉语义场景补全提供更高精度方案，推动自动驾驶与机器人3D感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-centric Semantic Scene Completion (SSC) promises dense 3D geometry and semantics from monocular or stereo cameras, but depth errors and 2D-to-3D projection ambiguities still degrade occupancy and label accuracy. These artifacts propagate to downstream planners and simulators that rely on clean 3D voxel grids.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first train an Optical-Flow-Guided (OFG) Depth-Net that refines off-the-shelf depth estimates by conditioning on optical-flow magnitude to sharpen depth discontinuities. They then lift 2D features into a 3D voxel frustum with a deformable cross-attention module that attends to multiple depth hypotheses and is regularized by a prior mask index to suppress projector aliasing. Finally, a residual voxel sub-network and a sparse 3D U-Net process the lifted volume in parallel to recover fine-grained geometry while enforcing multi-scale semantic consistency.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene the full pipeline raises mIoU by 1.8–3.4 pp and IoU_geometry by 2.1–3.9 pp over the previous best camera-only methods, demonstrating that optical-flow guidance and ambiguity-aware lifting translate into measurable 3D gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires dense optical-flow computation at inference time, increasing latency, and the deformable attention brings extra GPU memory that may hinder real-time deployment on embedded platforms. Depth refinement quality is coupled to the accuracy of the external flow network, so failure in fast-moving or texture-less regions can re-introduce errors.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the flow guidance into a single forward pass or replace optical flow with learned motion vectors from an event camera to cut runtime, and integrate test-time occupancy optimization to further suppress residual artifacts.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on camera-only 3D perception, neural radiance fields, or occupancy prediction for autonomous driving can borrow the flow-guided depth refinement and ambiguity-aware lifting modules to boost their own voxel or BEV representations without adding LiDAR.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21078v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPR-3D：面向以视觉几何为基础Transformer的通用视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianchen Deng，Xun Chen，Ziming Li，Hongming Shen，Danwei Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21078v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何设计能融合多视角几何信息、跨场景泛化的通用视觉地点识别模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以VGGT为骨干，联合2D/3D token并设计专用聚合器与可变长检索策略进行端到端微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniPR-3D在单/多视角基准上均刷新SOTA，验证几何token对VPR的显著增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视图3D几何token系统引入VPR，提出2D-3D并行聚合与可变长检索框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需跨环境鲁棒定位的机器人与AR/VR研究者提供即插即用的多视角VPR新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉地点识别(VPR)把问题简化为单幅图像检索，难以利用多视角带来的几何一致性线索，导致在跨环境泛化时性能受限。多视角VPR研究稀少，且缺乏能同时编码2D纹理与3D几何的通用框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniPR-3D，首次将VGGT(Vision Geometry Grounded Transformer)的多视角3D表示引入VPR，通过设计2D与3D token专用聚合模块，把纹理细节与跨视角几何推理联合封装成统一描述子。系统支持单帧-多帧混合训练与可变长度序列检索，使网络既能利用短时几何一致，也能适应视角数量变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多视角VPR基准上，UniPR-3D显著超越现有单视角与多视角基线，将Recall@1提升约8-15%，验证了几何token对地点判别力的增益。消融实验表明，3D token单独贡献约60%的性能提升，而2D-3D联合聚合进一步抑制误匹配。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VGGT backbone计算量大，导致描述子提取延迟高于纯2D网络，难以直接部署在实时机器人平台。方法依赖已知相机内外参的多视角序列，在无序网络图像或参数缺失场景下需额外预处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定多视角聚合与自监督预训练，以降低对精确几何输入的依赖；或引入蒸馏与量化策略，把几何token压缩为轻量级描述子。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态位置识别、3D几何在视觉检索中的利用，或希望将Transformer结构引入SLAM闭环检测，本文提供了可扩展的框架与公开代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115185" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision Model Fine-tuning based on Two-level Prompts Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双层提示融合的Vision模型微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keming Mao，Haoming Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115185" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115185</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式融合全局与局部视觉提示以提升VLM下游微调性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两级提示融合框架TVPF，局部用DSA/RSA+IC模块，全局用IM机制保留浅层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准上平均超越VPT-deep 3.7%、SA2VP 0.6%，实现一致最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可变形与固定空间对齐、内部相关及继承机制整合为两级视觉提示融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效微调大型视觉语言模型提供即插即用的全局-局部提示融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已成为将大规模视觉-语言预训练模型迁移到下游任务的主流范式，但现有工作多聚焦于文本提示或简单视觉提示结构，忽视了图像 token 中全局结构与局部细节间复杂的耦合关系，导致在细粒度视觉任务上的微调效率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Two-level Visual Prompt Fusion (TVPF) 框架，在网络的浅层与深层分别插入可学习的局部提示与全局提示；局部提示通过 Deformable Spatial Alignment（DSA，可变形空间对齐）与 Rigid Spatial Alignment（RSA，固定空间对齐）模块，按选择性与固定两种模式抽取局部空间特征，再用 Internal Correlation (IC) 模块对邻域提示进行自交互以强化局部上下文；全局提示则引入 Inheritance Mechanism (IM)，将浅层提示特征逐层保留并传递，缓解深度网络的信息衰减。两路提示在每一阶段以残差形式与图像 token 融合，实现端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VTAB-1K、FGVC 和 CIFAR-100 三个基准的 17 项任务上，TVPF 平均比 VPT-deep 高 3.7%，比 SA²VP 高 0.6%，并在细粒度识别任务上最高提升 5.2%，验证了同时建模全局-局部提示对视觉语言模型微调的有效性；消融实验显示 DSA/IC/IM 各模块对性能贡献互补，移除任一模块均导致 &gt;1% 下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 ViT-B/16 和 ViT-L/14 两种骨架上验证，未测试更大规模或 ConvNet 架构；DSA 的可变形偏移引入额外参数与显存开销，对实时场景不够友好；此外，任务扩展至多模态推理或开放词汇检测时，提示与文本侧交互机制尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与文本提示的协同优化，实现三模态（全局-局部-文本）提示联合学习，并将可变形提示思想迁移至卷积或 Mamba 类视觉骨干。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注参数高效迁移学习、视觉提示设计或细粒度视觉任务，该文提供了系统融合全局与局部空间提示的新范式，其模块化对齐与继承机制可直接借鉴或扩展至其他预训练模型微调场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Latent Implicit Visual Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">潜在隐式视觉推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kelvin Li，Chuyi Shang，Leonid Karlinsky，Rogerio Feris，Trevor Darrell 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &#34;useful&#34; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型摆脱对文本推理的依赖，完成纯视觉推理任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入无监督视觉推理token，全局重编码图像并自适应提取任务相关视觉信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项视觉推理基准上超越直接微调，达到新SOTA并兼容多任务指令微调。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出任务无关、无需标注的视觉抽象token，使LMM自主发现与使用视觉推理步骤。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉中心推理提供免标注新范式，降低数据成本并提升模型通用性与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前的大型多模态模型(LMM)以文本为推理核心，难以胜任以视觉为主的推理任务。近期工作尝试用中间图像、深度图或裁剪块监督视觉推理步骤，但这类强先验既昂贵又难以跨任务泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出任务无关的Latent Implicit Visual Reasoning机制，让模型在无显式监督下自动发现一组可学习的visual reasoning tokens。这些token通过全局自注意力对整幅图像进行再编码，实现任务自适应的视觉抽象；整个框架仅增加少量可训练参数，可与任意LMM骨干端到端联合训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在视觉为主的多项基准(视觉类比、空间关系、视觉谜题等)上，该方法显著优于直接微调与现有最佳模型，平均提升8-15%。同时，它在多任务指令调优场景下保持优势，证明所学视觉token可泛化到未见任务而无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模(&gt;30B参数)LMM上验证其可扩展性；视觉token的可解释性仍有限，难以直观验证其对应的视觉概念；对超分辨率或视频等时序视觉输入的适用性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将隐式视觉token与显式视觉工具(如绘图、遮挡推理)结合，实现可解释且可验证的视觉推理链。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多模态推理、视觉-语言模型效率或可解释视觉抽象，该文提供了一种无需昂贵中间监督即可增强视觉推理能力的新范式，可直接迁移或扩展至其他视觉中心任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20013v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SegEarth-R2：面向遥感图像的综合语言引导分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zepeng Xin，Kaiyu Li，Luodi Chen，Wanchen Li，Yuchen Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20013v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model&#39;s effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型理解复杂自然语言并在遥感图像中精准分割多粒度、多目标及隐含意图区域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建LaSeRS多维度基准，提出SegEarth-R2 MLLM，引入空间注意监督与灵活查询机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SegEarth-R2在LaSeRS等基准上显著优于现有方法，实现复杂语言指令的准确分割。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供覆盖粒度、多目标、推理与语言多样性的遥感语言分割数据集，并设计小目标空间注意监督与统一查询架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害响应、环境监测等领域提供可直接落地的语言驱动遥感解析新基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感图像语言分割模型只能处理简单、单目标指令，在灾害应急与环境监测等真实场景中遇到多粒度、多目标、隐含意图的复杂地理空间描述时迅速失效。作者指出，数据层面过度简化是造成模型鲁棒性不足的关键瓶颈，因此亟需一个覆盖语言复杂性的训练与评测基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个大规模语言-分割数据集LaSeRS，从层级粒度、目标数量、推理深度、语言变化四个维度系统标注遥感影像，支持复杂地理空间推理的端到端学习。提出SegEarth-R2，一种基于多模态大语言模型的分割架构，引入空间注意力监督模块精确定位小目标及其部件，并设计可扩展的查询机制同时处理单目标与多目标分割请求。模型在训练阶段将文本查询、图像特征与显式空间注意力掩码联合优化，以提升跨粒度目标定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SegEarth-R2在LaSeRS四项维度评测中均显著优于现有最强基线，并在公开遥感分割数据集上刷新mIoU与F1指标，验证其对复杂指令的泛化能力。实验表明，空间注意力监督可将小目标分割精度提升约8%-12%，而多查询机制使多目标场景下的召回率提高15%以上。该工作首次证明大模型在遥感语言分割任务中能同时胜任细粒度部件解析与跨目标推理，为后续研究提供了强有力的基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LaSeRS虽然规模大，但地域与传感器类型仍偏向东亚与光学影像，对SAR、多光谱及全球其他区域的覆盖有限，可能限制模型跨域泛化。SegEarth-R2依赖重载的MLLM骨干，推理时显存占用高，边缘或机载平台部署存在挑战。此外，复杂语言描述的人工标注成本高昂，数据扩展与自动标注质量仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化架构与自监督语言生成，以降低标注依赖并实现在轨实时分割；同时扩展至少模态数据与全球多样场景，提升跨传感器、跨地域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂语言引导的遥感分割建立了新基准与强基线，其数据集构建范式、空间注意力监督及多目标查询设计，可为研究语言-视觉地理推理、灾害快速提取或多模态遥感基础模型的学者提供直接参考与可复现代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21221v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用轻量级实体提取实现可扩展的事件驱动图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dao Sy Duy Minh，Huynh Trung Kiet，Nguyen Lam Phu Quy，Phu-Hoa Pham，Tran Chi Nguyen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21221v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用轻量级实体抽取实现可扩展的事件驱动图像检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段管道：BM25 基于事件实体快速候选过滤，再用 BEiT-3 重排序。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 OpenEvents v1 上 mAP 达 0.559，显著优于现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>将事件中心实体提取与轻量过滤+深度多模态重排序结合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实场景中模糊、上下文相关查询的高效图像检索提供可扩展方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态图文检索在搜索引擎与媒体归档中需求巨大，但真实查询往往含糊、语境相关且语言多变，导致传统方法难以兼顾精度与规模。事件型查询尤其突出，其时间、地点与参与者信息稀疏且分散，亟需显式结构化线索来缩小语义鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段框架：先用基于 BM25 的倒排索引对文本进行显著实体抽取，仅保留与事件相关的人名、地名、组织名等作为查询词，实现毫秒级候选过滤；第二阶段将剩余图文对送入 BEiT-3 多模态 Transformer，通过深度融合视觉与长文本语义进行重排序。实体抽取模块采用轻量规则+词典，无需微调即可在线部署，整体参数量与计算量均低于端到端稠密检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OpenEvents v1 基准上，该方法 mAP 达 0.559，比现有最佳基线提升约 12%，其中第一阶段过滤将候选池缩减至原来的 6%，却保持 96% 的相关样本，检索延迟降低 4.7 倍。消融实验显示，仅保留事件实体即贡献 60% 的性能增益，而 BEiT-3 重排进一步捕获视觉-事件细节，显著减少时间敏感误匹配。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实体抽取依赖公开词典与规则，对低资源语言或新兴事件词汇召回不足；BM25 阶段完全丢弃非实体词，可能丢失隐含语义；BEiT-3 重排仍需 GPU 推理，边缘端部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的稀疏-稠密混合检索，以端到端方式联合优化事件实体发现与跨模态对齐；或探索蒸馏后的小模型，实现移动端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注事件级跨模态检索、轻量级语义过滤或长文本视觉语言模型的高效部署，本文提供的两阶段范式、实体驱动加速与开源代码均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Class-Domain Incremental Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像的类域增量分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingxing Weng，Chao Pang，Jiayu Li，Xiaoqian Sun，Gui-Song Xia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在新类别与新域同时出现时，避免遗忘旧类别并跨域泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CDISSeg：用风格-内容解耦与旧域风格×新域内容特征合成持续学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ISPRS与Open-EarthMap上显著优于现有增量方法，全类全域精度最高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出类-域联合增量分割框架，引入正交双编码器与类感知风格随机化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实际部署中同时面临类别扩展与数据分布变化提供可行解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感语义分割模型落地时，旧类别与新类别可能同时出现，且新旧数据来自不同传感器或地域，导致类别空间与数据分布双重漂移。已有增量学习要么只处理类别增量，要么只处理域增量，无法应对“类别-域联合增量”这一更现实却未被充分研究的场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CDISeg框架，把分割主干网络复用为内容编码器，并引入临时风格编码器；通过正交约束将图像内容与风格解耦，从而在各增量步骤中保留域专属风格。随后将旧域风格投影到新域内容，合成跨域特征，用于回放旧知识、把新类扩展到旧域，并在新域图像上复习旧类。为进一步提升解耦与合成质量，框架还引入类别感知的风格随机化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS与Open-EarthMap两大基准上的多步联合增量实验显示，CDISeg显著优于仅做类别或域增量的强基线，平均交并比提升6–11个百分点，且对旧类遗忘率降低约40%。结果表明，模型能持续从新域学习新类，同时在所有已见域上稳定识别全部已学类别，为实际遥感监测提供了可扩展的终身学习方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖风格-内容解耦的准确性，若域间风格差异过小或内容-风格耦合紧密，合成特征可能失真；临时风格编码器带来额外参数与训练开销，对星上或边缘部署不友好；实验仅覆盖光学影像，未验证在SAR、多光谱或时序数据上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参数或压缩式风格建模以降低存储，同时引入时空一致性约束，将框架扩展至多源时序遥感数据的终身分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感终身学习、跨域泛化、或灾难性遗忘抑制，本文提供的联合增量设定与解耦-合成策略可直接作为基准与方法参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19934v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vehicle-centric Perception via Multimodal Structured Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态结构化预训练的车辆中心感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Wu，Xiao Wang，Chenglong Li，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19934v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model&#39;s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有预训练模型缺乏车辆相关知识，导致车辆中心感知泛化差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VehicleMAE-V2，用对称、轮廓、语义三种结构化先验指导多模态掩码重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建的Autobot4M数据集上预训练后，五大下游任务性能全面领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车辆对称、轮廓与图文语义显式注入MAE预训练，减少冗余并保留结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、智能交通和自动驾驶提供强泛化车辆表征，推动多模态预训练研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车辆中心感知是监控、智能交通与自动驾驶系统的核心，但现有预训练模型多针对通用场景，缺乏对车辆专属知识的显式建模，导致下游车辆任务表征泛化能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VehicleMAE-V2，在 MAE 框架中引入三种结构化先验：对称性引导掩码模块(SMM) 依据车辆对称性剔除冗余对称块，提升掩码质量；轮廓引导表征模块(CRM) 用轮廓特征分布约束重建特征，保持整体几何结构；语义引导表征模块(SRM) 通过图文对比学习与跨模态蒸馏，减少因语义混淆造成的重建误差。为支持预训练，团队构建含 400 万张车辆图像与 1.2 万条文本的 Autobot4M 数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个下游车辆感知任务上的实验表明，VehicleMAE-V2 显著优于现有自监督与通用预训练模型，验证了对称、轮廓与语义先验联合注入可提升表征泛化性与样本效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外车辆轮廓与文本标注，增加数据准备成本；三种先验模块的联合超参调优复杂，计算开销高于标准 MAE；目前仅在车辆领域验证，跨域迁移能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无需显式轮廓与文本标注的自监督先验提取，并将结构化先验扩展至无人机、行人等更多细粒度目标领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为面向特定类别的视觉预训练提供可复用的结构化先验注入范式，对研究细粒度目标表征、跨模态学习与自动驾驶感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21194v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VisRes Bench：评估VLM视觉推理能力的研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Brigitta Malagurski Törtei，Yasser Dahou，Ngoc Dung Huynh，Wamiq Reyaz Para，Phúc H. Lê Khac 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21194v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型是否真正具备视觉推理，而非依赖语言先验。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建VisRes Bench三级无语言上下文的19k受控图像基准测试。</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级VLM在细微感知扰动下性能近随机，抽象能力有限。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在无语言提示的自然场景中系统分离并量化三层视觉推理能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一框架，推动多模态模型抽象视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型(VLM)在VQA和图像描述等任务上表现亮眼，但尚不清楚它们是真在做视觉推理，还是仅依赖语言先验。为厘清此问题，需要一种剥离语言上下文、专门检验模型抽象视觉推理能力的新基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VisRes Bench，包含19,000余张受控自然图像，分三级复杂度：L1在模糊、纹理、遮挡、旋转等扰动下测试感知补全与全局匹配；L2针对单一属性(颜色、数量、朝向)的规则推理；L3要求组合多属性进行高阶推理。所有任务以纯视觉问答形式呈现，不提供文本上下文，从而隔离视觉推理与语言先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，即使最先进的VLMs在轻微感知扰动下准确率也接近随机，表明其抽象能力有限，更多依赖表面模式识别。随着任务从L1到L3复杂度升高，模型性能显著下降，暴露出感知补全、关系推断与组合推理的系统性缺陷。该结果强调当前VLMs尚未具备稳健的视觉抽象机制。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖静态图像场景，未涉及时序或交互式推理；任务设计虽自然但仍属实验室受控条件，与真实世界复杂性有差距；此外，受测模型范围有限，尚不清楚结论是否推广至所有VLM架构。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将VisRes扩展为视频推理基准，引入动态场景与因果链；同时结合神经-符号方法，探索提升模型组合抽象能力的新训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注多模态推理、鲁棒视觉理解或下一代VLM评估框架的研究者而言，该文提供了可复用的诊断工具与明确的性能缺口，助力开发真正具备抽象视觉推理能力的多模态系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20907v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PanoGrounder：利用全景场景表示桥接2D与3D以实现基于VLM的三维视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seongmin Jung，Seongho Choi，Gunwoo Jeon，Minsu Cho，Jongwoo Lim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20907v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据稀缺下实现可泛化的3D视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用全景渲染+3D特征作为2D VLM输入，三阶段推理再升回3D框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanRefer/Nr3D达SOTA，对未见数据集与文本改写泛化显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将360°全景作为2D-3D桥梁，直接赋能预训练VLM完成3D定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供轻量、强泛化的3D语言交互方案，无需大量3D标注。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D视觉定位(3DVG)是连接视觉-语言感知与机器人应用的关键任务，但现有监督方法受限于3D-文本配对数据稀缺，且推理能力远不及现代大规模视觉-语言模型(VLM)。作者观察到，直接把3D点云输入VLM会遭遇模态鸿沟，而纯2D裁剪图又丢失空间上下文，因此需要一种能兼顾2D可迁移性与3D几何信息的中间表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PanoGrounder提出“全景-3D耦合”思路：先将场景点云渲染成覆盖360°的多模态全景图，每幅图同步编码RGB、深度与语义特征；随后用轻量级适配把全景图喂给冻结的2D VLM完成文本指代推理；最后通过三阶段流程——基于场景几何布设少量全景视点→每视点输出热图与文本相关度→将多视2D预测反投影并融合为单一3D框——实现零样本或弱监督3D定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer和Nr3D基准上，PanoGrounder达到新SOTA，且无需任何3D-文本再训练即可直接泛化到SR3D、ScanNet++等未见数据集；对同一指代表述的多种口语改写也表现出鲁棒性，验证了大模型2D推理能力向3D任务的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖稠密全景渲染，计算与显存开销高于纯点云方案；对严重遮挡或镜面区域，深度不连续会导致反投影误差；此外，VLM的2D先验可能把共现物体误关联，尤其在物体密集且颜色相似的场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级稀疏全景或分层采样策略以降低计算，同时引入自监督深度补全与3D-文本对比学习，进一步缩小2D-3D语义鸿沟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D理解、大模型迁移或机器人视觉-语言交互，本文提供了“2D VLM→3D任务”的新范式与可复现代码框架，可直接对比或扩展至指代分割、导航等下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648127" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM-FuseNet: Segment Anything Guided Multi-Modal Fusion for RGB–Thermal Aerial Robotic Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM-FuseNet：Segment Anything引导的多模态融合用于RGB-热航空机器人感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyang Zhu，Jierui Wang，Lanlan Zhang，Jia Liang，Qianxiao Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648127" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648127</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal semantic segmentation with RGB and Thermal (RGB–T) inputs is critical for robust aerial robotic perception, yet existing methods face challenges in effectively adapting pretrained representations and balancing complementary cues under variable illumination.To overcome these challenges, we introduce Segment Anything Guided Multi-Modal Fusion Network (SAM-FuseNet), a novel RGB–T framework that leverages modality-specific encoders built on the Segment Anything Model 2 (SAM2) Vision Transformer (ViT) backbone, enhanced with Low-Rank Adaptation (LoRA) modules for efficient modality-specific fine-tuning without full model retraining. Cross-modal fusion is achieved via a pixel-wise spatial attention mechanism combined with channel recalibration, allowing adaptive weighting of contributions from each modality. A dual-pathway segmentation head further improves prediction quality: the first pathway progressively refines coarse-to-fine masks under SAM2 guidance, while the second pathway performs hierarchical feature fusion to capture long-range contextual information. Extensive experiments on the Caltech RGB–T and PST900 benchmarks demonstrate the effectiveness of SAM-FuseNet, achieving improvements of up to 3.93% in mean Intersection-over-Union (mIoU) while reducing the trained model parameters by more than 50%. Qualitative results further demonstrate sharper boundaries, robustness under underexposed conditions, and resilience to modality degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光照变化下RGB-T航拍图像语义分割的鲁棒性与高效适应预训练模型难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SAM2 ViT主干，用LoRA微调双模编码器，像素空间注意+通道重标定融合，双路径分割头</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Caltech RGB-T与PST900上mIoU提升3.93%，参数量减半，边界清晰且抗曝光不足与模态退化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM2与LoRA引入RGB-T融合，提出像素级空间-通道自适应融合及SAM2引导的渐进-层级分割头</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机多模态感知提供轻量、高精度的分割框架，可推广至其他遥感与机器人视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T语义分割对空中机器人在昼夜变化、光照不足等场景下的鲁棒感知至关重要，但现有方法难以同时利用大规模预训练表征并保持跨模态互补平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAM-FuseNet，以SAM2 ViT为双模态编码器主干，在各模态支路插入LoRA实现轻量级微调；像素级空间注意力与通道重标定模块动态融合RGB与热红外特征；双路分割头中，一路在SAM2掩码先验下由粗到精迭代优化，另一路通过层级特征融合捕获长程上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Caltech RGB-T与PST900基准上，SAM-FuseNet将mIoU提升最多3.93%，同时训练参数量减少逾50%；可视化结果显示其边界更锐利，并在曝光不足与单模态退化条件下保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估两个公开数据集，尚未验证在更高分辨率或实时机载推理中的延迟与内存占用；LoRA秩的选择及注意力模块计算开销可能对极小无人机平台仍显繁重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应秩分配与量化技术以进一步压缩模型，并在连续帧时空一致性上引入轻量级时序融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一种将大模型先验高效迁移到RGB-T分割的范式，对研究多模态遥感、夜间机器人导航及边缘部署的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19354v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReasonCD：面向隐式兴趣变化语义挖掘的多模态推理大模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyang Huang，Xiao Yu，Yi Zhang，Decheng Wang，Hang Ruan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19354v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users&#39; CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users&#39; implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users&#39; implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅依据隐式文本描述准确检测遥感图像中用户真正关心的变化区域</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用预训练大语言模型的推理能力，提出多模态推理变化检测框架ReasonCD</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BCDD数据集F1达92.1%，能挖掘隐式意图并解释推理过程</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大模型推理用于隐式兴趣语义挖掘，摆脱对显式CRoI描述的依赖</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供可解释、用户意图感知的智能解译新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测的核心是识别用户感兴趣的变化区域(CRoI)，但现有基于多模态大模型的方法高度依赖对CRoI的显式文本描述，一旦用户给出隐式、模糊或间接的语义表达，检测性能几乎完全失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReasonCD，将预训练大语言模型的链式推理能力引入变化检测流程：先通过视觉-语言编码器对双时相影像与隐式文本进行联合嵌入，再由大语言模型在提示驱动下生成对“用户真正想找什么变化”的多步推理结果，最后将推理出的显式语义注入轻量级变化解码器得到像素级变化图。整个框架采用端到端训练，仅冻结视觉编码器与大语言模型主干，微调跨模态映射层与解码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开BCDD数据集上ReasonCD取得92.1% F1，比现有最佳语义引导方法提升约6个百分点；作者还基于SECOND子集标注了1 200条隐式查询-推理标注对，实验表明模型在隐式场景下F1仅下降2.3%，而对比方法下降超过20%，且可自动生成人类可读的推理链条辅助决策。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开隐式查询标注集与代码，难以复现；推理阶段需多次调用大语言模型，导致单幅512×512影像推断耗时约8.7 s，难以实时部署；此外，隐式意图的评估指标仍依赖人工判断，缺乏统一客观的“意图-变化”一致性度量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建面向遥感变化检测的隐式意图基准与评价协议，并探索大模型蒸馏或侧缓存机制，在保持推理能力的同时实现毫秒级响应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多模态大模型在遥感解释中的应用、隐式语义挖掘或人类意图驱动的视觉任务，该文提供了将链式推理与变化检测耦合的新范式及初步实验证据，可直接借鉴其提示设计与跨模态微调策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20557v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在4D中学习推理：面向视觉语言模型的动态空间理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shengchao Zhou，Yuxin Chen，Yuying Ge，Wei Huang，Jiehong Lin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20557v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型具备随时间演化的3D动态空间推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建4D数据流水线生成DSR-Train/Bench，并设计轻量Geometry Selection Module注入几何先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Qwen2.5-VL-7B结合DSR-Train与GSM后动态空间推理显著提升且通用视频理解性能不降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提可扩展的in-the-wild 4D问答数据生成框架与问题驱动的几何先验选择模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏4D训练资源的领域提供数据、基准与即插即用模块，推动动态空间智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在静态图像与文本对齐上表现优异，但在4D动态空间推理(DSR)——即理解物体几何与三维关系随时间演化的能力——仍显著落后，主因是缺乏可扩展的4D感知训练数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整套资源与模块被封装为DSR Suite，可直接插入现有VLM进行端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>人类评估表明，模型生成的过程化解释在物理合理性与细粒度描述上优于现有最佳基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GSM仍需要额外的4D先验网络，在推理时增加显存与延迟；数据与基准主要覆盖英语问答，跨语言泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将GSM升级为稀疏激活结构以降低开销，并引入可微分物理模拟器实现自监督4D预训练；同时扩展至开放式文本生成与机器人规划任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、4D表征学习、视频-语言数据集构建或希望将几何先验注入大模型，本工作提供了可直接使用的数据、评测与即插即用模块，显著降低进入门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20255v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BiCoR-Seg：用于高分辨率遥感图像分割的双向协同精修框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinghao Shi，Jianing Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20255v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感图像语义分割中类间相似高、类内差异大导致边界模糊与类别混淆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双向协同精修框架BiCoR-Seg，含HBIS模块、分层监督与跨层Fisher判别损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LoveDA、Vaihingen、Potsdam数据集上取得领先分割精度并具可解释热图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用类热图建立特征图与类别嵌入的双向信息流，并以热图直接监督低层特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语义分割提供高判别、可解释新范式，可直接提升地物分类与边界精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割是地球观测的核心任务，但长期受困于类间相似度高、类内差异大，导致边界模糊和类别混淆。现有方法难以把抽象而判别性强的语义知识注入像素级特征学习，亟需一种能双向协同精炼特征与语义的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BiCoR-Seg，其核心是Heatmap-driven Bidirectional Information Synergy模块：通过生成类别级热图，在特征图与类别嵌入之间建立双向信息流，实现彼此迭代优化。在此基础上，引入分层监督策略，将每层HBIS产生的可解释热图直接作为低分辨率分割预测进行监督，从而增强浅层特征的判别力。为进一步紧致类内、分离类间，设计了跨层类别嵌入的Fisher判别损失，整体框架端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LoveDA、Vaihingen和Potsdam三个主流数据集上的大量实验表明，BiCoR-Seg在mIoU、F1、边界精度等指标上均取得领先水平，同时生成的热图具有良好可视解释性，验证了双向协同精炼的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外生成的类别热图，增加了显存与计算开销；Fisher判别损失引入的超参数对不同数据集敏感，需精细调优；目前仅在公开基准上验证，尚未在更大规模或跨传感器影像上测试泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化热图生成策略以降低计算成本，并将双向协同思想扩展到变化检测、多模态遥感等任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注高分辨率遥感语义分割、特征-语义协同学习、可解释深度学习或判别性损失设计的研究者，该文提供了新的双向精炼思路与可直接运行的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02652-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Consistency Matching for Semi-Supervised Semantic Correspondence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双重一致性匹配用于半监督语义对应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hailong Jin，Huiying Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02652-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02652-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅1%–5%像素级标注下获得高质量跨图像语义对应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出教师-学生框架DCM，结合邻域偏移一致性与语义一致性生成伪标签，并引入部件感知原型学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPair-71k上1%标注达79.6%PCK@0.1，5%标注达86.9%，超越全监督模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将子像素邻域偏移一致性与语义一致性联合用于半监督语义对应，并辅以部件原型空间约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺场景提供高精度对应方案，推动半监督视觉匹配与大模型微调研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义对应旨在把同一类别不同图像中的像素或区域一一对应起来，但类内外观差异与背景杂讯使该任务极具挑战。完全监督方法依赖稠密像素级标注，而人工标注代价高昂，促使研究者探索半监督学习以降低标注需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出教师-学生框架 Dual Consistency Matching (DCM)，通过邻域偏移一致性在子像素级别估计几何一致匹配，并利用语义一致性过滤掉几何虽一致但语义不符的伪对应。框架还引入部件感知原型学习，将物体关键部位作为空间约束，进一步提升教师模型生成的伪标签质量。整个网络以大规模预训练视觉模型为骨干，并在少量标注数据上微调以增强表征能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SPair-71k 上仅用 1% 标注数据（每类约 29 张）即达到 79.6% PCK@0.1；当标注比例升至 5% 时，PCK@0.1 进一步提升至 86.9%，超越使用全部标注的完全监督基线。实验表明 DCM 在几何与语义一致性约束下生成的伪标签显著提高了半监督对应学习的样本效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练大模型的初始表征质量，若目标类别与预训练数据差异过大，伪标签噪声可能增加。部件原型假设物体具有可区分的关键部位，对弱结构化或对称物体可能失效。教师-学生自训练框架也存在误差累积风险，需要谨慎设计更新策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨类别迁移的元学习策略，使 DCM 在极少甚至零标注新类别上快速适应；同时结合扩散模型或视觉大语言模型，进一步降低伪标签噪声并扩展至视频时序对应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为语义对应、半监督学习与自监督伪标签生成提供了可扩展的教师-学生范式，其几何-语义双重一致性思路可直接迁移至姿态估计、目标跟踪与三维重建等需要稠密匹配的任务，对研究低标注成本下视觉对应问题的学者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20042v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越视觉：基于多模态检索的上下文增强图像描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nguyen Lam Phu Quy，Pham Phu Hoa，Tran Chi Nguyen，Dao Sy Duy Minh，Nguyen Hoang Minh Ngoc 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20042v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让图像字幕包含背景、时间、结果与命名实体等不可见上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用BEIT-3/SigLIP检索相似图→ORB/SIFT重排→语义搜索相关文章→QLoRA微调Qwen3融合Instruct BLIP生成字幕。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEvents v1上，新字幕信息量比传统方法显著提升，事件要素覆盖更全面。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何重排的外部图文检索与QLoRA大模型融合，实现非视觉上下文注入的端到端字幕生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为新闻、教育、档案等需深度图像理解场景提供了可落地的富语境描述方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统图像字幕仅描述可见内容，在新闻、教育等场景常遗漏事件背景、时间线索与命名实体等关键语境，导致信息贫乏。作者指出这一“视觉之外”的缺口严重限制了深度图像理解与实际应用价值，因此提出引入外部文本知识增强字幕。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>系统以InstructBLIP(Vicuna-7B)生成基础字幕，同时用BEIT-3和SigLIP So-384编码图像并在Flickr30k&amp;COCO上检索语义相似图片，再用ORB+SIFT几何重排序精选样本。随后对关联新闻文章进行语义搜索抽取上下文，将视觉特征、基础字幕与外部文本拼接，通过QLoRA微调后的Qwen3融合生成事件级 enriched caption。整套流程在OpenEvents v1上评估，以信息量和实体覆盖为主要指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，所提方法在OpenEvents v1上比纯视觉基线字幕的信息量显著提升，BERTScore、CIDEr与实体召回率平均提高约15-20%，能生成含背景、时间、结果与命名实体的长描述。人工评测亦表明读者认为新字幕对事件理解更有帮助，验证了多模态检索+大模型融合在真实场景中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部新闻语料的质量与可检索性，若相关报道稀缺则增强效果下降；多阶段检索-重排-融合增加计算延迟，难以实时部署；未公开代码与详细超参，复现性与通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索端到端训练以减小延迟，并引入知识图谱或时序事件库提升对冷门事件的覆盖；同时研究轻量化检索策略实现移动端实时 enriched captioning。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要将视觉内容扩展为富含背景知识的描述的研究者提供完整范式，其多模态检索+LLM微调的框架可直接迁移到新闻存档、教育图解、文化档案等课题，对从事跨模态理解、外部知识增强生成或事件级视觉叙事的研究具有启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IAENet：面向3D点云异常检测的重要性感知集成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuanming Cao，Chengyu Tao，Yifeng Cheng，Juan Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104097</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在没有强大3D预训练骨干的条件下，用点云精准定位工业表面缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>IAENet框架，以Importance-Aware Fusion模块动态加权2D与3D专家异常得分并联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec 3D-AD点级定位SOTA、Eyecandies双级第一，同时显著降低误报。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出可学习的IAF模块及配套损失，实现跨模态贡献自适应加权，保留各专家优势。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D质检提供即插即用的2D-3D集成范式，缓解3D预训练不足并提升工业部署可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业表面缺陷检测长期依赖2D图像，但3D点云包含更丰富的几何信息，却缺乏像2D那样强大的预训练基础模型，导致3D异常检测性能受限。作者认为缺少可迁移的3D预训练骨干是当前瓶颈，因此提出利用2D预训练专家来弥补3D表征能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IAENet构建了一个双分支集成框架，一支采用2D预训练视觉Transformer提取RGB特征，另一支使用3D点云网络提取几何特征；核心贡献是Importance-Aware Fusion模块，它在每个样本上动态估计两种模态的置信度并重新加权异常得分。为了训练IAF，作者设计了专门的重要性一致性损失和保真损失，使融合结果既保留各专家独特优势又抑制不可靠预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec 3D-AD基准上，IAENet在点级缺陷定位达到新SOTA，并在物体级指标获得第二名；在Eyecandies数据集上同时取得点级与物体级最佳成绩。相比现有3D检测方法，IAENet显著降低假阳率，提升工业部署的可靠性。消融实验表明IAF模块对性能提升贡献最大，验证了动态加权策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖2D预训练大模型，若测试场景纹理缺失或光照极端，2D分支可能失效并拉低整体性能。IAF需额外参数估计重要性，增加推理延迟，对实时产线提出更高算力要求。论文仅在两个公开数据集验证，尚未在真实产线大规模数据上测试泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督预训练3D骨干以减少对2D预训练依赖，并将IAF思想扩展到多模态工业检测（如红外、X-ray）。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业异常检测、多模态融合或3D点云表征，该文提供了利用2D预训练知识提升3D检测性能的可复用框架，其动态加权策略对解决模态不一致问题具有通用参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20735v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VL4Gaze: Unleashing Vision-Language Models for Gaze Following
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VL4Gaze：释放视觉-语言模型用于视线跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shijing Wang，Chaoqun Cui，Yaping Huang，Hyung Jin Chang，Yihua Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20735v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视觉-语言模型能否理解并定位图像中人物的注视方向与目标？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建48.9万问答对的VL4Gaze基准，以VQA形式统一四类注视任务并系统评测与微调主流VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>通用VLM零样本注视推理弱；经VL4Gaze多任务训练后所有模型在语义与定位指标上显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出大规模注视理解VQA基准，将注视描述、方向、定位与歧义识别整合为统一评测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言社区提供标准数据与训练策略，推动注视理解在人机交互、社交机器人等领域的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视线（gaze）是推断人类注意力、意图与社会互动的关键线索，现有的大规模视觉-语言模型（VLMs）尚未被系统性地检验或训练用于 gaze 理解，导致其是否能在通用预训练中自发涌现 gaze 能力仍属空白。作者认为缺乏专门 benchmark 是阻碍该方向发展的核心瓶颈，因此亟需构建一个面向 gaze 的 VLM 评测与训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 VL4Gaze，首个 489 K QA 对、覆盖 124 K 图像的大规模 gaze 基准，将 gaze 理解统一为 VQA 形式并划分为四个互补子任务：描述被注视物体、描述视线方向、定位注视点坐标，以及识别模糊提问。数据集通过自动 pipeline 结合 gaze 估计模型与语言模板生成，再经人工校验保证质量。作者对闭源与开源 VLM 分别进行 in-context learning 和全参数微调对比，以量化任务特定监督带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，即使规模最大的通用 VLM 在未针对 gaze 训练时，其语义推理与空间定位准确率仍显著低于随机猜测之上界，表明 gaze 理解不会自然涌现。相较之下，使用 VL4Gaze 多任务监督后，所有模型在四个子任务上均取得一致且大幅度的性能跃升，验证专门 benchmark 对激活 VLM gaze 能力的关键作用。作者进一步分析发现，联合训练四个子任务比单任务训练更能提升泛化，且定位任务受益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据生成依赖现有 gaze 估计模型，其自身误差会传导至 QA 标注，可能引入系统偏差；benchmark 目前仅覆盖静态图像，缺少视频时序与多模态上下文（如语音、手势）的复杂交互。此外，自动模板生成的问答多样性有限，可能不足以评估模型对罕见 gaze 模式的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 VL4Gaze 至视频域，引入跨帧时序一致性任务，并探索将 gaze 作为提示信号反哺 VLM 的自监督预训练，以实现更细粒度的人机交互理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注人类行为理解、注意力建模或多模态学习，该文提供了首个可复现的 VLM gaze benchmark 与训练策略，可直接用于评估模型在社交场景、人机交互或辅助驾驶等应用中的视线推理能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19693v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">棱镜假设：通过统一自编码协调语义与像素表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weichen Fan，Haiwen Diao，Quan Wang，Dahua Lin，Ziwei Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19693v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder&#39;s feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一语义与像素两种模态的表征，使其在同一潜在空间中互补共存</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统分析编码器频谱特性，提出 Prism 假设，并设计带频率调制器的统一自编码器 UAE</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义编码器主要捕获低频抽象信息，像素编码器保留高频细节，二者可在统一频谱中共存</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将编码器功能角色与其特征频谱对应，提出 Prism 假设并用 UAE 实现跨模态频带调制融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言等多模态任务提供统一表征框架，提升生成与理解性能，启发频谱视角下的模型设计</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态表征学习长期面临语义抽象与像素保真难以兼顾的矛盾：语义编码器侧重高层抽象，像素编码器保留细节却缺乏语义判别力。作者从频谱视角重新审视不同编码器，发现其功能差异与特征谱能量分布高度耦合，由此提出统一视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先对CLIP、DINOv2等语义编码器和MAE、ConvNext等像素编码器做离散余弦变换，统计各频带能量占比，证实语义编码器能量集中在低频而像素编码器覆盖全频。据此提出Prism假设：世界共享同一特征谱，不同模态只是对该谱的带通投影。UAE在潜空间引入可学习频率带调制器，将低频语义与高频细节按通道-频率掩码相加，再用共享解码器重建图像与文本，实现单潜码同时支持语义分类与像素重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet线性探针与微调实验显示，UAE在相同计算量下Top-1分别比MAE提升3.8和1.4个百分点；MS-COCO零样本检索mAP@1提升4.7，图像生成FID从14.3降至11.9。可视化表明统一潜码既能激活高层语义神经元，也能保持边缘纹理高频响应，首次在单一模型内实现两种功能的最佳平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在分类、检索、重建三类任务验证，未涉及视频或3D模态；频带调制器依赖手工划分的低频/高频阈值，缺乏理论最优划分依据；训练开销比纯像素自编码增加约38%，对资源受限场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可微分频带划分策略让模型自动学习最优截止频率，并将UAE扩展至视频-音频-文本三模态，验证Prism假设在时序与动态谱上的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态统一表征、生成与理解一体化，或希望用频谱工具解释神经网络行为，该文提供可验证的频谱-功能对应关系与即插即用的调制模块，可直接迁移至VLM、扩散模型或跨模态检索框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向形状重建与姿态估计的扩散驱动自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingtao Sun，Yaonan Wang，Mingtao Feng，Chao Ding，Mike Zheng Shou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647855</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用形状先验、无需标注，实现多物体类别级6-DoF位姿估计与形状重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散驱动的自监督网络，含SE(3)等变金字塔3D点Transformer与先预训练后精调范式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开及自建数据集上，自监督性能超越现有最佳并优于部分全监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散机制引入自监督多任务框架，实现位姿与形状联合学习且无需合成数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供零标注、高精度的类别级位姿与形状感知新途径。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>类别级6-DoF位姿估计传统上依赖全监督，需要为每个新实例手工标注大量6-DoF标签，成本极高。近期自监督方法试图用合成数据或CAD模型替代人工标注，但大多仅解决单物体位姿，忽略了形状重建等多任务需求。本文动机在于仅用类别形状先验，同时完成多物体形状重建与类别级位姿估计，彻底摆脱对任何标注或合成数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出扩散驱动的自监督网络，核心包含：1) Prior-Aware Pyramid 3D Point Transformer，利用径向核点卷积提取SE(3)等变位姿特征，并用3D尺度不变图卷积编码物体级形状表示；2) Pretrain-to-Refine自监督训练范式，先通过扩散模型将形状先验映射到观测空间，再迭代细化，以捕捉同一类别内形状变化并建立先验-观测关联，实现无标注联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开数据集和一个自建数据集上，该方法显著优于现有自监督类别级基线，在形状重建误差（CD/EMD）和位姿误差（5°5cm、10°10cm指标）上平均降低20-30%。更令人惊讶的是，它在无标注条件下甚至超过了一些全监督实例级和类别级方法，证明扩散机制能有效弥补监督缺失，实现形状与位姿的互助提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量的类别级形状先验，若先验分布与测试域差异大，性能会明显下降；扩散迭代细化带来额外推理时间，实时性不如纯前馈网络；此外，目前仅在刚性物体上验证，对含有铰接或可变形的类别尚未探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的先验生成模块，以缓解对固定形状先验的依赖；同时将扩散细化蒸馏为轻量级前馈网络，提升实时性，并扩展到铰接或非刚性物体领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究无监督/自监督3D感知、类别级位姿估计、或多任务形状-位姿联合学习，该文提供了用生成式扩散先验替代标注的新范式，可直接借鉴其SE(3)等变网络设计与Pretrain-to-Refine训练流程，快速迁移到新类别或新传感器数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TPIN：融合模态共有与模态特定特征的文本并行交互网络用于多模态情感分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changbin Wang，Fengrui Ji，Baolin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104087</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾模态共性与个性并突出文本情感线索以提升多模态情感分析性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TPIN，含共性对比学习TSCL与文本引导融合TG-DSA，及特异动态路由MSIP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CMU-MOSI/MOSEI上全面指标提升0.5%–1.2%，达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>TSCL+HNM缓解异构，TG-DSA文本主导融合，动态路由保留视音模态特异信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MSA提供兼顾共性与特异且文本增强的新框架，可推广至多模态理解任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)需要把文本、视觉、声学三种异质信号融合成统一表征，但现有方法常忽视各模态的特异性且对文本中的情感线索利用不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Text-based Parallel Interaction Network(TPIN)，由Modality-Common Information Processing(MCIP)和Modality-Specific Information Processing(MSIP)并行支路组成；MCIP中设计Two-Stage Contrastive Learning(TSCL)并引入Hard Negative Mining以缓解异质性，同时用Text-Guided Dynamic Semantic Aggregation(TG-DSA)在文本引导下完成深度融合；MSIP则通过动态路由机制迭代优化权重，显式保留视觉与声学模态的特有信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSI与CMU-MOSEI基准上，TPIN在Acc、F1、MAE等主要指标上均取得SOTA，平均提升0.5%–1.2%，验证了对模态共性与特异性的权衡策略有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数细节，难以复现；动态路由增加计算开销，对长视频序列的可扩展性未讨论；实验仅覆盖英文视频数据，跨语言或跨文化泛化能力未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索TPIN在直播、跨语言社交媒体及缺失模态场景下的鲁棒性，并引入轻量化路由以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、对比学习或文本主导的情感计算，该文提供的并行共性-特异性框架与TG-DSA、动态路由设计可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Causal HyperPrompter：一种无偏的高光谱伪装目标跟踪框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanzheng Wang，Wei Li，Xiang-Gen Xia，Qian Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&#39;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除RGB偏见与模板-搜索关联缺失，实现高光谱伪装目标稳健跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建结构因果模型+反事实干预去偏，设计局部光谱角令牌嵌入，并发布BihoT-130k数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新框架在多个大规模数据集上显著优于现有方法，验证去偏与光谱关联增强的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断与反事实去偏引入高光谱跟踪，并提出局部光谱角令牌关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为克服高光谱视觉中RGB迁移偏见与数据稀缺提供可扩展因果框架和基准资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱伪装目标跟踪因目标与背景在纹理和颜色上极度相似而极具挑战；现有方法普遍把高光谱数据压缩成伪彩三通道图像，再用 RGB 预训练跟踪器微调，导致 RGB 域混淆效应，使模型学到虚假相关而忽视光谱判别性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Causal HyperPrompter 框架，先用结构因果模型显式解耦跟踪中的专属因果因子，并通过反事实干预去除来自 RGB 模型的混杂偏差；设计新的 token-type 嵌入模块，在模板-搜索 token 间引入局部光谱角建模，强化语义关联与定位敏感度；为缓解数据稀缺，发布含 130750 帧标注的大规模高光谱伪装数据集 BihoT-130k。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个大规模高光谱跟踪基准上的实验表明，该方法显著优于现有最佳基线，平均成功率提升约 6–8%，且对初始框扰动更具鲁棒性；消融验证显示因果干预与光谱角嵌入分别贡献约 60% 与 30% 的性能增益；BihoT-130k 使训练数据量扩大一个数量级，预训练模型迁移到 RGB-热红外伪装任务亦取得 SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高光谱完整波段，若存在严重波段缺失或噪声，因果因子估计会失效；反事实干预增加约 35% 的推理时间，实时性仍低于轻量级 RGB 跟踪器；数据集虽大，但场景以草地-林地为主，城市/海域伪装样本相对不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可研究自适应波段选择机制以降低采样负担，并将因果干预蒸馏为实时网络；探索跨模态因果对齐，把高光谱因果因子迁移到 RGB-深度或热红外跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱成像、因果推理在视觉跟踪中的应用，或伪装目标检测的数据稀缺与域偏差问题，该文提供了可复现的因果框架和大规模基准，可直接扩展至其他模态的隐蔽目标分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">神经隐式场中面向物体姿态估计的正激励点采样学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifei Shi，Boyan Wan，Xin Xu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647829</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&#39;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&#39;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&#39;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在严重遮挡或新物体下，用神经隐式场准确估计6-DoF位姿。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SO(3)-等变卷积隐网络+动态正激励点采样，用教师模型生成伪真值训练稀疏采样器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三数据集SOTA，5°2cm指标达0.63，训练更快、预测更准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SO(3)-等变隐网络与输入自适应正激励采样结合，用自监督伪真值训练稀疏关键点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡/新物体位姿估计提供高鲁棒隐式表示与高效采样范式，可直接提升AR/机器人抓取等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经隐式场因其任意分辨率表示3D形状的能力，在形状重建、新视角合成和物体位姿估计中迅速兴起；然而，相机空间不可见区域的规范坐标缺乏直接观测信号，导致模型只能依赖泛化、不确定性高，密集采样会引入错误并拖累训练与性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SO(3)-等变卷积隐式网络，在任意查询点保持旋转等变地估计点级属性；同时设计正激励点采样策略，由轻量估计网络依据输入动态生成少量高置信度采样点，这些点的特征足以确定物体全部6-DoF位姿；估计网络用教师模型自动生成的伪真值进行训练，从而避免额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个标准位姿估计数据集上方法达到SOTA，5°2cm指标达0.63，显著优于现有基线；在高度遮挡和新形状场景下，密集对应精度与位姿误差均大幅降低；训练阶段采样点减少约60%，收敛速度提升，验证了采样策略的效率与等变网络的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖教师模型生成伪真值，若教师本身在极端视角或严重遮挡下失效，会传播错误；SO(3)-等变卷积增加网络参数量与计算开销，对实时应用构成挑战；正激励采样虽稀疏，但对估计网络的超参数敏感，需交叉验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无教师自监督伪真值生成，并将等变网络蒸馏为轻量级前馈模型以实现实时推理；结合语义或运动先验进一步提升对动态遮挡的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将神经隐式场与等变网络引入6-DoF位姿估计，并首次提出动态置信度采样策略，为研究遮挡、新形状泛化及高效训练的研究者提供了可复用的网络模块与采样框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647928" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S
                    &lt;sup&gt;2&lt;/sup&gt;
                    EDL: Selective Semantic Efficient Distillation Learning for Large-Scale Remote Sensing Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²EDL：面向大规模遥感表征的选择性语义高效蒸馏学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wu Wen，Jinghui Luo，Lizhuang Tan，Konstantin Igorevich Kostromitin，Jian Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647928" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647928</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾全局可分性与局部细节、提升大规模遥感自监督表征质量与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>教师-学生蒸馏框架，教师提供多级语义，学生选择性重建高信息掩码区并辅以对比分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多下游任务实验显示S2EDL预训练模型性能优于主流SSL方法，表征更全面高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出选择性语义高效蒸馏学习，动态聚焦高价值掩码区并融合MIM与CL优势。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大数据自监督预训练提供兼顾精度与计算效率的新范式，可直接提升下游应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模遥感影像缺少昂贵的人工标注，自监督学习(SSL)成为提取通用视觉表征的主流手段。现有对比学习(CL)擅于全局可分性却易丢失局部细节，而掩码图像建模(MIM)能捕捉局部空间信息却缺乏全局一致性与计算效率。为此，作者提出选择性语义高效蒸馏学习(S²EDL)，以兼顾全局判别力与局部细粒度感知。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²EDL采用教师-学生知识蒸馏框架：教师网络对完整增广影像编码并输出多级语义监督；学生网络通过“选择性语义MIM”动态评估每个掩码区域的信息量，仅对信息量最高的子集进行重建与损失回传，从而把计算量降低约40%。框架并行引入对比学习分支，利用教师全局特征作正样本，使学生表征同时满足全局可分性与局部判别性，整体训练在单张V100上即可处理1.2亿参数模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet、EuroSAT、SEN12MS等五个下游任务上，S²EDL预训练模型平均mIoU和Top-1精度分别比当前最佳SSL方法提升2.7%和3.4%，参数效率提升1.8倍。可视化表明其激活图在建筑物边缘、农田纹理等细粒度区域响应更集中，验证了局部细节保持优势。消融实验显示选择性掩码策略贡献约60%的性能增益，证明信息导向的稀疏重建对遥感影像尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师网络质量，若教师表征存在偏差会直接影响学生；选择性掩码的阈值与信息量度量目前为固定启发式，对不同传感器或分辨率需重新调参。此外，实验主要聚焦多光谱与RGB影像，尚未验证在SAR、LiDAR等模态上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的稀疏掩码策略，实现任务驱动的动态阈值；并探索跨模态教师-学生协同，以统一框架自监督融合光学、SAR与激光雷达数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大规模遥感自监督预训练、细粒度空间信息保持或知识蒸馏在地球观测中的应用，S²EDL提供了一种兼顾效率与性能的新范式，可直接借鉴其选择性语义掩码与双分支协同策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VOIC：可见-遮挡解耦的单目三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zaidao Han，Risa Higashita，Jiang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像做3D语义场景补全时，可见区与遮挡区互相干扰导致性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VRLE离线提取可见区标签，并设计VOIC双解码器分别处理可见感知与遮挡推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI和SSCBench-KITTI360上几何与语义指标均达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦可见-遮挡任务，用双解码交互完成全局一致3D补全。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉方案提供更高精度的3D场景理解，推动自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D语义场景补全(SSC)是自动驾驶与机器人理解的核心任务，但现有端到端2D-3D提升方法在单张图像输入下，常把高置信可见区域感知与低置信遮挡区域推理混在一起，导致特征稀释与误差传播。作者观察到这一干扰是性能瓶颈，因此提出显式解耦可见-遮挡空间，以净化监督信号并提升整体补全质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先设计离线Visible Region Label Extraction(VRLE)策略，从密集3D真值中分离出仅含可见体素的监督，形成两个互补子任务的纯净标签。随后提出双解码器框架VOIC：基础网络将图像特征与单目深度估计的占用栅格融合，得到初始3D体素特征；可见解码器在该特征上输出高保真几何与语义先验；遮挡解码器再以这些先验为条件，通过跨模态交互完成全局遮挡区域推理，最终合并两路输出得到完整SSC结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI和SSCBench-KITTI360两大基准上，VOIC在几何补全度(Completeness)与语义分割mIoU均优于此前所有单目SSC方法，取得新SOTA；消融实验显示VRLE监督解耦带来+2.8 mIoU提升，而双解码器交互机制额外带来+1.9 mIoU增益，验证了可见-遮挡解耦策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VRLE依赖精确的3D真值与相机位姿，在真值稀疏或标定不准的数据集上难以复现；双解码器设计使参数量与推理时间相比单解码器增加约40%，对实时车载系统构成负担；方法仍基于单帧输入，对动态物体和时序一致性未做建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线VRLE以适配无真值场景，并引入时序融合或多帧自监督，进一步提升动态区域与长距离遮挡的补全精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D感知、遮挡推理或语义-几何联合补全，VOIC提供的可见-遮挡解耦思想与双解码器交互机制可直接迁移到BEV分割、3D检测或 occupancy network等任务，以缓解单目输入下的特征混叠问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21333v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast SAM2 with Text-Driven Token Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Fast SAM2：基于文本驱动的Token剪枝方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Avilasha Mandal，Chaoning Zhang，Fachrina Dewi Puspitasari，Xudong Wang，Jiaquan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21333v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改动SAM2架构的前提下，用文本线索提前剔除冗余视觉token，降低视频分割计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在编码后传播前，引入轻量路由，融合局部视觉、文本语义及不确定性，对token排序并剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>剪枝后推理提速42.5%，GPU内存降37.4%，J&amp;F指标与原版SAM2持平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本驱动的早期token剪枝用于SAM2视频分割，实现无需改架构的高效时序推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时与资源受限场景提供可即插即用的SAM2加速方案，推动大模型在视频任务中的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在提示驱动的视频目标分割上表现优异，但需逐帧处理全部视觉 token，导致时间注意力随帧数呈二次增长，推理与显存开销大，难以在实时或资源受限场景落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出文本引导的 token 剪枝框架，在图像编码后、记忆传播前插入轻量路由器，对每帧 token 按局部视觉上下文、文本语义相关度（用户给定或自动生成的目标描述）和模型不确定性三因子打分并排序，仅保留前 k% 的“高信息” token 进入后续记忆更新与掩码解码，无需改动 SAM2 原结构即可实现端到端推理加速。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DAVIS17、DAVIS16、YouTube-VOS 等基准上，剪枝后推理速度提升 42.5%，GPU 显存降低 37.4%，而 J&amp;F 指标仅下降 0.6–1.2 个百分点，仍与完整模型持平；消融实验表明文本语义项对保留目标区域 token 贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>剪枝比例需手动设定，对极细小或快速形变目标可能误剪关键边缘 token；文本描述若与目标不符，路由器会引入语义偏差；方法目前仅在单卡 GPU 上测试，未验证分布式或移动端部署效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的动态剪枝比例，并结合跨帧 token 预测实现自适应稀疏化；探索无文本提示的纯视觉-不确定性自监督剪枝以扩展适用场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视频 Transformer、token 稀疏化、或想将 SAM2 类基础模型压缩到边缘设备，该文提供了即插即用且开源可复现的文本-视觉联合剪枝思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20936v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">推理驱动的非模态补全：协作智能体与感知评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongxing Fan，Shuyu Zhao，Jiayang Ao，Lu Sheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20936v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遮挡推理中语义一致性与结构完整性难以兼顾、渐进式方法误差累积的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多智能体协作框架，将语义规划与视觉生成分离，并引入自验证与多样假设生成机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>单次生成即可实现显著优于现有方法的语义与结构一致性，在多个数据集上取得SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦语义规划与像素合成，提出自纠正验证智能体与MAC-Score人类对齐评估指标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡推理提供稳定、可解释且可评估的新范式，推动视觉理解与生成的协同研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Amodal completion 需要在仅给出可见部分的情况下推断被遮挡物体的完整形状与外观，传统逐阶段生成方法因误差累积和推理不稳定而难以保持语义一致与结构完整。作者观察到，若能在像素生成前先做显式语义规划，可显著降低一次生成的失败率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Collaborative Multi-Agent Reasoning Framework，将语义规划与视觉生成解耦：先由专用推理代理输出结构化计划，再由生成网络一次完成补全。框架包含自纠正 Verification Agent，利用 Chain-of-Thought 推理精修可见区域分割并检测残留遮挡物；同时引入 Diverse Hypothesis Generator，在语义层面产生多种合理解释而非仅依赖随机种子。整个流程在单阶段内完成，无需迭代细化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCOA、KINS、D2SA 等数据集上，该方法在 IoU、FID 及新提出的 MAC-Score 上均显著优于现有最佳算法，MAC-Score 与人工评分的 Kendall τ 达 0.81，验证了其对结构完整性与语义一致性的可靠评估能力。实验表明，引入显式语义规划可将错误累积降低约 30%，并提升遮挡物边缘的清晰度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多代理间的链式推理，计算开销高于单网络方案；MAC-Score 基于现成 MLLM，其偏见可能随模型版本变化；对极端遮挡比例 (&gt;80 %) 的场景，假设多样性仍可能遗漏稀有语义。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级代理蒸馏以加速推理，并将框架扩展到视频时序一致补全或开放世界文本引导的 amodal 生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遮挡推理、语义生成评估或多代理协同框架，本文提供的显式规划-生成解耦思路及 MAC-Score 可为后续实验与评测提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>