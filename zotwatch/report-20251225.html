<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-25</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-25 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">1天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户聚焦计算机视觉中的场景理解，尤其偏好场景图生成与表示；同时对视觉问答和场景分类保持同步关注，体现出对“图像→结构化语义→高层推理”链条的整体兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在极短收藏周期内，场景图相关文献占比高达2/3，说明该方向是其当前最集中、最优先深入阅读的核心领域；视觉问答与场景分类的并列收藏也显示出对跨任务关联性的主动探索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读材料全部落在CS-CV与CS-AI交叉地带，尚未出现与机器人、图形学或人文社科的明显融合，表明用户目前仍以纯视觉-AI方法研究为主。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单日集中收藏6篇最新文献，且时间跨度覆盖2022-2025，显示用户正快速补齐该方向的前沿进展；全部选取近四年工作，预示其兴趣向最新模型、数据集和评测基准迅速靠拢。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续跟进场景图与大型多模态模型（如Vision-Language Transformers）的结合，以及3D场景图、动态视频场景图等新扩展；同时关注Visual Reasoning、Embodied QA等需要场景图支撑的高层任务。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(2 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 6/6 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-25 10:27 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '视觉问答', '场景分类'],
            datasets: [{
              data: [4, 2, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 4 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u751f\u6210\u4e0e\u56fe\u50cf\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u6587\u672c-\u573a\u666f\u56fe\u57ce\u5e02\u611f\u77e5",
            size: 2,
            keywords: ["Computer Science - Artificial Intelligence", "Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.7276801213949997}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于遥感多模态的论文、1篇关于注视估计的论文、1篇关于医学诊断的论文和1篇关于高效融合的论文。</p>
            
            <p><strong class="text-accent">遥感多模态</strong>：《Bridging Semantics and Geometry》提出将语言推理与像素预测解耦的LVLM-SAM框架，实现遥感影像推理分割；《Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network》设计双提示感知跨模态交互，缓解遥感图像描述中的模态鸿沟。</p>
            
            <p><strong class="text-accent">注视估计</strong>：《VL4Gaze》首次把视觉-语言模型引入注视跟随任务，利用文本线索提升复杂场景下的人眼注视定位精度。</p>
            
            <p><strong class="text-accent">医学诊断</strong>：《Beyond CLIP》在糖尿病视网膜病变诊断中引入医学知识增强的多模态Transformer，超越通用CLIP实现更精准的跨模态对齐。</p>
            
            <p><strong class="text-accent">高效融合</strong>：《CASA》提出“自注意力做交叉注意力”的CASA模块，用共享参数高效完成视觉-语言融合，降低计算量并保持性能。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于3D感知与场景理解、6篇关于视觉-语言跨模态、5篇关于自监督/半监督表示学习、4篇关于文档与图像检索、3篇关于人体与行人再识别、2篇关于语义对应与匹配、2篇关于扩散模型与生成建模的论文。</p>
            
            <p><strong class="text-text-secondary">3D感知与场景理解</strong>：该主题聚焦单目或多视角图像恢复完整3D几何与语义，如《Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion》联合深度补全与语义分割，《PanoGrounder》用全景场景表征桥接2D-3D实现3D视觉定位，《UniPR-3D》以视觉几何Transformer做通用视觉地点识别，《Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation》在隐式场中采样关键点估计6-DoF位姿。</p>
            
            <p><strong class="text-text-secondary">视觉-语言跨模态</strong>：论文探索视觉与文本对齐以完成定位、检索与表征统一，《PanoGrounder》将VLM用于3D视觉定位，《Vision-Language Models for Person Re-identification: A Survey and Outlook》系统梳理VLM在行人重识别中的应用，《The Prism Hypothesis》提出统一自编码框架调和语义与像素表征。</p>
            
            <p><strong class="text-text-secondary">自监督/半监督表示学习</strong>：研究通过自监督或半监督信号减少标注依赖，《Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation》用扩散模型生成伪标签进行类别级位姿自监督，《Dual Consistency Matching for Semi-Supervised Semantic Correspondence》在半监督下保持几何-语义双重一致性学习跨图像对应。</p>
            
            <p><strong class="text-text-secondary">文档与图像检索</strong>：关注从富视觉文档或自然语言描述中检索关键信息，《SynJAC》提出合成数据驱动的联合粒度适应与校准用于扫描文档关键信息抽取，《Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval》以轻量级实体抽取实现事件导向图像检索。</p>
            
            <p><strong class="text-text-secondary">人体与行人再识别</strong>：聚焦跨摄像头行人检索与人体相关任务，《Vision-Language Models for Person Re-identification: A Survey and Outlook》综述VLM增强的ReID方法，提升文本-行人检索与遮挡鲁棒性。</p>
            
            <p><strong class="text-text-secondary">语义对应与匹配</strong>：研究同类图像间密集语义对应，《Dual Consistency Matching for Semi-Supervised Semantic Correspondence》引入几何-语义双向一致性提升半监督匹配精度。</p>
            
            <p><strong class="text-text-secondary">扩散模型与生成建模</strong>：利用扩散过程进行生成式自监督，《Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation》将扩散先验融入形状重建与位姿估计，显著提升无标注场景下的泛化性能。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 39%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19302v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">桥接语义与几何：面向遥感推理分割的解耦LVLM-SAM框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Junyao Ge，Yang Zheng，Kaitai Guo，Jimin Liang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19302v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有LVLM遥感推理分割因端到端微调导致几何定位弱、泛化差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Think2Seg-RS，用RL训练LVLM生成结构化几何提示驱动冻结SAM分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>EarthReason达SOTA，零-shot泛化多基准；语义级监督下小分割器优于大模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将语义推理与像素预测解耦，用掩码RL让LVLM学会提示SAM实现遥感推理分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可解释、统一的LVLM-SAM范式，推动语义级地理空间理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像的“推理式分割”要求模型先理解自然语言中的复杂语义，再给出像素级掩膜，但现有方法把语言推理与几何预测耦合在端到端微调中，导致几何定位弱、跨任务泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Think2Seg-RS，将大视觉-语言模型（LVLM）与冻结的 Segment Anything Model (SAM) 解耦：LVLM 仅作为“提示器”，通过强化学习在掩膜奖励下学习输出结构化几何提示（点框等），直接驱动 SAM 生成掩膜，而无需更新 SAM 参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EarthReason 基准上达到新 SOTA；学到的提示策略零样本迁移到多个指代分割数据集，揭示语义级与实例级定位存在明显鸿沟；实验还发现紧凑 SAM 变体在语义监督下优于更大模型，且负提示在航空异质背景中无效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在公开遥感数据集验证，未测试跨传感器、跨分辨率或时序一致性；强化学习训练需设计掩膜奖励，收敛慢且对超参敏感；LVLM 提示空间离散，可能丢失精细几何细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将提示空间扩展为连续向量或扩散式几何嵌入，实现亚像素级精度；引入时序-多视角数据，研究动态地表目标的持续推理分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把“语义推理”与“几何分割”解耦并用 RL 驱动冻结 SAM，为研究遥感 LVLM、提示学习、零样本迁移或弱监督分割的学者提供可复现的新范式与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20735v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VL4Gaze: Unleashing Vision-Language Models for Gaze Following
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VL4Gaze：释放视觉-语言模型用于视线跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shijing Wang，Chaoqun Cui，Yaping Huang，Hyung Jin Chang，Yihua Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20735v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视觉-语言模型能否理解并定位图像中人物的注视方向与目标？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建48.9万问答对的VL4Gaze基准，以VQA形式统一四类注视任务并系统评测与微调主流VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>通用VLM零样本注视推理弱；经VL4Gaze多任务训练后所有模型在语义与定位指标上显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出大规模注视理解VQA基准，将注视描述、方向、定位与歧义识别整合为统一评测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言社区提供标准数据与训练策略，推动注视理解在人机交互、社交机器人等领域的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视线（gaze）是推断人类注意力、意图与社会互动的关键线索，现有的大规模视觉-语言模型（VLMs）尚未被系统性地检验或训练用于 gaze 理解，导致其是否能在通用预训练中自发涌现 gaze 能力仍属空白。作者认为缺乏专门 benchmark 是阻碍该方向发展的核心瓶颈，因此亟需构建一个面向 gaze 的 VLM 评测与训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 VL4Gaze，首个 489 K QA 对、覆盖 124 K 图像的大规模 gaze 基准，将 gaze 理解统一为 VQA 形式并划分为四个互补子任务：描述被注视物体、描述视线方向、定位注视点坐标，以及识别模糊提问。数据集通过自动 pipeline 结合 gaze 估计模型与语言模板生成，再经人工校验保证质量。作者对闭源与开源 VLM 分别进行 in-context learning 和全参数微调对比，以量化任务特定监督带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，即使规模最大的通用 VLM 在未针对 gaze 训练时，其语义推理与空间定位准确率仍显著低于随机猜测之上界，表明 gaze 理解不会自然涌现。相较之下，使用 VL4Gaze 多任务监督后，所有模型在四个子任务上均取得一致且大幅度的性能跃升，验证专门 benchmark 对激活 VLM gaze 能力的关键作用。作者进一步分析发现，联合训练四个子任务比单任务训练更能提升泛化，且定位任务受益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据生成依赖现有 gaze 估计模型，其自身误差会传导至 QA 标注，可能引入系统偏差；benchmark 目前仅覆盖静态图像，缺少视频时序与多模态上下文（如语音、手势）的复杂交互。此外，自动模板生成的问答多样性有限，可能不足以评估模型对罕见 gaze 模式的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 VL4Gaze 至视频域，引入跨帧时序一致性任务，并探索将 gaze 作为提示信号反哺 VLM 的自监督预训练，以实现更细粒度的人机交互理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注人类行为理解、注意力建模或多模态学习，该文提供了首个可复现的 VLM gaze benchmark 与训练策略，可直接用于评估模型在社交场景、人机交互或辅助驾驶等应用中的视线推理能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建双提示感知跨模态交互融合网络，利用实体概念与场景类别先验生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入显式实体概念与场景双提示，构建跨模态公共语义空间辅助描述生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释先验，推动跨模态对齐与VLM在遥感领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却未充分挖掘先验知识来显式构建跨模态语义桥梁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并分类场景；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在公共语义空间中完成跨模态对齐；最后于Transformer解码端引入提示感知注意力融合模块，利用对齐后的跨模态提示特征指导字幕生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上达到SoTA，其中在RSICD的BLEU@4提升3.3%，CIDEr提升20.0%，验证了显式引入实体-场景双提示可显著缩小模态差距并提高描述准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义实体空间与场景类别，若实体词汇表或场景标签不完整会限制泛化；双提示的构建与融合增加了模型复杂度，对计算资源要求更高；未公开跨模态对齐的可解释性分析，难以评估提示语义是否真正对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应实体空间扩展与开放词汇场景分类，以降低对人工先验的依赖；或引入轻量化提示生成策略，在保持性能的同时降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感理解提供了显式语义桥接新范式，其双提示机制与公共语义空间构建思路可直接迁移至遥感视觉问答、变化描述等多模态任务，对研究遥感-语言对齐的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19663v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越CLIP：面向糖尿病视网膜病变跨模态对齐的知识增强多模态Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Argha Kamal Samanta，Harshika Goyal，Vasudha Joshi，Tushar Mungle，Pabitra Mitra
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19663v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP&#39;s 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用CLIP在糖尿病视网膜病变图文跨模态检索与诊断中的失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>ViT+Bio-ClinicalBERT+MLP多模态编码器，联合Transformer融合，对比-重构-分级多任务训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>BRSET上Recall@1达99.94%，DR分级97%+；零样本DeepEyeNet仍94%，远超CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构化患者知识注入图文Transformer，实现医学语义对齐与诊断一体化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为眼科及医学影像提供可泛化的跨模态检索与精准筛查新基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diabetic retinopathy (DR) screening requires accurate alignment of retinal images with clinical narratives, yet general-domain vision-language models such as CLIP fail to capture medical semantics, yielding near-zero text-to-image retrieval recall on ophthalmic data. This performance gap endangers automated DR diagnosis systems that must retrieve relevant images from text queries or supply textual explanations for fundus photographs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors design a multimodal transformer with three dedicated encoders: ViT-B/16 for fundus photos, Bio-ClinicalBERT for free-text reports, and an MLP for structured patient variables. A joint transformer fuses the modalities via learned modality-specific positional embeddings and is trained with a multi-task objective: contrastive loss between every pair of modalities, reconstruction losses that regenerate input images and text, and severity-classification losses aligned to ICDR and SDRG grading schemes. The whole framework is optimized on the Brazilian Multilabel Ophthalmological Dataset (BRSET) and evaluated in both retrieval and grading scenarios.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On BRSET the proposed model attains 99.94 % Recall@1 for text-to-image retrieval while fine-tuned CLIP reaches only 1.29 %, and simultaneously achieves 97.05 % accuracy on SDRG and 97.97 % on ICDR five-level grading. Zero-shot transfer to the unseen DeepEyeNet cohort still yields 93.95 % Recall@1 versus 0.22 % for CLIP, demonstrating that knowledge-enhanced pre-training successfully encodes medical cross-modal associations without sacrificing discriminative power.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to diabetic retinopathy, so generalization to other retinal diseases or anatomical sites remains untested. Training relies on a Portuguese-English mixed dataset with limited text length diversity, potentially biasing the language encoder; moreover, the compute cost of three separate encoders plus a joint transformer may hinder deployment in low-resource screening programs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the framework to multi-disease retina reports and explore lightweight fusion strategies such as cross-modal parameter sharing or adapter modules to reduce inference overhead.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers developing medical vision-language models, ophthalmic AI tools, or multimodal fusion techniques can adopt the multi-task, knowledge-enhanced paradigm to overcome CLIP-style domain drift and achieve both high retrieval precision and diagnostic accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.29</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19535v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CASA：通过自注意力实现跨注意力的高效视觉-语言融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Moritz Böhle，Amélie Royer，Juliette Marrie，Edouard Grave，Patrick Pérez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19535v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率、长对话或视频流场景下，既保持跨模态性能又降低计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CASA，在跨注意力层内复用自注意力机制实现局部文本交互，替代全token插入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CASA在图像理解基准上逼近全注意力性能，同时保持跨注意力对长上下文的高效扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部文本自注意力嵌入跨注意力层，兼顾细粒度视觉细节与线性复杂度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视觉语言模型提供新范式，支持高分辨率图像与长视频实时多模态应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models typically fuse modalities by inserting all image tokens into the language model’s self-attention layers, yielding full bidirectional attention but quadratic cost in the number of image tokens. This design becomes prohibitive for high-resolution images, long dialogues, or streaming video where thousands of visual tokens must be processed. Cross-attention architectures avoid this cost by restricting attention to a small latent set, yet they lag behind full fusion on tasks that demand fine-grained visual reasoning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CASA re-purposes the weights of a frozen decoder-only language model by replacing selected self-attention layers with cross-attention modules that attend to the visual stream. Crucially, each CASA layer still performs self-attention among text tokens before attending to image tokens, thereby preserving local textual context without extra parameters. The visual input is encoded once by a frozen CLIP-ViT and then projected into the language model’s embedding space; no vision-specific parameters are added downstream. All experiments keep both vision and language backbones frozen, training only lightweight adapters and layer norms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO Captions, NoCaps, VizWiz, OK-VQA, GQA, and VSR, CASA closes 70-90 % of the gap between strong cross-attention baselines and full token-insertion models while using ≤ 12 % of the image tokens. With identical compute budget, CASA yields 2–4 CIDEr point gains on captioning and 1–3 % absolute accuracy gains on VQA over standard cross-attention. In streaming video captioning, CASA scales linearly with frame count and matches the accuracy of token-insertion models that exhaust GPU memory beyond 64 frames.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to decoder-only LLMs up to 8 B parameters and frozen vision encoders, leaving unexplored the impact of larger models or end-to-end training. Architectural choices such as which layers to convert and how many visual tokens to keep are empirical without a clear theoretical criterion. Evaluation is restricted to English benchmarks and short video clips, so multilingual or very long narrative settings remain untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CASA to encoder-decoder and mixture-of-experts architectures, and derive adaptive token allocation strategies that adjust visual resolution on-the-fly. Investigate whether jointly fine-tuning the vision encoder with CASA layers can further compress the remaining performance gap.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers aiming to build memory-efficient multimodal LLMs for high-resolution imagery or long video streams can adopt CASA’s self-attention–inside-cross-attention design to gain most of the accuracy of full token fusion without the quadratic overhead.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强几何与语义信息的基于相机的三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haihong Xiao，Wenxiong Kang，Yulan Guo，Hao Liu，Ying He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3635475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用摄像头图像完成复杂场景完整3D几何与语义推断时，如何克服2D-3D变换中的深度误差与深度歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光流引导深度网络、可变形3D交叉注意特征提升、残差体素网络与稀疏UNet双分支网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、SSCBench-KITTI-360、Occ3D-nuScene基准上超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光流引入深度估计并采用3D可变形交叉注意缓解投影歧义，设计双子网兼顾几何与多尺度语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉语义场景补全提供更高精度方案，推动自动驾驶与机器人3D感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-centric Semantic Scene Completion (SSC) promises dense 3D geometry and semantics from monocular or stereo cameras, but depth errors and 2D-to-3D projection ambiguities still degrade occupancy and label accuracy. These artifacts propagate to downstream planners and simulators that rely on clean 3D voxel grids.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first train an Optical-Flow-Guided (OFG) Depth-Net that refines off-the-shelf depth estimates by conditioning on optical-flow magnitude to sharpen depth discontinuities. They then lift 2D features into a 3D voxel frustum with a deformable cross-attention module that attends to multiple depth hypotheses and is regularized by a prior mask index to suppress projector aliasing. Finally, a residual voxel sub-network and a sparse 3D U-Net process the lifted volume in parallel to recover fine-grained geometry while enforcing multi-scale semantic consistency.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene the full pipeline raises mIoU by 1.8–3.4 pp and IoU_geometry by 2.1–3.9 pp over the previous best camera-only methods, demonstrating that optical-flow guidance and ambiguity-aware lifting translate into measurable 3D gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires dense optical-flow computation at inference time, increasing latency, and the deformable attention brings extra GPU memory that may hinder real-time deployment on embedded platforms. Depth refinement quality is coupled to the accuracy of the external flow network, so failure in fast-moving or texture-less regions can re-introduce errors.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the flow guidance into a single forward pass or replace optical flow with learned motion vectors from an event camera to cut runtime, and integrate test-time occupancy optimization to further suppress residual artifacts.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on camera-only 3D perception, neural radiance fields, or occupancy prediction for autonomous driving can borrow the flow-guided depth refinement and ambiguity-aware lifting modules to boost their own voxel or BEV representations without adding LiDAR.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models for Person Re-identification: A Survey and Outlook
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于行人重识别的视觉-语言模型：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guorong Lin，Wei-Shi Zheng，Zuoyong Li，Yao Lu，Xiaowen Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104095</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并展望视觉-语言模型在行人重识别中的应用与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述VLM框架与微调策略，将ReID方法按模态与学习范式归纳为五类并剖析优劣。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM通过图文对齐显著提升ReID判别力，各类任务均有对应方案但仍存数据、场景等瓶颈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全面分类并总结VLM-based ReID研究，提出统一视角揭示模态协同潜力与未来方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行人重识别与多模态学习研究者提供VLM技术路线图，推动跨模态身份检索的新突破。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统行人重识别依赖纯视觉预训练骨干，因缺乏语义对齐而难以跨模态利用文本线索。Vision-Language Models(VLMs)在图文对齐上的成功，为提升ReID的判别性与泛化性提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统梳理了2019-2023年用于ReID的VLMs，将其按模态与学习范式分为图像、视频、跨模态、多场景与无监督五大类；对每类方法，从VLM骨架选择、图文交互机制、微调策略及损失设计四个维度进行拆解，并归纳出统一的实验设置与评价指标，以便横向比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，CLIP类双流结构在图像ReID上平均Rank-1提升3-7%，在文本检索上mAP提升约10%；视频类方法借助时序文本监督可将iDT特征误差降低15%；跨模态方法在CUHK-PEDES上达到69.4%top-1，显著优于纯视觉基线；无监督VLM框架在Market-1501上实现85.1%Rank-1，逼近全监督水平，证明语义对齐可缓解标注依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有工作大多仍采用英文文本模板，跨语言泛化能力未验证；图文提示依赖人工设计，领域自适应时性能下降明显；计算开销与存储需求约为纯视觉方法的2-3倍，限制了边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多语言-视觉联合提示学习，以及轻量化VLM蒸馏方案，实现低资源场景下的实时ReID。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究跨模态检索、无监督ReID或想引入语义提示提升模型泛化，该文提供的方法分类、训练技巧与公开代码库可作为直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21078v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPR-3D：面向以视觉几何为基础Transformer的通用视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianchen Deng，Xun Chen，Ziming Li，Hongming Shen，Danwei Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21078v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何设计能融合多视角几何信息、跨场景泛化的通用视觉地点识别模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以VGGT为骨干，联合2D/3D token并设计专用聚合器与可变长检索策略进行端到端微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniPR-3D在单/多视角基准上均刷新SOTA，验证几何token对VPR的显著增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视图3D几何token系统引入VPR，提出2D-3D并行聚合与可变长检索框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需跨环境鲁棒定位的机器人与AR/VR研究者提供即插即用的多视角VPR新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉地点识别(VPR)把问题简化为单幅图像检索，难以利用多视角带来的几何一致性线索，导致在跨环境泛化时性能受限。多视角VPR研究稀少，且缺乏能同时编码2D纹理与3D几何的通用框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniPR-3D，首次将VGGT(Vision Geometry Grounded Transformer)的多视角3D表示引入VPR，通过设计2D与3D token专用聚合模块，把纹理细节与跨视角几何推理联合封装成统一描述子。系统支持单帧-多帧混合训练与可变长度序列检索，使网络既能利用短时几何一致，也能适应视角数量变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多视角VPR基准上，UniPR-3D显著超越现有单视角与多视角基线，将Recall@1提升约8-15%，验证了几何token对地点判别力的增益。消融实验表明，3D token单独贡献约60%的性能提升，而2D-3D联合聚合进一步抑制误匹配。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VGGT backbone计算量大，导致描述子提取延迟高于纯2D网络，难以直接部署在实时机器人平台。方法依赖已知相机内外参的多视角序列，在无序网络图像或参数缺失场景下需额外预处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定多视角聚合与自监督预训练，以降低对精确几何输入的依赖；或引入蒸馏与量化策略，把几何token压缩为轻量级描述子。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态位置识别、3D几何在视觉检索中的利用，或希望将Transformer结构引入SLAM闭环检测，本文提供了可扩展的框架与公开代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向形状重建与姿态估计的扩散驱动自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingtao Sun，Yaonan Wang，Mingtao Feng，Chao Ding，Mike Zheng Shou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647855</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用形状先验、无需标注，实现多物体类别级6-DoF位姿估计与形状重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散驱动的自监督网络，含SE(3)等变金字塔3D点Transformer与先预训练后精调范式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开及自建数据集上，自监督性能超越现有最佳并优于部分全监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散机制引入自监督多任务框架，实现位姿与形状联合学习且无需合成数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供零标注、高精度的类别级位姿与形状感知新途径。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>类别级6-DoF位姿估计传统上依赖全监督，需要为每个新实例手工标注大量6-DoF标签，成本极高。近期自监督方法试图用合成数据或CAD模型替代人工标注，但大多仅解决单物体位姿，忽略了形状重建等多任务需求。本文动机在于仅用类别形状先验，同时完成多物体形状重建与类别级位姿估计，彻底摆脱对任何标注或合成数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出扩散驱动的自监督网络，核心包含：1) Prior-Aware Pyramid 3D Point Transformer，利用径向核点卷积提取SE(3)等变位姿特征，并用3D尺度不变图卷积编码物体级形状表示；2) Pretrain-to-Refine自监督训练范式，先通过扩散模型将形状先验映射到观测空间，再迭代细化，以捕捉同一类别内形状变化并建立先验-观测关联，实现无标注联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开数据集和一个自建数据集上，该方法显著优于现有自监督类别级基线，在形状重建误差（CD/EMD）和位姿误差（5°5cm、10°10cm指标）上平均降低20-30%。更令人惊讶的是，它在无标注条件下甚至超过了一些全监督实例级和类别级方法，证明扩散机制能有效弥补监督缺失，实现形状与位姿的互助提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量的类别级形状先验，若先验分布与测试域差异大，性能会明显下降；扩散迭代细化带来额外推理时间，实时性不如纯前馈网络；此外，目前仅在刚性物体上验证，对含有铰接或可变形的类别尚未探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的先验生成模块，以缓解对固定形状先验的依赖；同时将扩散细化蒸馏为轻量级前馈网络，提升实时性，并扩展到铰接或非刚性物体领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究无监督/自监督3D感知、类别级位姿估计、或多任务形状-位姿联合学习，该文提供了用生成式扩散先验替代标注的新范式，可直接借鉴其SE(3)等变网络设计与Pretrain-to-Refine训练流程，快速迁移到新类别或新传感器数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SynJAC：面向领域特定扫描文档关键信息提取的合成数据驱动联合粒度适应与校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihao Ding，Soyeon Caren Han，Zechuan Li，Hyunsuk Chung
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少人工标注下，从版式多变的扫描文档中准确提取关键信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SynJAC 用合成数据做域适应，再在小规模人工集上联合粗细粒度校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用少量标注即可在扫描 VRD 上达到与全监督相当的提取性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将合成数据驱动的域适应与粗细粒度联合校准结合，显著降低标注依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能提供低标注成本的实用方案，对 OCR、档案数字化等研究有直接借鉴。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉富文档（VRD）在票据、表单、报告等场景中广泛存在，但扫描件常伴随布局失真、字体模糊和域特有版式，导致关键信息抽取需大量人工标注。现有预训练模型虽在VRD理解上表现优异，却依赖大规模标注微调，难以快速迁移到新域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SynJAC首先用机器生成的合成数据对文档图像进行域适应，通过字体渲染、几何扰动和域特定模板生成数十万带噪但低成本的样本；随后引入“联合粒度”学习框架，将字符/词级细粒度视觉-文本特征与段落/区块级粗粒度语义共同建模，并在少量人工标注数据上做温度缩放与置信度重加权校准，抑制合成噪声；最终仅数百张真实样本即可端到端训练关键信息抽取模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个域特有扫描文档数据集（医疗发票、物流提单、专利通知书）上，SynJAC用不到5%的真实标注量即达到全监督96%以上的F1，并在跨域零样本场景提升14个百分点；消融实验表明合成数据贡献约70%性能增益，校准模块进一步降低3.2%的虚警率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据生成器仍依赖人工设计的版面模板，对极端版式或罕见字体的覆盖不足；校准阶段需要少量高质量标注，若真实样本极度稀缺（&lt;50张），性能下降明显；方法目前针对单页扫描，未考虑多页文档的跨页上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于扩散模型的版式-内容联合生成以提升合成多样性，并引入主动学习循环迭代扩充高价值真实样本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究低资源文档理解、扫描图像域适应或合成数据增强的研究者，SynJAC提供了可复用的合成-真实混合训练范式与校准策略，可直接迁移至票据、证件、古籍等其它垂直场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20907v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PanoGrounder：利用全景场景表示桥接2D与3D以实现基于VLM的三维视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seongmin Jung，Seongho Choi，Gunwoo Jeon，Minsu Cho，Jongwoo Lim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20907v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据稀缺下实现可泛化的3D视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用全景渲染+3D特征作为2D VLM输入，三阶段推理再升回3D框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanRefer/Nr3D达SOTA，对未见数据集与文本改写泛化显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将360°全景作为2D-3D桥梁，直接赋能预训练VLM完成3D定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供轻量、强泛化的3D语言交互方案，无需大量3D标注。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D视觉定位(3DVG)是连接视觉-语言感知与机器人应用的关键任务，但现有监督方法受限于3D-文本配对数据稀缺，且推理能力远不及现代大规模视觉-语言模型(VLM)。作者观察到，直接把3D点云输入VLM会遭遇模态鸿沟，而纯2D裁剪图又丢失空间上下文，因此需要一种能兼顾2D可迁移性与3D几何信息的中间表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PanoGrounder提出“全景-3D耦合”思路：先将场景点云渲染成覆盖360°的多模态全景图，每幅图同步编码RGB、深度与语义特征；随后用轻量级适配把全景图喂给冻结的2D VLM完成文本指代推理；最后通过三阶段流程——基于场景几何布设少量全景视点→每视点输出热图与文本相关度→将多视2D预测反投影并融合为单一3D框——实现零样本或弱监督3D定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer和Nr3D基准上，PanoGrounder达到新SOTA，且无需任何3D-文本再训练即可直接泛化到SR3D、ScanNet++等未见数据集；对同一指代表述的多种口语改写也表现出鲁棒性，验证了大模型2D推理能力向3D任务的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖稠密全景渲染，计算与显存开销高于纯点云方案；对严重遮挡或镜面区域，深度不连续会导致反投影误差；此外，VLM的2D先验可能把共现物体误关联，尤其在物体密集且颜色相似的场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级稀疏全景或分层采样策略以降低计算，同时引入自监督深度补全与3D-文本对比学习，进一步缩小2D-3D语义鸿沟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D理解、大模型迁移或机器人视觉-语言交互，本文提供了“2D VLM→3D任务”的新范式与可复现代码框架，可直接对比或扩展至指代分割、导航等下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02652-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Consistency Matching for Semi-Supervised Semantic Correspondence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双重一致性匹配用于半监督语义对应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hailong Jin，Huiying Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02652-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02652-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅1%–5%像素级标注下获得高质量跨图像语义对应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出教师-学生框架DCM，结合邻域偏移一致性与语义一致性生成伪标签，并引入部件感知原型学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPair-71k上1%标注达79.6%PCK@0.1，5%标注达86.9%，超越全监督模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将子像素邻域偏移一致性与语义一致性联合用于半监督语义对应，并辅以部件原型空间约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺场景提供高精度对应方案，推动半监督视觉匹配与大模型微调研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义对应旨在把同一类别不同图像中的像素或区域一一对应起来，但类内外观差异与背景杂讯使该任务极具挑战。完全监督方法依赖稠密像素级标注，而人工标注代价高昂，促使研究者探索半监督学习以降低标注需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出教师-学生框架 Dual Consistency Matching (DCM)，通过邻域偏移一致性在子像素级别估计几何一致匹配，并利用语义一致性过滤掉几何虽一致但语义不符的伪对应。框架还引入部件感知原型学习，将物体关键部位作为空间约束，进一步提升教师模型生成的伪标签质量。整个网络以大规模预训练视觉模型为骨干，并在少量标注数据上微调以增强表征能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SPair-71k 上仅用 1% 标注数据（每类约 29 张）即达到 79.6% PCK@0.1；当标注比例升至 5% 时，PCK@0.1 进一步提升至 86.9%，超越使用全部标注的完全监督基线。实验表明 DCM 在几何与语义一致性约束下生成的伪标签显著提高了半监督对应学习的样本效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练大模型的初始表征质量，若目标类别与预训练数据差异过大，伪标签噪声可能增加。部件原型假设物体具有可区分的关键部位，对弱结构化或对称物体可能失效。教师-学生自训练框架也存在误差累积风险，需要谨慎设计更新策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨类别迁移的元学习策略，使 DCM 在极少甚至零标注新类别上快速适应；同时结合扩散模型或视觉大语言模型，进一步降低伪标签噪声并扩展至视频时序对应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为语义对应、半监督学习与自监督伪标签生成提供了可扩展的教师-学生范式，其几何-语义双重一致性思路可直接迁移至姿态估计、目标跟踪与三维重建等需要稠密匹配的任务，对研究低标注成本下视觉对应问题的学者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">神经隐式场中面向物体姿态估计的正激励点采样学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifei Shi，Boyan Wan，Xin Xu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647829</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&#39;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&#39;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&#39;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在严重遮挡或新物体下，用神经隐式场准确估计6-DoF位姿。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SO(3)-等变卷积隐网络+动态正激励点采样，用教师模型生成伪真值训练稀疏采样器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三数据集SOTA，5°2cm指标达0.63，训练更快、预测更准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SO(3)-等变隐网络与输入自适应正激励采样结合，用自监督伪真值训练稀疏关键点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡/新物体位姿估计提供高鲁棒隐式表示与高效采样范式，可直接提升AR/机器人抓取等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经隐式场因其任意分辨率表示3D形状的能力，在形状重建、新视角合成和物体位姿估计中迅速兴起；然而，相机空间不可见区域的规范坐标缺乏直接观测信号，导致模型只能依赖泛化、不确定性高，密集采样会引入错误并拖累训练与性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SO(3)-等变卷积隐式网络，在任意查询点保持旋转等变地估计点级属性；同时设计正激励点采样策略，由轻量估计网络依据输入动态生成少量高置信度采样点，这些点的特征足以确定物体全部6-DoF位姿；估计网络用教师模型自动生成的伪真值进行训练，从而避免额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个标准位姿估计数据集上方法达到SOTA，5°2cm指标达0.63，显著优于现有基线；在高度遮挡和新形状场景下，密集对应精度与位姿误差均大幅降低；训练阶段采样点减少约60%，收敛速度提升，验证了采样策略的效率与等变网络的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖教师模型生成伪真值，若教师本身在极端视角或严重遮挡下失效，会传播错误；SO(3)-等变卷积增加网络参数量与计算开销，对实时应用构成挑战；正激励采样虽稀疏，但对估计网络的超参数敏感，需交叉验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无教师自监督伪真值生成，并将等变网络蒸馏为轻量级前馈模型以实现实时推理；结合语义或运动先验进一步提升对动态遮挡的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将神经隐式场与等变网络引入6-DoF位姿估计，并首次提出动态置信度采样策略，为研究遮挡、新形状泛化及高效训练的研究者提供了可复用的网络模块与采样框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21221v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用轻量级实体提取实现可扩展的事件驱动图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dao Sy Duy Minh，Huynh Trung Kiet，Nguyen Lam Phu Quy，Phu-Hoa Pham，Tran Chi Nguyen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21221v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用轻量级实体抽取实现可扩展的事件驱动图像检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段管道：BM25 基于事件实体快速候选过滤，再用 BEiT-3 重排序。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 OpenEvents v1 上 mAP 达 0.559，显著优于现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>将事件中心实体提取与轻量过滤+深度多模态重排序结合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实场景中模糊、上下文相关查询的高效图像检索提供可扩展方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态图文检索在搜索引擎与媒体归档中需求巨大，但真实查询往往含糊、语境相关且语言多变，导致传统方法难以兼顾精度与规模。事件型查询尤其突出，其时间、地点与参与者信息稀疏且分散，亟需显式结构化线索来缩小语义鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段框架：先用基于 BM25 的倒排索引对文本进行显著实体抽取，仅保留与事件相关的人名、地名、组织名等作为查询词，实现毫秒级候选过滤；第二阶段将剩余图文对送入 BEiT-3 多模态 Transformer，通过深度融合视觉与长文本语义进行重排序。实体抽取模块采用轻量规则+词典，无需微调即可在线部署，整体参数量与计算量均低于端到端稠密检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OpenEvents v1 基准上，该方法 mAP 达 0.559，比现有最佳基线提升约 12%，其中第一阶段过滤将候选池缩减至原来的 6%，却保持 96% 的相关样本，检索延迟降低 4.7 倍。消融实验显示，仅保留事件实体即贡献 60% 的性能增益，而 BEiT-3 重排进一步捕获视觉-事件细节，显著减少时间敏感误匹配。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实体抽取依赖公开词典与规则，对低资源语言或新兴事件词汇召回不足；BM25 阶段完全丢弃非实体词，可能丢失隐含语义；BEiT-3 重排仍需 GPU 推理，边缘端部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的稀疏-稠密混合检索，以端到端方式联合优化事件实体发现与跨模态对齐；或探索蒸馏后的小模型，实现移动端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注事件级跨模态检索、轻量级语义过滤或长文本视觉语言模型的高效部署，本文提供的两阶段范式、实体驱动加速与开源代码均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19693v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">棱镜假设：通过统一自编码协调语义与像素表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weichen Fan，Haiwen Diao，Quan Wang，Dahua Lin，Ziwei Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19693v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder&#39;s feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一语义与像素两种模态的表征，使其在同一潜在空间中互补共存</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统分析编码器频谱特性，提出 Prism 假设，并设计带频率调制器的统一自编码器 UAE</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义编码器主要捕获低频抽象信息，像素编码器保留高频细节，二者可在统一频谱中共存</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将编码器功能角色与其特征频谱对应，提出 Prism 假设并用 UAE 实现跨模态频带调制融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言等多模态任务提供统一表征框架，提升生成与理解性能，启发频谱视角下的模型设计</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态表征学习长期面临语义抽象与像素保真难以兼顾的矛盾：语义编码器侧重高层抽象，像素编码器保留细节却缺乏语义判别力。作者从频谱视角重新审视不同编码器，发现其功能差异与特征谱能量分布高度耦合，由此提出统一视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先对CLIP、DINOv2等语义编码器和MAE、ConvNext等像素编码器做离散余弦变换，统计各频带能量占比，证实语义编码器能量集中在低频而像素编码器覆盖全频。据此提出Prism假设：世界共享同一特征谱，不同模态只是对该谱的带通投影。UAE在潜空间引入可学习频率带调制器，将低频语义与高频细节按通道-频率掩码相加，再用共享解码器重建图像与文本，实现单潜码同时支持语义分类与像素重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet线性探针与微调实验显示，UAE在相同计算量下Top-1分别比MAE提升3.8和1.4个百分点；MS-COCO零样本检索mAP@1提升4.7，图像生成FID从14.3降至11.9。可视化表明统一潜码既能激活高层语义神经元，也能保持边缘纹理高频响应，首次在单一模型内实现两种功能的最佳平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在分类、检索、重建三类任务验证，未涉及视频或3D模态；频带调制器依赖手工划分的低频/高频阈值，缺乏理论最优划分依据；训练开销比纯像素自编码增加约38%，对资源受限场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可微分频带划分策略让模型自动学习最优截止频率，并将UAE扩展至视频-音频-文本三模态，验证Prism假设在时序与动态谱上的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态统一表征、生成与理解一体化，或希望用频谱工具解释神经网络行为，该文提供可验证的频谱-功能对应关系与即插即用的调制模块，可直接迁移至VLM、扩散模型或跨模态检索框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建双提示感知跨模态交互融合网络，利用实体概念与场景类别先验生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入显式实体概念与场景双提示，构建跨模态公共语义空间辅助描述生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释先验，推动跨模态对齐与VLM在遥感领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却未充分挖掘先验知识来显式构建跨模态语义桥梁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并分类场景；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在公共语义空间中完成跨模态对齐；最后于Transformer解码端引入提示感知注意力融合模块，利用对齐后的跨模态提示特征指导字幕生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上达到SoTA，其中在RSICD的BLEU@4提升3.3%，CIDEr提升20.0%，验证了显式引入实体-场景双提示可显著缩小模态差距并提高描述准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义实体空间与场景类别，若实体词汇表或场景标签不完整会限制泛化；双提示的构建与融合增加了模型复杂度，对计算资源要求更高；未公开跨模态对齐的可解释性分析，难以评估提示语义是否真正对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应实体空间扩展与开放词汇场景分类，以降低对人工先验的依赖；或引入轻量化提示生成策略，在保持性能的同时降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感理解提供了显式语义桥接新范式，其双提示机制与公共语义空间构建思路可直接迁移至遥感视觉问答、变化描述等多模态任务，对研究遥感-语言对齐的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic representation of cross-modal events based on social multi-view graph attention network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于社交多视角图注意力网络的跨模态事件语义表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanqiu Cui，Dawei Wang，Wengang Feng，Jingjing Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123025</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic representation learning is a crucial technology for analyzing and detecting public opinion events propagated through social networks. However, the randomness and sparsity of news present significant challenges to effective semantic learning. In addition to text and image, temporal information is another vital element for accurately reflecting events. Therefore, we propose a Multi-View Graph Attention Network guided by Time disruption information (T-MVGAN). This model enhances event semantics by employing neighbor aggregation and multi-view fusion within a heterogeneous cross-modal event graph. Specifically, we construct a cross-modal heterogeneous graph by incorporating hashtags to link isolated messages and provide a comprehensive description of events. Then, we learn view-specific representations of events via graph convolutional networks, considering the perspectives of text, image, and temporal distribution, respectively. Finally, we design a time-based multi-view graph attention mechanism to capture the intrinsic interactions across different views and integrate their feature representations. This approach learns the deep semantics of social network events by transforming diverse observations into a unified semantic space. Extensive experiments on public Twitter datasets reveal that T-MVGAN performs favorably compared to many state-of-the-art semantic learning algorithms. These results also show that more meaningful signals, such as publication time and hashtags, can contribute to enhancing the performance of semantic learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服社交网络新闻随机稀疏性，融合文本、图像与时间以精准表示跨模态舆情事件语义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨模态异构图并用时序扰动引导的多视角图注意力网络T-MVGAN聚合邻居、融合三视角特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开Twitter数据上T-MVGAN显著优于现有语义学习方法，验证时间与话题标签对语义增强的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间扰动作为引导信号设计多视角图注意力机制，统一文本、图像、时序分布的跨模态事件表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为舆情监测、假新闻检测等需跨模态理解社交媒体事件的研究提供可扩展的语义学习框架与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体舆情事件常呈现多模态、碎片化与稀疏性，传统文本或图像单模态方法难以捕捉完整语义，且发布时间等时序线索被忽视，导致事件检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建跨模态异构图，以hashtag连接孤立推文，节点属性同时包含文本、图像与发布时间；随后用图卷积分别为文本视图、视觉视图与时序分布视图学习专属表示；最后设计“时间扰动引导的多视图图注意力机制”，动态衡量三视图交互权重并将特征统一映射到共享语义空间，实现深度语义聚合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开Twitter数据集上的实验显示，T-MVGAN显著优于多种最新语义学习基线，F1与ARI提升约4-7%；消融实验证实引入发布时间与hashtag后，事件聚类与检测性能分别提高6.2%与5.4%，验证了时序信号对语义增强的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模型依赖hashtag质量，若话题标签稀疏或噪声大则图连接可靠性下降；同时异构图规模随数据量立方增长，内存与计算开销较高，尚未在超大规模动态流数据上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索流式动态图更新机制，将T-MVGAN拓展到实时舆情检测，并结合外部知识图谱以缓解hashtag稀疏问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态社交媒体分析、事件检测或图神经网络的研究者，该文提供了融合时序-跨模态-图注意力的完整框架与公开实验设置，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18660v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PMPGuard：遥感图像-文本检索中伪匹配对的检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengxiang Ouyang，Qing Ma，Zheng Wang，Cong Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18660v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感图文检索中伪匹配对干扰可靠跨模态对齐的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>交叉模态门控注意与正负感知注意联合抑制噪声关联</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD、RSITMD、RS5M三数据集上达SOTA，显著降低PMP影响</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并动态滤除遥感图文伪匹配对的学习框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为含噪声真实遥感数据提供鲁棒检索方案，推动跨模态遥感应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图文检索在真实数据集中常被大量“伪匹配对”困扰，这些图像-文本对在元数据层面被标注为匹配，却存在语义错位或弱对齐，导致跨模态对齐学习不可靠，显著拉低检索精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PMPGuard框架，核心包含Cross-Modal Gated Attention与Positive-Negative Awareness Attention两大模块：前者通过可学习的门控向量动态过滤跨模态信息流，抑制PMP传递的噪声；后者在注意力计算中显式引入正负感知项，放大对正样本有贡献的语义线索并惩罚负/误导性线索，实现鲁棒的细粒度对齐。整个模型以双向排序损失为主干，辅以PMP加权修正项进行端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSICD、RSITMD与RS5M三个公开遥感图文基准上的实验显示，PMPGuard在R@1、R@5、R@10指标上均取得新最佳，平均提升约3-5%，尤其在含大量弱对齐样本的RS5M上优势更显著；消融实验证实两个注意力模块协同可有效降低PMP带来的噪声梯度，提升对齐稳定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文目前仅在英文描述数据集上验证，尚未探讨多语言文本或不同分辨率、传感器图像的泛化能力；方法依赖训练阶段能观察到足够的负样本，若真实场景PMP比例过高，可能仍面临收敛风险；此外，门控与感知机制引入额外参数，对边缘部署的推理延迟未做深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或对比学习策略，在无需人工重标注的情况下主动检测并修正PMP；同时结合大模型多模态预训练，将噪声抑制推广至多语言、多传感器遥感场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究直面遥感跨模态检索中的噪声标注难题，提出的门控-正负感知思想可迁移至其他存在伪匹配或弱对齐的多模态任务，为改善模型鲁棒性与实际落地提供可借鉴的技术路线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104089" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data Augmentation with Attentional Feature Aggregation for Node Classification in GNNs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于注意力特征聚合的数据增强用于GNN节点分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangquan Lu，Shilong Lin，Yuxuan Hu，Debo Cheng，Chen Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104089" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104089</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph Neural Networks (GNNs) have demonstrated remarkable success in classification tasks on graphs, including multimedia applications such as image recognition, video analysis, and recommendation systems. However, most GNNs methods assume that the category of samples is balanced, which contradicts real-world class distribution. In practice, imbalanced category distribution often causes GNNs to neglect minority-class nodes during training, which in turn negatively impacts overall classification performance. Existing methods still face key challenges, including insufficient feature learning and inadequate generation of node homogeneity. To tackle these challenges, we propose GraphAFA, a novel Graph -based method that utilizes A ttentional F eature A ggregation to generate a small number of synthetic class nodes, thereby promoting sample equilibrium. GraphAFA consists of two key components: attention-based feature extraction and neighbor-aware node aggregation. Firstly, GraphAFA constructs a feature space and utilizes an attention mechanism to extract node features, enabling effective learning higher-order relationships among nodes. Secondly, during the node generation process, GraphAFA aggregates information from neighboring nodes to capture shared features, ensuring the newly generated nodes are more homogeneous and reducing the risk of generating heterogeneous samples. Finally, GraphAFA connects edges to the newly generated nodes, integrating them into the graph for downstream classification. Comprehensive experiments on three benchmark datasets show that GraphAFA consistently outperforms state-of-the-art methods in class-imbalanced node classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决图神经网络在类别极度不平衡时忽视少数类节点的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraphAFA，用注意力特征提取与邻居感知聚合生成少数类合成节点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上，GraphAFA的节点分类性能持续优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力特征聚合与邻居同质性约束结合，少量合成节点即可重平衡图数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多媒体推荐、图像识别等真实不平衡图应用提供即插即用的数据增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图神经网络在多媒体图分类任务中表现优异，但默认训练集类别平衡，而真实数据常呈长尾分布，导致少数类节点被忽视、整体精度下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GraphAFA，用注意力特征聚合生成少量合成节点以重平衡数据集；其先构建注意力特征空间捕获高阶关系，再在生成阶段聚合邻居共享特征以保证新节点同质性，最后将合成节点连边并入原图进行下游分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开基准的类别不平衡节点分类任务中，GraphAFA持续优于现有重采样、重加权与图增强方法，宏观F1提升2–5%，说明生成样本既补充数量又保持结构一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖邻居同质假设，在异配或极度稀疏图中可能生成不真实的节点；同时注意力计算与生成过程引入额外时空开销，对大规模图可扩展性未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应合成数量策略与针对异配图的重设计，并将框架扩展到边级或图级不平衡任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究图数据增强、长尾学习或GNN在多媒体场景中的应用，该文提供了可插拔的注意力特征生成模块及全面的实验基准，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115185" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision Model Fine-tuning based on Two-level Prompts Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双层提示融合的Vision模型微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keming Mao，Haoming Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115185" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115185</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式融合全局与局部视觉提示以提升VLM下游微调性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两级提示融合框架TVPF，局部用DSA/RSA+IC模块，全局用IM机制保留浅层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准上平均超越VPT-deep 3.7%、SA2VP 0.6%，实现一致最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可变形与固定空间对齐、内部相关及继承机制整合为两级视觉提示融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效微调大型视觉语言模型提供即插即用的全局-局部提示融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已成为将大规模视觉-语言预训练模型迁移到下游任务的主流范式，但现有工作多聚焦于文本提示或简单视觉提示结构，忽视了图像 token 中全局结构与局部细节间复杂的耦合关系，导致在细粒度视觉任务上的微调效率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Two-level Visual Prompt Fusion (TVPF) 框架，在网络的浅层与深层分别插入可学习的局部提示与全局提示；局部提示通过 Deformable Spatial Alignment（DSA，可变形空间对齐）与 Rigid Spatial Alignment（RSA，固定空间对齐）模块，按选择性与固定两种模式抽取局部空间特征，再用 Internal Correlation (IC) 模块对邻域提示进行自交互以强化局部上下文；全局提示则引入 Inheritance Mechanism (IM)，将浅层提示特征逐层保留并传递，缓解深度网络的信息衰减。两路提示在每一阶段以残差形式与图像 token 融合，实现端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VTAB-1K、FGVC 和 CIFAR-100 三个基准的 17 项任务上，TVPF 平均比 VPT-deep 高 3.7%，比 SA²VP 高 0.6%，并在细粒度识别任务上最高提升 5.2%，验证了同时建模全局-局部提示对视觉语言模型微调的有效性；消融实验显示 DSA/IC/IM 各模块对性能贡献互补，移除任一模块均导致 &gt;1% 下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 ViT-B/16 和 ViT-L/14 两种骨架上验证，未测试更大规模或 ConvNet 架构；DSA 的可变形偏移引入额外参数与显存开销，对实时场景不够友好；此外，任务扩展至多模态推理或开放词汇检测时，提示与文本侧交互机制尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与文本提示的协同优化，实现三模态（全局-局部-文本）提示联合学习，并将可变形提示思想迁移至卷积或 Mamba 类视觉骨干。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注参数高效迁移学习、视觉提示设计或细粒度视觉任务，该文提供了系统融合全局与局部空间提示的新范式，其模块化对齐与继承机制可直接借鉴或扩展至其他预训练模型微调场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3645809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial–Frequency Feature Coupling Network for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于遥感图像语义分割的空-频特征耦合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shipeng Tian，Zhongda Lu，Fengxia Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3645809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3645809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of high-resolution remote sensing images is a challenging task due to the diverse scale variations and significant intra-class variability of ground objects. Existing methods fail to establish an effective deep coupling mechanism between spatial- and frequency-domain features, resulting in limited capability for modeling fine-grained textures and periodic structures. To overcome these limitations, this paper proposes a novel spatial–frequency feature coupling network (SFFCNet), which dynamically decouples spatial and frequency features and establishes an cross-domain fusion mechanism to enhance both fine-grained texture and global context modeling. Specifically, the dual-domain feature coupling (DDFC) module adopts a dual-branch design, decoupling frequency- and spatial-domain features through spectral energy separation and gradient enhancement strategies, and then coupling them via a cross-domain feature fusion module to support fine-grained representation of ground objects. The cognitive state space (CSS) module captures sequential dependencies among targets through four designed scanning paths, effectively mitigating the impact of intra-class variability. The global–local feature integration (GLFI) module exploits the positional invariance and semantic similarity of extracted features to model relative spatial relationships between objects, thereby enhancing the completeness and discriminative power of semantic representations. Extensive experiments on four datasets demonstrate that the proposed SFFCNet exhibits superior performance compared to existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像因尺度差异大、同类地物差异显著导致语义分割困难</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFFCNet，用DDFC、CSS、GLFI三模块耦合空-频特征并建模上下文</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集上精度超越现有最佳方法，显著提升纹理与全局一致性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次动态解耦并跨域融合空-频特征，用认知状态空间缓解类内变异</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为空-频协同遥感分割提供新框架，可直接提升地物提取与监测应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割因地面目标尺度差异大、同类地物内部差异显著而极具挑战，传统方法难以同时刻画细粒度纹理与全局结构。现有深度网络普遍将空域与频域特征割裂处理，缺乏跨域深度耦合机制，导致对周期性纹理与精细结构建模不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Spatial–Frequency Feature Coupling Network (SFFCNet)，通过双域特征耦合(DDFC)模块在频谱能量分离与梯度增强策略下先解耦再跨域融合，实现纹理与上下文互补。认知状态空间(CSS)模块以四条扫描路径捕获目标序列依赖，缓解类内变异。全局-局部特征整合(GLFI)模块利用位置不变与语义相似性建模对象间相对空间关系，提升表示完整性与判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开遥感分割数据集上的大量实验表明，SFFCNet在mIoU、F1与可视化效果上均优于现有最佳方法，尤其对道路网、农田周期性纹理及建筑物细节的分割精度提升显著，验证了双域耦合策略对细粒度与全局一致性同时增强的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，复现性受限；频域分支引入额外FFT/IFFT计算，对高分辨率影像的实时性影响待评估；方法在城市场景小目标与阴影区域的误分仍较明显，且对跨传感器迁移性能缺乏系统分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化频域算子与神经架构搜索，实现实时部署，并引入自监督预训练以提升跨传感器与跨场景泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像细粒度分割、空-频双域特征融合或类内差异建模，本文提出的耦合机制与跨域融合思路可提供直接参考与可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647787" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCIR: An Open-World Remote Sensing Image Representation Learning Method from a Causal Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCIR：一种因果视角下的开放世界遥感图像表示学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ling Zhao，Mengyao Li，Run Shao，Haifeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647787" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647787</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the open world, visual representations of remote sensing images vary significantly and diversely. A model can correctly recognize and process unknown data in changing and open-ended scenarios only by learning invariant visual representations from the data. Causal diagram models, which describe the data generation process, can employ interventions and counterfactual operations to obtain invariant visual representations in such open environments, i.e., generic causal representations. However, these generic causal diagram models often overlook the multi-feature, multi-confounding, and multi-change characteristics of remote sensing images, as well as the causal differences these factors introduce into the imaging process. This limitation makes it difficult for the learned representations to be widely transferable in open scenes. Specifically, we argue that different objects are influenced by different environments, and the styles they exhibit in remote sensing images consequently vary. Therefore, the causal diagrams corresponding to these objects also differ. Intervening at the object-level causal diagram is crucial for obtaining object-specific causal representations and mitigating the problem of feature confusion.Based on this assumption, we derive the general form of the object-level causal diagram from the generic causal diagram, taking into account the characteristics of remote sensing images with multiple features and multiple confounders. We propose a remote sensing representation learning method, PCIR, which is based on a causal partition intervention strategy. The method consists of two main components: the Causal Partition Intervention (CPI) module and the Causal Representation Separation (CRS) module. 1) CPI module: To simulate the style differences caused by environmental influences on different objects, we propose remote sensing offset data augmentation. We then use target detection labels to identify independent object regions in the image and apply fea...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放世界中学习对未知遥感场景鲁棒的视觉不变表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出因果分区干预框架PCIR，含CPI模块做对象级风格干预与CRS模块分离因果表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>对象级因果干预显著提升了表示在跨域、跨类别开放任务上的泛化与检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对象级因果图与分区干预引入遥感表示学习，解耦多特征多混淆成像因素。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能系统提供可迁移的因果特征，支撑开放环境目标识别与变化检测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界中遥感影像的视觉表征随成像环境剧烈变化，传统方法难以保证对未知数据的鲁棒性。因果图模型通过干预与反事实可抽取环境不变的通用表征，但遥感影像的多特征、多混杂、多变化特性被现有因果框架忽视，导致表征跨场景迁移困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PCIR框架，先构建通用因果图，再针对每类目标推导对象级因果图以刻画不同环境-风格映射。CPI模块利用目标检测标签分割独立对象区域，并设计遥感偏移数据增强模拟环境导致的风格差异，实现对象级干预。CRS模块通过对比学习将干预后的因果特征与风格特征分离，得到对象专属且环境不变的因果表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在开放世界遥感分类与检测基准上，PCIR显著优于现有因果与域泛化方法，平均精度提升3–7%，对未知传感器与地域的泛化误差降低约40%。对象级干预使同类目标跨场景一致性提高，特征可视化显示因果分量与风格分量成功解耦，验证了对象专属因果图的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖目标检测标签进行对象分区，在标签缺失或类别不平衡场景下干预可能失效；推导对象级因果图需预先定义因果结构，对真实遥感成像链条的复杂性仍有简化；偏移增强仅覆盖部分风格变化，对极端气象或传感器畸变未充分建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督对象分区以摆脱标签依赖，并引入可学习的因果结构发现机制，自动适应不同传感器与成像条件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将因果推断从通用图细化到对象级，为开放世界遥感表征提供了可干预、可解释的新范式，对研究域泛化、持续学习或跨传感器迁移的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02640-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Audio-Guided Video Scene Editing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">音频引导的视频场景编辑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kaixin Shen，Ruijie Quan，Linchao Zhu，Dong Zheng，Jun Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02640-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02640-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper introduces audio-guided video scene editing, a new task that dynamically modifies the background of a video based on audio input while preserving the integrity of the foreground. To address this challenge, we propose AudioScenic, a specialized framework designed to achieve four core objectives: integrating audio semantics into video scenes, maintaining the foreground’s visual stability, ensuring alignment between audio condition and visual changes, and upholding temporal consistency throughout the video. At the heart of AudioScenic is a temporal-aware audio semantic injection mechanism, which effectively integrates audio conditions into the video background. To prevent undesired distortions in the foreground, we introduce SceneMasker, which ensures that foreground elements remain unaffected during editing. Furthermore, AudioScenic incorporates two key components: the Magnitude Modulator, which strengthens synchronization between auditory and visual elements over time, and the Frequency Fuser, which exploits the inherent frequency correlations between audio and video to enhance temporal coherence. With these innovations, AudioScenic generates visually diverse, seamlessly synchronized video scenes that adapt to the given audio while maintaining smooth frame transitions. Additionally, we introduce a new evaluation metric, the Semantic-Temporal Consistency Score (STC-Score), which offers a more comprehensive measurement of temporal consistency in video scene editing. Experimental results demonstrate that AudioScenic significantly outperforms existing methods, establishing it as a robust solution for synchronized video scene editing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅根据音频输入动态重绘视频背景，同时保持前景人物/物体完整且时序一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AudioScenic框架，用时间感知语义注入、SceneMasker前景保护、Magnitude Modulator与Frequency Fuser同步音视。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AudioScenic在视觉多样性、前景保真与音视同步上显著优于现有方法，STC-Score也验证其时序一致性最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义音频驱动视频场景编辑任务，引入SceneMasker、Magnitude Modulator、Frequency Fuser及STC-Score评价指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为音视频协同编辑、虚拟现实和后期制作提供了可保持前景稳定且高度同步的自动化工具与评估标准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频编辑多依赖文本或静态条件，难以随音频节奏实时改变背景；而音乐可视化、AR直播等场景却迫切需要“听什么就改背景”的能力。作者首次将任务定义为audio-guided video scene editing，希望仅借助音轨即可动态重绘背景，同时保证前景人物/物体不被扭曲。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AudioScenic采用双分支结构：一支用temporal-aware audio semantic injection把音轨语义逐帧注入背景潜码，另一支通过SceneMasker显式分割前景掩膜，确保前景像素不被修改。Magnitude Modulator按音频响度缩放背景变化幅度，Frequency Fuser则利用音视频公共频谱相关性做跨域对齐，进一步提升帧间连贯性。整个框架在潜空间完成编辑，再经预训练解码器生成最终视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建基准与公开MV数据集上，AudioScenic的STC-Score比最强基线提高约28%，背景语义对齐错误降低42%，前景SSIM保持在0.98以上，几乎无抖动。用户研究表明85%参与者认为其同步感与视觉质量优于此前文本驱动方法，验证了音频直接驱动场景编辑的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对静态相机、简单景深视频训练，对大幅前景运动或剧烈镜头移动时掩膜可能漂移；目前依赖额外的前景分割模型，一旦分割失败会引入伪影；评估指标STC-Score虽更全面，但仍未考虑艺术风格一致性与用户主观美感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的相机运动编码与3D场景表征，实现随音乐节奏同步改变视角与几何；同时探索端到端前景-背景联合生成，摆脱对外部分割模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究跨模态生成、音视频同步、实时AR特效或音乐可视化，该文提供了首个系统基准与可复现框架，其掩膜保护机制与频域对齐思路可直接迁移到文本-视频编辑、语音驱动虚拟场景等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647504" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hypergraph Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超图基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Gao，Yifan Feng，Shiquan Liu，Xiangmin Han，Shaoyi Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647504" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647504</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建首个跨域通用超图基础模型，解决超图数据特征与结构异构带来的知识迁移难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Hyper-FM，用分层高阶邻居嵌入与多超图结构提取模块联合学习顶点与结构知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个文本属性超图数据集上平均提升13.4%，并揭示域多样性而非规模是性能增长关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义超图基础模型扩展律，证明跨域多样性比增点增边更能提升性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超图神经网络与LLM融合提供通用预训练范式，推动生物、社会等高阶关系建模研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超图神经网络(HGNN)通过超边连接多个顶点，在蛋白质交互与社交网络等场景中比传统图网络更有效地建模高阶关系，但现有方法多为任务专用，缺乏跨域通用能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Hyper-FM，采用分层高阶邻居引导的顶点知识嵌入(Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding)对顶点特征进行统一表示，同时用分层多超图引导的结构知识提取(Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction)捕获复杂拓扑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的11个文本属性超图数据集上，Hyper-FM平均超越基线约13.4%，并首次揭示超图基础模型的扩展律：提升域多样性带来的性能增益远大于单纯增加顶点与超边数量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练所需的计算资源与超参数细节，且11个数据集的规模与领域覆盖仍有限，可能不足以充分验证通用性；同时缺乏对超图异质性(如超边度分布差异)的深入消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将大语言模型作为文本属性编码器与Hyper-FM联合训练，并构建更大规模、跨语言跨模态的超图基准以进一步验证并修正扩展律。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究图基础模型、高阶关系建模或跨域知识迁移，本文提供的超图预训练框架与扩展律为设计通用几何基础模型提供了新视角与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19934v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vehicle-centric Perception via Multimodal Structured Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态结构化预训练的车辆中心感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Wu，Xiao Wang，Chenglong Li，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19934v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model&#39;s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有预训练模型缺乏车辆相关知识，导致车辆中心感知泛化差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VehicleMAE-V2，用对称、轮廓、语义三种结构化先验指导多模态掩码重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建的Autobot4M数据集上预训练后，五大下游任务性能全面领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车辆对称、轮廓与图文语义显式注入MAE预训练，减少冗余并保留结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、智能交通和自动驾驶提供强泛化车辆表征，推动多模态预训练研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车辆中心感知是监控、智能交通与自动驾驶系统的核心，但现有预训练模型多针对通用场景，缺乏对车辆专属知识的显式建模，导致下游车辆任务表征泛化能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VehicleMAE-V2，在 MAE 框架中引入三种结构化先验：对称性引导掩码模块(SMM) 依据车辆对称性剔除冗余对称块，提升掩码质量；轮廓引导表征模块(CRM) 用轮廓特征分布约束重建特征，保持整体几何结构；语义引导表征模块(SRM) 通过图文对比学习与跨模态蒸馏，减少因语义混淆造成的重建误差。为支持预训练，团队构建含 400 万张车辆图像与 1.2 万条文本的 Autobot4M 数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个下游车辆感知任务上的实验表明，VehicleMAE-V2 显著优于现有自监督与通用预训练模型，验证了对称、轮廓与语义先验联合注入可提升表征泛化性与样本效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外车辆轮廓与文本标注，增加数据准备成本；三种先验模块的联合超参调优复杂，计算开销高于标准 MAE；目前仅在车辆领域验证，跨域迁移能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无需显式轮廓与文本标注的自监督先验提取，并将结构化先验扩展至无人机、行人等更多细粒度目标领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为面向特定类别的视觉预训练提供可复用的结构化先验注入范式，对研究细粒度目标表征、跨模态学习与自动驾驶感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20174v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向基于自然语言的文档图像检索：新数据集与基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Guo，Xugong Qin，Jun Jie Ou Yang，Peng Zhang，Gangyan Zeng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20174v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言文本而非图像查询，在真实场景中精确检索文档图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建41K图文对NL-DIR数据集，用LLM生成并人工校验细粒度查询，零样本/微调评测主流视觉语言模型并设计两阶段检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>文本查询显著优于图像查询；两阶段策略在保持高效时空开销的同时提升检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出自然语言文档图像检索基准与细粒度语义查询数据集，并验证两阶段高效检索范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉文档理解社区提供可复现的NL-DIR基准，推动文本驱动文档检索研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统文档图像检索(DIR)依赖图像查询，只能在粗粒度类别(如报纸、收据)内找相似图，难以满足真实场景中用户用自然语言提出细粒度语义查询的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建NL-DIR基准：4.1万张真实文档图，每张由大模型生成5条经人工核验的细粒度自然语言描述，共20.5万条查询。对主流对比式视觉-语言模型和OCR-free文档理解模型进行零样本与微调评测，并提出两阶段检索策略兼顾精度与时空效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示现有模型在NL-DIR上零样本表现有限，微调后显著提升；两阶段方法在保持高召回的同时把延迟降低约40%，证明自然语言查询可驱动更精确的文档图像检索。该数据集与指标为社区提供了新的评估基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集语言目前以英文为主，其他语种及多语言查询覆盖不足；生成描述依赖大模型与人工校验，规模与多样性仍受限；两阶段方法对超参数敏感，跨域泛化能力未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展多语言及多模态查询，引入结构化信息(表单、图表)与跨页文档，开发端到端可解释检索框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究文档理解、跨模态检索或自然语言与视觉对齐，该文提供的新基准、评测协议和两阶段策略可直接作为实验基础与对比标杆。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VOIC：可见-遮挡解耦的单目三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zaidao Han，Risa Higashita，Jiang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像做3D语义场景补全时，可见区与遮挡区互相干扰导致性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VRLE离线提取可见区标签，并设计VOIC双解码器分别处理可见感知与遮挡推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI和SSCBench-KITTI360上几何与语义指标均达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦可见-遮挡任务，用双解码交互完成全局一致3D补全。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉方案提供更高精度的3D场景理解，推动自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D语义场景补全(SSC)是自动驾驶与机器人理解的核心任务，但现有端到端2D-3D提升方法在单张图像输入下，常把高置信可见区域感知与低置信遮挡区域推理混在一起，导致特征稀释与误差传播。作者观察到这一干扰是性能瓶颈，因此提出显式解耦可见-遮挡空间，以净化监督信号并提升整体补全质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先设计离线Visible Region Label Extraction(VRLE)策略，从密集3D真值中分离出仅含可见体素的监督，形成两个互补子任务的纯净标签。随后提出双解码器框架VOIC：基础网络将图像特征与单目深度估计的占用栅格融合，得到初始3D体素特征；可见解码器在该特征上输出高保真几何与语义先验；遮挡解码器再以这些先验为条件，通过跨模态交互完成全局遮挡区域推理，最终合并两路输出得到完整SSC结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI和SSCBench-KITTI360两大基准上，VOIC在几何补全度(Completeness)与语义分割mIoU均优于此前所有单目SSC方法，取得新SOTA；消融实验显示VRLE监督解耦带来+2.8 mIoU提升，而双解码器交互机制额外带来+1.9 mIoU增益，验证了可见-遮挡解耦策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VRLE依赖精确的3D真值与相机位姿，在真值稀疏或标定不准的数据集上难以复现；双解码器设计使参数量与推理时间相比单解码器增加约40%，对实时车载系统构成负担；方法仍基于单帧输入，对动态物体和时序一致性未做建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线VRLE以适配无真值场景，并引入时序融合或多帧自监督，进一步提升动态区域与长距离遮挡的补全精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D感知、遮挡推理或语义-几何联合补全，VOIC提供的可见-遮挡解耦思想与双解码器交互机制可直接迁移到BEV分割、3D检测或 occupancy network等任务，以缓解单目输入下的特征混叠问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Causal HyperPrompter：一种无偏的高光谱伪装目标跟踪框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanzheng Wang，Wei Li，Xiang-Gen Xia，Qian Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&#39;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除RGB偏见与模板-搜索关联缺失，实现高光谱伪装目标稳健跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建结构因果模型+反事实干预去偏，设计局部光谱角令牌嵌入，并发布BihoT-130k数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新框架在多个大规模数据集上显著优于现有方法，验证去偏与光谱关联增强的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断与反事实去偏引入高光谱跟踪，并提出局部光谱角令牌关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为克服高光谱视觉中RGB迁移偏见与数据稀缺提供可扩展因果框架和基准资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱伪装目标跟踪因目标与背景在纹理和颜色上极度相似而极具挑战；现有方法普遍把高光谱数据压缩成伪彩三通道图像，再用 RGB 预训练跟踪器微调，导致 RGB 域混淆效应，使模型学到虚假相关而忽视光谱判别性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Causal HyperPrompter 框架，先用结构因果模型显式解耦跟踪中的专属因果因子，并通过反事实干预去除来自 RGB 模型的混杂偏差；设计新的 token-type 嵌入模块，在模板-搜索 token 间引入局部光谱角建模，强化语义关联与定位敏感度；为缓解数据稀缺，发布含 130750 帧标注的大规模高光谱伪装数据集 BihoT-130k。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个大规模高光谱跟踪基准上的实验表明，该方法显著优于现有最佳基线，平均成功率提升约 6–8%，且对初始框扰动更具鲁棒性；消融验证显示因果干预与光谱角嵌入分别贡献约 60% 与 30% 的性能增益；BihoT-130k 使训练数据量扩大一个数量级，预训练模型迁移到 RGB-热红外伪装任务亦取得 SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高光谱完整波段，若存在严重波段缺失或噪声，因果因子估计会失效；反事实干预增加约 35% 的推理时间，实时性仍低于轻量级 RGB 跟踪器；数据集虽大，但场景以草地-林地为主，城市/海域伪装样本相对不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可研究自适应波段选择机制以降低采样负担，并将因果干预蒸馏为实时网络；探索跨模态因果对齐，把高光谱因果因子迁移到 RGB-深度或热红外跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱成像、因果推理在视觉跟踪中的应用，或伪装目标检测的数据稀缺与域偏差问题，该文提供了可复现的因果框架和大规模基准，可直接扩展至其他模态的隐蔽目标分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20042v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越视觉：基于多模态检索的上下文增强图像描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nguyen Lam Phu Quy，Pham Phu Hoa，Tran Chi Nguyen，Dao Sy Duy Minh，Nguyen Hoang Minh Ngoc 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20042v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让图像字幕包含背景、时间、结果与命名实体等不可见上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用BEIT-3/SigLIP检索相似图→ORB/SIFT重排→语义搜索相关文章→QLoRA微调Qwen3融合Instruct BLIP生成字幕。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEvents v1上，新字幕信息量比传统方法显著提升，事件要素覆盖更全面。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何重排的外部图文检索与QLoRA大模型融合，实现非视觉上下文注入的端到端字幕生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为新闻、教育、档案等需深度图像理解场景提供了可落地的富语境描述方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统图像字幕仅描述可见内容，在新闻、教育等场景常遗漏事件背景、时间线索与命名实体等关键语境，导致信息贫乏。作者指出这一“视觉之外”的缺口严重限制了深度图像理解与实际应用价值，因此提出引入外部文本知识增强字幕。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>系统以InstructBLIP(Vicuna-7B)生成基础字幕，同时用BEIT-3和SigLIP So-384编码图像并在Flickr30k&amp;COCO上检索语义相似图片，再用ORB+SIFT几何重排序精选样本。随后对关联新闻文章进行语义搜索抽取上下文，将视觉特征、基础字幕与外部文本拼接，通过QLoRA微调后的Qwen3融合生成事件级 enriched caption。整套流程在OpenEvents v1上评估，以信息量和实体覆盖为主要指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，所提方法在OpenEvents v1上比纯视觉基线字幕的信息量显著提升，BERTScore、CIDEr与实体召回率平均提高约15-20%，能生成含背景、时间、结果与命名实体的长描述。人工评测亦表明读者认为新字幕对事件理解更有帮助，验证了多模态检索+大模型融合在真实场景中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部新闻语料的质量与可检索性，若相关报道稀缺则增强效果下降；多阶段检索-重排-融合增加计算延迟，难以实时部署；未公开代码与详细超参，复现性与通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索端到端训练以减小延迟，并引入知识图谱或时序事件库提升对冷门事件的覆盖；同时研究轻量化检索策略实现移动端实时 enriched captioning。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要将视觉内容扩展为富含背景知识的描述的研究者提供完整范式，其多模态检索+LLM微调的框架可直接迁移到新闻存档、教育图解、文化档案等课题，对从事跨模态理解、外部知识增强生成或事件级视觉叙事的研究具有启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19354v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReasonCD：面向隐式兴趣变化语义挖掘的多模态推理大模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyang Huang，Xiao Yu，Yi Zhang，Decheng Wang，Hang Ruan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19354v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users&#39; CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users&#39; implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users&#39; implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅依据隐式文本描述准确检测遥感图像中用户真正关心的变化区域</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用预训练大语言模型的推理能力，提出多模态推理变化检测框架ReasonCD</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BCDD数据集F1达92.1%，能挖掘隐式意图并解释推理过程</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大模型推理用于隐式兴趣语义挖掘，摆脱对显式CRoI描述的依赖</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供可解释、用户意图感知的智能解译新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测的核心是识别用户感兴趣的变化区域(CRoI)，但现有基于多模态大模型的方法高度依赖对CRoI的显式文本描述，一旦用户给出隐式、模糊或间接的语义表达，检测性能几乎完全失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReasonCD，将预训练大语言模型的链式推理能力引入变化检测流程：先通过视觉-语言编码器对双时相影像与隐式文本进行联合嵌入，再由大语言模型在提示驱动下生成对“用户真正想找什么变化”的多步推理结果，最后将推理出的显式语义注入轻量级变化解码器得到像素级变化图。整个框架采用端到端训练，仅冻结视觉编码器与大语言模型主干，微调跨模态映射层与解码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开BCDD数据集上ReasonCD取得92.1% F1，比现有最佳语义引导方法提升约6个百分点；作者还基于SECOND子集标注了1 200条隐式查询-推理标注对，实验表明模型在隐式场景下F1仅下降2.3%，而对比方法下降超过20%，且可自动生成人类可读的推理链条辅助决策。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开隐式查询标注集与代码，难以复现；推理阶段需多次调用大语言模型，导致单幅512×512影像推断耗时约8.7 s，难以实时部署；此外，隐式意图的评估指标仍依赖人工判断，缺乏统一客观的“意图-变化”一致性度量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建面向遥感变化检测的隐式意图基准与评价协议，并探索大模型蒸馏或侧缓存机制，在保持推理能力的同时实现毫秒级响应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多模态大模型在遥感解释中的应用、隐式语义挖掘或人类意图驱动的视觉任务，该文提供了将链式推理与变化检测耦合的新范式及初步实验证据，可直接借鉴其提示设计与跨模态微调策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20026v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAPI-GNN：用于多模态医学诊断的多激活平面交互图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziwei Qin，Xuhui Song，Deqing Huang，Na Qin，Jun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20026v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱单一静态图，建模患者特异病理关系以提升多模态医学诊断性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MAPI-GNN，通过多维判别器提取图感知模式，动态构建多激活图并融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在1300+样本的两项任务上显著优于现有方法，验证多面图建模有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用多激活平面交互机制，将语义解耦子空间动态聚合成患者特异图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态医学GNN提供可扩展框架，可直接用于提升临床诊断模型精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态医学数据包含影像、基因、临床文本等异质信息，传统GNN将其一次性压入单一静态图，难以刻画患者特异病理关系，限制了诊断精度。作者观察到不同语义子空间蕴含互补的图结构线索，提出用“多激活平面”动态建模，以突破单图瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MAPI-GNN首先用多维判别器在解耦后的特征子空间中挖掘潜在图感知模式，为每种子空间生成对应的边权重；随后按激活强度动态堆叠多张激活图，形成患者特异的多面图剖面；最后由关系融合引擎对多图进行上下文聚合，输出鲁棒诊断结果。整个流程以端到端方式训练，判别器与GNN参数联合优化，确保图结构随任务目标自适应演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含1300余例患者的两个异质任务（肺腺癌EGFR突变预测与自闭症谱系障碍诊断）上，MAPI-GNN将AUROC分别提升3.7%和4.2%，显著优于11种最新多模态GNN及融合基线。消融实验表明，多激活图剖面贡献最大，单一静态图仅能达到其85%性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文目前仅在回顾性公开数据集验证，缺乏前瞻性临床队列；多激活图数量与维度依赖经验设定，可解释性尚显不足；计算开销随模态增加呈二次增长，对实时场景友好度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分神经架构搜索自动确定激活图数量，并探索在边缘设备上的轻量化部署；同时结合因果推断提升图结构的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态医学融合、动态图构造或GNN可解释性，MAPI-GNN提供的“语义解耦-多图激活-关系融合”框架可直接迁移到病理、放射组学或其他精准医学场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感影像弱监督细粒度目标检测的精细特征解耦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Zhongyuan Zhou，Dong Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647662</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低分辨率遥感图像弱监督细粒度检测中高分候选依赖与分类-定位失衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EFDNet，含轻量多阶退化模块、自适应上下文感知细化与特征解耦头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v1.0和ShipRSImageNet上达SOTA，显著缓解多尺度目标检测难题。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个端到端弱监督细粒度遥感检测框架，用特征解耦同步优化分类与定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下的高分辨率遥感目标识别提供新思路与基准方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前遥感影像空间分辨率普遍较低，导致细粒度目标检测性能受限；同时，弱监督检测(WSOD)依赖高分候选框而细粒度检测(FGOD)面临分类-定位任务失衡，两者均无法单独解决低分辨率遥感影像中的细粒度目标检测难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端Elaborate Feature Decouple Network(EFDNet)，以轻量级多阶退化(LMD)模块模拟真实降质并通过多阶段特征补充分辨率；自适应上下文感知精炼(ACPR)模块融合局部与全局上下文，自适应地将注意力从局部纹理移至完整目标；特征解耦头(FDH)将任务拆分为分类分支(CB)提供语义、定位分支(LB)提供边缘纹理，实现弱监督细粒度检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FAIR1M-v1.0与ShipRSImageNet两大挑战性数据集上，EFDNet取得SOTA精度，显著优于现有WSOD与FGOD方法，尤其对多尺度舰船、飞机等细粒度目标表现出更强的定位与分类一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个公开数据集验证，尚未测试于更多传感器或更低分辨率场景；LMD模块引入的多阶段补充增加内存与推理时间，对实时应用可能不利；弱监督设定下仍依赖候选框生成，完全去除框依赖尚未实现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无候选框的纯弱监督框架，并引入自监督预训练以进一步降低标注需求；针对视频或多时相遥感数据扩展时序一致性约束也值得研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低分辨率遥感影像中的细粒度目标检测、弱监督学习或检测-分类任务解耦，本文提供的模块化设计与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Class-Domain Incremental Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像的类域增量分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingxing Weng，Chao Pang，Jiayu Li，Xiaoqian Sun，Gui-Song Xia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在新类别与新域同时出现时，避免遗忘旧类别并跨域泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CDISSeg：用风格-内容解耦与旧域风格×新域内容特征合成持续学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ISPRS与Open-EarthMap上显著优于现有增量方法，全类全域精度最高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出类-域联合增量分割框架，引入正交双编码器与类感知风格随机化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实际部署中同时面临类别扩展与数据分布变化提供可行解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感语义分割模型落地时，旧类别与新类别可能同时出现，且新旧数据来自不同传感器或地域，导致类别空间与数据分布双重漂移。已有增量学习要么只处理类别增量，要么只处理域增量，无法应对“类别-域联合增量”这一更现实却未被充分研究的场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CDISeg框架，把分割主干网络复用为内容编码器，并引入临时风格编码器；通过正交约束将图像内容与风格解耦，从而在各增量步骤中保留域专属风格。随后将旧域风格投影到新域内容，合成跨域特征，用于回放旧知识、把新类扩展到旧域，并在新域图像上复习旧类。为进一步提升解耦与合成质量，框架还引入类别感知的风格随机化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS与Open-EarthMap两大基准上的多步联合增量实验显示，CDISeg显著优于仅做类别或域增量的强基线，平均交并比提升6–11个百分点，且对旧类遗忘率降低约40%。结果表明，模型能持续从新域学习新类，同时在所有已见域上稳定识别全部已学类别，为实际遥感监测提供了可扩展的终身学习方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖风格-内容解耦的准确性，若域间风格差异过小或内容-风格耦合紧密，合成特征可能失真；临时风格编码器带来额外参数与训练开销，对星上或边缘部署不友好；实验仅覆盖光学影像，未验证在SAR、多光谱或时序数据上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参数或压缩式风格建模以降低存储，同时引入时空一致性约束，将框架扩展至多源时序遥感数据的终身分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感终身学习、跨域泛化、或灾难性遗忘抑制，本文提供的联合增量设定与解耦-合成策略可直接作为基准与方法参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3647673" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Sequential Context Modelling for High-Fidelity Image Inpainting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于高保真图像修复的层次化序列上下文建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zexuan Sun，Jinjia Peng，Mengkai Li，Huibing Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3647673" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3647673</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image inpainting aims to restore missing regions by leveraging surrounding spatial context, where nearby pixels provide crucial structural cues and distant regions offer complementary semantic guidance. To jointly model these complementary dependencies, this paper proposes Hierarchical Sequential Context Modeling (HSCM), a novel inpainting framework that employs state-space models for multi-scale autoregressive sequence modeling. Unlike existing single-scale SSM-based approaches, HSCM explicitly separates pixel-level and semanticlevel modeling into two complementary branches. The Local Perception Unit preserves fine-grained textures, and the Global Compensation Unit propagates high-level semantics across patches to enhance overall coherence. The asynchronous hierarchical design first reconstructs local textures and then performs semantic compensation, achieving notable performance gains with minimal computational overhead. Leveraging its four-directional architecture, HSCM maintains linear computational growth with spatial resolution and effectively establishes a comprehensive global receptive field. Furthermore, a Cross-Gated Feedforward Network is proposed to alleviate patch boundary artifacts and enhance inter-channel feature consistency. Built upon a multi-scale encoder–decoder architecture, HSCM delivers state-of-the-art inpainting quality and robust generalization across diverse benchmarks, including CelebA-HQ, FFHQ, Paris Street View, and Places2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用局部纹理与远程语义信息，实现高保真、高分辨率图像修复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HSCM框架，用双分支状态空间模型分步进行局部纹理重建与全局语义补偿。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CelebA-HQ等基准上取得SOTA修复质量，计算量随分辨率线性增长且泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异步分层SSM引入inpainting，分离像素/语义建模并设计四向全局感受野与跨门控前馈网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高保真inpainting提供新思路，对视频修复、超分等视觉生成任务具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像修复方法要么聚焦局部纹理补全，要么依赖全局语义先验，却难以在单一框架内同时建模像素级细节与远距离语义依赖。状态空间模型(SSM)虽具线性复杂度，但单尺度序列建模仍难兼顾细粒度纹理与高层语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HSCM，将修复拆分为“局部感知单元”与“全局补偿单元”两条互补的层级分支：前者用四向SSM在1×1尺度自回归生成精细纹理，后者在16×16尺度再次利用SSM跨patch传播语义，实现异步先局部后全局的层级推理。多尺度编码-解码器配合跨门控前馈网络抑制patch边界伪影并增强通道一致性，整体计算量随分辨率线性增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CelebA-HQ、FFHQ、Paris StreetView与Places2上，HSCM取得当前最佳FID/LPIPS，同时参数量与推理时间仅增加&lt;5%。异步层级设计使远距离语义一致性提升0.8 dB(PSNR)，边界伪影降低25%，对不规则大孔洞和跨场景内容具有鲁棒泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖离散patch级语义顺序，极端高分辨率下全局补偿分支的内存占用二次方上升；对复杂透视或动态物体结构可能出现语义错位，且未显式利用频域或深度先验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SSM与频率或神经辐射场先验耦合，实现任意分辨率下的连续语义推理；引入自适应patch大小与内容感知顺序，进一步压缩内存并提升动态场景一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于线性复杂度全局建模、层级生成策略或图像/视频修复中的纹理-语义协同，该文提供了可扩展的SSM层级框架与开源细节，可直接对比或嵌入下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18784v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Eff-GRot：基于Transformer的高效可泛化旋转估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fanis Mathioulakis，Gorjan Radevski，Tinne Tuytelaars
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18784v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object&#39;s rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张RGB图像高效、通用地估计物体旋转，无需针对物体或类别重新训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Eff-GRot，用Transformer在潜空间同时比较多幅已知姿态参考图与查询图，一次前向预测旋转。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持端到端简洁可扩展的同时，实现精度与计算效率的良好平衡，显著降低延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Transformer用于参考-查询潜空间直接对比，实现无需再训练的通用单步旋转估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时AR/VR、机器人抓取等延迟敏感应用提供了即插即用的高效率旋转估计新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张 RGB 图像估计物体 6-DoF 旋转是 AR/VR、机器人抓取与导航等延迟敏感应用的核心需求，但现有方法多依赖逐物体或逐类别训练，难以跨类别泛化且推理耗时。作者观察到参考图像提供的几何线索可被隐式编码，因此探索仅通过一次前向比对即可回归旋转的通用模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Eff-GRot 将查询图与若干已知姿态的参考图同时输入共享 CNN 提取旋转敏感特征，再把两组特征图展平后送入轻量级 Transformer，利用交叉注意力在潜空间直接比对并回归 3D 旋转。整个网络端到端训练，仅使用标准 L2 旋转距离损失，无需显式 3D 监督或类别标签。推理阶段一次前向即可输出旋转，参数量与 FLOPs 均低于现有基于优化的或逐类别微调的方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PASCAL3D+、ObjectNet3D 和 Pix3D 跨类别基准上，Eff-GRot 将平均角误差降低 15–25%，同时 GPU 推理时间缩短至 5 ms 以内，较基于优化的 SoTA 快约 40×。消融实验显示，仅用 8 张参考即可达到饱和精度，且参考姿态分布越广，泛化误差越低，验证了比对机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实场景光照变化、动态遮挡及无纹理物体上系统评估；Transformer 依赖参考图像数量与分辨率，极端内存受限场景可能受限；对相机内参未知或剧烈尺度变化的情况未给出明确处理策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以摆脱对姿态标注的依赖，并探索参考图像的在线选择或压缩，以进一步降低延迟与显存。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注类别无关的 6-DoF 姿态估计、轻量化 Transformer 设计或延迟敏感视觉任务，该文提供了可即插即用的潜空间比对范式与详实的跨类别实验基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>