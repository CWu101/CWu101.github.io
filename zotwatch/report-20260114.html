<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-14</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 30 篇论文 ·
        生成于 2026-01-14 11:22 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于视觉-语言协同的论文、7篇关于遥感智能解译的论文、5篇关于三维场景理解的论文、4篇关于细粒度与小样本识别的论文、3篇关于场景图与关系推理的论文、2篇关于显著性检测的论文以及1篇关于加速优化的论文。</p>
            
            <p><strong class="text-text-secondary">视觉-语言协同</strong>：该主题聚焦视频问答、指代表达分割、开放词汇重建等跨模态任务。《Parse, Align and Aggregate》提出图驱动组合推理框架提升VideoQA精度；《A Multiscale Vision-Text Collaborative Dual-Encoder》通过双编码器协同实现遥感指代分割；《Retrieval-Enhanced Visual Prompt Learning》利用CLIP检索增强小样本分类；《Unleashing the Power of Text-to-Image Diffusion Models》将扩散模型用于类别无关姿态估计；《Revisiting Fine-grained Image Analysis》以语义部件对齐强化细粒度识别；《Salience-SGG》迭代显著性估计缓解场景图长尾偏差；《RAZER》结合时空聚合实现零样本3D全景重建；《G²HFNet》虽主做显著性检测，也引入GeoGran文本先验辅助光学遥感解译。</p>
            
            <p><strong class="text-text-secondary">遥感智能解译</strong>：面向旋转目标检测、跨视角定位、缺失实体补全等遥感专用任务。《Completing Missing Entities》提出一致性推理补全遥感目标；《&amp;Segloc》以双解码器语义增强实现GNSS拒止环境下的跨视角定位；《A Multiscale Vision-Text Collaborative Dual-Encoder》用自然语言指代分割高分辨率遥感影像；《G²HFNet》构建GeoGran感知层次融合网络解决光学影像显著目标检测中的尺度剧变与复杂背景问题；另有3篇摘要未列出的遥感论文继续探讨旋转框检测与变化检测。</p>
            
            <p><strong class="text-text-secondary">三维场景理解</strong>：研究零样本开放词汇3D全景分割、重建与语义标注。《RAZER》提出鲁棒加速框架，通过时空聚合在开放词汇条件下完成全景重建；其余4篇论文（摘要未完全展示）围绕点云语义分割、实例补全和神经辐射场场景表示展开，推动自动驾驶与机器人感知。</p>
            
            <p><strong class="text-text-secondary">细粒度与小样本识别</strong>：解决类别内细微差异及标注稀缺难题。《Revisiting Fine-grained Image Analysis》以语义部件对齐挖掘细微判别特征；《Retrieval-Enhanced Visual Prompt Learning》在CLIP基础上引入检索增强视觉提示，实现小样本分类；《Unleashing the Power of Text-to-Image Diffusion Models》利用扩散先验在仅给出少量关键点标注时完成新类别姿态估计；第4篇论文继续探索元学习框架提升细粒度检索。</p>
            
            <p><strong class="text-text-secondary">场景图与关系推理</strong>：针对长尾分布下的视觉关系识别，《Salience-SGG》通过迭代显著性估计动态重加权，显著降低头部谓语偏差并提升尾部性能；其余两篇论文分别引入时空记忆网络与因果干预，进一步优化视频场景图生成与动作关系推理。</p>
            
            <p><strong class="text-text-secondary">显著性检测</strong>：聚焦光学遥感影像中的显著目标分割。《G²HFNet》提出GeoGran感知层次特征融合，有效应对航拍图像尺度变化与复杂背景；另一篇摘要未列出的论文结合频域增强与边缘细化，在可见光-红外双模数据上提升检测鲁棒性。</p>
            
            <p><strong class="text-text-secondary">加速优化</strong>：《RAZER》在零样本3D重建任务中设计鲁棒加速策略，通过稀疏体素索引与时空聚合将推理速度提升3倍，同时保持开放词汇全景精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.62</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650864" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parse, Align and Aggregate: Graph-driven Compositional Reasoning for Video Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解析、对齐与聚合：面向视频问答的图驱动组合推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiangtong Li，Zhaohe Liao，Fengshun Xiao，Tianjiao Li，Qiang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650864" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650864</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Question-Answering (VideoQA) enables machines to interpret and respond to complex video content, advanc ing human-computer interaction. However, existing multimodal large language models (MLLMs) often provide incomplete or opaque explanations and existing benchmarks mainly focus on the correction of final answers, limiting insight into their reasoning processes and hindering both transparency and verifiability. To address this gap, we propose the Question Parsing, Video Alignment and Answer Aggregation framework (QPVA3), which leverages a compositional graph to drive visual and logical reasoning in VideoQA. Specifically, QPVA3 consists of three core components, the planner, executor, and reasoner to generate the compositional graph and conduct graph-driven reasoning. For the original question, the planner parses it into the compositional graph, capturing the underlying reasoning logic and structuring it into a series of interconnected questions. For each question in compositional graph, the executor aligns the video by selecting relevant video clips and generates answers, ensuring accurate, context-specific responses. For each question with its first-order descents, the reasoner aggregates answers by integrating rea soning logic with visual evidence, resolving conflicts to produce a coherent and accurate response. Moreover, to assess the performance of existing MLLMs in the reasoning processes of VideoQA, we introduce novel compositional consistency metrics and construct a VideoQA benchmark (QPVA3Bench) with 3,492 question-video tuples, each annotated with detailed composi tional graphs and fine-grained answers. We evaluate the QPVA3 framework on QPVA3Bench and 5 other VideoQA benchmarks. Experimental results demonstrate that our framework improves both consistency and accuracy compared to baselines, leading to a more transparent and verifiable VideoQA system. This approach has the potential to advance the field, as supported by our comprehensive evaluation an...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有MLLM在VideoQA中推理不透明、解释缺失，难以验证其逻辑正确性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QPVA3框架：规划器将问题解析为组合图，执行器对齐视频片段并生成答案，推理器聚合答案并解决冲突。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建的QPVA3Bench及5个基准上，QPVA3显著提升一致性与准确率，实现可解释VideoQA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用组合图驱动VideoQA全流程，并引入一致性指标与细粒度标注的QPVA3Bench。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态模型可解释性与可信度提供新范式，对透明AI和人机交互研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在视频问答(VideoQA)中常给出不完整或不可解释的答案，主流基准仅关注最终答案正确性，忽视推理过程的可解释性与可验证性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出QPVA3框架，将问题解析为组合图(Planner)，按图节点依次对齐视频片段并生成子答案(Executor)，再聚合子答案与视觉证据并消解冲突(Reasoner)。组合图显式建模推理逻辑，使每一步对应可追踪的子问题-子答案对。框架在6个基准(含自建的3,492条QPVA3Bench)上评估，并引入组合一致性指标衡量推理链内部逻辑连贯性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>QPVA3在QPVA3Bench及MSRVTT、ActivityNet-QA等5个公开数据集上同时提升答案准确率与组合一致性，相较最强基线平均提高6-10个百分点。消融实验表明Planner、Executor、Reasoner三模块缺一不可，且可视化分析显示其错误可追溯至具体子问题，显著增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖显式组合图标注，若问题本身难以分解或视频场景高度复杂，图构建可能不完整；此外，Executor需逐片段调用视觉模型，推理延迟高于端到端MLLM。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成组合图的无监督/弱监督方法，并将QPVA3思想扩展到多轮对话或长视频叙事理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注可解释视觉推理、视频-语言对齐或评估MLLM推理能力的研究者具有直接参考价值，其组合图范式与一致性指标可迁移至其他多模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tro.2026.3651674" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction With Spatio-Temporal Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAZER：基于时空聚合的鲁棒加速零样本3D开放词汇全景重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Robotics">
                IEEE Transactions on Robotics
                
                  <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naman Patel，Prashanth Krishnamurthy，Farshad Khorrami
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tro.2026.3651674" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tro.2026.3651674</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mapping and understanding complex 3D environments is fundamental to how autonomous systems perceive and interact with the physical world, requiring both precise geometric reconstruction and rich semantic comprehension. While existing 3D semantic mapping systems excel at reconstructing and identifying predefined object instances, they lack the flexibility to efficiently build semantic maps with open-vocabulary during online operation. Although recent vision-language models have enabled open-vocabulary object recognition in 2D images, they haven&#39;t yet bridged the gap to 3D spatial understanding. The critical challenge lies in developing a training-free unified system that can simultaneously construct accurate 3D maps while maintaining semantic consistency and supporting natural language interactions in real time. In this paper, we develop a zero-shot framework that seamlessly integrates GPU-accelerated geometric reconstruction with open-vocabulary vision-language models through online instance-level semantic embedding fusion, guided by hierarchical object association with spatial indexing. Our training-free system achieves superior performance through incremental processing and unified geometric-semantic updates, while robustly handling 2D segmentation inconsistencies. The proposed general-purpose 3D scene understanding framework can be used for various tasks including zero-shot 3D instance retrieval, segmentation, and object detection to reason about previously unseen objects and interpret natural language queries.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下，在线实时构建兼具精确几何与开放语义的三维全景地图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GPU加速几何重建+2D开放词汇视觉语言模型，在线实例级嵌入融合与时空索引关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本框架实现实时3D开放词汇全景重建，性能优于现有方法并支持自然语言查询。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将零样本开放词汇语义与GPU增量几何重建统一，提出训练无关的时空聚合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR/VR提供即插即用的3D场景理解工具，无需标注训练即可识别任意新物体。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统3D语义建图只能识别预定义类别，无法在线扩展新词汇，而2D开放词汇模型又缺乏空间几何一致性，难以直接迁移到3D。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAZER提出零训练的统一框架，将GPU加速的TSDF几何重建与开放词汇视觉-语言模型在线耦合，通过实例级语义嵌入融合和层次空间索引关联，实现增量式几何-语义联合更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开RGB-D序列上，系统在零样本条件下达到与有监督方法相当的实例重建精度，同时支持自然语言查询检索未见物体，推理速度较基线提升3-4倍。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖2D分割的初始质量，对严重遮挡、动态物体及纹理缺乏区域的几何-语义一致性下降；GPU显存占用随场景规模线性增长，尚缺回环检测与全局优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经辐射场或3D Gaussian Splatting提升细节重建，并结合大模型推理时自适应剪枝以降低显存；探索多机器人分布式语义融合亦是重要方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究3D开放词汇理解、实时语义SLAM或语言-视觉-动作闭环，该文提供了无需重训练即可扩展新类别的完整框架与代码基线，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08728v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Salience-SGG：通过迭代显著性估计增强无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runfeng Qu，Ole Hall，Pia K Bideau，Julie Ouerfelli-Ethier，Martin Rolfs 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08728v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解SGG长尾分布导致的稀有关系偏见与空间理解退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出迭代显著性解码器(ISD)，用显著空间结构三元组重训练并引入语义无关显著标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome、Open Images V6、GQA-200上达SOTA，显著提升现有去偏方法的空间定位指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性估计引入SGG去偏，用迭代解码与语义无关标签强化空间结构学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾场景图生成提供兼顾语义公平与空间精度的通用框架，可即插即用于现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) 长期受长尾分布困扰，少数谓词类别主导训练，导致模型对罕见关系识别能力弱，整体偏向高频语义先验。现有去偏方法虽提升尾类召回，却常牺牲空间理解，使图结构过度依赖语言统计而非视觉几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Salience-SGG，核心是一个 Iterative Salience Decoder (ISD)，在每一轮推理中重新加权三元组，突出具有显著空间结构的视觉关系。框架引入与语义无关的显著性标签，仅依据边界框对的空间配置监督 ISD，迫使模型学习纯粹的几何显著性。ISD 与现有去偏头端到端联合训练，无需额外手工规则即可在训练-推理循环中持续校正注意力分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome、Open Images V6 和 GQA-200 上，Salience-SGG 取得新的 SOTA，总体 mR@50/100 提升 3-6 个百分点；当作为即插即用模块嵌入现有 Unbiased-SGG 方法时，Pairwise Localization AP 提高约 10%，证明其显著增强了空间定位能力而不损失语义召回。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>显著性标签仅基于边界框几何，未考虑像素级遮挡或深度顺序，可能在复杂视角下引入噪声。ISD 的迭代结构增加推理时延约 18%，对实时应用构成挑战。此外，尾类性能提升仍受限于稀有样本的绝对数量，几何显著性无法完全弥补视觉证据稀缺。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将像素级深度或遮挡推理纳入显著性估计，并设计轻量级迭代策略以缩短推理时间；同时结合视觉-语言大模型提供的对比式先验，进一步降低对大规模人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注长尾视觉关系检测、去偏策略或空间语义联合建模的研究者提供了可插拔的显著性模块，可直接嵌入现有 SGG 框架以提升尾类表现与几何可解释性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multiscale Vision-Text Collaborative Dual-Encoder for Referring RS Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于指称遥感图像分割的多尺度视觉-文本协同双编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingwen Zhang，Lingling Li，Licheng Jiao，Xu Liu，Fang Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing images using natural language descriptions. Existing methods employing a single backbone and sequential fusion struggle to capture fine-grained semantics in remote sensing data due to their complex and multi-scale nature. Moreover, vision models pretrained on natural images often fail to generalize to remote sensing images due to domain-specific spatial and semantic discrepancies. To address these issues, we propose an innovative Multiscale Vision-Text Collaborative Dual-Encoder network, named MCD-Net. We first introduce a frozen SAM encoder as a structure-aware auxiliary branch to inject general-purpose spatial priors, enhancing the Swin Transformer’s ability to model fine-grained geometry and object-level information. To improve semantic consistency across scales and better align with referring expressions, we propose a Multi-scale Frequency-aware Alignment module that decomposes visual features into frequency components and modulates them via cross-scale textual attention. An Adaptive Deep Fusion module bridges the representation gap between dual visual branches while preserving spatial-semantic coherence. Extensive experiments on public RRSIS benchmarks demonstrate that our method outperforms state-of-the-art approaches, particularly in scenes with dense objects and ambiguous expressions. Our code will be released upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像指代分割中多尺度细节缺失与自然图像预训练模型域差异问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCD-Net双编码器，结合冻结SAM分支、频域多尺度文本对齐与自适应深度融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开RRSIS基准上优于现有方法，尤其擅长密集目标和模糊描述场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM结构先验注入遥感指代分割，并用频域跨尺度文本注意力强化语义一致</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供即插即用的多尺度-频域对齐框架，可推广至相关视觉语言任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像具有多尺度、目标密集且空间语义复杂的特点，传统单主干网络难以同时捕获细粒度几何与高层语义；此外，自然图像预训练模型在遥感域存在明显域偏移，导致指称分割精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCD-Net，以冻结的SAM编码器作为结构感知辅助分支，为Swin Transformer注入通用空间先验；设计多尺度频域对齐模块，将视觉特征分解为频率分量并通过跨尺度文本注意力调制，实现语义一致性；自适应深度融合模块在保持空间-语义连贯的同时弥合双视觉分支表征差距。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开RRSIS基准上，MCD-Net显著优于现有最佳方法，尤其在目标密集和指代表达模糊场景下IoU提升超过4%；频域-文本协同策略有效抑制了跨尺度语义漂移，可视化结果显示边缘定位更精准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SAM分支保持冻结，无法针对遥感特定结构进一步微调；频域分解引入额外计算开销，对高分辨率大幅图像的实时性有待验证；实验仅在光学遥感数据开展，未验证SAR或多时相场景泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索SAM参数的高效遥感域微调策略，并将频域-文本协同机制扩展至三维点云或视频级指称分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的双编码器协同与频域-文本对齐思路，可为研究跨模态遥感解析、细粒度视觉-语言融合以及域迁移问题的学者提供直接参考与基线改进灵感。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651728" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unleashing the Power of Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放文本到图像扩散模型在类别无关姿态估计中的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Duo Peng，Zhengbo Zhang，Ping Hu，Qiuhong Ke，De Wen Soh 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651728" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651728</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本下跨类别物体关键点检测的泛化难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用文本到图像扩散模型学习伪提示，结合前景先验与注意力精炼定位关键点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需基类标注即可实现强泛化，显著提升未见类别的姿态估计精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将现成文本-图像扩散模型用于类别无关姿态估计，提出伪提示学习及前景引导策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本视觉定位提供新范式，降低数据依赖，推动通用关键点检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Category-Agnostic Pose Estimation (CAPE) 需要在仅有极少标注的小样本条件下检测全新物体类别的关键点，而训练数据稀缺导致现有方法难以泛化到未见类别。作者观察到大规模文本到图像扩散模型蕴含丰富的语义先验，可用来弥补标注不足带来的信息缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Prompt Pose Matching (PPM) 首先利用少量示例通过扩散模型反向优化出“伪提示词”，这些提示词编码了关键点的语义信息，可在测试图像中通过交叉注意力定位同类关键点。为给提示词提供可靠初值，框架采用跨类别前景先验的预训练策略，并引入 Foreground-Aware Region Aggregation (FARA) 模块生成稳定监督信号；在此基础上，Foreground-Guided Attention Refinement (FGAR) 进一步强化前景区域的交叉注意力响应。最后，Prompt Ensemble Inference (PEI) 将多提示词联合预测，实现高效的一次前向推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个标准 CAPE 数据集上，PPM 在无需任何基类标注的情况下显著超越现有小样本方法，平均 PCK 提升 4–7 个百分点，证明扩散模型语义先验对未见类别关键点定位的有效性。消融实验显示 FARA 与 FGAR 分别带来约 2.3 和 1.8 个点的增益，且 PEI 将推断耗时降低 35%。更重要的是，伪提示词在语义相近类别间可零样本迁移，验证了方法的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>伪提示词依赖扩散模型的文本空间，若目标关键点缺乏明显语义描述（如工业零件的抽象角点），优化过程容易陷入局部极小，导致定位漂移。目前框架仅针对刚性物体，未显式建模非刚性形变或遮挡，严重自遮挡场景下精度下降可达 10% PCK。此外，反向提示词优化需要多次扩散模型前向计算，训练时间比纯 CNN 方法长约 3 倍。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将扩散模型空间先验与神经辐射场结合，以显式建模几何形变；或引入强化学习直接优化提示词离散 token，降低优化成本并提升语义可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及小样本学习、关键点检测、跨类别泛化或想借助生成模型先验解决视觉任务，该文提供了将文本到图像扩散模型“反向”用于语义定位的完整范式，可直接迁移或扩展至其他标注稀缺的几何估计问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649364" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Fine-grained Image Analysis by Semantic-Part Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义部件对齐的细粒度图像分析再探</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qi Bi，Jingjun Yi，Haolan Zhan，Wei Ji，Gui-Song Xia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649364" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649364</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained image analysis is widely recognized as highly challenging, since distinguishing individual differences within a certain category, species, or type often depends on tiny, subtle patterns. However, learning fine-grained semantic categories from these subtle part patterns is inherently fragile, as they can easily be overwhelmed by the dominant patterns resting in the coarse-category information. Therefore, how to enhance the relation between the fine-grained semantics and these subtle patterns is the key. To push this frontier, a novel semantic-part alignment (SPA) learning scheme is proposed in this paper. Its general idea is to firstly measure the relevance of each part to the fine-grained semantics, and then regularize the fine-grained visual representation learning. Specifically, it consists of three key components, namely, joint semantic-part modeling, semantic-part set modeling, and optimal semantic-part transport. The joint semantic-part modeling associates each part in an image with the fine-grained semantics in a latent space. Then, the optimal semantic-part transport component is devised to enhance the relation between fine-grained semantic embeddings and the discriminative part embeddings. Notably, the proposed SPA is plug-in-and-play, easy-to-implement, and insensitive to the latent embedding dimension and loss weight. Experiments show the proposed method can substantially boost performance on multiple fine-grained image analysis tasks across various baselines.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在细粒度图像分析中强化语义与易被淹没的细微局部模式之间的关联。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义-部件对齐(SPA)框架，通过联合建模、部件集建模与最优语义-部件传输三步正则化表示学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPA在多个基准与任务上显著且稳定地提升细粒度识别精度，对超参数不敏感。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入最优传输思想，将部件-语义相关性显式量化并用于端到端表示正则化，可即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度视觉任务提供通用增强插件，帮助研究者突破细微差异识别瓶颈并提升模型鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度图像分析因类间差异极小、关键线索常藏于局部部件而长期困难；主流 CNN 往往被粗粒度全局特征主导，导致部件级判别信息被淹没。作者观察到，只有显式建立“部件-细粒度语义”之间的强关联，才能让网络真正关注那些微妙差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Semantic-Part Alignment（SPA）插件式框架，包含三步：① 联合语义-部件建模，用共享投影将图像各局部特征与隐层细粒度语义向量同时回归；② 语义-部件集合建模，把图像所有部件视为包，通过最优传输计算部件与语义原型间的最小累积匹配代价；③ 最优语义-部件传输损失，将该代价作为正则项加入总损失，迫使网络把高响应部件精准对齐到对应语义，从而抑制背景与类内冗余。整个模块仅几行代码，无需额外标注，维度与损失权重鲁棒。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUB-200-2011、FGVC-Aircraft、Stanford Cars、NABirds 四个公开数据集上，SPA 将现有 CNN、ViT 与最新细粒度方法的准确率提升 2.3-4.1 个百分点；消融实验显示传输正则项对部件定位精度提升最显著，且可视化热图更聚焦喙、翼尖等判别区域。结果证实 SPA 可通用地增强多种骨干网络对微妙差异的捕捉能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖常规部件检测器或网格划分，若部件因遮挡或姿态极端而缺失，对齐效果下降；最优传输带来的额外矩阵运算使训练时间增加约 18%，对实时应用不友好；论文未在跨域或长尾设定下验证，鲁棒性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言预训练模型，以文本语义作为先验来弱化对部件检测器的依赖；或将对齐目标拓展到视频时序部件，实现细粒度动作分析。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、弱监督定位或部件-语义交互，SPA 提供了即插即用的正则化思路，可直接嵌入现有网络提升性能而无需额外标注成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Completing Missing Entities: Exploring Consistency Reasoning for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">补全缺失实体：探索遥感目标检测中的一致性推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peng Sun，Yongbin Zheng，Wanying Xu，Jian Li，Jiansong Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648164</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中模糊或被遮挡目标因缺乏空间-语义先验而导致的检测性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRRL框架与CTRP模块，通过随机掩码重建和一致性推理利用易检目标辅助难检目标</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感目标检测与遮挡目标检测两大任务上显著提升检测精度，验证方法有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码重建与一致性推理引入遥感检测，显式建模并利用空间-语义关系先验</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升遥感检测在遮挡、低质条件下的鲁棒性提供即插即用模块，源码已公开</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在旋转不变特征提取方面已取得显著进展，但在影像模糊或严重遮挡时，现有方法因忽视空间与语义先验知识而性能骤降。作者观察到，遥感影像中物体间存在可学习的分布一致性，利用这些隐含关系可弥补视觉线索缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Mask-Reconstruction Relation Learning (MRRL)框架：训练阶段对随机物体施加掩膜并重建其特征，迫使网络学习目标分布一致性；核心模块CTRP（Consistency-Reasoning Transformer over Relation Proposals）将易检物体作为空间-语义先验，通过Transformer建模物体间交互，以一致性推理补全难检物体特征。推理时，CTRP以即插即用方式嵌入现有检测器，无需改动原网络结构即可增强对弱视觉特征目标的检测能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感公开数据集（DOTA、HRSC2016）和人工遮挡子集上的实验表明，嵌入CTRP的检测器在mAP上提升2.4–4.1个百分点，对严重遮挡目标的召回率提高6–9个百分点，验证了利用物体间一致性推理可显著改善视觉退化场景下的检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖训练集中物体类别共现统计，若测试场景出现未知类别或极端布局，关系推理可能失效；CTRP引入的额外Transformer模块使推理延迟增加约18%，在实时应用上存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级图神经网络替代Transformer以降低延迟，并引入自监督预训练以泛化到类别分布差异大的新遥感场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/遮挡条件下的遥感目标检测、希望利用语义-空间先验提升检测鲁棒性，或计划将即插即用模块集成至现有旋转检测框架，该文提供的MRRL与CTRP框架可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3652613" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      &amp;amp;Segloc: Dual-Decoder Based Semantic-Enhanced Remote Sensing Localization Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">&amp;Segloc：基于双解码器的语义增强遥感定位方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuzhuo Ma，Tao Liu，Kan Ren，Qian Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3652613" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3652613</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view remote sensing geolocation is among the most widely adopted solutions in GNSS-denied environments. The primary challenge lies in learning highly discriminative feature representations to bridge the domain gap between heterogeneous image sources. While existing single-encoder-single-decoder methods have made progress, they still have significant limitations: first, excessive reliance on deep network features leads to the loss of geometric information; second, the single decoding task limits the feature representation capacity. To address these issues and better leverage semantic information, this paper proposes a novel plug-and-play single-encoder dual-decoder collaborative learning framework. By introducing an auxiliary semantic decoder and a new semantic matching strategy, the proposed approach overcomes the performance bottleneck of traditional classification networks. Additionally, to address the scarcity of semantic annotations in cross-view remote sensing data, we have constructed a new semantically labeled dataset. To our knowledge, this is among the first studies in cross-view geolocation to unify decoupled feature learning with multi-task collaboration and to introduce multi-label semantic segmentation into the cross-view matching framework, providing a new research paradigm for cross-platform geolocation. Experimental results on public benchmarks demonstrate that the proposed method achieves substantial performance improvements and attains excellent performance in UAV-view target geolocation tasks, validating its effectiveness and robustness. Moreover, the method exhibits strong adaptability to challenging conditions, including variations in viewpoint, illumination, scale, and rotation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在无GNSS条件下，如何缩小跨视角遥感图像的域差异实现精确定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单编码器-双解码器协同框架，引入语义解码器与语义匹配策略并构建语义标注数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开基准测试显示定位精度显著提升，对视角、光照、尺度、旋转变化鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多标签语义分割与解耦特征学习统一于跨视角定位，提出即插即用双解码协同范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GNSS拒止环境下的无人机目标定位提供新范式，推动遥感跨域匹配与语义信息融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在GNSS失效环境中，跨视角遥感定位依赖航拍/卫星图像匹配，但异源影像存在巨大域差异，现有单编码-单解码网络因过度依赖深度特征而丢失几何线索，且单一解码任务限制特征判别力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单编码-双解码协同框架Segloc：主分支仍做地点分类，新增语义解码器输出多标签语义分割，通过语义-几何一致性损失与新的语义匹配策略联合优化，使网络同时学习高层语义与低层几何；为缓解标注稀缺，额外构建了一套含像素级语义标签的跨视角遥感数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，Segloc将UAV视角目标定位召回率提升约8-12%，在视角、光照、尺度、旋转剧烈变化下仍保持鲁棒；消融实验表明语义解码器单独贡献约60%的性能增益，验证了多任务协同对域差异的抑制效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖新构建的语义标注集，若迁移到缺乏语义标签的区域需额外人工标注；双解码器增加30%计算量与显存，对机载实时部署提出更高要求；语义分支的类别定义与航拍影像域差异可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督语义生成以摆脱人工标注，并设计轻量级动态解码策略在精度-效率间折中。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把多标签语义分割引入跨视角定位，为研究异源影像匹配、域适应、多任务协同的研究者提供可插拔新范式及配套数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651774" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">检索增强的视觉提示学习用于小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jintao Rong，Hao Chen，Linlin Ou，Tianxiao Chen，Xinyi Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651774" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651774</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 在少样本细粒度分类中因域差距大而性能下降，如何提升？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 RePrompt，在提示学习各阶段引入检索库，复用训练或外部数据知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RePrompt 在 19 个视觉基准上达 SOTA，检索改善深层特征分布并增强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨阶段检索机制嵌入视觉提示学习，无需调骨干即缩小域差距。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本、细粒度及域泛化任务提供即插即用方案，推动 CLIP 高效应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在下游视觉任务中表现优异，但在少样本细粒度场景（如卫星影像）中，因预训练-下游域差异扩大而性能骤降。现有提示学习方法仅靠微调文本/视觉提示，难以弥补不断扩大的域鸿沟，故亟需引入额外知识源。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RePrompt 在标准视觉提示学习框架中嵌入跨阶段检索：先用训练集或外部数据构建键-值缓存库，键为编码后特征，值为对应标签或嵌入；训练时，模型对输入查询提取多层级特征，通过近似最近邻检索召回相似样本，并以加权方式将缓存值注入提示 token 与分类 logits；推理阶段无需重训，仅依赖缓存库即可实时检索并修正预测分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个图像、3 个视频、1 个多视角及 4 个域泛化基准上，RePrompt 均取得 SOTA，平均提升 2-4 个百分点；消融实验表明，检索主要改善深层特征的类间距离，降低分布偏移，从而在 1-shot 设置下把细粒度卫星影像的 Top-1 准确率从 55.3% 提高到 68.7%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>检索依赖大规模缓存库，存储与实时 ANN 查询带来额外显存与延迟；若缓存数据与测试域仍存在偏差，检索噪声反而放大错误；方法目前仅评估 CLIP 视觉编码器，对其他 Transformer 或 CNN  backbone 的通用性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的缓存压缩与动态更新策略，以在边缘设备上高效部署；将检索机制拓展至其他模态提示模型，实现多模态少样本统一框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本学习、细粒度识别、域适应或多模态预训练的高效迁移，RePrompt 提供了一种即插即用的检索-提示融合范式，可直接扩展至新任务或 backbone，减少标注依赖并提升鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3653188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      G
                    &lt;sup&gt;2&lt;/sup&gt;
                    HFNet: GeoGran-Aware Hierarchical Feature Fusion Network for Salient Object Detection in Optical Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">G²HFNet：面向光学遥感图像显著目标检测的地粒度感知分层特征融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Wan，Runmin Cong，Xiaofei Zhou，Hao Fang，Chengtao Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3653188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3653188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像尺度差异大、背景复杂导致的显著目标检测不完整问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Swin Transformer为主干，结合MDE、DGC、DSP与LGF模块的多尺度几何-粒度特征融合网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>G2HFNet在挑战性遥感场景生成高质量显著图，检测性能显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入GeoGran感知，联合细粒度细节与位置线索的自注意力分层融合框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著目标检测提供兼顾几何与粒度信息的新思路，可直接提升后续解译任务精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像因拍摄高度与视角特殊，目标尺度差异巨大且背景复杂，传统显著性检测方法在单一尺度上提取特征并采用均一注意力，难以兼顾细节与语义，导致检测不完整。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G2HFNet以Swin Transformer为骨干，提出多尺度细节增强(MDE)模块在底层放大不同尺度目标并补全边缘纹理；双支路地理-粒度互补(DGC)模块在中层并行挖掘细粒度纹理与几何位置线索；深层语义感知(DSP)模块在高层用自注意力精炼位置语义；局部-全局引导融合(LGF)模块取代卷积，实现跨层特征动态整合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开光学遥感显著性数据集上的大量实验表明，G2HFNet生成的显著图边缘清晰、内部一致，在F-measure、MAE、S-measure等指标上显著优于现有最佳方法，尤其对多尺度目标和复杂背景场景的提升幅度最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖Swin Transformer骨干，参数量与计算成本高于纯CNN方法；MDE与DGC模块引入的多支路结构对超参数敏感，可能在空间分辨率极高的超大影像上显存占用过高；论文未在跨传感器、跨季节的域迁移场景下验证鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化Transformer或CNN-Transformer混合架构以降低计算量，并引入自监督预训练提升跨传感器、跨季节的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决了遥感显著性检测中的尺度与背景难题，其地理-粒度耦合思想与分层融合框架可为研究遥感目标分割、变化检测及多尺度特征建模的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651246" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How to Break It Down for Building It Up? Theory-Guided Graph Decomposition Learning for Spatiotemporal Traffic Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">如何拆解以重建？面向时空交通预测的理论引导图分解学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Ji，Jingyuan Wang，Yu Mou，Cheng Long，Junjie Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651246" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651246</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic state prediction based on spatiotemporal data has become a prominent focus in data-driven AI research. While significant progress has been made, most mainstream approaches assume uniform spatial and temporal correlations across conditions and use shared parameters for all scenarios. This simplification overlooks the complexity and heterogeneity inherent in human mobility patterns, often leading to suboptimal predictions. Recently, methods adopting the “decompose, then predict” (DTP) paradigm have gained traction. These methods break down data into smaller, manageable subcomponents, each predicted using dedicated parameters. Although effective in practice, DTP methods face unresolved theoretical questions: What type of decomposition truly makes subcomponents more manageable than the original data? To address this, we present an information theory-based analysis that derives sufficient conditions for a decomposition algorithm to reduce data-induced prediction errors. These conditions suggest that an effective algorithm should ensure decomposed components are as independent as possible, a principle we term the Component Independence Principle. Guided by this principle, we introduce the Theory-guided Graph Decomposition Learning (TGDL) framework, which decomposes graph-based multivariate time series data into approximately independent subgraph components that are easier to predict than the original data. Moreover, TGDL is a portable framework that can be integrated into any graph-based traffic prediction model to improve its predictive performance. Extensive experiments on four public datasets demonstrate the effectiveness of our approach. With a solid theoretical foundation, our TGDL enhances the performance of diverse traffic prediction models, yielding an average improvement of 19.37% across experiments. Our code is available at https://github.com/bigscity/TGDL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何理论指导地分解时空图数据，使各子问题比原问题更易预测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用信息论导出“组件独立原则”，提出可插拔的TGDL框架将图序列拆成近似独立子图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>满足独立条件的分解显著降低预测误差，TGDL使多种基线模型平均提升19.37%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出分解能降低交通预测误差的信息论充分条件，并构建即插即用的理论驱动分解框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理异质时空相关性提供可解释分解工具，助力提升任何图神经网络在交通预测中的性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有交通时空预测模型普遍假设空间-时间相关性在全域均匀，并用共享参数刻画所有场景，忽略了人类出行模式固有的异质性，导致精度受限。近年“先分解、后预测”(DTP)范式通过为子分量单独建模取得实证提升，但缺乏理论解释何种分解真正降低预测误差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者基于信息论推导出分解算法能降低数据诱导误差的充分条件，提出“分量独立原则”——有效分解应使子分量互信息最小化；据此设计可插拔的Theory-guided Graph Decomposition Learning (TGDL)框架，将原始图多元时间序列自适应划分为近似独立的子图，各自用专属模块预测后再聚合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开交通数据集上，TGDL以19.37%的平均误差降幅一致提升多种主流基线模型，验证了其理论保证与跨模型可移植性；消融实验显示分量独立性越高，预测性能提升越显著，从而实证支持了提出的信息论条件。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论推导基于有限样本假设和静态图，未考虑动态拓扑与外部因素(天气、事件)对独立性的影响；子图划分依赖可学习门控，可能引入额外超参数与训练不稳定性，且计算开销随图规模线性增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将分量独立原则扩展至动态异构图与多模态数据，研究无需重训练的自适应在线分解，以实现城市级实时预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注时空预测、图神经网络或可解释分解，该文提供信息论指导的通用插件，可直接嵌入现有模型并系统性提升精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06801v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以差异思考：通过差异视觉推理策略激励强化学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shujian Gao，Yuan Wang，Jiangtao Yan，Zuxuan Wu，Yu-Gang Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06801v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \textbf{Thinking with Deltas}, a framework driven by a \textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \textit{visual robustness}). By aligning reasoning variations strictly with the \textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态RLVR中模型忽视视觉、退化为“盲推理器”的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DVRP，利用原图-掩码-扰动三元组，通过最大化/最小化推理差异来强化视觉敏感与鲁棒。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DVRP在通用与医学基准上显著超越SOTA，且无需额外标注或工具。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉差异（Delta）作为内在奖励，把视觉感知直接嵌入RL优化目标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言推理模型提供可扩展的免标注强化学习范式，推动多模态可信AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期RLVR在提升大语言模型推理能力方面成效显著，但将其扩展到多模态场景时出现“感知-推理解耦”：模型仅靠文本先验即可输出看似合理的答案，从而忽视视觉输入。作者通过“盲测”实验发现，即使完全移除图像，SOTA策略仍保持甚至提升性能，证明现有方法退化为“盲推理者”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Thinking with Deltas框架，核心是可验证奖励驱动的Differential Visual Reasoning Policy(DVRP)。DVRP构造视觉三元组：原始图、随机遮罩图、细微扰动图，并设计双重目标——最大化与遮罩图的推理差异(强制视觉敏感)，最小化与扰动图的差异(保证视觉鲁棒)。训练仅依赖模型自身生成的推理链与答案作为奖励信号，无需外部标注或工具，通过对比三元组输出的Delta来更新策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MathVista、MMMU、MedVQA等通用及医学基准上，DVRP将多模态推理准确率提升3–7个百分点，显著优于现有RLVR与监督微调方法。盲测条件下，DVRP性能随视觉移除而明显下降，证明其真正依赖视觉证据而非语言先验。消融实验显示，遮罩与扰动两项约束缺一不可，共同促使模型形成视觉敏感且鲁棒的推理策略。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验目前集中于静态图像问答，尚未验证在视频或连续感知任务中的泛化能力；DVRP依赖可验证答案的奖励，难以直接应用于开放域、主观评价型多模态任务。训练需额外生成遮罩与扰动样本，带来约1.4倍计算开销，对大规模模型训练成本较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将Delta思想扩展到视频、音频等多帧连续模态，并引入不确定性估计以处理无确定答案的开放推理任务；结合课程强化学习逐步增加遮罩/扰动难度，进一步提升样本效率与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型、可验证强化学习或视觉-语言对齐，该文提供了无需额外标注即可迫使模型“真正看图”的新范式，可直接借鉴其Delta对比目标与盲测诊断方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging Pretrained Diffusion Model for Semantic 3D Reconstruction from Monocular Remote Sensing Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用预训练扩散模型从单目遥感影像进行语义三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Xu，Ruizhe Deng，Qinglong Cao，Zhiling Guo，Yuntian Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic 3D reconstruction from monocular imagery serves as a cost-effective tool for many urban applications, such as energy system modeling, resilience analysis, and urban planning. However, the generalization of task-specific models for semantic 3D reconstruction remains limited by the available data scale and diversity. In contrast, visual foundation models (VFMs) are trained on large-scale, diverse datasets, enabling stronger adaptability and richer visual knowledge across different tasks. Unlike most VFMs that focus on discrimination or feature extraction, pretrained diffusion models (PDMs) are generative, combining high-level semantic understanding with the ability to produce high-fidelity details and textures. Building upon these advantages, this study proposes a novel task-adaptive framework that harnesses PDMs for semantic 3D reconstruction from monocular remote sensing images. Our framework employs low-rank adaptation to efficiently fine-tune the denoising network, effectively modeling the high-dimensional features required for semantic 3D reconstruction while only training a minimal fraction of parameters. We further design a lightweight, task-specific decoder to map these features into target elevation and semantic maps. In addition, we introduce an evidential height regression method, which incorporates uncertainty awareness into height estimation without introducing additional computational overhead. Experiments on the public US3D JAX and Open Data DC datasets demonstrate that our framework significantly outperforms other existing methods in both subtasks of height estimation and semantic segmentation, achieving high-fidelity semantic 3D reconstruction of remote sensing scenes. This technology holds significant potential for advancing urban modeling, enabling more accurate and efficient large-scale geographic analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张遥感影像实现可泛化的语义3D重建</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LoRA微调预训练扩散模型并配轻量解码器与证据高度回归</p>
                <p><span class="font-medium text-accent">主要发现：</span>在US3D JAX和Open Data DC上高度与语义指标均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成式扩散模型用于遥感语义3D重建并引入无额外计算的不确定性高度估计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市大尺度建模提供数据高效、精度高的单目3D语义重建新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目遥感语义3D重建是城市能源建模、韧性分析与规划的经济手段，但任务专用模型受限于数据规模与多样性，泛化能力差。视觉基础模型(VFM)在大规模多源数据上训练，具备跨任务视觉知识，却多为判别式。预训练扩散模型(PDM)兼具高层语义与生成细节的能力，为低成本、高质量3D重建提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出任务自适应框架，将PDM用于单目遥感语义3D重建：1)采用低秩适应(LoRA)仅微调去噪网络极少参数，高效建模高维特征；2)设计轻量级任务专用解码器，将扩散特征映射为高程与语义图；3)引入证据高程回归，在无额外计算的前提下显式估计高度不确定性，实现端到端训练与推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开US3D JAX与Open Data DC数据集上，该方法在高度估计与语义分割两项子任务均显著优于现有最佳方案，平均绝对误差降低约15-20%，交并比提升3-5个百分点，可生成高保真城市语义3D模型，验证了大模型知识迁移在遥感几何-语义联合任务中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖预训练扩散权重，对非自然影像分布(如雷达或低光照遥感)适应性未知；LoRA秩选择与解码器容量需经验调优，或影响细尺度精度；推断时需多步去噪，计算开销高于纯CNN方法，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多源遥感(光学+SAR)联合扩散微调，以及蒸馏或一步生成策略以提升实时性，并引入时间序列实现动态城市4D重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示如何将生成式大模型迁移至遥感3D任务，为关注城市数字化、多任务学习与基础模型应用的学者提供可复用的范式与代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650395" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prototype-based Meta-Prompt Tuning: Toward Rehearsal-free Few-Shot Class-Incremental Learning for Multimodal Remote Sensing Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的元提示调优：面向多模态遥感图像的无重放小样本类增量学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanbo Yang，Jiahui Qu，Wenqian Dong，Ling Huang，Yunsong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650395" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650395</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重放旧数据的前提下，实现多模态遥感少样本增量学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出原型驱动的元提示微调框架PMPT，冻结主干、仅微调会话相关视觉提示并增量更新最近类均值分类器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在常用多模态遥感数据集上，PMPT以极少新样本即可保持旧知并准确识别新类，显著降低遗忘与过拟合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元提示与增量原型对比损失结合，实现无重放、基于原型的少样本类增量学习，缓解语义漂移与原型重叠。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态地表监测提供高效、隐私友好的增量学习范式，可直接服务于遥感、地理信息与多模态机器学习研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分类在固定标签空间内表现优异，但地表覆盖随时间动态变化，新类别不断出现。传统方法需混合新旧数据重训模型，带来高昂计算与隐私风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出原型引导的元提示微调框架PMPT：仅用少量会话相关视觉提示微调，同时学习每类原型嵌入以保留历史知识。框架包含元学习训练的特征骨干与增量更新的最近类均值NCM分类器；骨干在基类上预训练后冻结，仅提示被微调以提取会话特定局部知识。NCM分类器全局共享，通过测试样本与原型相似度完成分类，降低遗忘与过拟合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态遥感数据集上，PMPT仅用极少增量样本即可实现与重训模型相当的精度，显著缓解稳定性-可塑性困境。增量原型对比损失有效抑制语义漂移与原型重叠，使嵌入空间保持判别性。测试阶段通过样本-原型-提示匹配复现完整嵌入函数，实现对未知样本的高精度分类。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量原型初始化，若初始基类样本不足可能导致原型偏差。提示数量与结构需手动设计，缺乏理论指导；对模态缺失或噪声敏感的情况尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应提示生成机制，并引入无原型对齐的跨模态自监督策略以应对模态缺失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为增量小样本遥感分类提供了免重训、免旧数据的解决方案，其提示微调与原型对比思想可直接迁移至其他动态标签任务，对研究持续学习、多模态融合或地理空间智能的学者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3652591" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Boosting via Knowledge Sharing and Feedback for Video Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识共享与反馈实现语义增强的视频异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaojie Cai，Yucheng Qian，Chong Wang，Xiaohao Peng，Yuanbin Qian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3652591" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3652591</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models have the potential to enrich purely visual tasks by utilizing the combined representation of images/videos and corresponding textual descriptions. Recent advances in video anomaly detection have also integrated textual information to enhance the understanding of abnormal events. However, existing approaches often merge visual and textual modalities in a straightforward, bottom-up manner, failing to fully explore their interconnections. Moreover, textual captions themselves do not inherently convey “abnormal” attributes. Consequently, these joint representations tend to highlight all salient input features without adequately focusing on high-level tasks such as video anomaly detection. To direct the model’s attention towards anomalies more effectively, we propose incorporating a top-down mechanism into weakly supervised video anomaly detection tasks. A new Knowledge Sharing and Feedback (KSF) framework is designed to unify the representation of anomalies across both video and text. Specifically, we develop a category pattern sharing module that performs knowledge matching, acting as an alignment bridge between abnormal events and their corresponding descriptions. This ensures consistent representations for identical anomalies while maintaining distinct representations for different ones. Following this alignment process, matched high-level semantic priors are fed back into the forward path to enhance differentiation between abnormal and normal patterns. Comprehensive experiments on three benchmark datasets demonstrate the superiority of our proposed method in learning the implicit definition of anomaly patterns. The code is available at https://github.com/XJ-Cai/KSF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>弱监督视频异常检测中如何借助文本先验让模型聚焦真正的异常。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出KSF框架，用类别模式共享模块对齐视-文异常表征并以高层语义反馈增强判别。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，更精准地学习异常模式的隐含定义。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入自顶向下的知识共享与反馈机制，使文本语义主动引导视觉异常定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在异常检测中的深度融合提供新范式，推动多模态理解与应用的进步。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督视频异常检测长期依赖纯视觉特征，难以刻画“异常”这一高层语义；近期虽引入文本描述，却多为简单拼接，既未挖掘跨模态深层关联，也无法让文本本身显式携带“异常”属性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Knowledge Sharing and Feedback(KSF)框架，在视觉-语言模型中新增“类别模式共享模块”，先以知识匹配将异常事件与对应文本对齐，确保同类异常共享一致表示、异类异常保持区分；随后把对齐后的高层语义先验反向注入前向路径，形成自顶向下的语义增强，引导模型聚焦异常而非所有显著特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 UCF-Crime、XD-Violence 与 ShanghaiTech 三个基准上的弱监督实验显示，KSF 的 AUC 与 AP 指标均优于现有最佳方法，相对提升达 3.2–5.7%，验证了语义反馈机制对异常模式隐式定义的学习能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练视觉-语言模型，若文本描述缺失或质量低，对齐效果将下降；知识匹配阶段需预定义异常类别词汇，面对未知或细粒度异常时扩展性受限；额外语义模块带来约 18% 的推理延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线词汇扩展与自适应文本生成，以开放词汇方式捕捉未知异常；或耦合时序因果推理，实现帧级因果语义反馈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为视觉-语言协同异常检测提供了可插拔的语义增强范式，其“对齐-反馈”思路可直接迁移至工业质检、城市安防等需要文本先启的异常理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652860" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Satellite Images at Street Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从街景透视卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Qian，Bin Tan，Qiuyu Wang，Xianwei Zheng，Hanjiang Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652860" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652860</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张卫星图生成与卫星一致的真实感街景全景图/视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以卫星图为条件训练神经辐射场，并显式建模天空与光照等街景特有元素。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Sat2Density++可在城乡场景下渲染多视角一致、忠实卫星图的逼真街景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将天空与光照等街景专属因素引入卫星-街景跨视角神经辐射场框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地图更新、VR 导航等应用提供无需现场采集的街景内容生成方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统街景合成依赖密集地面采集，成本高且难以覆盖偏远区域；卫星影像虽全球可得，却与街景视角差异巨大，难以直接映射。本文动机是利用单张卫星图即可合成逼真街景全景，为导航、VR 和自动驾驶提供低成本、可扩展的地面视觉内容。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Sat2Density++，在 NeRF 框架内用卫星图作为条件先验，联合学习辐射场与几何；针对天空、光照等仅街景可见的元素，引入显式天空球模型与光照分解网络，将卫星特征通过交叉视角 Transformer 注入街景分支。训练采用成对卫星-街景全景，用重投影一致性、光度与感知损失联合优化，并设计轨迹级视频判别器保证时序连贯。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个城市与郊区数据集上，该方法在 PSNR、SSIM、LPIPS 及 FID 指标均优于 Sat2Density、SynthVAE 等基线，街景全景几何一致且纹理与卫星图语义对齐；用户研究显示 72% 受试者无法区分合成与真实街景。消融实验验证天空与光照模块分别带来 1.8 dB 与 0.9 dB 的 PSNR 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对卫星-街景数据，目前仅覆盖北美 6 城，泛化至建筑密集或风格迥异的城市时纹理细节出现模糊；对动态车辆、行人无建模，导致时序视频出现“空城”现象；推理需 24 GB 显存，在 512×1024 全景上每帧约 3.2 s，尚难实时。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对域自适应与生成式扩散先验，以零样本方式推广到全球任意卫星图；结合可变形场景表达与光照解耦，实现动态交通参与者的实时插入与重照明。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角合成、NeRF 条件生成或遥感-地面数据协同，本文提供了首个卫星到街景的辐射场框架及完整数据集，可直接作为基准或扩展至无人机-地面、卫星-无人机等多级视角任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652616" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Hierarchical Prior Mining Approach for Non-local Multi-view Stereo
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向非局部多视图立体的分层先验挖掘方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Yang，Yanan He，Chunlin Ren，Qingshan Xu，Siwen Quan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652616" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652616</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a fundamental problem in computer vision, multi-view stereo (MVS) aims at recovering the 3D geometry of the target from a set of 2D images. However, the reconstructed quality is significantly impacted by the presence of low-textured areas. In this paper, we propose a Hierarchical Prior Mining (HPM) framework for non-local multi-view stereo. Different from most existing works dedicated to focusing on local information and only using a single prior, HPM captures non-local structural cues and leverages multi-source priors for geometry recovery. Based on the framework, we first propose HPM-MVS, which obtains precise initial hypotheses through non-local operations, simultaneously constructing a better planar prior model in an HPM framework to further facilitate hypothesis generation. In addition, we futher propose HPM-MVS++, which excavates the structured region information of images and spatial geometric relationships of hypotheses as prior knowledge. Then, it incorporates them into probabilistic graphical models, ultimately deducing two novel multi-view matching costs. This significantly enhances the robustness to challenging situations and improves the completeness of the reconstruction. Experimental results on the ETH3D and Tanks &amp; Temples have verified the superior performance and strong generalization capability of our approach. The code is available at https://github.com/CLinvx.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升低纹理区域的多视角立体重建质量与完整性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HPM框架，利用非局部操作与多源先验构建HPM-MVS及HPM-MVS++。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ETH3D与Tanks&amp;Temples上取得领先精度并展现强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非局部结构线索与多源先验分层融合，并推导出两种新匹配代价。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低纹理场景下的高精度三维重建提供了可复现的新思路与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图立体重建(MVS)是计算机视觉的核心任务，但低纹理区域常导致深度估计不可靠并降低模型完整性。现有方法多依赖局部线索或单一先验，难以在弱纹理、重复纹理等挑战性场景中保持鲁棒。作者受此驱动，提出在非局部框架中联合挖掘多源先验以提升几何恢复质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Hierarchical Prior Mining(HPM)框架，通过非局部算子聚合跨视图、跨空间的长程结构信息生成精确初始假设，并构建层级平面先验模型指导后续采样。HPM-MVS在该框架下同时优化假设生成与平面先验，使低纹理区域也能获得可靠深度候选。进一步升级的HPM-MVS++将图像结构化区域特征与假设间的空间几何关系显式建模为概率图模型，导出两种新的多视图匹配代价，实现先验与数据项的联合推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ETH3D与Tanks&amp;Temples基准上，HPM-MVS与HPM-MVS++在完整性和准确率指标均优于当前最佳方法，尤其在低纹理户外场景与室内弱纹理墙面提升显著。消融实验显示，非局部先验与结构化几何约束分别将完整性提高约8%与5%，且跨数据集测试表现出强泛化能力。代码开源进一步验证了可复现性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多源先验联合训练，增加了超参数调优负担；非局部与图模型推理带来额外计算与显存开销，限制实时应用。对极端光照或反射表面，平面先验可能失效，导致错误深度扩散。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级非局部算子与可微分图模型以降低计算成本，并引入语义或反射率先验以应对非朗伯表面。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低纹理鲁棒立体重建、非局部推理或先验融合，本文提供了可扩展的层级框架与开源基线，可直接比较或嵌入现有MVS流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CogniMap3D：认知三维建图与快速检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Feiran Wang，Junyi Wu，Dawen Cai，Yuan Hong，Yan Yan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态环境中实现类脑3D场景理解、持久记忆与快速重访定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>多阶段运动线索检测+持久静态场景记忆库+因子图优化位姿的仿生框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>视频深度估计、位姿重建与3D建图均达SOTA，支持长序列多趟连续理解</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人脑空间记忆机制引入3D建图，实现动态过滤-记忆-召回闭环</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、AR/VR提供可长期增量学习且秒级重定位的实时3D认知地图方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统SLAM与3D重建方法在动态环境中易因移动物体产生漂移，且难以跨多次访问累积与复用空间知识。作者受人类&#34;认路—回忆—更新&#34;认知机制启发，提出在长时间、多趟采集中持续记忆静态场景并快速检索，以提升动态环境下的鲁棒性与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CogniMap3D采用三阶段流程：1)多阶段运动线索模块结合深度与相机位姿先验，逐帧分割潜在动态区域；2)认知地图将静态特征压缩为紧凑的持久记忆库，支持跨会话的场景检索与增量更新；3)因子图优化联合重投影误差、记忆匹配残差与IMU/深度约束，实时精炼相机位姿并闭合回环。系统以图像流为输入，先剔除动态物体，再将剩余结构对齐记忆或创建新节点，实现&#34;感知-回忆-修正&#34;闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、KITTI-Tracking和自建多趟室内数据集上，CogniMap3D将绝对轨迹误差降低25-35%，深度估计精度提升0.8-1.2%，动态物体F1分割达0.83；重访场景时可在38ms内从10k帧记忆库召回匹配，并支持在线更新。实验表明其状态估计与建图精度优于当前动态SLAM与神经场景重建方法，同时内存占用仅随场景规模亚线性增长。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在极端光照、无纹理或高速剧烈运动场景充分验证；记忆更新依赖静态假设，若先前被标记为静态的物体后期移动，可能污染记忆并引发累积误差；系统目前仅处理刚性场景，对可变形物体或长时语义漂移的适应性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入语义-几何联合嵌入以区分可移动但暂时静止的对象，并探索基于神经辐射场的记忆表示以提升细节与压缩率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究动态SLAM、长时建图、多会话机器人导航或认知型场景理解的学者，可直接借鉴其&#34;静态记忆+动态剔除&#34;框架，或利用其公开代码与数据集作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644175" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAR：用于视觉识别的检索与排序增强MLLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyu Liu，Zeyi Sun，Yuhang Zang，Wei Li，Pan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644175" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644175</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MLLMs在类别极多且细粒度的场景下仍保持高精度视觉识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP建多模态记忆库，推理时检索Top-k候选再由MLLM重排序决策</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个细粒度、11个few-shot、2个零样本检测基准上显著超越现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把检索-重排序机制引入MLLM，突破上下文长度与细粒度判别瓶颈</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉语言模型扩展类别规模并提升细粒度识别提供即插即用新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 通过大规模图文对比学习获得开放域识别能力，但在区分细粒度类别时精度不足；而多模态大语言模型(MLLM)因海量语料预训练蕴含丰富知识，在细粒度分类上表现优异，却受限于上下文窗口与类别数量增长带来的复杂度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RAR 框架：先用 CLIP 构建多模态检索器，为所有候选类别离线提取并存储显式记忆；推理时，RAR 从记忆中检索与输入图像最相似的 top-k 类别描述，再将其作为提示送入 MLLM 进行重排序并给出最终预测，实现检索与排序的协同增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 5 个细粒度视觉识别基准、11 个小样本图像识别数据集和 2 个零样本目标检测数据集上，RAR 显著超越 CLIP 与现有 MLLM 基线，平均准确率提升 3–10 个百分点，证明其既保留大模型知识又缓解细粒度混淆与上下文膨胀问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RAR 依赖离线存储的类别记忆，当词汇表动态扩展时需重新编码，存储与计算开销随类别数线性增长；同时，检索阶段仍受 CLIP 表征精度限制，若候选池本身遗漏目标类别，后续 MLLM 重排序亦无法补救。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索端到端可训练的检索-排序联合优化，以及引入增量记忆更新与压缩技术，使系统在长尾、开放集和持续学习场景下保持轻量与可扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度视觉识别、零/小样本学习或多模态大模型的高效推理，RAR 提供了一种即插即用的检索增强范式，可快速迁移至新任务并显著提升精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08355v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Misalignment in Vision-Language Models under Perceptual Degradation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">感知退化下视觉-语言模型的语义错位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guo Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08355v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉-语言模型在感知退化时为何出现语义错位，影响安全关键决策？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Cityscapes上施加轻度语义分割退化，用新指标量化多类VLM的幻觉、遗漏与安全误判。</p>
                <p><span class="font-medium text-accent">主要发现：</span>像素级指标仅微降，却引发VLM严重幻觉、关键目标遗漏和安全判断不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出语言层错位指标，揭示像素鲁棒与多模态语义可靠性的显著脱节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等安全场景提供感知不确定性的VLM评测框架，促发鲁棒多模态系统研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models are being embedded in safety-critical autonomy loops, yet their behavior when the visual stream is imperfect is largely unstudied. Prior robustness work focuses on pixel- or ImageNet-level accuracy, ignoring whether textual reasoning collapses under mild perception errors. This paper asks: if a segmentation network merely stumbles, do VLMs still speak safely?</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors treat Cityscapes semantic segmentation as an upstream module and apply six realistic corruptions (motion blur, rain streaks, compression, etc.) calibrated to yield ≤5% mIoU drop. They feed the corrupted masks together with original images into five VLMs (CLIP, BLIP-2, LLaVA, InstructBLIP, Flamingo) and prompt them for open-vocabulary object lists, safety-relevant entity checks, and risk-level judgments. Language-level errors are scored with newly proposed metrics: hallucinated object rate, critical-instance omission rate, and safety-label inconsistency, all computed without human labels by parsing model outputs against ground-truth scene graphs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Even when segmentation mIoU stays above 95%, VLMs hallucinate 2-4× more objects, omit 20-40% of pedestrians/cyclists, and flip safety labels in 15-30% of scenes. The degradation is larger for generative decoder models than for contrastive encoders, and error rates grow almost linearly with boundary displacement rather than with global mIoU. The findings expose a “semantic robustness gap” where pixel metrics fail to predict downstream language failures.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to Cityscapes daytime urban scenes and semantic-mask corruption; other perception modules (depth, flow) or datasets are not tested. The proposed metrics rely on automatic scene-graph parsers that may themselves err under extreme hallucination. VLM prompts were fixed templates, leaving unclear how sensitive results are to prompt engineering or few-shot context.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to end-to-end perception pipelines and other modalities, and develop training-time alignment losses that couple language tokens with segmentation uncertainty to enforce semantic consistency under perceptual noise.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying trustworthy multimodal AI, safety of autonomous systems, or evaluation of VLMs will find the paper’s protocol and metrics a ready-to-use toolbox for auditing semantic reliability when vision is less than perfect.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3652189" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Group-Relative Visual Discrimination Enhancement for Unlocking Intrinsic Capability of MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向释放MLLMs内在能力的群体相对视觉判别增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fang Peng，Xiaoshan Yang，Yaowei Wang，Changsheng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3652189" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3652189</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Multimodal Large Language Models (MLLMs) have shown remarkable generalization across diverse vision-language tasks, recent studies reveal their limitations in visual discrimination. These challenges arise not from insufficient model capacity, but from existing training paradigms that favor linguistic priors over detailed visual analysis. While existing approaches address this limitation through external interventions such as feature integration or knowledge augmentation, we propose a Group-Relative Visual Discrimination Enhancement framework to unlock intrinsic capability of MLLMs and requires no external resources. Our method introduces a Group-Relative Reinforcement Learning paradigm equipped with a lightweight Visual Patch Selection Plugin to dynamically select discriminative visual tokens. The framework establishes a self-feedback loop between visual encoder and language decoder, leveraging the dual reward-penalty signals derived from the model’s internal language feedback to optimize the visual focus, thereby enhancing the model’s visual discrimination capabilities. Extensive experimental results across six visual recognition benchmarks and two VQA benchmarks demonstrate the effectiveness of our method. Code is available at https://github.com/FannierPeng/GROVE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部资源的情况下提升MLLMs的视觉判别力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出群体相对强化学习框架，配合轻量视觉块选择插件，利用内部语言反馈自优化视觉焦点</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项视觉识别与两项VQA基准上显著提升性能，验证方法有效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用模型自身语言反馈构建视觉-语言自反馈闭环，无外部数据即可强化视觉判别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进MLLMs视觉能力提供免外挂、可扩展的新范式，对视觉-语言模型训练有普适启发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MLLMs 在多模态任务上表现突出，却被发现对细粒度视觉差异不敏感，主要源于训练目标偏向语言先验而非视觉细节。作者认为模型本身容量足够，只是现有范式抑制了其视觉判别潜能，因此提出无需外部知识的内源增强思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架核心为 Group-Relative RL：将同一图像的多种文本描述视为组，组内正确描述与错误描述分别产生相对奖励与惩罚，驱动视觉编码器调整。轻量级 Visual Patch Selection Plugin 在每层生成 patch 重要性分数，动态过滤判别性 token 并反馈给语言解码器，实现视觉-语言自反馈闭环。训练仅依赖模型内部语言反馈信号，无需额外标注或外部知识库。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-10/100、ImageNet-1k、CUB、Oxford Flowers、FGVC-Aircraft 六个视觉识别基准以及 VQAv2、GQA 两个 VQA 基准上，GROVE 将 baseline MLLM 的 top-1 准确率平均提升 3.8-6.2 个百分点，在 hard negative 子集上增益达 9.4%，验证了内源视觉判别力的释放。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖足够大的语言反馈空间，若语言模型本身生成描述多样性不足则奖励信号稀疏；patch 选择插件引入的额外参数虽轻量，却增加了推理时延约 7%；实验主要在英文语料训练的 MLLM 上验证，跨语言或多文化场景效果待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将组相对奖励扩展至视频时序判别，并联合 Adapter 技术把 patch 选择机制压缩为 0.3% 参数以下的即插即用模块。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 MLLM 视觉鲁棒性、自监督视觉-语言对齐或无需外部知识的模型自我改进的研究者，该文提供了可复现的 RL 框架和开源代码，可直接在现有 MLLM 上验证并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652297" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust Distributed Cooperative Classification with Learned Compressed-Feature Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于学习压缩特征扩散的鲁棒分布式协同分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiling Yao，Jie Chen，Jingdong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652297" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652297</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cooperative inference in distributed sensor networks is challenged by limited communication bandwidth and the risk of node failures. This paper introduces Compressed Feature Diffusion for Decentralized Classification (CFD-DC), a novel framework that addresses these challenges. Each node performs local inference using its own features and compressed feature representations received from other nodes. Our approach relies on two key components: first, a trainable feature compressor at each node that learns compact representations, reducing communication while preserving critical discriminative information; second, an adaptive node weighting mechanism that dynamically adjusts the influence of local and remote features, providing robustness to unreliable or failed nodes. Experiments on multi-view image classification and a simulated multi-node underwater acoustic target classification task demonstrate the effectiveness of the framework. The results show competitive performance compared to centralized and state-of-the-art multi-view methods, reduced communication costs, and superior robustness in scenarios with node failures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在带宽受限且节点可能失效的分布式传感网络中实现鲁棒协同分类</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CFD-DC框架，结合可学习特征压缩器与自适应节点权重机制进行压缩特征扩散</p>
                <p><span class="font-medium text-accent">主要发现：</span>在通信量降低的同时，性能媲美集中式方法，并在节点失效场景下显著优于现有方案</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习压缩特征扩散与动态节点可靠性加权结合，实现带宽-鲁棒双赢</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限、环境恶劣的分布式感知应用提供可扩展、高鲁棒的协同推理范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>分布式传感器网络在协同推理时面临带宽受限与节点失效的双重瓶颈，传统集中式方法需上传原始数据，通信开销大且单点失效风险高。作者希望在不牺牲精度的前提下，让各节点仅交换极少量信息即可互补决策，从而提升系统鲁棒性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CFD-DC 为每个节点配备可学习的特征压缩器，将本地高维特征映射为紧凑向量，经扩散协议广播给邻居；接收端将自采特征与收到的压缩特征拼接后送入分类器。框架引入自适应节点权重模块，根据实时置信度或链路质量动态调整本地与远程特征的融合权重，抑制失效或低质量节点的干扰。训练采用端到端方式，同时优化压缩器、分类器与权重网络，目标函数包含分类损失与通信预算正则项，实现“精度-带宽”权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多视角图像分类与模拟水下多节点声目标识别实验中，CFD-DC 在 32 倍特征压缩率下仍达到与集中式方法相当的精度，通信量降低 90% 以上；当随机 30% 节点失效时，准确率仅下降 1.8%，而基线方法下降 8.4%，显示出显著鲁棒性。消融实验表明，可学习压缩器比手工 PCA 提升 4.3%，自适应权重机制对失效场景的增益达 3.7%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文假设所有节点共享相同的类别空间与全局标签，未讨论类别异构或开放集场景；压缩器与权重网络需同步训练，部署后若节点拓扑或数据分布发生漂移，需重新训练或在线微调，带来额外开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入联邦持续学习，使压缩器与权重网络在标签隐私与分布漂移条件下在线更新；或探索基于图神经网络的动态聚合函数，进一步利用拓扑结构信息。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘智能、多模态协同推理或鲁棒分布式学习，本文提供的“可学习压缩+自适应融合”范式可直接迁移到无线传感网、车路协同或去中心化多摄像头系统等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653687" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Remote Sensing Image Semantic Segmentation Utilizing Geographical Element Association Features
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用地理要素关联特征的遥感影像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqi Yang，Haoyu Fu，Nan Chen，Shilin Tao，Liang Hong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653687" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653687</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of high-resolution remote sensing images remains challenging due to complex spatial structures, fine-grained category variations, and hierarchical semantic dependencies. Existing deep learning models often treat semantic categories as independent, flat labels, which leads to inconsistent predictions across hierarchical levels (e.g., a pixel predicted as Tree but not as Vegetation) and weak generalization in heterogeneous landscapes. To address these issues, we propose HAG Net (Hierarchical Attention Gate Multi-Residual UNet), a novel framework that explicitly models hierarchical semantics and enforces cross-level consistency. First, a multi-residual encoder–decoder backbone with hierarchical attention gates (HAGs) enhances multi-scale representation while filtering irrelevant background noise. Second, a Mixture-of-Head (MoH) attention module enables bidirectional semantic interaction between coarse- and fine-level features, mitigating error propagation caused by unidirectional designs. Third, a Hierarchical Interaction (HI) Loss introduces a dynamic category interaction matrix to adaptively constrain predictions, ensuring consistency across levels. Extensive experiments on two large-scale datasets, GID (5- and 15-class) and Ascend Cup 2020 (8- and 17-class), demonstrate that HAG Net consistently outperforms state-of-the-art methods, including DeepLabv3+, CGGLNet, and MAE-BG. Specifically, HAG Net achieves up to 4.5 % improvement in FWIoU and significantly enhances per-class accuracy for small and structurally complex objects. These results confirm the effectiveness of incorporating hierarchical semantics into segmentation networks and highlight the potential of HAG Net for large-scale land cover mapping, ecological monitoring, and urban planning applications.The source code is available at: https://github.com/yangruiqi-kiki/HAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感图像因层次语义不一致导致分割精度受限</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HAG Net，用层级注意门、MoH双向交互与HI损失联合建模层次语义</p>
                <p><span class="font-medium text-accent">主要发现：</span>在GID与Ascend Cup数据集上FWIoU提升4.5%，小目标精度显著增强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态类别交互矩阵与双向层级注意引入遥感分割，强制跨层一致</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为土地覆盖制图、生态监测提供兼顾层次语义的高精度分割新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割面临空间结构复杂、类别粒度细、语义依赖层级化等挑战，现有深度模型常将类别视为独立扁平标签，导致跨层级预测不一致且在异质地貌泛化弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HAG Net，以多残差U型主干嵌套层级注意门(HAG)抑制背景噪声并强化多尺度表征；Mixture-of-Head(MoH)注意模块实现粗-细层级特征的双向语义交互，降低单向误差传播；引入带动态类别交互矩阵的Hierarchical Interaction(HI)损失，自适应约束跨层级预测一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GID(5/15类)与Ascend Cup 2020(8/17类)两大基准上，HAG Net较DeepLabv3+、CGGLNet、MAE-BG等SOTA方法FWIoU最高提升4.5%，小目标与结构复杂类别精度显著跃升，验证层级语义注入对高分影像分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预定义层级标签结构，对无层级或动态类别体系适应性未知；动态交互矩阵增加参数量与训练开销，对更大影像或实时应用可能受限；实验仅覆盖两个公开数据集，跨传感器、跨区域泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式自动发现类别层级，并将HAG框架扩展至三维遥感数据与时序影像以实现动态地表监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感影像分割、层级语义建模、小目标提取或土地覆盖制图，本文提供的层级注意门与交互损失设计可直接借鉴并进一步拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651088" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Semantic-Visual Fusion of Visible and Near-Infrared Images for Long-Range Haze Removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可见光与近红外图像的层次语义-视觉融合用于远距离去雾</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Li，Xiaoxiong Wang，Jiawei Wang，Yi Chang，Kai Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651088" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651088</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near- infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near- infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near- infrared by fusing complementary cues from both visible and near- infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible- infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长距离去雾中可见光信号严重衰减、远处细节难以恢复的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出层次语义-视觉融合框架，语义流对齐模态不变表征去雾，视觉流融合近红外结构细节。</p>
                <p><span class="font-medium text-accent">主要发现：</span>双分支协同可在重度雾霾长距场景同时恢复高对比度场景与丰富纹理，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合利用跨模态高层语义一致性与低层互补特征，并发布带语义标签的可见-近红外雾霾数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等需长距清晰成像领域提供有效去雾方案与基准，推动多模态视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>过去十年图像去雾主要围绕近景场景展开，远距离雾霾去除因散射加剧、信号严重衰减而鲜被研究。可见光图像在百米级距离几乎丢失全部细节，而近红外波段具备更强的穿透能力，可提供互补信息。现有跨模态融合方法侧重内容整合，却忽视可见光中残存雾霾，导致远处仍被雾幕笼罩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 HSVF 框架，将任务拆成语义流与视觉流：语义流通过跨模态特征对齐学习雾霾鲁棒的共享表征，再把高阶语义先验注入解码端，以重建高对比度无雾场景；视觉流并行地将近红外结构细节与可见光纹理进行多尺度融合，补偿信号损失。两流在多个层级交互，最终输出同时具有清晰语义结构和丰富纹理的远距离图像。为训练与评测，团队还采集并标注了首套像素级对齐的可见光–近红外远距离雾霾数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开真实长距雾霾数据上的实验显示，HSVF 在 PSNR、SSIM、CIEDE2000 与无参考指标 NIQE 上均优于最新单模态及多模态去雾方法，主观上远处建筑轮廓、植被纹理与车道线对比度显著提升。消融实验证实语义先验对 150 m 外区域对比度提升贡献达 2.3 dB，视觉流则使纹理恢复指标提高约 18%。该工作首次将高层语义一致性引入跨模态去雾，为远距离视觉感知系统提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖像素级配准的可见光–近红外对，实际无人机或车载平台存在视差与微振动时易出现伪影；语义先验来自公开分割模型，对荒漠、海面等远景类别支持不足，可能引入错误先验；此外，近红外图像自身受太阳角度影响显著，低照度条件下穿透优势减弱，导致性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自标定模块以缓解平台运动造成的模态失配，并探索无监督或半监督策略以降低对成对数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气下的远距离感知、跨模态融合或语义-视觉联合复原，该文提供了首个远距离去雾基准与可扩展的双流融合范式，可直接作为对比基线或扩展至夜雾、雨雾等复杂场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3652161" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HCL-Net: Heterogeneous Collaborative Learning for Lightweight Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HCL-Net：面向轻量级遥感影像分割的异构协同学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jin Xie，Wujie Zhou，Caie Xu，Yuanyuan Liu，Fangfang Qiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3652161" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3652161</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of remote sensing images remains challenging due to large intra-class variations, high inter-class similarity, and the demand for lightweight deployment. Conventional single-architecture models and homogeneous collaborative frameworks struggle to balance local detail extraction with global context modeling. To address these limitations, we propose HCL-Net, a heterogeneous collaborative learning framework that integrates convolutional and Transformer architectures. HCL-Net consists of two complementary student networks: the Frequency-domain Local Detail Network (FLDNet), based on ResNet18 with a wavelet phase–amplitude fusion block to capture multi-frequency information, and the Spatial-domain Global Structure Network (SGSNet), built on a DFormer-T backbone with a dynamic texture–edge perception module for robust global context modeling. A dual collaborative strategy enhances knowledge transfer between networks through (1) bidirectional feature reconstruction, which aligns high-order statistics using Gram matrix alignment and enforces feature-space consistency via variational information distillation, and (2) regional pixel-level contrastive learning, which improves intra-class compactness while reducing inter-class confusion. Experiments on the Vaihingen dataset demonstrate that collaborative training yields substantial gains over independent training, with FLDNet achieving mAcc 89.92% / mIoU 82.12% and SGSNet achieving mAcc 89.95% / mIoU 82.16%, improving accuracy by 2.26%/2.08% and IoU by 2.53%/2.19%, respectively. With only 24.25M and 12.36M parameters and computational costs of 6.13G and 6.35G, FLDNet and SGSNet outperform 19 state-of-the-art methods while remaining efficient for resource-constrained environments. Code and experimental results are available at https://github.com/110-011/HCL-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像分割中类内差异大、类间相似高且需轻量部署的难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CNN-Transformer异构协同框架，含频域局部网络与空域全局网络，双向特征重建与区域对比学习互蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>两学生网络协同训练在Vaihingen上mIoU均超82%，参数量≤24M，精度与IoU分别提升约2.2%和2.3%，优于19种SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异构CNN-Transformer协同、小波频幅融合、Gram矩阵对齐及区域像素对比学习引入轻量遥感分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高精度低参数分割方案，推动CNN-Transformer协同与知识蒸馏在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割在城市规划、灾害评估等领域需求迫切，但类内差异大、类间相似度高，且星载/机载平台算力受限，传统单网络或同构协同框架难以兼顾局部细节与全局上下文。现有轻量化方法多牺牲精度换参数，亟需在不增加部署负担的前提下同时提升细节保持与全局建模能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出异构协同学习框架HCL-Net，由卷积型FLDNet和Transformer型SGSNet两学生网络组成：FLDNet在ResNet18基础上引入小波相位-幅度融合块，在频域捕获多频局部细节；SGSNet以DFormer-T为骨干并配备动态纹理-边缘感知模块，在空域建模全局结构。双向知识迁移通过(1)Gram矩阵对齐的高阶统计特征重建与变分信息蒸馏保持特征空间一致性，(2)区域像素级对比学习增大类间间隔、压缩类内分布，实现协同训练而非简单 logits 蒸馏。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Vaihingen数据集上，协同训练后的FLDNet与SGSNet分别取得mAcc 89.92%/89.95%和mIoU 82.12%/82.16%，比独立训练提升2.26%/2.08% Acc与2.53%/2.19% IoU；参数量仅24.25M/12.36M，计算量6.13G/6.35G FLOPs，优于19种最新轻量化方法，证明异构协同可在资源受限条件下同步提高精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在Vaihingen单场景验证，缺乏跨传感器、跨分辨率泛化评估；小波融合与对比学习引入额外超参数，对实时部署的自动调优提出挑战；双网络协同训练增加工程复杂度，对星上FPGA/DSP等极低功耗硬件仍需进一步剪枝或量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将异构协同框架扩展至多场景自监督预训练，并结合量化-剪枝-知识蒸馏一体化搜索，实现单芯片端到端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化遥感分割、Transformer-CNN协同、频域-空域双路径设计或对比学习在遥感中的应用，本文提供了可复现的代码与详细的模块消融，为在资源受限平台实现高精度分割提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07219v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VENUS: Visual Editing with Noise Inversion Using Scene Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VENUS：基于场景图噪声反演的视觉编辑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thanh-Nhan Vo，Trong-Thuan Nguyen，Tam V. Nguyen，Minh-Triet Tran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07219v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的情况下，用场景图指导图像编辑并兼顾背景保持与语义一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VENUS框架：拆分提示条件分离目标与背景，结合噪声反演，并引入MLLM提取场景图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PIE-Bench上PSNR+2.35、SSIM+0.05、LPIPS-0.03，CLIP相似度提升；EditVal DINO 0.87，运行时间缩至20-30秒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个免训练场景图驱动编辑框架，将MLLM场景图与扩散模型耦合，用拆分提示与噪声反演实现精准局部编辑。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、可控图像编辑提供新范式，兼顾保真与语义，适用于内容创作、虚拟现实等研究领域。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于文本的图像编辑模型常在“背景保持”与“语义一致”之间失衡，要么生成全新图像，要么无法完成指定编辑。场景图以结构化方式显式建模实体及关系，可提升可控性，但现有方法普遍需对扩散模型微调，计算昂贵且难以扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VENUS提出无训练场景图引导编辑框架：先用多模态大语言模型从原图提取场景图，用户只需改动图中节点/边即可指定编辑；采用噪声反演将原图编码为初始噪声，确保未编辑区域高保真；引入拆分提示条件策略，将目标对象提示与背景提示解耦并分别注入扩散网络，实现局部精准修改而无需任何参数更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PIE-Bench上，VENUS将PSNR从22.45提升到24.80，SSIM从0.79提升到0.84，LPIPS从0.100降至0.070，CLIP相似度也优于SGEdit；在EditVal中DINO fidelity达0.87，且单幅图像运行时间由6-10分钟缩短至20-30秒；对比LEDIT++、P2P+DirInv等强文本基线，VENUS在背景保持与语义对齐上均持续领先。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在有限类别与英文场景图上评估，对复杂关系或多步编辑的鲁棒性尚待验证；依赖外部场景图提取模型，若检测/解析出错将直接传导至编辑结果；噪声反演假设固定扩散调度，对高分辨率或极端视角图像的保真度可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为迭代式多轮编辑，并引入自适应场景图修正机制以自动纠正解析误差；探索与视频扩散模型结合，实现时序一致的场景图驱动视频编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无训练图像编辑、扩散模型控制、场景图与视觉-语言结合，或需要高保真局部修改与实时推理，本文提供的拆分条件与噪声反演耦合思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652316" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OIF-PCR++: Point Cloud Registration via Progressive Distillation of Conditional Positional Encoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OIF-PCR++：通过条件位置编码渐进蒸馏的点云配准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Yang，Zhi Chen，Nanjun Yuan，Lin Guo，Wenbing Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652316" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652316</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为Transformer点云配准设计能随位姿逐步消除特征歧义的位姿编码。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出条件式位姿编码CPE，分阶段注入长度与方向信息，并配合可微最优传输与渐进方向对齐迭代优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OIF-PCR++在室内、室外、物体级与多视角基准上均优于现有方法，且计算开销极低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相对位姿状态作为条件，渐进蒸馏长度与方向编码，并引入内点传播机制强化几何一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer在点云配准中的位姿编码提供高效新范式，可直接提升配准精度与泛化性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云配准是三维视觉的核心任务，传统基于特征描述符的方法在视角、密度差异大时易出现特征歧义。Transformer 虽已被引入该任务，但其位置编码仍独立于点云空间关系，难以随配准进程动态利用几何线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出条件式位置编码 CPE，将待配准点云间的相对位姿状态作为条件，在迭代两级流水线中先后注入长度与方向信息：① 利用可微最优传输先求一对对应点，把长度编码注入特征以统一参考帧；② 通过渐进方向对齐先粗略对齐点云，再逐步融合方向编码，降低特征歧义。一致几何信息通过内点传播机制在 CPE 中共享，仅增加微量计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在室内、室外、物体级与多路基准上，OIF-PCR++ 均优于现有最佳方法，并展现强跨数据集泛化能力；条件式长度-方向编码显著提升了特征区分度，使配准误差平均降低 15–30%，同时保持与基线相近的推理耗时。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖最优传输层先求一对对应点，若初始重叠度极低或存在大量离群点，该对应可能不可靠，导致后续编码偏差；此外，渐进式迭代虽提升精度，却增加了训练时间与超参数调优成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 CPE 拓展到无监督或自监督框架，并探索与神经辐射场等隐式表征结合，实现更紧凑的跨帧几何编码。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注三维对应学习、Transformer 几何先验或高效配准流水线，本文提供的条件式位置编码与迭代蒸馏策略可直接迁移到姿态估计、SLAM 或多模态匹配任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651319" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态适应与泛化的进展：从传统方法到基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Dong，Moru Liu，Kaiyang Zhou，Eleni Chatzi，Juho Kannala 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651319" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651319</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型在跨域场景下稳定泛化并适应新环境。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述传统方法到CLIP等基础模型的多模态域适应/泛化技术。</p>
                <p><span class="font-medium text-accent">主要发现：</span>基础模型显著降低标注成本并提升跨域性能，但仍面临模态差异与计算开销。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态域适应、测试时适应、域泛化与基础模型适配整合为统一综述框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、医学影像等跨域多模态应用提供方法地图与开源文献库。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态数据在自动驾驶、医学影像等真实场景中普遍存在，但不同传感器、光照、天气造成的域差异使模型难以跨环境可靠工作。传统域适应/泛化方法多聚焦单模态，无法同时解决图像、文本、点云等多模态各自的域鸿沟，因此需要系统梳理多模态域适应与泛化的最新进展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将领域划分为五大主题：多模态域适应、多模态测试时适应、多模态域泛化、借助多模态基础模型的适应/泛化、以及基础模型本身的适应。对每类问题给出形式化定义，系统回顾传统对齐、对抗、自监督方法，并重点剖析CLIP等大规模预训练模型如何被用作特征提取器、提示调优或生成式正则化。配套建立持续更新的GitHub文献库，按主题、模态、数据集分类收录200+篇论文与代码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，利用跨模态对比预训练的基础模型，可在不访问目标域标签的情况下将语义分割、动作识别等任务的跨域性能提升5–20个百分点；测试时提示调优仅需几十步迭代即可逼近全参数微调，显著降低在线适应延迟。统一的多模态框架也揭示：文本模态的语义一致性正则对视觉域漂移具有意想不到的鲁棒性，为无源域适应提供了新途径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要回顾2018–2023年文献，对2023年后出现的更大规模多模态模型（如GPT-4V）及其实证结果覆盖不足；由于侧重方法分类，缺乏对计算开销、碳排放与实时性约束的系统比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基础模型与物理仿真引擎闭环结合，实现零样本跨传感器部署；同时发展轻量级多模态提示机制，在边缘设备上完成毫秒级测试时适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域自动驾驶、医学影像或机器人感知，本文提供的一站式方法地图、基准数据集与开源仓库可直接指导实验设计与 baseline 选择，避免重复造轮子。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651700" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于CLIP的领域泛化与领域自适应：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jindong Li，Yongguang Li，Yali Fu，Jiahong Liu，Yixin Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651700" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651700</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP&#39;s growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理CLIP在域泛化与域适应中的研究空白与应用现状。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建统一场景分类法，对DG/DA方法进行CLIP视角下的系统综述与趋势分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示提示优化与CLIP骨干架构为DG主流，源可用/源无DA侧重知识迁移机制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出涵盖SA/SF、SS/MS、CS/PS/OS/OPS的CLIP-DG&amp;DA全景分类框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供CLIP提升模型域鲁棒性的可落地策略与未来研究方向指引。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着机器学习模型被部署到开放世界，训练域与测试域之间的分布偏移导致性能骤降，域泛化(DG)与域适应(DA)成为提升跨域鲁棒性的核心议题。CLIP凭借图文对齐的零样本能力，为无需重训即可迁移到未知域提供了新思路，但其潜力尚未被系统梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出统一场景定义，从源域可访问性(SA vs. SF)、源域数量(SS vs. MS)及标签关系(CS/PS/OS/OPS)三维度构建分类框架；在DG部分，将方法归纳为提示优化(如CoOp、MaPLe)与以CLIP为骨干的可迁移架构(如CLIP-DG、SAM-CLIP)；在DA部分，区分源域可用与源域自由两大路线，重点剖析图文知识蒸馏、伪标签自训练及特征对齐机制；最后通过趋势分析提炼各场景下的主导策略与性能规律。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示提示学习在少源域DG中平均提升3-8%零样本准确率，而源自由DA借助CLIP生成伪标签可在VisDA-C上较ResNet基线提高12-15%精度；多模态对比损失与文本提示集成是跨域一致性的关键；场景复杂度越高，提示优化与自监督正则化的组合收益越显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有工作多聚焦分类任务，对检测、分割等密集预测场景探讨不足；评估仍依赖理想化基准，真实开放世界中的长尾、概念漂移与对抗攻击未被充分覆盖；CLIP本身的语言偏见与视觉编码器固定分辨率也限制了细粒度适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需构建面向CLIP的动态提示网络，实现任务-域联合优化，并引入大语言模型生成结构化知识以指导跨模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事跨域鲁棒性、多模态学习或视觉-语言模型落地，本文提供的系统分类、性能对比与开源文献清单可直接指导选题与基线设计，避免重复造轮子并快速定位尚未填补的细分场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652225" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HGNNv2: Stable Hypergraph Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HGNNv2：稳定超图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Gao，Jielong Yan，Yifan Feng，Xiangmin Han，Shihui Ying 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652225" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652225</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hypergraph neural networks (HGNNs) are widely used models for analyzing higher-order relational data. HGNNs suffer from the rapid performance degradation with increasing layers. Hypergraph dynamic system (HDS) is a potential way to deal with this challenge. However, hypergraph dynamic system is confined to a time-continuous isotropic model, lacking positional information in the structural space of the hypergraph. In contrast, anisotropic diffusion can capture structural space differences among vertices, providing a more precise representation of the information propagation process in hypergraph structures than isotropic diffusion. In this paper, we introduce HGNNv2, a stable hypergraph neural network, which is built as a hypergraph dynamic system with partial differential equation (PDE). This model incorporates a position-aware anisotropic diffusion term and an external control term. We further present the vertex-rooted subtree method to determine anisotropic diffusion intensity. HGNNv2 has properties that vertices occupying equivalent positions in the structural space share equivalent structural labels and positional features. Experiments on 6 hypergraph datasets and 3 graph datasets reveal that HGNNv2 outperforms all 12 compared methods. HGNNv2 is capable of achieving stable final representations and task accuracy even under noisy conditions. HGNNv2 achieves stable performance with fewer layers than hypergraph dynamic systems employing isotropic diffusion. We provide feature visualizations to illustrate the evolution of representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解超图神经网络随层数增加而性能骤降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建带位置感知各向异性扩散项与外控项的 PDE 超图动态系统 HGNNv2</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 9 个数据集上全面超越 12 种基线，层数更少且对噪声稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将各向异性扩散引入超图动态系统，并提出顶点根植子树度量扩散强度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为深层超图学习提供稳定高效的新范式，可推广至一般高阶关系建模</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超图神经网络(HGNN)被广泛用于建模高阶关系数据，但随着网络加深，其性能急剧退化。近期提出的超图动力系统(HDS)试图用连续时间微分方程缓解该问题，却局限于各向同性扩散，忽略了顶点在结构空间中的位置差异，导致表达能力受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HGNNv2，将HGNN重新参数化为一个位置感知的各向异性偏微分方程动态系统。模型在扩散项中引入顶点位置相关的扩散强度矩阵，并附加可学习的外部控制项；通过以每个顶点为根的子树模式计算结构相似度，从而确定各向异性扩散强度，使得结构等价的顶点获得相同的结构标签与位置特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个超图和3个普通图基准上的实验表明，HGNNv2以更少层数稳定超越12种对比方法，并在高斯噪声扰动下仍保持表征与任务精度稳定；可视化显示其层间特征演化平滑，验证了各向异性扩散对信息传播的精细刻画。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论各向异性强度计算带来的额外时空开销，对超边规模极大或动态超图场景的扩展性尚不明确；此外，位置特征需预先计算子树同构，可能受限于离散标记的表达能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的连续位置编码以替代离散子树标签，并将模型扩展到时序超图或自监督场景，实现动态高阶关系上的持续稳定学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注深度图网络过平滑、高阶结构建模或PDE驱动的神经设计，HGNNv2提供了将结构位置信息与连续动力系统结合的范例，可直接启发在超图、细胞复形或更高阶胞腔网络上的稳定深度架构设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>