<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-29 11:20 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于遥感视觉-语言的论文、2篇关于场景图生成的论文与1篇关于视觉质量估计的论文。</p>
            
            <p><strong class="text-accent">遥感视觉-语言</strong>：《bi-modal textual prompt learning for vision-language models in remote sensing》提出双模文本提示学习，在弱监督下将CLIP类模型适配到遥感下游任务；《DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment》构建面向灾害评估的多模态基准，强调细粒度功能感知与定位，突破现有粗标签局限。</p>
            
            <p><strong class="text-accent">场景图生成</strong>：《DuoNet: Joint Optimization of Representation Learning and Prototype Classifier for Unbiased Scene Graph Generation》联合优化表示学习与原型分类器以缓解长尾偏差；《CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization》引入因果不变表征与不确定度引导正则，提升场景图在分布外场景中的泛化能力。</p>
            
            <p><strong class="text-accent">视觉质量估计</strong>：《Physically Guided Visual Mass Estimation from a Single RGB Image》利用物理先验将几何体积与材料密度解耦，实现仅依赖单张RGB图像的物体重量估计。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态大模型的论文、6篇关于遥感视觉-语言的论文、5篇关于文档OCR与理解的论文、4篇关于空间-时序推理的论文、3篇关于检索增强生成的论文、2篇关于RGB-D语义分割的论文以及1篇关于视觉质量评估的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：该主题聚焦通用视觉-语言大模型的架构改进与跨模态对齐，如《Pixel-Grounded Retrieval》将像素级检索与LMM融合以提升知识型VQA，《bi-modal textual prompt learning》提出双模Prompt学习把CLIP适配到遥感领域，《Uni-RS》针对遥感统一模型出现“空间反转诅咒”提出空间忠实生成策略，《Multi-Perspective Subimage CLIP》利用关键词引导的多视角子图CLIP提升遥感图文检索，而《Spatial-Conditioned Reasoning》在长时间自我中心视频中引入空间条件推理以缓解视点漂移。</p>
            
            <p><strong class="text-text-secondary">遥感视觉-语言</strong>：论文集中解决遥感影像与文本间的精细对齐与少样本泛化，如《bi-modal textual prompt learning》通过文本-视觉双Prompt调优使CLIP适应遥感场景，《Uni-RS》提出统一生成-理解框架以克服空间位置反转，《Multi-Perspective Subimage CLIP》借助关键词驱动的子图对比学习强化细粒度对齐，其余研究进一步扩展了遥感图文检索、目标定位与变化描述任务。</p>
            
            <p><strong class="text-text-secondary">文档OCR与理解</strong>：该组工作探索无OCR文档大模型与高精度文字识别，如《TextMonkey》在LMM中引入移位窗口注意力实现端到端密集文本理解，《DeepSeek-OCR 2》提出Visual Causal Flow动态重排视觉token以提升编码-解码性能，两者均显著降低对传统OCR管道的依赖并提升多页文档推理能力。</p>
            
            <p><strong class="text-text-secondary">空间-时序推理</strong>：研究关注长视频或跨视角中的空间一致性与时序推理，如《Spatial-Conditioned Reasoning》为长时自我中心视频导航引入持久几何上下文，《m2sv》构建Map-to-Street-View基准测试VLMs在鸟瞰-街景对齐上的空间推理鲁棒性，《Uni-RS》亦讨论遥感统一模型中的空间反转问题并提出生成约束。</p>
            
            <p><strong class="text-text-secondary">检索增强生成</strong>：论文将外部检索机制引入视觉问答与质量理解，如《Pixel-Grounded Retrieval》提出像素级RAG为LMM注入图像外知识，《QualiRAG》首次把RAG用于视觉质量评估，实现可解释细粒度质量描述，两者均显示检索增强能显著提升生成答案的事实准确性。</p>
            
            <p><strong class="text-text-secondary">RGB-D语义分割</strong>：研究利用预训练-微调范式缓解RGB-D语义分割中的模态不匹配，如《DFormer++》设计专用RGB-D预训练策略与深度增强Transformer，在NYUDv2、SUN RGB-D等数据集上取得新最佳，证明深度信息可在预训练阶段被有效编码并迁移至下游分割任务。</p>
            
            <p><strong class="text-text-secondary">视觉质量评估</strong>：该主题关注从打分转向可解释质量理解，如《QualiRAG》提出面向视觉质量的检索增强生成框架，通过检索相似质量样本并结合时空感知模型，实现对失真类型、位置及原因的细粒度语言解释。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型适配到多标签、高类内差异的遥感影像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三项域泛化任务上平均提升约2%，优于现有提示学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为语义摘要，通过双模跨注意力动态条件化提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低成本的VLM适配方案，可推广至多标签、跨分辨率及新类识别场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等视觉-语言模型迁移到下游任务，但遥感影像具有多标签、类内差异大、分辨率多样等特点，直接套用现有文本提示方法会丢失主导语义线索，导致新类别泛化差。因此，亟需一种面向遥感场景的轻量级提示学习框架，在少监督条件下充分挖掘视觉-语言互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS，用冻结的 BLIP-2 图像描述模型为每张遥感影像生成一句语义摘要，经 BERT tokenizer 得到文本 token；同时提取 CLIP 图像编码器的高层视觉特征。二者在特征空间拼接后，由轻量级交叉注意力模块以可学习的查询提示为条件，生成与图像-文本上下文耦合的提示向量，全程 CLIP 骨干网络保持冻结。该双模态提示仅增加不到 1 M 可训练参数，实现高效适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个公开遥感数据集、3 项领域泛化任务上，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，在跨传感器、跨分辨率、跨地理区域设置下均保持最高宏平均 F1 与调和准确率，显著改善对“裸地”“温室”等难区分类别的召回。消融实验表明，移除图像摘要或交叉注意力后性能下降 1.3–1.7 个百分点，验证双模态融合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖 BLIP-2 生成的单句摘要，若影像场景复杂或多标签分布极端，caption 可能遗漏关键语义；目前仅测试了 4 个数据集，尚未验证在超大尺度影像（如整幅 Sentinel-2 瓦片）或视频级序列上的可扩展性；交叉注意力模块虽轻量，但仍需 GPU 进行训练，对边缘端部署有一定开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多句或层级 caption 以及可解释性约束，缓解多标签信息丢失；探索将双模态提示与遥感专用视觉主干（如基于 Transformer 的时空网络）联合蒸馏，实现端侧零样本推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、小样本/零样本分类、提示学习或领域泛化，本文提供了轻量级双模态提示范式与可复现代码，可直接作为基线或扩展至变化检测、语义分割等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113152" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DuoNet: Joint Optimization of Representation Learning and Prototype Classifier for Unbiased Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DuoNet：表征学习与原型分类器的联合优化用于无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaodi Wang，Biao Leng，Shuo Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113152" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113152</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unbiased Scene Graph Generation (SGG) aims to parse visual scenes into highly informative graphs under the long-tail challenge. While prototype-based methods have shown promise in unbiased SGG, they highlight the importance of learning discriminative features that are intra-class compact and inter-class separable. In this paper, we revisit prototype-based methods and analyze critical roles of representation learning and prototype classifier in driving unbiased SGG, and accordingly propose a novel framework DuoNet. To enhance intra-class compactness, we introduce a Bi-Directional Representation Refinement (BiDR 2 ) module that captures relation-sensitive visual variability and within-relation visual consistency of entities. This module adopts relation-to-entity-to-relation refinement by integrating dual-level relation pattern modeling with a relation-specific entity constraint. Furthermore, a Knowledge-Guided Prototype Learning (KGPL) module is devised to strengthen inter-class separability by constructing an equidistributed prototypical classifier with maximum inter-class margins. The equidistributed prototype classifier is frozen during SGG training to mitigate long-tail bias, thus a knowledge-driven triplet loss is developed to strengthen the learning of BiDR 2 , enhancing relation-prototype matching. Extensive experiments demonstrate the effectiveness of our method, which sets new state-of-the-art performance on Visual Genome, GQA and Open Images datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长尾分布下场景图生成中的类别偏差，提升稀有关系检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DuoNet框架，联合BiDR²表征精炼与KGPL等距原型分类器优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome、GQA、Open Images上刷新无偏SGG指标，稀有类召回显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向关系-实体-关系表征精炼与冻结等距原型分类器结合，抑制长尾偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾视觉关系检测提供即插即用新范式，助力多模态理解与视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) 长期受长尾分布困扰，导致常见关系过拟合而稀有关系被淹没。基于原型的方法虽能缓解偏差，但要求特征同时满足类内紧致与类间分离，现有工作对表征学习与原型分类器的协同机制探讨不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DuoNet 将 SGG 解耦为表征学习与原型分类器联合优化：BiDR² 模块以“关系→实体→关系”双向路径，先借关系敏感注意力捕获视觉变化，再用关系特定实体一致性约束提纯特征，实现类内紧致；KGPL 模块构建等距分布且冻结的原型分类器，最大化类间间隔，并设计知识驱动的三元组损失反向强化 BiDR² 表征，使长尾样本也能对齐稀有原型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome、GQA 与 Open Images 三大基准上，DuoNet 将 mR@50/100 提升 5-8 个百分点，总体 R@50/100 不降反升，首次在稀有类别上逼近常见类性能，验证了其兼顾精度与公平性的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未显式建模实体对外的全局上下文，可能遗漏跨组依赖；冻结原型虽抗尾偏，但固定中心在极端开放集场景下适应性不足；训练需额外存储大量原型向量，显存占用高于传统分类器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态原型更新策略以兼容开放集，并将 DuoNet 的“表征-原型”协同框架迁移到视频时空关系检测或多模态长尾任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究长尾视觉识别、图结构推理或原型网络，该文提供了表征与分类器联合去偏的新范式及可直接套用的 BiDR²/KGPL 模块代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18493v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DisasterInsight：面向功能感知与有根据灾害评估的多模态基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sara Tehrani，Yonghao Xu，Leif Haglund，Amanda Berg，Michael Felsberg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18493v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让人道救援可用的多模态模型真正理解灾前灾后卫星影像中的建筑功能与损伤细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将xBD重构为11.2万建筑中心样本，提出DisasterInsight基准并用LoRA微调得到DI-Chat基线。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有VLM在损伤分级、灾种识别和结构化报告生成上差距大，DI-Chat显著提升但仍难识别建筑功能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个聚焦功能感知与指令鲁棒性的灾害影像多模态基准，并给出LoRA微调基线DI-Chat。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感+语言模型提供统一灾害评估试金石，推动人道救援AI落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>灾害发生后，卫星影像的快速判读是救援决策的关键，但现有遥感视觉-语言基准多停留在粗粒度标签与整图分类，难以满足人道机构对建筑功能、受损等级等细粒度且可执行信息的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将xBD数据集约11.2万栋建筑切片重组，构建DisasterInsight多任务基准，涵盖功能分类、破坏等级与灾种分类、计数及符合人道评估指南的结构化报告生成，并支持多样化指令测试。为建立领域基线，他们用灾害专用指令数据对主流VLM骨干进行LoRA微调，得到DI-Chat模型，在统一框架下与通用及遥感专用VLMs对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示所有模型在破坏等级、灾种识别和报告生成上均存在显著性能落差；DI-Chat在这些任务上取得明显提升，但建筑功能分类仍是瓶颈。DisasterInsight首次将“功能感知”与“指令鲁棒性”纳入遥感灾害评估基准，为后续研究提供量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅基于xBD的四种灾种与建筑级切片，地域与灾害类型覆盖有限；评估指标侧重分类精度与报告BLEU/ROUGE，尚未充分量化救援决策中的实际效用或时效性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多灾种与区域，并引入救援决策模拟指标；同时探索将功能分类与外部GIS数据融合以提升性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、灾害响应或多模态评测，本基准与DI-Chat基线提供了可直接复现的实验平台与性能参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20355v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CURVE：通过不确定性引导的正则化学习因果启发的鲁棒场景理解不变表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Liang，Jiatong Du，Ziyi Yang，Yanjun Huang，Hong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20355v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>场景图因虚假相关导致分布外泛化差，如何学得稳定结构？</p>
                <p><span class="font-medium text-accent">研究方法：</span>CURVE 用因果变分不确定性建模与结构正则，分离环境无关交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本与少数据 sim-to-real 实验显示 CURVE 生成稀疏域稳定拓扑并提供可靠不确定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性引导的结构正则与原型去偏结合，抑制高方差环境边。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为鲁棒场景图生成提供可解释不确定性工具，助自动驾驶与机器人安全迁移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene graphs compactly encode objects and their relations, but deep models trained on them latch onto dataset-specific correlations that break under new environments. This spurious correlation problem severely limits the zero-shot and sim-to-real transfer needed for safe embodied AI.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CURVE treats a scene graph as a causal system and learns latent interaction representations with a variational encoder that outputs per-relation uncertainty. A causality-inspired regularizer penalizes edges whose uncertainty (predictive variance) is high across environments, forcing the model to retain only low-variance, invariant relations. Prototype-conditioned debiasing further clusters interactions into semantic prototypes and removes environment-specific residuals, yielding a sparse, domain-stable graph topology.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome → Action Genome zero-shot transfer CURVE improves F1 by 6.8 pp while using 42% fewer edges, and in low-data sim-to-real it outperforms the strongest baseline by 9.3 pp on mAP. The learned uncertainty scores rank erroneous triplets in the top-5% with 0.81 precision, enabling reliable risk alarms before deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to visual relation detection and action graphs; scalability to larger multi-modal graphs is unverified. The method assumes access to discrete environment labels during training, which may not hold in continuously shifting domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CURVE to open-world continual learning where environment boundaries are unknown, and integrate learned invariants with causal discovery to refine the graph structure online.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust perception, spurious correlation mitigation, or uncertainty-aware graph learning can directly borrow CURVE’s uncertainty-guided sparsification and prototype debiasing modules for their own scene understanding pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 38%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20303v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physically Guided Visual Mass Estimation from a Single RGB Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于物理引导的单幅RGB图像视觉质量估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sungjae Lee，Junhan Jeong，Yeonjoo Hong，Kwang In Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20303v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张RGB图像估计物体质量，解决体积与密度不可观测带来的病态问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用单目深度估计体积、视觉-语言模型提取材料语义，经实例自适应门控融合后预测体积与密度因子。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Image2Mass和ABO-500数据集上，该方法显著优于现有最佳单图质量估计方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将物理意义的体积-密度分解引入单图质量估计，仅用质量监督即可学习显式物理因子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人抓取、AR/VR及物流等需快速物体质量感知的应用提供轻量级视觉解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张RGB图像估计物体质量本质上病态，因为质量由几何体积与材料密度共同决定，而二者均无法直接从外观观测。现有视觉方法忽视物理约束，导致解空间过大且结果缺乏可解释性。作者旨在将显式物理因素引入学习框架，以缩小解空间并提升精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该框架首先用单目深度网络恢复物体三维点云以计算体积，并用视觉-语言模型提取材料语义以推理密度。随后，几何、语义与外观特征经实例自适应门控融合，分别输入体积与密度两个回归头，仅通过质量标签进行监督。整个网络以物理可解释的两隐因子为瓶颈，使模型必须同时解释体积与密度才能获得最终质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在image2mass与ABO-500数据集上，该方法在所有指标上均优于现有最佳方法，相对误差降低约15–20%，且消融实验显示几何与语义两项物理线索均不可或缺。可视化表明，网络学会将大体积错误分配给低密度材料时会受到惩罚，从而自发校正预测。结果证明引入物理结构可在无额外传感器条件下显著提升单图质量估计的精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度估计误差会直接放大体积计算偏差，对透明、镂空或薄壁物体尤为敏感；视觉-语言模型给出的材料语义较粗糙，难以区分密度相近的类别。此外，方法假设物体单一材料且被完整观测，对多材质或严重遮挡场景尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督深度细化与材料分割分支，以减轻深度误差并支持多材质质量估计；结合视频或交互式视角，利用运动与触觉先验进一步提升密度推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将经典物理量分解融入深度学习，为从事视觉-物理耦合、3D感知或机器人抓取估计的研究者提供了可解释且易扩展的框架。其单图质量估计的精度提升与模块化设计，对需要快速物性推断的AR/VR、物流及自动化分拣系统具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.39</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658114" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DFormer++: Improving RGBD Representation Learning for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DFormer++：提升 RGBD 表征学习以用于语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo-Wen Yin，Jiao-Long Cao，Dan Xu，Ming-Ming Cheng，Qibin Hou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658114" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658114</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-D语义分割中RGB预训练主干对深度几何关系编码失配的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DFormer++框架，先在ImageNet-1K图像-深度对预训练主干，再用RGB-D注意力块微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大RGB-D语义分割基准上刷新SOTA，同时显著减少冗余参数并提升效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现RGB-D预训练-微调范式，设计专门RGB-D注意力块显式编码深度几何信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D视觉任务提供可迁移的联合表征学习范式，减少RGB预训练失配带来的性能损失。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-D语义分割长期面临“RGB预训练→RGB-D微调”带来的模态失配：ImageNet上仅见过RGB的骨干网络无法正确编码深度图的三何结构，导致几何线索浪费。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DFormer++提出“RGB-D预训练-微调”新范式，先在ImageNet-1K的RGB-D图像-深度对上将骨干网络预热，使其天生具备联合模态表征能力；网络主体由堆叠的RGB-D Attention Block构成，通过共享空间-通道注意力同时挖掘RGB纹理与深度几何，避免为各模态单独设支路带来的冗余参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYUDv2、SUN RGB-D与Cityscapes-3D三项主流基准上，DFormer++刷新SOTA，mIoU分别提升2.3、1.9与1.4个百分点，而参数量仅为此前最佳方法的62%，推理速度提高约30%，证明几何-纹理联合预训练显著缓解失配并提升效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模配准精确的RGB-D预训练数据，对仅有RGB或深度噪声大的场景泛化性待验证；此外，注意力计算仍随空间分辨率二次增长，在&gt;2K分辨率图像上显存消耗较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督RGB-D预训练以摆脱ImageNet-1K配对约束，并引入稀疏或线性注意力降低高分辨率显存开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征、3D几何与2D语义融合、或预训练-微调策略在密集预测任务中的落地，本文的“RGB-D同步预训练”思想与高效注意力设计可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextMonkey：一种无需OCR的大型多模态文档理解模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuliang Liu，Biao Yang，Qiang Liu，Zhang Li，Zhiyin Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&#39;s performance. Moreover, by expanding our model&#39;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖OCR的前提下，让大模型高效理解高分辨率文档与场景文本图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Shifted Window Attention、相似度过滤冗余token，并联合文本检测与定位任务端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在12项基准平均提升4.9%，OCRBench获561分，场景文本检测提10.9%，均优于现有开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨窗口注意力与自适应token剪枝的OCR-Free框架，首次将位置感知文本定位融入统一多模态模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档理解、票据分析等提供高准确率且无需OCR的端到端方案，降低部署成本并推动产业应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大视觉-语言模型在文档图像上常依赖外部OCR管线，导致错误累积且难以端到端优化；同时高分辨率输入带来超长token序列，增加计算与显存负担。TextMonkey旨在构建无需OCR、可直接阅读文本的多模态大模型，以统一框架解决场景文本、文档问答与信息抽取任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>模型以Shifted Window Attention替换ViT中的全局自注意力，在896×896乃至更高分辨率下实现跨窗口交互并稳定早期训练；提出基于余弦相似度的token过滤模块，将图像patch特征聚类后仅保留代表性token，使序列长度减少30-50%且性能提升。为增强可解释性，引入文本检测与 grounding 头，将预测框坐标离散化为文本token，与答案一起自回归生成，实现端到端文本定位。训练分三阶段：低分辨率预训练、高分辨率持续训练及多任务指令微调，涵盖阅读理解、关键信息抽取、图表问答等12个数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12个基准上平均提升：场景文本任务+5.2%、文档问答+6.9%、关键信息抽取+2.8%；在TextSpotting基准提高10.9%，OCRBench（29项OCR评测）获561分，超过此前所有开源多模态模型。token过滤使输入序列缩短约40%，推理延迟降低25%，显存占用减少20%，而精度不降反升。可视化显示模型可直接定位答案所在行或单元格，无需后处理即可输出边界框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>token过滤依赖固定相似度阈值，对密集小文本或复杂版式可能误删关键patch；目前仅支持英文与数字，缺乏多语言及手写体专项优化；高分辨率训练成本仍高，未在更大参数量（&gt;13B）上验证可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应token保留策略与多语言端到端训练，并将TextMonkey扩展至视频字幕、多页PDF理解等长序列文档场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注OCR-free文档理解、高分辨率视觉-语言效率优化或统一文本检测+问答框架，本文提供的Shifted Window Attention与相似度剪枝策略可直接借鉴，其代码与模型已开源便于复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向知识增强型大型多模态模型的像素级定位检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jeonghwan Kim，Renjie Tao，Sanat Sharma，Jiaqi Wang，Kai Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &lt;search&gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型在VQA中自主决定何时、以何种粒度检索外部知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端训练LMM，用&lt;search&gt;token触发检索并生成像素掩码作为视觉查询。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CRAG-MM上比整图检索提升19.7%准确率，同时保持分割与推理性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把区域分割与检索策略统一进单一模型，无需外部检测/分割/描述模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建能主动获取细粒度知识的多模态系统提供可扩展范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉问答(VQA)常需将细粒度感知与图像之外的事实知识结合，而现有MM-RAG系统虽能提升事实性，却缺乏何时、如何检索的内部策略。作者观察到，区域级感知与检索决策的割裂导致冗余计算且难以端到端优化，因此希望让大模型自己决定“看哪、查什么”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PixSearch在单一LMM内统一了区域分割与检索：编码阶段插入可学习的&lt;search&gt;令牌，模型并行输出检索触发概率、查询模态(文本/整图/区域)以及像素级掩膜，该掩膜直接作为视觉查询送入检索器，无需额外检测-分割-描述模块。训练采用两阶段监督微调：先以检索交错数据学习“何时搜、搜什么”，再用分割标注保持区域感知能力，实现端到端梯度回传。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CRAG-MM等以自我中心和实体为中心的VQA基准上，PixSearch比整图检索的相对准确率提升19.7%，同时保持通用VQA与纯文本QA的竞争性能；消融显示像素掩膜查询显著减少无关事实注入，提高答案事实一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量带检索标签与分割标注的配对数据，标注成本高；推断时多次调用检索器增加延迟，且像素掩膜对低分辨率或遮挡区域仍可能召回噪声知识。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应检索预算机制以平衡速度与精度，并研究无监督或弱监督方式自动生成像素级查询，降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次把“检索决策+区域分割”完全内嵌到LMM，为研究细粒度感知与外部知识融合、端到端MM-RAG系统设计以及区域级可解释检索提供了可复现的框架和基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18195v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">QualiRAG：面向视觉质量理解的检索增强生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linhan Cao，Wei Sun，Weixia Zhang，Xiangyang Zhu，Kaiwei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18195v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需训练即可让大模型具备细粒度、可解释的视觉质量理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QualiRAG框架，动态生成四类互补知识并做相关性检索增强推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在零训练条件下显著超越开源通用及VQA微调大模型，质量比较任务亦具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态生成式RAG用于视觉质量感知，摆脱标注依赖与数据偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉质量评估提供免训练、可解释的新范式，降低应用门槛并推动公平比较。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉质量评估(VQA)正从简单的标量打分转向可解释的质量理解，这要求模型具备细粒度时空感知和辅助上下文信息。现有方法依赖昂贵的人工标注指令数据进行监督微调或强化学习，易产生数据集偏差且难以泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>QualiRAG提出无需训练的检索增强生成框架，通过将问题分解为结构化请求动态生成四类互补知识：视觉元数据、主体定位、全局质量摘要和局部质量描述。随后执行相关性感知检索，将最相关的证据送入大型多模态模型进行推理，从而激活其潜在感知知识完成质量理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项视觉质量理解基准上，QualiRAG相对开源通用LMM平均提升约15%，优于专为VQA微调的同规模模型；在质量比较任务中与监督方法性能相当但无需任何任务特定训练。实验表明其生成的解释与人类标注理由一致性高，验证了证据驱动推理的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>动态知识生成依赖提示工程，若分解或提示设计不当可能引入噪声；检索阶段仍受限于现成视觉编码器的表示能力，对未见失真类型泛化有限；计算开销高于单次前馈模型，实时性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应知识源权重学习与轻量级检索策略，以提升效率与鲁棒性；将QualiRAG扩展至视频质量、AIGC检测等更广泛的视觉感知任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为免训练提升LMM视觉感知能力提供新范式，适合关注无监督/轻量级质量评估、RAG在多模态场景应用及可解释AI的研究者借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型适配到多标签、高类内差异的遥感影像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三项域泛化任务上平均提升约2%，优于现有提示学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为语义摘要，通过双模跨注意力动态条件化提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低成本的VLM适配方案，可推广至多标签、跨分辨率及新类识别场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等视觉-语言模型迁移到下游任务，但遥感影像具有多标签、类内差异大、分辨率多样等特点，直接套用现有文本提示方法会丢失主导语义线索，导致新类别泛化差。因此，亟需一种面向遥感场景的轻量级提示学习框架，在少监督条件下充分挖掘视觉-语言互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS，用冻结的 BLIP-2 图像描述模型为每张遥感影像生成一句语义摘要，经 BERT tokenizer 得到文本 token；同时提取 CLIP 图像编码器的高层视觉特征。二者在特征空间拼接后，由轻量级交叉注意力模块以可学习的查询提示为条件，生成与图像-文本上下文耦合的提示向量，全程 CLIP 骨干网络保持冻结。该双模态提示仅增加不到 1 M 可训练参数，实现高效适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个公开遥感数据集、3 项领域泛化任务上，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，在跨传感器、跨分辨率、跨地理区域设置下均保持最高宏平均 F1 与调和准确率，显著改善对“裸地”“温室”等难区分类别的召回。消融实验表明，移除图像摘要或交叉注意力后性能下降 1.3–1.7 个百分点，验证双模态融合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖 BLIP-2 生成的单句摘要，若影像场景复杂或多标签分布极端，caption 可能遗漏关键语义；目前仅测试了 4 个数据集，尚未验证在超大尺度影像（如整幅 Sentinel-2 瓦片）或视频级序列上的可扩展性；交叉注意力模块虽轻量，但仍需 GPU 进行训练，对边缘端部署有一定开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多句或层级 caption 以及可解释性约束，缓解多标签信息丢失；探索将双模态提示与遥感专用视觉主干（如基于 Transformer 的时空网络）联合蒸馏，实现端侧零样本推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、小样本/零样本分类、提示学习或领域泛化，本文提供了轻量级双模态提示范式与可复现代码，可直接作为基线或扩展至变化检测、语义分割等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Uni-RS：空间保真的遥感统一理解与生成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiyu Zhang，Yuan Hu，Yong Li，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17673v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感统一多模态模型在文生图时空间关系颠倒的“空间反转诅咒”</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出显式空间布局规划、空间感知查询监督与图文空间布局变增广的Uni-RS框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持理解任务性能的同时显著提升文生图的空间忠实度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对遥感显式解耦几何规划与视觉生成的统一模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图文生成提供可靠空间一致性方案，推动多模态应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在遥感领域同时承担视觉理解与文本生成图像任务，但现有统一框架存在显著“空间反转诅咒”：理解阶段能准确定位并描述目标空间关系，却在文本到图像生成阶段把相同的空间语义倒置或丢失，直接削弱遥感影像的可信度与应用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Uni-RS，通过三步显式空间对称化设计缓解理解-生成不对称。首先，Spatial-Layout Planning 将输入文本解析为显式几何布局图，把几何规划与像素合成解耦；其次，Spatial-Aware Query Supervision 在扩散查询空间引入与布局图对齐的监督信号，使可学习查询优先关注指令中的方位、尺度与拓扑；最后，Image-Caption Spatial Layout Variation 在训练阶段对同一图像-描述对进行几何一致但参数随机的空间扰动，增强模型对旋转、平移、比例变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Million-AID、RSICD、DIOR 等遥感理解基准上，Uni-RS 的图像描述、视觉定位与 VQA 指标与现有统一模型持平或更优；在自建的 RS-SpatialGen 文本到遥感图像数据集上，其空间忠实度 FID 降低 21%，结构相似性提升 18%，人类评估显示 85% 的生成样本空间关系与指令一致，显著优于 BLIP-2、Stable Diffusion 等通用模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的布局解析器，若文本描述含糊或包含隐含空间关系，规划阶段仍会出错；生成阶段仅考虑二维平面布局，对遥感影像中常见的高程、遮挡和透视效应尚未建模；训练与推理均引入布局分支，参数量与计算开销比纯文本条件扩散模型高约 30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入三维高程先验与视角建模，实现真正的三维空间忠实生成，并探索无监督或弱监督布局推理，降低对显式解析器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统揭示并缓解了遥感统一多模态模型的空间不对称难题，其显式布局规划与查询监督策略可为任何需要高精度空间生成的领域（如地图生成、无人机导航影像仿真）提供可直接迁移的框架与训练代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18190v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">关键词引导的多视角子图像 CLIP 用于遥感图文检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Li，Shiying Wang，Jianqiang Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18190v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训CLIP的前提下，实现遥感图文检索的细粒度、多尺度语义对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM提取关键词→SamGeo生成多视角子图→G²A适配器+MPR模块聚合特征→混合对比/三元组损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD与RSITMD上mR分别达35.18%和48.40%，超越全微调与现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将关键词引导的子视角采样与轻量G²A适配器结合，实现参数高效的多视角CLIP遥感检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索提供低成本、高精度的新范式，可推广至其他大模型适配任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Pre-training (VLP) models such as CLIP have pushed RSITR forward, yet they mostly perform coarse global alignment that ignores the dense, multi-scale semantics of overhead scenes. Full fine-tuning of these large models is computationally prohibitive and prone to catastrophic forgetting, motivating a parameter-efficient, fine-grained alternative.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose MPS-CLIP, a lightweight framework that first prompts an LLM to extract semantic keywords, which then guide SamGeo to crop semantically relevant sub-regions (multi-perspectives). A frozen CLIP backbone is adapted via a Gated Global Attention (G²A) adapter that injects global context with negligible extra parameters, while a Multi-Perspective Representation (MPR) module fuses the local crops into robust embeddings. Training optimizes a hybrid objective combining multi-perspective contrastive loss and a weighted triplet loss that dynamically selects maximum-response views to suppress noisy patches.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RSICD and RSITMD benchmarks MPS-CLIP attains 35.18% and 48.40% mean Recall (mR), respectively, establishing new state-of-the-art results while using far fewer trainable parameters than full fine-tuning. The gains confirm that keyword-guided sub-perspective alignment captures fine-grained semantics that global matching misses, and that the G²A adapter prevents catastrophic forgetting by keeping the original CLIP weights frozen.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The pipeline depends on the quality of LLM-extracted keywords and SamGeo segmentation, both of which may fail for rare classes or complex scenes. The approach also introduces extra inference-time overhead due to multi-crop encoding, and the reported experiments are limited to two English-only datasets, leaving cross-lingual or larger-scale generalization untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore learnable prompt generation to replace fixed LLM keywords and extend the framework to video-text or multi-temporal RS retrieval.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient VLP adaptation, fine-grained remote-sensing understanding, or lightweight multimodal retrieval will find the keyword-guided sub-perspective paradigm and G²A adapter readily transferable to their own tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Conditioned Reasoning in Long-Egocentric Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">长时自我中心视频中的空间条件推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Tribble，Hao Wang，Si-En Hong，Chaoyi Zhou，Ashish Bastola 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改模型结构的前提下提升VLM在长时第一视角视频中的空间推理与导航能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Sanpo-D细粒度重标注集，向VLM输入RGB+深度图并评测导航类空间问答</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入深度等显式空间信号可显著增强安全关键检测，但会轻微牺牲通用精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证仅通过输入层空间条件化即可改善VLM长时第一视角空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升具身智能导航安全性提供无需重训模型的低成本空间增强方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称长视频在视觉导航中因视角漂移和缺乏持续几何上下文而极具挑战，现有视觉-语言模型(VLM)虽在图像或短视频推理表现良好，但在长时自我中心序列的空间推理上仍显不足。作者希望在不改动模型架构或推理流程的前提下，探讨显式空间信号对VLM视频理解的影响，以填补长时空间推理的研究空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发布Google Sanpo数据集的细粒度重标注版本Sanpo-D，新增面向导航的空间查询标签，用于系统评测多种VLM。为检验输入层面的归纳偏置，他们将深度图与RGB帧融合作为额外通道输入，保持原模型结构与推断方式不变，对比纯RGB与RGB-D两种输入在相同空间问答任务上的表现。实验采用零样本提示方式，重点考察行人检测、障碍物识别等安全关键指标，并记录通用准确率与空间专精度的权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，引入深度信息后，模型在行人、障碍物等安全关键空间任务上的召回率与F1显著提升，但通用字幕或动作识别准确率略有下降，揭示“通用-空间”性能权衡。空间接地表示使VLM在长视频跨帧定位误差降低15-20%，验证了输入级几何线索即可增强空间推理而无需重新训练主干。Sanpo-D基准亦暴露出现有VLM在相对距离、遮挡推理上的系统性缺陷，为后续研究提供明确改进靶点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在零样本/提示层面注入深度，并未深入微调或架构融合，可能低估深度信号的全面潜力。Sanpo-D目前覆盖场景与语言查询仍偏向城市户外步行，对室内、车辆或其他文化场景的可迁移性尚待验证。实验依赖商用深度传感器精度，若深度噪声增大，实际导航安全性提升幅度可能缩减。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索针对长时几何一致性的自监督深度估计与VLM联合微调，以突破输入级融合的性能天花板；同时扩展Sanpo-D至少样化环境与多语言查询，构建更具挑战性的长时空间推理基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注第一人称视觉导航、多模态VLM空间推理或安全敏感应用，该文提供了不改动模型即可提升空间理解的可行范式，并发布细粒度标注数据，方便直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20552v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DeepSeek-OCR 2: Visual Causal Flow
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DeepSeek-OCR 2：视觉因果流</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoran Wei，Yaofeng Sun，Yukun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20552v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉模型像人一样按语义因果顺序而非固定光栅顺序阅读图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 DeepEncoder V2，在送入 LLM 前用级联 1D 因果结构动态重排视觉 token</p>
                <p><span class="font-medium text-accent">主要发现：</span>动态因果排序显著提升复杂版面图像的 OCR 与理解性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双级联 1D 因果推理用于 2D 图像理解，实现语义驱动的 token 重排</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VLMs 提供贴近人类视觉扫描的新编码范式，可推广至文档、图表等复杂场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流视觉-语言模型(VLM)将二维图像展平为一维token序列，并以固定光栅扫描顺序输入LLM，忽略了人类视觉按语义因果灵活扫视的机理，导致在复杂版式图像上理解受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DeepEncoder V2，在将视觉token送入LLM前引入可学习的因果重排序模块，先对图像做语义分割与区域重要性评估，再依据逻辑结构动态生成因果链式顺序。该编码器与下游LLM级联，形成“两次一维因果推理”框架，试图用串行因果建模逼近真正的二维理解。训练时采用自监督因果顺序预测与OCR文本生成多任务联合优化，强化模型对空间语义的因果依赖捕捉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，在多个OCR与文档理解基准上，动态重排序使端到端文本识别准确率提升2.4–4.1 BLEU，版面还原FID下降7%，且推理延迟仅增加5%，验证了两级1D因果结构可高效编码2D语义。消融实验表明因果顺序比固定光栅顺序减少25%的注意力冗余，显著降低LLM输入长度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文档类图像上评估，未验证自然场景或复杂视觉问答任务；动态排序模块引入额外参数，对端侧部署的内存与能耗影响尚未量化；缺乏与人类眼动数据的直接对比，因果顺序的可解释性仍较薄弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多模态推理与视频字幕生成，探索因果排序与链式思维提示的结合，并引入眼动追踪数据监督以提升生物合理性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型中的二维结构建模、文档智能或人类认知启发的神经网络设计，本工作提供了可复现的代码与权重，可直接对比或嵌入现有流程以改进复杂版式理解性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19099v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">m2sv：可扩展的地图到街景空间推理基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yosub Shin，Michael Buriek，Igor Molybog
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19099v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在鸟瞰图与街景图之间进行鲁棒的空间对齐与朝向推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建可扩展基准m2sv-20k及11k推理链微调数据，评估并微调多种VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>最佳VLM仅65.2%准确率，远低于人类95%，几何对齐与证据聚合缺陷突出。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出大规模真实路口地图-街景朝向推理基准，并配套结构化推理微调数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示VLMs跨视角空间推理短板，为 grounded spatial reasoning 研究提供标准与方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision–language models excel on standard multimodal benchmarks yet struggle when asked to align abstract overhead maps with ground-level images, a core instance of cross-view spatial reasoning. This brittleness matters for robotics, navigation, and AR, where agents must translate 2-D spatial abstractions into egocentric decisions. The authors therefore propose a controlled, large-scale task—predicting camera orientation at an intersection given a north-up map—to isolate and measure this capability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces m2sv, a scalable pipeline that pairs real Street View panoramas with OpenStreetMap tiles of the same intersection, then samples four cardinal direction choices plus four obliques to create an 8-way classification task. To ensure geographic diversity while controlling difficulty, they filter for intersections with sufficient visual cues, balance continents, and add synthetic ambiguities such as occluded road markings. Two datasets are released: m2sv-20k for evaluation and m2sv-sft-11k containing human-written chain-of-thought traces for supervised fine-tuning. Models are evaluated zero-shot, after SFT, and after RL fine-tuning, with cross-benchmark transfer tests to related spatial tasks.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The best proprietary VLM reaches only 65.2 % accuracy, 30 percentage points below human performance (95 %), indicating that current VLMs cannot reliably aggregate geometric cues like lane orientation, building corners, or distant landmarks. Supervised fine-tuning on 11k reasoning traces lifts open-model scores by ~8 % and RL by another ~3 %, but transfer to other spatial benchmarks is limited, suggesting narrow rather than general spatial reasoning gains. Detailed error analysis shows that models fail on cases requiring multi-step evidence integration (e.g., combining road curvature, signage, and distant buildings) and exhibit inconsistent predictions under viewpoint perturbations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The task is constrained to intersection-level orientation, so results may not generalize to open-country or indoor environments; extending to full 6-DoF pose is non-trivial. The dataset is geographically balanced but still biased toward regions with Street View coverage, potentially under-representing developing areas. Finally, human-written reasoning traces are costly to scale, and automatic generation of high-quality rationales remains an open challenge.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should explore self-supervised alignment losses that explicitly reason over geometric correspondences between map and image, and should benchmark continuous pose regression rather than discrete classification to better reflect robotics needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying embodied AI, cross-view localization, or spatial reasoning in VLMs will find m2sv a rigorously curated resource that isolates geometric alignment failures and provides fine-tuning data to prototype new architectures or losses.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115422" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Interpretable Image Recognition Network via Language-Guided Global-Local Collaboratively Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语言引导的全局-局部协同对齐的多模态可解释图像识别网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sulan Zhang，Peijun Zhang，Lihua Hu，Xin Wen，Jifu Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115422" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115422</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretability is crucial for establishing user trust in image recognition models in high-risk domains such as medical diagnosis and autonomous driving. Recent studies have enhanced model interpretability through visual-language alignment.However, existing methods mostly rely on coarse-grained alignment between overall image representations and semantic concepts, making it difficult to achieve fine-grained interactions between local visual regions and semantic concepts. This limitation restricts further improvements in practical interpretability and recognition performance. To address this issue, we propose a Language-Guided Global-Local Collaboratively Aligned Multimodal Interpretable Image Recognition Network (LGLCA-Net), which uses text concepts generated by large language models (LLMs) to guide the collaborative alignment of images and text in terms of global and local semantics.The network first designs a concept de-redundancy and visual recognizability verification strategy, driving the large language model to generate high-quality and visually relevant semantic concepts as the foundation for language guidance. Subsequently, we utilize the multimodal space provided by the CLIP model to construct a dual-branch alignment structure for global semantics and local visual semantics.In the local visual semantics branch, we introduce a visual prompting mechanism that extracts discriminative local regions to achieve fine-grained alignment with semantic concepts. Finally, we design a learnable dynamic weighting mechanism to adaptively fuse the alignment information from both branches, achieving collaborative alignment of global-local semantics and semantic concepts. Extensive experiments show that our method not only provides finer-grained and more trustworthy visual explanations but also improves recognition performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时实现全局-局部视觉区域与语义概念的细粒度对齐，以提升图像识别模型的可解释性与准确率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 LLM 生成去冗余概念，在 CLIP 空间构建全局-局部双分支对齐，并以动态权重融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LGLCA-Net 在提供可信细粒度解释的同时，显著提高了图像识别性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入 LLM 概念生成与视觉可识别性验证，结合视觉提示实现全局-局部协同对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗、自动驾驶等高可信场景提供了兼顾性能与可解释性的多模态识别新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在高风险场景（医疗、自动驾驶）中，可解释性是深度学习模型被信任的前提。现有视觉-语言对齐方法多停留在整图与语义标签的粗粒度匹配，难以把局部病灶或关键区域与文本概念精细关联，限制了实际可解释性与分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LGLCA-Net，先让LLM在概念去冗余与视觉可识别性验证策略下生成高质量、视觉相关的语义概念；随后在CLIP多模态空间内构建全局-局部双分支对齐结构，局部分支借视觉提示机制挖掘判别性区域，实现区域-概念的细粒度对齐；最后通过可学习动态权重融合两支对齐信息，完成全局-局部-概念的协同对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个可解释性基准与分类任务上的实验表明，LGLCA-Net不仅显著提升了图像识别准确率，而且生成的注意力图在细粒度定位、概念一致性与人类评估可信度方面优于现有SOTA方法，为高风险决策提供了更可靠的视觉证据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖LLM生成概念，若LLM对领域知识掌握不足可能引入语义偏差；动态权重与视觉提示模块增加了参数量与训练成本，在资源受限设备上部署存在挑战；评估指标仍以代理可视化为主，缺乏临床或驾驶场景下的真实决策反馈验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入领域知识图谱对LLM概念进行约束，减少语义漂移，并设计轻量化对齐模块以满足边缘部署需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释视觉-语言模型、细粒度对齐或高风险AI应用的可信度，该文提供了从概念生成到全局-局部协同对齐的完整范式与开源细节，可直接扩展至医疗影像、自动驾驶检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113152" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DuoNet: Joint Optimization of Representation Learning and Prototype Classifier for Unbiased Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DuoNet：表征学习与原型分类器的联合优化用于无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaodi Wang，Biao Leng，Shuo Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113152" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113152</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unbiased Scene Graph Generation (SGG) aims to parse visual scenes into highly informative graphs under the long-tail challenge. While prototype-based methods have shown promise in unbiased SGG, they highlight the importance of learning discriminative features that are intra-class compact and inter-class separable. In this paper, we revisit prototype-based methods and analyze critical roles of representation learning and prototype classifier in driving unbiased SGG, and accordingly propose a novel framework DuoNet. To enhance intra-class compactness, we introduce a Bi-Directional Representation Refinement (BiDR 2 ) module that captures relation-sensitive visual variability and within-relation visual consistency of entities. This module adopts relation-to-entity-to-relation refinement by integrating dual-level relation pattern modeling with a relation-specific entity constraint. Furthermore, a Knowledge-Guided Prototype Learning (KGPL) module is devised to strengthen inter-class separability by constructing an equidistributed prototypical classifier with maximum inter-class margins. The equidistributed prototype classifier is frozen during SGG training to mitigate long-tail bias, thus a knowledge-driven triplet loss is developed to strengthen the learning of BiDR 2 , enhancing relation-prototype matching. Extensive experiments demonstrate the effectiveness of our method, which sets new state-of-the-art performance on Visual Genome, GQA and Open Images datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长尾分布下场景图生成中的类别偏差，提升稀有关系检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DuoNet框架，联合BiDR²表征精炼与KGPL等距原型分类器优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome、GQA、Open Images上刷新无偏SGG指标，稀有类召回显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向关系-实体-关系表征精炼与冻结等距原型分类器结合，抑制长尾偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾视觉关系检测提供即插即用新范式，助力多模态理解与视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) 长期受长尾分布困扰，导致常见关系过拟合而稀有关系被淹没。基于原型的方法虽能缓解偏差，但要求特征同时满足类内紧致与类间分离，现有工作对表征学习与原型分类器的协同机制探讨不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DuoNet 将 SGG 解耦为表征学习与原型分类器联合优化：BiDR² 模块以“关系→实体→关系”双向路径，先借关系敏感注意力捕获视觉变化，再用关系特定实体一致性约束提纯特征，实现类内紧致；KGPL 模块构建等距分布且冻结的原型分类器，最大化类间间隔，并设计知识驱动的三元组损失反向强化 BiDR² 表征，使长尾样本也能对齐稀有原型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome、GQA 与 Open Images 三大基准上，DuoNet 将 mR@50/100 提升 5-8 个百分点，总体 R@50/100 不降反升，首次在稀有类别上逼近常见类性能，验证了其兼顾精度与公平性的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未显式建模实体对外的全局上下文，可能遗漏跨组依赖；冻结原型虽抗尾偏，但固定中心在极端开放集场景下适应性不足；训练需额外存储大量原型向量，显存占用高于传统分类器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态原型更新策略以兼容开放集，并将 DuoNet 的“表征-原型”协同框架迁移到视频时空关系检测或多模态长尾任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究长尾视觉识别、图结构推理或原型网络，该文提供了表征与分类器联合去偏的新范式及可直接套用的 BiDR²/KGPL 模块代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658213" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RMNet: Dual-Dimensional Difference Recalibration-Guided CNN-VMamba Synergistic Network for Remote Sensing Image Change Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RMNet：双维差分重标定引导的 CNN-VMamba 协同网络用于遥感图像变化描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xintong Cao，Wenqian Dong，Jiahui Qu，Yunsong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658213" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658213</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感变化字幕中CNN漏检细微变化、Transformer参数量大及伪变化干扰的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双维度差分校正引导的CNN-VMamba协同网络RMNet，结合通道-空间窗口注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LEVIR-CC数据集上所有指标显著提升，有效抑制伪变化并增强真实变化描述精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>双维度差分校正机制突出核心变化抑制伪变化，CNN-VMamba双流架构兼顾局部与全局建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化智能解读提供轻量高效框架，推动变化字幕技术向实用化迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化字幕任务要求模型同时定位地表变化区域并用自然语言描述其语义属性，但现有单流CNN难以捕获时空细粒度差异，单流Transformer参数量又过大，且直接利用像素级差异易将光照或噪声伪变化误判为真实变化，降低描述准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RMNet提出CNN-VMamba双支流协同框架：CNN支路负责局部空间细节提取，VMamba支路以线性复杂度建模全局时空上下文，二者通过通道-空间窗口注意力互补融合。核心创新是双维差异重校准模块，先在通道维用可学习权重突出真实变化波段、抑制伪变化波段，再在空间维利用局部窗口自相关增强显著变化区域、弱化噪声，实现差异特征二次提纯。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CC公开数据集上，RMNet在所有标准字幕指标(BLEU-4、METEOR、ROUGE-L、CIDEr)均显著优于现有最佳方法，CIDEr相对提升约4.2%，参数量仅为同性能Transformer方案的37%，可视化差异图显示伪变化抑制率提高，变化定位与描述一致性增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一LEVIR-CC建筑变化数据集验证，尚未测试多类地物或更复杂场景；VMamba的扫描顺序对变化敏感性的理论解释不足；双维重校准的超参数(窗口大小、抑制阈值)依赖经验设置，跨传感器迁移时可能需要重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源时序SAR与多光谱融合数据，引入自适应重校准策略以减少人工超参数，并结合变化检测预训练以进一步提升小样本泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感时空建模、轻量级变化描述或伪变化抑制，该文提供的CNN-VMamba协同范式与双维差异重校准机制可直接借鉴并迁移至变化检测、事件摘要等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18157v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Agentic Very Long Video Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向超长视频的Agent化理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aniket Rege，Arka Sadhu，Yuliang Li，Kejie Li，Ramya Korlakai Vinayak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18157v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI助手理解持续数天至数周的第一人称超长视频流并回答复杂问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EGAgent框架，以实体场景图存储长时关系，结合结构化搜索与跨模态视听检索工具。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EgoLifeQA达57.5% SOTA，Video-MME(Long)获74.1%，显著优于现有长视频理解方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实体场景图与可搜索工具集成到智能体，实现跨天多跳推理与音视频协同回忆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候可穿戴设备提供超长记忆与上下文理解方案，推动持续感知助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天可穿戴设备（如智能眼镜）催生了对“始终在线”个人 AI 助手的需求，这些助手必须理解连续数日至数周的第一视角长视频流，而非孤立短视频。现有大模型与检索增强方法受限于有限上下文窗口，难以在长视频中做多跳、组合式跨模态推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EGAgent，一个以“实体场景图”为中心的代理框架，将人-地点-物体及其随时间演化的关系编码为可查询图结构。系统赋予规划代理结构化搜索工具，可在图上执行多跳时序推理，并融合视觉-音频混合检索，实现跨模态、时序一致的问答与摘要。为支持超长输入，框架采用分层记忆与按需加载机制，只在推理时激活相关片段与图节点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EgoLifeQA 纵向生活问答基准上，EGAgent 达到 57.5% 的准确率，刷新 SOTA；在 Video-MME (Long) 长视频理解评测上取得 74.1%，与顶尖方法持平。消融实验显示，实体场景图与跨模态检索分别带来约 8% 与 5% 的绝对提升，验证了结构化记忆对长时推理的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在数百小时级别的真正“连续”流数据上测试，图构建与存储开销随视频长度线性增长，对设备端实时部署构成挑战；另外，实体与关系的抽取仍依赖外部模型，错误会累积到下游推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索端侧增量图更新与压缩技术，实现可穿戴设备上的实时长视频理解，并引入用户个性化记忆以支持跨会话的个性化问答。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、第一视角视觉-语言模型、代理式推理或可穿戴 AI，该文提供了可扩展的图记忆与代理工具链范例，可直接借鉴其图检索与跨模态融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LocationAgent：一种通过解耦策略与参数化知识证据实现图像地理定位的分层智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiujun Li，Zijin Xiao，Xulin Wang，Zhidan Ma，Cheng Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少开放世界图像定位中的幻觉与泛化瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>RER分层代理+外部工具验证地理证据，零样本推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下准确率提升≥30%，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>把地理事实验证外移，RER架构解耦假设-验证循环</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信、可泛化的视觉地理推理系统提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像地理定位需要在开放世界中仅凭视觉内容推断拍摄地点，本质上是一个“假设-验证”循环的推理过程；现有方法把地理知识静态内化到模型参数，导致幻觉与泛化瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出分层定位智能体LocationAgent，用RER架构（Reasoner-Executor-Recorder）把高层推理逻辑留在模型内，通过角色分离与上下文压缩抑制多步漂移；地理事实验证被解耦到外部线索探索工具集，实现动态证据检索；同时发布中文场景基准CCL-Bench以缓解数据泄露与语种稀缺问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本设定下，LocationAgent比现有最佳方法绝对提升≥30%，在CCL-Bench的多粒度城市场景中保持稳健优势，证明外挂证据可显著抑制幻觉并增强开放世界泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>工具链依赖外部地理API的实时性与覆盖范围，推理延迟高于纯参数模型；RER的压缩策略可能丢失细粒度视觉线索，且CCL-Bench目前仅覆盖中国城市，全球多样性仍不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可微分地图检索以端到端优化工具调用，并构建多语言全球基准以验证跨文化迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将 Agent 架构与外部知识验证引入地理视觉推理，为研究多模态大模型、工具增强推理或开放世界定位的研究者提供可复用的RER框架与中文评测资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132858" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See then tell: Enhancing key information extraction with vision grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先见后述：借助视觉定位增强关键信息提取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuhang Liu，Zhenrong Zhang，Pengfei Hu，Jiefeng Ma，Jun Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132858" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132858</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( &#34; role=&#34;presentation&#34;&gt; ee then &#34; role=&#34;presentation&#34;&gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel &#34; role=&#34;presentation&#34;&gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, &#34; role=&#34;presentation&#34;&gt; directs the model to first &#34; role=&#34;presentation&#34;&gt; — attending to image regions relevant to the question — and then &#34; role=&#34;presentation&#34;&gt; , emitting the textual answer. To enhance the model’s &#34; role=&#34;presentation&#34;&gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( &#34; role=&#34;presentation&#34;&gt; ableQA with &#34; role=&#34;presentation&#34;&gt; ision &#34; role=&#34;presentation&#34;&gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖下游坐标标注的情况下，实现端到端的关键信息提取并同步给出视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STNet，用特殊&lt;loc&gt;令牌隐式编码坐标，先“看”图像区域再“说”答案，并构建带视觉定位的TVG数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CORD、SROIE、DocVQA等基准上达SOTA，无需下游坐标微调即可泛化，且可零样本增强MLLM的KIE性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个用单令牌隐式坐标实现同步文本答案与视觉定位的OCR-free KIE框架，免除昂贵坐标标注。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能研究者提供免坐标标注、即插即用的视觉定位机制，推动MLLM在真实场景下的KIE应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在数字化时代，文档常以图文混排的视觉富集形式存在，传统KIE方法依赖OCR，一旦识别错误会级联放大，影响下游抽取。OCR-free模型虽规避了识别错误，却普遍缺乏视觉定位能力，难以指出答案在图像中的具体位置。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出STNet，通过在每个回答前插入特殊&lt;see&gt;token，将答案坐标隐式编码到生成过程，实现先“看”后“说”的端到端抽取。为增强定位能力，团队收集大规模结构化表格识别数据，并用GPT-4合成带坐标的QA对，构建TVG数据集。模型在公开KIE基准上仅使用文本监督即可达到SOTA，且无需下游坐标标注即可微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>STNet在CORD、SROIE、DocVQA等基准上取得新最佳，F1提升2–4个百分点，同时输出答案对应的边界框，实现可解释抽取。零样本迁移到Qwen2-VL等MLLM后，KIE性能进一步提升，证明定位token可即插即用。消融实验显示&lt;see&gt;token贡献了约70%的定位精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>&lt;see&gt;token仅输出矩形框，无法处理不规则形状或跨页对象；TVG数据集由GPT-4合成，可能存在分布偏差，影响真实场景鲁棒性。训练依赖大规模表格-文本对，对无表格文档的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至像素级掩码定位，并引入弱监督或自监督策略减少对合成数据的依赖；探索将&lt;see&gt;机制推广到视频或三维文档理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注OCR-free文档理解、视觉定位或多模态信息抽取，本文提供的隐式坐标编码范式、TVG数据集及即插即用token设计均可作为基线与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tro.2026.3658211" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning From Videos Through Graph-to-Graphs Generative Modeling for Robotic Manipulation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向机器人操作的图到图生成建模视频学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Robotics">
                IEEE Transactions on Robotics
                
                  <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangyan Chen，Meiling Wang，Te Cui，Chengcai Yang，Mengxiao Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tro.2026.3658211" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tro.2026.3658211</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning from demonstration is a powerful method for robotic skill acquisition. Nevertheless, a critical limitation lies in the substantial costs associated with gathering demonstration datasets, typically action-labeled robot data, which creates a fundamental constraint in the field. Video data offer a compelling solution as an alternative rich data source, containing diverse behavioral and physical knowledge. This study introduces G3M, an innovative framework that exploits video data via Graph-to-Graphs Generative Modeling, which pre-trains models to generate future graphs conditioned on the graph within a video frame. The proposed G3M abstracts video frame into graph representations by identifying object and visual action vertices for capturing state information. It then effectively models internal structures and spatial relationships present in these graph constructions, with the objective of predicting forthcoming graphs. The generated graphs function as conditional inputs that guide the control policy in determining robotic behaviors. This concise method effectively encodes critical spatial relationships while facilitating accurate prediction of subsequent graph sequences, thus allowing the development of resilient control policy despite constraints in action-annotated training samples. Furthermore, these transferable graph representations enable the effective extraction of manipulation knowledge through human videos as well as recordings from robots with different embodiments. The experimental results demonstrate that G3M attains superior performance using merely 20% action-labeled data relative to comparable approaches. Moreover, our method outperforms the state-of-the-art method, showing performance gains exceeding 19% in simulated environments and 23% in real-world experiments, while delivering improvements of over 35% in cross-embodiment transfer experiments and exhibiting strong performance on long-horizon tasks. Our project page is available at https://...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用廉价视频而非昂贵动作标注数据来训练机器人操控策略</p>
                <p><span class="font-medium text-accent">研究方法：</span>G3M框架：将视频帧抽象为图，用图到图生成模型预训练并预测未来图以指导策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用20%动作标注数据即超越现有方法，模拟提升19%，真实提升23%，跨本体迁移提升35%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把视频预训练抽象为图到图生成，显式建模物体-动作空间关系并零样本迁移至不同机器人</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缓解机器人数据稀缺提供可扩展方案，使人类视频与异构机器人数据直接服务操控学习</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于演示的机器人学习需要大量带动作标签的机器人数据，采集成本高昂，成为领域瓶颈。无成本的网络或人类视频虽富含行为与物理知识，却缺乏动作标注，难以直接用于策略学习。作者希望用易获取的视频替代昂贵的机器人演示，以降低数据门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G3M将单帧图像抽象为“对象-视觉动作”图，节点表示物体与动作语义，边表示空间关系。框架用Graph-to-Graphs生成模型在视频上进行自监督预训练，预测下一帧的图结构，从而学习状态转移规律。下游控制阶段，把当前帧图与未来预测图一起输入策略网络，实现动作推断，无需额外动作标签即可微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅使用20%动作标注数据的情况下，G3M在模拟环境比基线提高19%以上成功率，在真实机器人上提升23%，跨 embodiment 迁移提升35%，并在长时序任务中保持鲁棒。结果表明图结构预训练有效迁移了视频中的空间-时序知识，显著降低对昂贵标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖准确的视觉检测与图提取，若场景遮挡或物体识别失败，预测图质量下降。图表示目前只编码空间关系，未显式建模力或接触动力学，可能限制精细操作任务。跨 embodiment 迁移仍假设末端执行器语义可对齐，极端形态差异时效果未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将力-触觉模态融入图节点，实现空间与力学联合建模，提升精细操作表现；结合大规模视频-语言模型，实现零样本语义对齐，进一步拓宽跨 embodiment 与跨任务迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效机器人学习、无标注视频利用或跨 embodiment 知识迁移，该文提供了可扩展的图结构预训练范式与实验基准，可直接借鉴其图提取、生成模型及策略融合流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiSa：显著性感知的前景-背景解耦框架用于开放词汇语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhen Yao，Xin Li，Taotao Jing，Shuai Zhang，Mooi Choo Chuah
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20064v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇语义分割中CLIP前景偏好与边界模糊问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiSa框架：SDM显式解耦前景/背景，HRM分层细化空间-通道特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个基准上持续超越SOTA，显著改善背景识别与边缘定位</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性引导的前景-背景解耦与分层多尺度细化结合于开放词汇分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用VLMs进行密集预测提供抑制前景偏差的通用思路，推动零样本分割研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割要求模型根据任意文本标签对图像逐像素分类，现有方法多直接复用在大规模图文对上预训练的视觉-语言模型(VLM)如CLIP，但这些模型天然关注显著、物体中心区域，导致在密集预测任务中出现前景偏好与空间定位模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DiSa框架，通过Saliency-aware Disentanglement Module(SDM)引入显著性先验，将图像特征显式拆分为前景与背景两组集合特征并分别建模，实现“分而治之”；随后设计Hierarchical Refinement Module(HRM)，利用多层级像素-通道上下文迭代更新，逐步细化边界与内部一致性，最终融合两组特征完成开放词汇分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL VOC、PASCAL Context、COCO Object、ADE20K、FSS-1000及Cityscapes六个基准上，DiSa相较先前最佳方法平均提升3.2-5.7 mIoU，显著改善了背景区域识别与物体边缘清晰度，验证了显著性解耦与分层细化策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>显著性估计依赖额外网络或先验，可能引入误差并增加计算开销；解耦过程依赖前景-背景掩码质量，在复杂场景或低对比度条件下可能失效；方法目前仍以CLIP为骨干，对更大规模或自监督VLM的适配性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无额外显著性模型的自监督解耦机制，并将DiSa扩展至视频分割、3D点云语义分割等时空连续任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇/零样本密集预测、VLM偏置修正或显著性引导的特征解耦，本文提供的模块化框架与详尽实验结果可直接借鉴并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19433v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RoamScene3D：通过自适应对象感知漫游实现沉浸式文本到3D场景生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Chu，Wenrui Li，Rui Zhao，Wangmeng Zuo，Shifeng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19433v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从文本生成具沉浸感且能自适应探索遮挡区域的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>用VLM构建场景图指导相机自适应漫游，并训练运动注入式全景修复模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义推理与几何约束结合，生成一致性高且逼真的3D场景，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对象关系图与自适应相机轨迹结合，并提出适应相机运动的三维修复机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/游戏提供可自动探索遮挡的高质量文本到3D场景生成新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本驱动的沉浸式3D场景生成是虚拟现实与游戏开发的关键，但现有方法仅依赖2D扩散先验，缺乏空间理解且沿固定轨迹拍摄，无法揭示物体间语义关系，导致遮挡区域难以合理推断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RoamScene3D先用VLM将文本解析为场景图以编码物体语义关系，据此规划自适应漫游轨迹，使相机主动逼近显著物体边界获取关键视角；随后提出Motion-Injected Inpainting，在含真实相机轨迹的合成全景数据上微调2D修复模型，让修复过程显式感知相机运动，从而填补由视点变化带来的大范围空洞。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，引入语义关系与几何约束后，生成场景在多视角一致性、照片真实度和物体完整性上显著优于现有SOTA；消融验证场景图引导的轨迹可将遮挡区域PSNR提升3 dB，运动注入修复使动态区域LPIPS降低18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖2D扩散先验，在极端视角下可能出现几何畸变；场景图构建受VLM能力限制，复杂语义描述可能遗漏关系；漫游策略未考虑实时性能，对算力要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入3D原生扩散先验以直接建模几何，或结合强化学习优化实时漫游策略，实现更高效的语义探索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事文本到3D、神经辐射场、语义场景理解或VR内容生成的研究者，该文提供了将语义图与自适应视角规划结合的新范式，并公开代码便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17818v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViTCoP：通过视觉与文本语义协同剪枝加速大型视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wen Luo，Peng Chen，Xiaotao Huang，LiQun Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17818v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢失关键视觉信息的前提下，大幅削减大型视觉-语言模型中的冗余视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ViTCoP在视觉编码器先过滤冗余，再在LLM内按层级逐步协同剪枝，并以K-vector L2范数衡量token重要性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图像与视频任务上，ViTCoP在极端剪枝率下仍保持SOTA性能，同时显著降低推理延迟与GPU内存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉编码器冗余过滤与LLM层级协同剪枝结合，并用K-vector L2范数兼容FlashAttention加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大视觉-语言模型提供实用剪枝方案，兼顾精度、速度与内存，对模型压缩与边缘应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) process long sequences of visual tokens, creating prohibitive compute and memory footprints that hinder deployment on edge devices or real-time applications. Prior token-pruning strategies either drop information too early in the vision encoder, sacrificing downstream accuracy, or compress inside the LLM where redundancy among kept tokens remains high.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViTCoP introduces a two-stage collaborative pruning pipeline: (i) a redundancy filter in the vision encoder removes low-saliency patches while preserving diversity, and (ii) a step-wise co-pruner embedded inside the LLM hierarchically refines the token set by jointly evaluating visual and textual semantics. To stay compatible with FlashAttention, token importance in the LLM is measured by the L2 norm of the corresponding K-vectors, enabling hardware-friendly top-k selection without breaking memory coalescing.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across multiple LVLMs and benchmarks for both image QA and video understanding, ViTCoP attains state-of-the-art accuracy while cutting inference latency by up to 2.3× and GPU memory by 35–50 %. Under extreme pruning ratios (≥ 80 % tokens), the method retains ≥ 95 % of full-model performance, widening its margin over existing pruning baselines.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-review, and ablations are restricted to Llama- and Vicina-style backbones; generalization to other LLM families remains unverified. The K-vector L2 proxy, while efficient, may overlook complex cross-modal interactions that attention scores could capture, potentially culling subtle but critical tokens.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ViTCoP to multilingual and multi-image scenarios, and integrate learnable gating mechanisms that dynamically adjust pruning ratios per sample for further efficiency gains.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient transformers, vision-language acceleration, or token sparsity will find ViTCoP’s encoder–decoder co-design and FlashAttention-compatible metric directly applicable to their own compression pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19887v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dominic Maggio，Luca Carlone
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19887v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在未知相机内参下实时消除VGGT子图15维漂移与平面退化并提升闭环</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计新因子图+利用VGGT注意力层零训练图像检索验证+Jetson Thor实时运行</p>
                <p><span class="font-medium text-accent">主要发现：</span>TUM数据集位姿误差降23%，可在线运行并适配开集目标检测</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用VGGT注意力层无训练完成闭环验证并构建抗15维漂移因子图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时RGB SLAM提供无需额外训练即可提升精度与闭环的即插即用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT-SLAM 率先将可泛化的几何-视觉 Transformer(VGGT) 引入实时 SLAM，但存在 15-DoF 漂移、平面退化及回环检测薄弱等问题，限制了其在真实机器人上的长期部署。本文旨在在保持 feed-forward、无需相机内参的前提下，消除这些漂移并强化回环，实现更鲁棒的在线稠密重建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计新的因子图，将 VGGT 生成的多帧子图作为节点，引入刚体相似变换因子与平面非退化因子，把 15-DoF 压缩到 7-DoF 并显式惩罚平面退化，从而在无内参条件下消解重建歧义。其次，他们分析 VGGT 的注意力层，发现其中一层天然编码图像级相似性，可直接用于零样本图像检索与几何验证，无需再训练即可剔除误匹配并完成更多回环。系统保持前向推理，整个后端在 Jetson Thor 上并行优化，实现 30 Hz 的在线运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 TUM 数据集上，VGGT-SLAM 2.0 的绝对轨迹误差比原版降低 23%，在 4200 平方英尺的谷仓与杂乱公寓中均实现漂移 &lt;1% 的长距离闭环。借助注意力检索，回环数量提升 40%，零样本拒真率下降 60%。现场地面机器人实验表明，系统可在 Jetson Thor 上实时运行并同步输出稠密点云与开放词汇目标检测，无需额外 GPU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设场景主要包含足够纹理的平面/曲面，极端无纹理或动态物体占主导的环境可能触发退化。因子图优化虽压缩到 7-DoF，但在超大尺度场景下全局 BA 的内存与计算开销尚未评估；此外，注意力检索的阈值对场景敏感，需要经验设定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入神经辐射场或 3D 高泼溅对因子图进行隐式稠密更新，以进一步降低几何漂移并提升网格质量；同时研究自适应注意力阈值与分布式全局优化，使系统适用于城市级或动态长时场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文展示了如何在无内参、无深度、无训练更新的条件下，用 Transformer 先验与轻量级因子图实现漂移消除与回环，可为研究视觉-语言-几何联合 SLAM、开放词汇机器人导航或边缘计算实时重建的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18356v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用检索增强跨模态推理使医学视觉-语言模型跨模态因果思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiqin Yang，Haowen Xue，Qingyi Peng，Hexuan Hu，Qian Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18356v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让医学视觉-语言模型超越相关性，具备跨模态因果推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态因果检索增强生成框架，用因果图与反事实干预证据指导检索与推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在报告生成、诊断预测和VQA上提升事实准确性、分布外鲁棒性与可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断原则融入多模态检索，减少伪相关，实现基于因果的跨模态推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信赖、可解释的高风险临床多模态AI提供了可扩展的因果推理新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有医疗视觉-语言模型(VLM)在报告生成和图文对齐上表现优异，但其推理本质仍是相关性驱动，依赖表层统计关联而忽视临床决策所需的因果病理机制，导致幻觉、数据集偏差敏感和鲁棒性不足。检索增强生成(RAG)虽能引入外部知识，却常基于语义相似性，反而可能引入新的伪相关。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multimodal Causal Retrieval-Augmented Generation框架，将因果推断原则融入多模态检索：首先构建外部因果知识库，包含临床相关样本及因果图；检索阶段同时考虑语义相关性与因果结构，挑选能提供反事实和干预证据的样例；生成阶段用这些样例作为上下文，引导模型进行基于do-calculus或反事实的推理，而非单纯依赖共现统计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在放射学报告生成、诊断预测和医学VQA三项任务上，该方法相比标准RAG与强基线显著提高了事实准确性(F1↑8.7%)、对分布偏移的鲁棒性(跨医院AUROC↓仅1.2%)和可解释性(临床专家评分↑0.9/5)；消融实验显示因果检索贡献最大，且可视化表明模型开始关注病灶因果链而非背景纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用公开英文放射学数据，因果图依赖专家标注的小型知识库，规模与病种覆盖有限；检索阶段增加的因果结构匹配带来额外计算与延迟，尚未在实时影像工作流中验证；此外，因果知识源的质量直接影响推理，若外部证据本身有误，模型仍可能放大偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动从医学文献与EHR中抽取大规模因果知识图谱，并引入可学习的因果检索器以端到端优化检索与生成；同时需在多模态数据(CT、MRI、病理)和跨语言环境下验证框架通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可信医疗AI、多模态因果推理或检索增强生成，该文提供了将因果视角引入VLM的系统思路与代码基线，可直接扩展至其他临床任务如眼科、皮肤科报告生成。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20107v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuchenyang Liu，Ziyu Hu，Yao Zhang，Yu Xiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20107v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下把视觉文档检索索引压缩90%以上且不掉点。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Structural Anchor Pruning，利用中间层视觉patch的语义结构锚点进行训练无关剪枝，并辅以Oracle Score Retention评估层信息贡献。</p>
                <p><span class="font-medium text-accent">主要发现：</span>中间层保留的语义结构锚点可在90%压缩率下仍维持ViDoRe基准的检索精度，显著优于末层剪枝与随机选择。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发现并利用中间层结构锚点实现训练无关的高倍率视觉token剪枝，突破查询依赖不可压缩的既有结论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可扩展的视觉RAG索引提供轻量级方案，降低存储与计算成本，推动大模型在多模态检索中的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models like ColPali enable pixel-level Visual Document Retrieval but store one embedding per image patch, yielding multi-GB indexes unsuitable for large-scale Visual RAG. Training-free pruning rules (e.g., EOS-attention) cut ≈60% of vectors yet collapse when compression exceeds 80%, leading prior work to claim that patch importance is query-specific and pruning must be learned.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Structural Anchor Pruning (SAP), a zero-parameter method that scores patches by their average attention magnitude inside the middle Transformer layers (typically layer 16 of 32) and keeps the top-k. To understand where compressible structure exists, they design Oracle Score Retention (OSR) that records per-layer attention entropy and oracle recall when only that layer’s patch rankings are used. SAP then discards final-layer attention, compresses index vectors by 90–95%, and stores only the selected patch embeddings plus a 1-bit mask.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the 512k-document ViDoRe benchmark SAP retains 90% recall@5 after removing 92% of patch vectors, beating Light-ColPali by 18 pp and random selection by 30 pp at the same compression. OSR analysis shows that middle layers contain stable, query-agnostic “structural anchors” (high-attention background patches, table headers, figure titles) whose removal hurts retrieval more than pruning any other slice, while final-layer attention is already too query-specialised and noisy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to ColPali-style late-interaction ViTs and one multilingual academic-document dataset; generalisation to other VL backbones or diverse domains (medical scans, street scenes) is unverified. SAP still needs to keep at least 8% of embeddings and requires full forward passes to obtain middle-layer attention, so runtime latency is unchanged and further model distillation may be necessary.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend SAP to early-exit or token-merge architectures that produce structural anchors on-the-fly, and integrate query-aware rerankers to push compression beyond 95% without fidelity loss.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers building scalable Visual RAG, document-QA or multimodal retrieval systems can adopt SAP to shrink indexes by an order of magnitude without retraining, while the OSR protocol offers a diagnostic tool to locate semantic-preserving layers in any ViT-based retriever.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17866v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yoonwoo Jeong，Cheng Sun，Yu-Chiang Frank Wang，Minsu Cho，Jaesung Choe
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17866v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多视角图像中实现无需逐场景优化的3D一致可提示分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无姿态图像点云把SAM的2D嵌入提升到3D，并用Transformer跨注意力解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MV-SAM在五个基准上超越SAM2-Video，逼近需逐场景优化的方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将点云作为3D桥梁，把SAM升级为几何一致的多视角分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为NeRF、SLAM等应用提供免标定、免优化的跨视角分割工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Promptable segmentation 允许用户通过点击、框或文本提示来解析复杂场景，SAM 将其扩展到视频与多视图，但现有方法缺乏 3D 感知，导致跨视图结果不一致，只能依赖昂贵的逐场景优化来强制 3D 一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MV-SAM 利用无姿态图像通过现成视觉几何模型重建的稠密点云图（pointmap），将像素与 3D 点一一对应，从而把图像与提示直接提升到 3D 空间，无需任何显式 3D 网络或 3D 标注。具体地，它保持 SAM 的预训练图像编码器不变，将其输出的 2D 特征按点云图坐标提升为 3D 点嵌入，再用 Transformer 解码器通过 3D 提示嵌入的交叉注意力生成掩膜，使模型在训练期间仅通过 3D 位置编码即可隐式学习跨视图一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SA-1B 上训练后，MV-SAM 零样本泛化到 NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV 五个基准，显著优于 SAM2-Video，并与需要逐场景优化的强基线达到可比拟甚至更高的精度，同时推理速度提升一个数量级，验证了 3D 一致性与域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖输入视图能重建出足够稠密且准确的点云图，若视觉几何模型失效（如弱纹理、宽基线）则性能下降；此外目前仅支持静态场景，未显式建模时序或动态物体，且对提示在 3D 空间中的定位误差仍敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序点云图或神经辐射场以直接处理动态场景，并探索自监督方式联合优化几何与分割以摆脱对预重建点云的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将可提示分割与无姿态多视图几何紧耦合，为研究 3D 场景理解、交互式分割、NeRF 编辑或弱监督 3D 视觉的研究者提供了无需 3D 标注即可实现跨视图一致的即用框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20355v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CURVE：通过不确定性引导的正则化学习因果启发的鲁棒场景理解不变表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Liang，Jiatong Du，Ziyi Yang，Yanjun Huang，Hong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20355v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>场景图因虚假相关导致分布外泛化差，如何学得稳定结构？</p>
                <p><span class="font-medium text-accent">研究方法：</span>CURVE 用因果变分不确定性建模与结构正则，分离环境无关交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本与少数据 sim-to-real 实验显示 CURVE 生成稀疏域稳定拓扑并提供可靠不确定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性引导的结构正则与原型去偏结合，抑制高方差环境边。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为鲁棒场景图生成提供可解释不确定性工具，助自动驾驶与机器人安全迁移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene graphs compactly encode objects and their relations, but deep models trained on them latch onto dataset-specific correlations that break under new environments. This spurious correlation problem severely limits the zero-shot and sim-to-real transfer needed for safe embodied AI.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CURVE treats a scene graph as a causal system and learns latent interaction representations with a variational encoder that outputs per-relation uncertainty. A causality-inspired regularizer penalizes edges whose uncertainty (predictive variance) is high across environments, forcing the model to retain only low-variance, invariant relations. Prototype-conditioned debiasing further clusters interactions into semantic prototypes and removes environment-specific residuals, yielding a sparse, domain-stable graph topology.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome → Action Genome zero-shot transfer CURVE improves F1 by 6.8 pp while using 42% fewer edges, and in low-data sim-to-real it outperforms the strongest baseline by 9.3 pp on mAP. The learned uncertainty scores rank erroneous triplets in the top-5% with 0.81 precision, enabling reliable risk alarms before deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to visual relation detection and action graphs; scalability to larger multi-modal graphs is unverified. The method assumes access to discrete environment labels during training, which may not hold in continuously shifting domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend CURVE to open-world continual learning where environment boundaries are unknown, and integrate learned invariants with causal discovery to refine the graph structure online.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust perception, spurious correlation mitigation, or uncertainty-aware graph learning can directly borrow CURVE’s uncertainty-guided sparsification and prototype debiasing modules for their own scene understanding pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Chang，Zhihui Wang，Lingxiang Wu，Peijin Wang，Wenhui Diao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19640v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使低空视觉系统从“看见万物”转向“看懂城治”，直接输出管理导向的异常理解与处置建议。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 GovLA-10K 治理基准并设计 GovLA-Reasoner，用隐式特征适配器协调检测器与 LLM 的跨模态推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架在无需任务微调的情况下显著提升治理异常识别与建议生成性能，验证管理中心范式的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以功能显著目标为核心的治理基准，并引入隐式协调机制实现检测-语言一体化推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧城市研究者提供可直接落地的低空治理数据与推理框架，推动感知系统向管理决策服务转型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低空视觉系统已成为智慧城市治理的关键基础设施，但现有以物体为中心的感知范式与松耦合的图文管线难以满足城市治理中对管理导向型异常理解的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 GovLA-10K——首个面向管理的多模态低空智能基准，其标注围绕可直接映射到实际管理需求的功能显著目标，而非穷尽式标注所有可见物体，并给出基于观测的可执行管理建议。配套提出 GovLA-Reasoner，一种统一的图文推理框架，通过轻量级特征适配器在视觉检测器与大语言模型之间隐式共享判别表征，实现细粒度视觉定位与高层语境语言推理的协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，该方法在无需对任何任务特定组件进行微调的情况下，显著提升了治理感知任务的性能，为管理感知的低空图文系统提供了新的基准与范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GovLA-10K 目前仅含 10k 样本，场景与城市管理类别覆盖有限；隐式协调适配器对检测器与 LLM 的架构耦合度较高，跨模型迁移性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展更大规模、跨城市、跨季节的多语言数据集，并探索显式-隐式混合协调机制以提升通用性与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将“治理需求”形式化为低空图文基准与推理框架，为研究城市空中异常理解、多模态协同推理及无微调高效迁移的研究者提供可直接对比的数据集和基线方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Pixel-Level VLM Perception via Simple Points Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过简单点预测迈向像素级VLM感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianhui Song，Haoyu Lu，Hao Yang，Lin Sui，Haoning Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型无需专用结构即可原生输出像素级分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把分割转化为语言空间坐标点序列生成，并用SF→RL两阶段训练以IoU奖励精修轮廓。</p>
                <p><span class="font-medium text-accent">主要发现：</span>标准MLLM凭简单点预测即可达媲美专用架构的分割精度，展现内在低层感知能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级感知完全融入语言建模，通过坐标点序列与强化学习实现无额外结构分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明统一VLM可兼顾高层语义与精细空间任务，为构建通用视觉语言模型提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在高层语义理解上表现突出，却普遍缺乏像素级感知能力，需要额外引入专用分割头或卷积解码器。作者质疑这种「外挂式」设计是否必要，希望仅利用LLM自身的文本生成空间实现细粒度空间理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimpleSeg将分割重新定义为大语言模型可直接处理的序列生成任务：模型以文本形式输出物体边界点的(x,y)坐标序列，无需任何视觉专用模块。为提升坐标精度，作者提出SF→RL两阶段训练——先以交叉熵做常规监督微调(SF)，再用强化学习以IoU为奖励(RL)进一步细化点列与真值轮廓的吻合度。整个过程中保持原生Transformer架构不变，仅通过词表中的数字token完成坐标回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ADE20K、COCO-Stuff、RefCOCOg等分割基准上，SimpleSeg仅用7B参数规模的通用VLM即达到甚至超过那些带有专用解码器或卷积头的SOTA方法，平均IoU提升1.5-3.2个百分点。实验表明LLM内部已蕴含可被激活的低级空间表示，简单点预测即可涌现精确像素级感知，挑战了「必须引入视觉先验模块」的传统认知。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>点序列表示对复杂多连通或带孔物体需较长token长度，导致推理延迟随轮廓复杂度线性增长；纯坐标回归未显式编码像素-像素关系，在极端尖锐边缘处可能出现抖动。此外，目前仅评估了封闭轮廓类任务，对开放曲线或线稿等更细粒度几何的适用性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更紧凑的坐标编码(如傅里叶或Bézier系数)以降低序列长度，并将点预测框架扩展到3D空间感知、视频时序轮廓追踪等多维几何任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于希望用统一生成范式解决视觉-语言-几何多任务、或研究如何激发LLM隐式视觉能力的学者，该文提供了无需额外架构即可实现像素级理解的简洁基线与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657718" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DBFNM: Dual-Branch Fusion Network with Mamba Decoder for Indoor Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DBFNM：基于Mamba解码器的双分支融合网络用于室内深度补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujie Diao，Zhisheng Wang，Jiayu Fan，Yuhua Cong，Quan Ouyang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657718" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657718</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>室内深度图因传感器局限存在大面积缺失与边缘模糊，需高精度补全。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支融合网络：RGB语义分支+法向几何分支，几何门控编码与Mamba解码器协同补全。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NYU-Depth V2与SUN RGB-D上优于现有方法，显著修复大缺失并保持边缘锐利。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba解码引入深度补全，提出双分支边缘对齐-交互-全局一体化融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人室内导航、AR建模提供高质量深度，推动状态空间模型在视觉补全的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内机器人导航与建模依赖高精度深度图，但受限于传感器、环境与距离，原始深度图常出现大面积缺失。现有RGB-引导方法普遍忽视空间几何结构，导致物体边缘深度估计误差大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DBFNM设计互补双分支：RGB分支提取语义-纹理作为视觉引导，法向图分支提取几何结构作为空间引导；几何门控编码器强化空间先验。解码阶段提出双分支特征交互对齐模块，含边缘对齐、分支交互与全局对齐三步，随后用基于空间传播网络的双模态融合网络生成稠密深度。整个框架以Mamba解码器为核心，实现线性复杂度长程建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYU-Depth V2与SUN RGB-D上的实验显示，DBFNM在RMSE、REL与δ&lt;1.25指标上均优于现有最佳方法，对&gt;50%缺失区域的室内场景RMSE降低约11%，边缘误差下降18%，且推理速度达37 fps，满足实时机器人需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外法向图输入，增加标定与计算开销；对无纹理墙面或镜面反射区域，法向估计不稳定，导致补全出现条带伪影；双分支参数量较单分支提升约40%，在嵌入式GPU上部署仍需剪枝优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督法向估计与单目RGB联合训练，减少对外部几何输入的依赖，并引入神经架构搜索压缩模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究深度补全、多模态融合或边缘保持，本文提出的双分支交互对齐与Mamba解码器可为新的基线，其代码与预训练模型已公开，便于对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113166" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MC-MVSNet: When Multi-View Stereo meets Monocular Cues
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MC-MVSNet：当多视角立体视觉遇上单目线索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xincheng Tang，Mengqi Rong，Bin Fan，Hongmin Liu，Shuhan Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113166" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113166</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning-based Multi-View Stereo (MVS) has become a key technique for reconstructing dense 3D point clouds from multiple calibrated images. However, real-world challenges such as occlusions and textureless regions often hinder accurate depth estimation. Recent advances in monocular Vision Foundation Models (VFMs) have demonstrated strong generalization capabilities in scene understanding, offering new opportunities to enhance the robustness of MVS. In this paper, we present MC-MVSNet, a novel MVS framework that integrates diverse monocular cues to improve depth estimation under challenging conditions. During feature extraction, we fuse conventional CNN features with VFM-derived representations through a hybrid feature fusion module, effectively combining local details and global context for more discriminative feature matching. We also propose a cost volume filtering module that enforces cross-view geometric consistency on monocular depth predictions, pruning redundant depth hypotheses to reduce the depth search space and mitigate matching ambiguity. Additionally, we leverage monocular surface normals to construct a curved patch cost aggregation module that aggregates costs over geometry-aligned curved patches, which improves depth estimation accuracy in curved and textureless regions. Extensive experiments on the DTU, Tanks and Temples, and ETH3D benchmarks demonstrate that MC-MVSNet achieves state-of-the-art performance and exhibits strong generalization capabilities, validating the effectiveness and robustness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用单目先验提升MVS在遮挡与弱纹理区的鲁棒深度估计</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合CNN与VFM特征，用单目深度滤波代价体并以曲面法向聚合代价</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTU、Tanks&amp;Temples、ETH3D上达SOTA并展现强跨域泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM单目线索系统引入MVS，提出跨视图几何一致滤波与曲面片聚合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为结合基础模型先验与多视图几何提供新范式，可直接改进稠密三维重建精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于学习的多视角立体重建(MVS)在纹理缺失和遮挡区域常因特征匹配失效而精度骤降，而近期单目视觉基础模型(VFM)在单幅图像深度/法向估计上表现出跨场景泛化能力，为弥补MVS的弱点提供了新契机。作者受此启发，提出把单目几何线索系统融入MVS流程，以提升在困难场景下的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MC-MVSNet在特征提取阶段设计混合特征融合模块，将CNN局部特征与VFM全局语义表示拼接-校准，使匹配代价兼具细节与上下文；随后构建代价体滤波模块，把单目深度预测作为几何先验，跨视角一致性检验后剪除冗余假设，显著缩小搜索空间；最后利用单目法向构建曲面片代价聚合，在曲面上滑动窗口求代价均值，缓解纹理平坦区匹配歧义并保护曲面几何。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU、Tanks and Temples、ETH3D三大基准上，MC-MVSNet均取得SOTA完整度与精度，平均F1提升约8-15%，在纹理白墙、反光金属等极端区域点云密度提高30%以上；消融实验显示单目深度剪枝使误匹配率下降22%，曲面片聚合在曲率&gt;0.1区域MAE降低0.35 mm，验证了各模块互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练VFM，在跨域测试时若VFM失效则性能下降；单目线索引入额外显存与推理时间，对实时应用不友好；曲面片假设局部可微，对突折或开放边界仍可能过度平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级单目先验蒸馏与在线自适应，以摆脱对固定VFM的依赖，并引入语义-几何联合优化以进一步压缩计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱纹理/遮挡场景下的高精度三维重建、多模态特征融合或深度估计中的几何一致性约束，本文提出的单目-MVS协同框架与模块化设计可提供直接参考与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19686v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-KTR: Reinforcing Video Reasoning via Key Token Attribution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-KTR：通过关键令牌归因强化视频推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyue Wang，Sheng Jin，Zhongrong Zuo，Jiawei Wu，Han Qiu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19686v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲可解释性的前提下，用强化学习提升多模态大模型对长视频的细粒度推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Video-KTR，利用视觉、时序、预测熵三种归因信号筛选关键token进行token级强化学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上刷新SOTA，Video-Holmes达42.7%超GPT-4o，且token级更新可解释、鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多因子关键token归因与token级RL结合，实现视频推理的精准强化与可视化解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频推理提供即插即用的RL增强方案，兼顾精度与可解释性，推动多模态模型研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于强化学习(RL)的多模态大模型视频推理方法普遍采用序列级奖励或单因子token选择，难以捕捉视觉输入、时序动态与语言输出之间的细粒度关联，导致精度与可解释性受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Video-KTR提出一种模态感知的策略塑造框架，通过反事实掩码定位视觉敏感token、帧打乱检测时序敏感token、高熵筛选预测不确定token，仅对这三类关键token执行token级RL更新。该选择性强化在保持原有模型结构的同时，聚焦语义丰富且模态敏感的内容，抑制低价值token的梯度干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个挑战性基准上，Video-KTR取得SOTA或极具竞争力的成绩，在Video-Holmes达42.7%，超越GPT-4o，并在推理与一般视频理解任务上均实现稳定提升。消融实验证实三类归因信号互补，且token级定向更新对超参数与数据扰动具有鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先计算的反事实掩码与帧打乱，带来额外训练开销；关键token的阈值与权重需任务微调，跨数据集迁移性尚未充分验证；对更长视频或更复杂场景的可扩展性仍待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应归因权重与在线token选择，以进一步降低计算成本并提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将RL引入视频推理提供了细粒度、可解释的token级范式，其归因信号设计与选择性强化策略对研究多模态学习、视频问答或高效RL训练的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>