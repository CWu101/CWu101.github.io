<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-06</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 30 篇论文 ·
        生成于 2026-01-06 10:56 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态理解的论文、7篇关于遥感智能的论文、6篇关于三维感知的论文、4篇关于视频理解的论文、2篇关于知识检索增强的论文与2篇关于模型泛化的论文。</p>
            
            <p><strong class="text-text-secondary">多模态理解</strong>：聚焦视觉-语言对齐与指代表达理解，代表作《BARE》提出偏置感知与推理增强的单塔视觉定位框架，《Text-Injected Discriminative Model》在遥感图文指代中注入文本判别特征，《AirSpatialBot》构建空间感知空中智能体实现细粒度车辆属性检索，《Cross-view and Multi-step Interaction》利用跨视角多步交互做变化描述，《Size-Aware Graph Embedding》引入对象相对尺度图嵌入提升遥感图像字幕质量。</p>
            
            <p><strong class="text-text-secondary">遥感智能</strong>：针对遥感图像匹配、字幕生成与视觉定位等任务，《MARSNet》以Mamba驱动自适应结构实现噪声场景下多源遥感半稠密匹配，《A Size-Aware Graph Embedding Approach》显式建模对象相对尺度生成精准字幕，《Text-Injected Discriminative Model》通过文本注入判别器提升遥感视觉定位精度，《AirSpatialBot》结合空间感知强化细粒度车辆检索。</p>
            
            <p><strong class="text-text-secondary">三维感知</strong>：围绕自动驾驶场景的三维语义占用预测与RGB-深度解析，《Hierarchical Context Alignment》解耦几何-时序建模完成相机语义占用预测，《Fully Exploiting Vision Foundation Model》充分挖掘ViT先验知识提升RGB-深度驾驶场景解析泛化性。</p>
            
            <p><strong class="text-text-secondary">视频理解</strong>：面向视频问答与片段定位，《FastV-RAG》提出快速细粒度视频问答检索增强生成框架，《GranAlign》构建粒度感知对齐实现零样本视频时刻检索。</p>
            
            <p><strong class="text-text-secondary">知识检索增强</strong>：探索将外部知识引入视觉推理，《FastV-RAG》在视频问答中结合RAG实现快速细粒度推理，为视觉-语言模型提供知识补充。</p>
            
            <p><strong class="text-text-secondary">模型泛化</strong>：关注跨域与零样本泛化能力，《Fully Exploiting Vision Foundation Model》利用ViT深度先验提升驾驶场景解析跨域性能，《GranAlign》通过粒度对齐实现零样本视频时刻检索无需任务训练数据。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">充分挖掘视觉基础模型的深层先验知识以实现可泛化的RGB-深度驾驶场景解析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sicen Guo，Tianyou Wen，Chuang-Wei Liu，Qijun Chen，Rui Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2025.3650682</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重训ViT的前提下，充分利用视觉基础模型先验知识实现可泛化RGB-深度驾驶场景解析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFIT侧适配器，将VFM的相对深度预测作为输入，高效提取并融合RGB与深度异构特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HFIT在Cityscapes与KITTI语义数据集上超越传统单模/融合网络、预训练VFM及ViT适配器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用VFM相对深度先验替代真实深度图，设计无需重训ViT的异构特征集成Transformer框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为驾驶场景解析提供即插即用的VFM融合策略，推动视觉基础模型在多模态自动驾驶感知中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）在纯RGB任务上表现卓越，但在RGB-深度联合的自动驾驶场景解析中潜力未被充分挖掘。现有方法要么重新训练大模型成本高昂，要么简单微调未能深度利用VFM的先验知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heterogeneous Feature Integration Transformer（HFIT），以ViT为骨干，通过侧向适配器结构将RGB与深度模态特征在Transformer内部异构融合，无需重训ViT。网络利用VFM输出的相对深度预测取代易缺失的绝对深度图，作为适配器输入，实现零额外深度传感器依赖。整体框架在保持VFM权重冻结的同时，仅训练轻量级融合模块，兼顾效率与泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes与KITTI Semantics基准上，HFIT的mIoU分别超过最佳单模RGB网络4.8%与5.3%，超过现有ViT适配器方案2.1%与2.7%，并在跨域评估中把性能下降控制在3%以内，验证其强泛化能力。消融实验显示，相对深度先验贡献了整体增益的约60%，证明充分挖掘VFM隐式几何知识是关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖VFM生成的相对深度质量，若VFM在夜间或恶劣天气失效则性能下降；侧适配器容量有限，对更高分辨率输入的扩展性尚未验证；实验仅覆盖驾驶场景，未测试室内或机器人导航等更一般RGB-D任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索HFIT在多帧时序融合与多任务联合训练中的扩展，以及结合扩散模型或自监督信号进一步提升深度先验的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态大模型高效迁移、自动驾驶环境感知或RGB-D语义分割的研究者，该文提供了不增加计算重担即可释放VFM先验的新范式，代码开源便于复现与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650788" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Size-Aware Graph Embedding Approach to Remote Sensing Image Captioning with Object Relative Size Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述生成的尺寸感知图嵌入方法：引入目标相对尺度信息</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Ni，Yinghao Xu，Weibo Zhang，Zhaoyun Zong，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650788" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650788</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感图像字幕常缺失或误述目标尺寸信息，如何显式建模相对大小以生成更准确的描述？</p>
                <p><span class="font-medium text-accent">研究方法：</span>三模块框架：目标确认与相对尺寸估计→尺寸感知图嵌入→字幕生成，并引入SizeNum-Meteor指标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UCM、Sydney、RSICD及扩展基准上，新方法在标准指标与SizeNum-Meteor均优于现有方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相对尺寸估计与图卷积结合用于字幕生成，提出专门评估尺寸与数量正确性的SizeNum-Meteor。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感字幕提供尺寸感知新范式，推动更精细场景理解及评估标准发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成旨在为卫星/航空影像自动生成自然语言描述，但现有方法普遍忽视目标尺寸信息，导致字幕要么漏掉“大/小”等形容词，要么给出模糊甚至错误的尺寸描述。作者认为尺寸是遥感场景语义的关键组成部分，其缺失会显著降低字幕的定量可理解性与实用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三模块框架：首先用检测网络确认目标类别并估计其相对尺寸（以场景最大目标为参照，输出1–5离散等级）；随后将目标作为节点、尺寸差异作为边权构建尺寸感知图，并用图卷积网络传播尺寸上下文；最后把增强后的节点特征送入Transformer字幕生成器，在解码端额外引入尺寸嵌入向量以显式预测“large/small”等形容词。训练阶段采用交叉熵+尺寸等级回归联合损失，并在推理时强制尺寸词束搜索约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM、Sydney、RSICD三个公开数据集及作者扩展的带尺寸标注版本（-N-S）上，新框架在BLEU-4、CIDEr等标准指标上平均提升2.3–4.1分；新指标SizeNum-Meteor（同时惩罚数量与尺寸错误）比最佳基线高出8.7分，人类评估显示尺寸描述准确率从62%提升到84%。消融实验表明相对尺寸估计模块贡献最大，单独移除即导致SizeNum-Meteor下降5.2分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预先训练的检测器，若检测漏检或尺寸估计错误会级联放大到字幕；相对尺寸仅用五级离散码，无法表达细粒度或绝对尺度；SizeNum-Meteor目前只支持英语且需人工规则匹配，跨语言或跨传感器通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入亚米级分辨率/多光谱信号直接回归目标物理长度，并探索连续向量尺寸表示以支持更细腻的形容词生成；同时扩展SizeNum-Meteor至少数民族语言及多源卫星数据，实现跨域尺寸一致性评估。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及遥感视觉-语言任务、目标属性描述或细粒度语义评价，该文提供的尺寸感知图嵌入范式、尺寸回归损失及SizeNum-Meteor代码可为实验基线与评价工具提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语义占位预测的分层上下文对齐：解耦几何与时序建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bohan Li，Jiajun Deng，Yasheng Sun，Xiaofeng Wang，Xin Jin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650478</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多帧特征聚合时同位置语义不一致导致的上下文错位问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分层对齐：先解耦几何与时间分支分别对齐，再按语义一致性全局融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、NuScenes-Occupancy与NuScenes LiDAR分割上超越SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何与时间上下文解耦并引入置信度-位姿先验的分层对齐框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为基于相机的3D语义占用预测提供更鲁棒的时空特征融合范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目相机只能给出 2D 观测，而自动驾驶等应用需要稠密 3D 语义占用预测；现有方法把多帧上下文直接拼在一起，却常因遮挡、动态物体或位姿误差导致同一空间位置在不同时刻的特征语义不一致，从而削弱融合效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-SOP 将多帧图像特征先解耦成“几何体积”和“时序体积”两条并行流：几何流利用深度置信度做局部邻域匹配，时序流利用相机位姿做帧间匹配，各自完成初步对齐；随后把两组对齐后的 4D 体积按语义一致性进行全局重对齐，并以可学习权重合成最终语义占用体积，实现层级式上下文校准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SemanticKITTI 与 NuScenes-Occupancy 语义场景补全任务上，Hi-SOP 将 mIoU 分别提升 2.8 和 3.4 个百分点，达到新的 SOTA；在 NuScenes LiDAR 语义分割任务上亦取得 1.6 mIoU 的增益，验证了几何-时序解耦对齐策略的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖准确的深度置信度和相机位姿，在剧烈颠簸或传感器失效场景下对齐质量可能下降；层级融合引入额外计算与显存开销，实时性仍逊于单帧方法；论文未探讨与基于激光雷达或毫米波雷达的融合潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自标定与不确定性估计，使对齐模块对位姿/深度误差更鲁棒；或把几何-时序解耦思想扩展到多模态输入，实现视觉-激光雷达统一占用预测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 语义感知、多帧融合或占用网格表示，本文提出的“解耦-对齐-合成”范式可直接迁移到其他传感器配置，也可作为特征一致性正则化策略嵌入现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的鲁棒多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声干扰下实现多源遥感影像的高效半稠密匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MARSNet，用Mamba捕获长程依赖、冻结DINOv2抗噪、自适应融合并改进线性注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在含噪与无噪多源遥感数据及大规模三维重建任务上，匹配精度与鲁棒性均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba结构引入遥感匹配，结合冻结DINOv2与自适应Mamba式线性注意力实现高效抗噪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像配准、三维重建提供兼顾效率与鲁棒性的新框架，推动噪声场景下多源数据融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和成像机理不一致的共同作用下，传统稀疏或半稠密匹配方法精度骤降，而无检测器方法在大视角与强噪声并存时效率低、鲁棒性差，已成为制约后续三维重建与变化检测的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MARSNet，将 Mamba 线性扫描结构嵌入 Transformer，以线性复杂度捕获长程依赖；冻结的 DINOv2 作为强噪声不变特征提取器，避免重新训练带来的过拟合；提出自适应跨模态融合模块，动态加权光学与 SAR/红外特征；最后用类 Mamba 线性注意力替代传统自注意力，在 1K×1K 影像上实现 O(N) 复杂度的半稠密匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的噪声-free、随机高斯噪声、周期性条带噪声三种多源遥感测试集上，MARSNet 的匹配召回率平均提升 8.7%，重投影误差降低 23%；在 1.2 M 张通用多视角数据集做位姿误差评估，SOTA 方法 0.67 px 的中误差被压缩到 0.48 px，验证了三维重建任务的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与多源遥感噪声数据集，难以复现；DINOv2 冻结导致网络对极端辐射差异的适应性仍受预训练分布限制；Mamba 扫描顺序在旋转大於 45° 时可能丢失跨行上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索扫描顺序的自学习策略，并将网络扩展到视频级多源时序匹配，以服务于动态目标监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究跨模态配准、三维重建或噪声鲁棒特征学习，该文提供了线性复杂度下兼顾精度与效率的新范式，可直接借鉴其 Mamba-Attention 混合架构与冻结基础模型的组合思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01526v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BARE：迈向偏见感知与推理增强的单塔视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbing Li，Linhui Xiao，Zihan Zhao，Qi Shen，Yixiang Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01526v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制单塔视觉定位中的模态偏差并强化指代推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>保留模态专属特征，引入语言显著调制、视觉偏差校正与指代关系增强三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上实现新SOTA，同时计算效率优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在单塔框架内联合显式去偏与指代语义推理，提出模块化三组件设计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效且可解释的多模态指代理解提供了去偏新思路与即插即用模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Grounding 要求模型仅依据自然语言表达式在图像中精确定位目标区域，是多模态理解的核心任务。近期单塔(one-tower)架构通过共享编码器融合图文特征，在零样本与迁移场景下取得进展，却暴露出模态偏差被过度放大、指代推理不足两大痛点，制约了鲁棒性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BARE 框架，在单塔结构内显式保持模态私有特征，通过三个协同模块进行偏差抑制与推理增强：i) Language Salience Modulator 先提取词汇显著性再动态重标定文本特征，抑制非关键词的干扰；ii) Visual Bias Corrector 利用对比学习校准视觉区域分布，削弱数据集固有的共现偏好；iii) Referential Relationship Enhancement 构建跨模态异构图并执行消息传递，迭代推理对象-属性-关系三元组以强化指代语义。整体采用端到端训练，仅增加轻量投影与图网络，保持单塔的高效推理优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO、RefCOCO+、RefCOCOg、Flickr30k Entities 与 Visual Genome 五项基准上，BARE 将平均 top-1 精度提升 2.1-4.8 个百分点，达到新 SOTA，同时推理速度比双塔方法快 1.6×，参数量减少 23%。消融实验显示各模块对偏差缓解与推理增益贡献互补，可视化表明激活区域与真实指代目标重叠率提高 9.3%，验证了偏差抑制与语义细化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英语指代表达与公开基准上验证，跨语言或复杂长句场景下的泛化能力尚不明确；偏差校正模块依赖统计先验，若测试分布与训练差异显著，可能引入新的估计误差。此外，指代图构建采用固定启发式规则，对隐含关系或多重指代的建模仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将 BARE 的偏差校正思想扩展至视频指代定位及视觉对话，实现时空一致的多轮推理；或引入可学习的关系挖掘替代手工图结构，进一步提升复杂指代场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合中的偏差治理、单塔架构的效率-性能权衡，或希望将语义推理模块迁移到 VQA、图像字幕等任务，BARE 提供了一套即插即用且开源的解决方案与详尽实验分析，可直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651125" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-view and Multi-step Interaction for Change Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨视角多步交互的变化描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tiantao Xian，Zhiheng Zhou，Wenlve Zhou，Delu Zeng，Bo Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651125" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651125</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视角偏移干扰下为图像对生成准确、稳定的真实变化语义表示并生成自然语言描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨视角多步交互网络：跨视角交互编码器增强单图内部关系，多步变化感知器由粗到精捕捉变化，融合模块生成细粒度表示，并辅以自监督反向重建模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开数据集上达到SOTA，显著抑制伪变化干扰，生成更精准的变化描述。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨视角交互与多步渐进式变化感知结合，并引入自监督反向重建提升变化语义捕获能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为变化描述、遥感监测、自动驾驶等需跨视角变化理解的领域提供鲁棒特征提取与描述生成新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change captioning 需要同时理解两张图像并精确定位语义差异，但现有方法在视角变化带来的伪变化干扰下容易生成不稳定或错误的描述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨视角多步交互网络：先用跨视角交互编码器在单图内部做交叉参照以强化特征，再用多步变化感知器由粗到细地抽取变化语义，随后通过融合模块动态整合成细粒度变化表示；另外设计反向表示重建模块，以自监督方式约束模型聚焦真实语义变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开数据集上达到 SOTA，显著降低视角偏移导致的伪变化误判，生成的变化描述在准确性与稳定性方面均优于现有方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对图像的精确对齐，对严重几何畸变或光照突变场景鲁棒性未验证；多步交互引入额外计算开销，实时性受限；反向重建损失的超参数需针对数据集仔细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无对齐假设的宽松匹配策略，并引入视觉-语言预训练以提升零样本泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决视角变化下的伪变化干扰，为跨视角差异检测、图像对比描述及自监督表示学习提供可复用框架与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01416v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AirSpatialBot：具备空间感知的空中智能体用于细粒度车辆属性识别与检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Zhou，Ran Ding，Xue Yang，Xue Jiang，Xingzhao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01416v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升遥感视觉-语言模型对无人机车辆影像的空间理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含3DBB的AirSpatial数据集，采用图像预训练+空间微调两阶段策略训练VLM</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提AirSpatialBot显著优于现有VLMs，暴露其空间推理缺陷</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入3DBB的遥感空间定位与问答数据集及可执行细粒度检索的空中智能体</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供大规模空间指令数据与空间感知训练范式，推动无人机应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉-语言模型(VLM)已在跨模态理解上取得显著进展，但在空间定位与推理方面仍明显落后于自然场景模型，制约了其在精细交通监控、智能巡检等真实任务中的可用性。无人机影像中车辆目标小、视角高、密集排列，对模型的细粒度属性识别与空间关系理解提出了更高要求，亟需专门的数据集与算法框架来填补空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了一个含20.6万条指令的无人机车辆影像数据集AirSpatial，首次引入3D边框(3DBB)并设计了两个新任务：空间定位(Spatial Grounding)与空间问答(Spatial QA)。训练采用两阶段策略：先在通用图文对上执行图像理解预训练，再在AirSpatial上进行空间理解微调，从而将已有VLM知识迁移到遥感空间域。基于微调后的空间感知VLM，作者开发了空中智能体AirSpatialBot，它通过动态任务规划、图像理解、空间推理与执行模块的闭环协作，实现任意自然语言查询下的细粒度车辆属性识别与检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，现有主流VLM在空间定位与问答任务上显著落后于AirSpatialBot，验证了对遥感空间理解进行专门训练的必要性。AirSpatialBot在细粒度属性识别与多约束检索准确率上均取得SOTA，且可解释地输出3D边框与推理链，为后续决策提供直观依据。消融实验显示，3DBB与两阶段训练分别带来约8%和12%的性能增益，证明空间先验与渐进式迁移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖无人机视角下的车辆类别，尚未扩展到多类遥感目标或卫星影像，限制了模型的通用性。3D边框依赖激光雷达或多视角重建，在纯RGB单图场景下无法获取，可能影响实际部署的可行性。此外，20万条指令虽规模可观，但语言模板仍可能带来隐性偏差，影响模型在开放词汇查询上的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入卫星与无人机多源影像，构建跨视角、跨分辨率的空间理解基准，并探索无3D监督下的单目深度估计与空间推理。结合链式思维(CoT)与大规模语言模型，实现更复杂的遥感时空问答与决策规划。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将3D空间定位与问答引入遥感VLM，为从事细粒度目标识别、跨模态检索、无人机智能体或遥感基础模型的研究者提供了公开数据集、训练策略和完整代码，可直接作为后续算法对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01513v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FastV-RAG：面向快速且细粒度的视频问答之检索增强生成方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gen Li，Peiyu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01513v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在视频问答中既快速又准确地利用外部知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VideoSpeculateRAG，用轻量草稿模型生成候选答案再由重模型验证，并引入相似度过滤修正实体</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持或提升准确率的同时将推理速度提高约2倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将投机解码与RAG结合用于视频QA，并用简单相似度过滤解决实体识别错误</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时响应的知识密集型多模态任务提供了高效可靠的RAG范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown strong visual reasoning skills but still lag when required to fuse external knowledge for video question answering. Existing RAG pipelines for video QA are computationally heavy and often degrade answer quality because retrieved knowledge is noisy and entity recognition is brittle.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose VideoSpeculateRAG, a two-stage speculative decoding framework: a lightweight draft VLM rapidly proposes multiple answer candidates, and a heavier verifier VLM re-scores and refines only the promising ones, cutting overall inference time. To curb entity mismatch errors in retrieved text, they add a cosine-similarity filter that aligns named entities in the passage with those in the question before the passage is fed to the model. All components are trained end-to-end with a contrastive objective that rewards both answer correctness and retrieval precision.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MSRVTT-QA, ActivityNet-QA and NExT-QA the system matches or exceeds standard RAG baselines while running ~2× faster, with absolute gains of +1.8–3.4 % in accuracy and a 46 % reduction in entity linking errors. Ablation shows speculative decoding contributes 70 % of the speed-up and the entity filter contributes 60 % of the accuracy improvement, verifying that both ideas are complementary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to short-form videos (&lt;120 s) and English captions; longer videos or multilingual settings may expose memory bottlenecks. The draft and verifier models share the same architecture family, so the approach may generalize poorly when the two models are structurally incompatible. Retrieval still relies on CLIP-style features, which can miss fine-grained temporal moments.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend speculative decoding to hierarchical video representations and integrate reinforcement learning to train the draft model to propose higher-recall candidates. Explore joint training of retrieval and generation to further tighten the knowledge gap.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal reasoning, speculative decoding, or knowledge-enhanced VLMs can borrow the two-stage verification and entity-filtering ideas to boost both speed and robustness in their own RAG pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00584v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GranAlign：面向零样本视频时刻检索的粒度感知对齐框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyu Jeon，Sunjae Yoon，Jonghee Kim，Junyeoung Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00584v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality&#39;s representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>零样本视频时刻检索中文本与视觉粒度失配导致定位不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>免训练框架GranAlign，用粒度感知查询重写与查询感知字幕生成对齐多粒度语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准刷新SOTA，QVHighlights mAP@avg提升3.23%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式调和预训练跨模态粒度差异，无需微调即可动态匹配多级语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无标注视频检索提供即插即用方案，推动零样本跨模态理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-shot video moment retrieval (ZVMR) 要求在不使用任何任务特定训练数据的情况下，用自然语言查询定位未修剪长视频中的目标片段；其核心难点是文本查询与视觉内容在语义粒度上天然不一致，导致跨模态对齐失败。现有工作依赖高质量预训练联合嵌入，却忽视了两模态对同一场景提供的知识粒度差异，从而限制了检索精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练框架 GranAlign，通过两种互补策略显式调和粒度差异：1) 粒度感知查询重写，将原始查询自动扩展为多个从粗到细的语义级版本；2) 查询感知字幕生成，把查询意图注入视觉内容，产生与查询相关的视频字幕。随后将多级查询分别与查询无关及查询感知的字幕进行相似度匹配，再融合分数以定位最相关时刻，实现无需任何参数更新的零样本推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 QVHighlights、Charades-STA 和 ActivityNet-Captions 三大主流基准上，GranAlign 全部刷新 SOTA，其中最具挑战的 QVHighlights 平均 mAP 提升 3.23 个百分点，验证了粒度对齐策略对缓解跨模态语义差异的有效性。实验还表明，两种粒度调和模块协同增益显著，单独使用任一模块均无法达到同等性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练字幕生成模型，若视觉内容过于复杂或字幕模型偏差大，会引入错误文本信号；同时查询重写需要外部语言模型，可能因改写不当而放大噪声。此外，计算开销随查询级数线性增加，对长视频实时应用仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应级数选择或强化学习驱动的动态粒度调整，以在精度与效率间取得更好平衡；也可将粒度对齐思想扩展到视频 grounding、多模态对话等更广泛的零样本任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无训练跨模态对齐提供了可复用的粒度调和范式，对从事零样本视频理解、图文检索或跨模态表征研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010161" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Injected Discriminative Model for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感视觉定位的文本注入判别模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minhan Hu，Keke Yang，Jing Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010161" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010161</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感视觉定位中同时提升视觉-语言语义聚焦与相似目标区分能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将文本线索轻量级注入视觉骨干，并引入框外注意力正则化损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上取得领先性能，验证简单融合与正则化策略有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需复杂融合模块的文本注入骨干及针对遥感场景的框外注意力正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态细粒度定位提供高效新范式，可直接嵌入现有检测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉定位(RSVG)任务要求模型根据自然语言描述在高分辨率遥感图像中精确标出目标边界框。传统两阶段方法先独立提取视觉与文本特征再进行融合，导致视觉编码阶段缺乏对描述中关键语义的关注，难以突出待定位对象。遥感影像中同类地物外观高度相似、背景复杂，进一步加剧了精确定位的难度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种“文本注入式判别模型”：在视觉骨干网络(如ResNet)的每个残差块末端，仅用1×1卷积将语言特征向量映射到视觉特征通道维度并逐通道相加，实现零参数增加的语言引导视觉增强。为抑制混淆背景，设计Attention Regularization Loss：在预测框外采样K个难分辨负样本区域，强制其注意力权重低于框内目标权重，从而放大目标-背景差异。整体训练仍沿用DETR风格的集合预测损失，仅额外加入λ·L_attn正则项。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个RSVG基准(NWPU VHR-RSVG、RSVG-1k、DIOR-RSVG)上，该方法以≤2%的参数量增幅将平均Top-1定位准确率提升3.1-4.7个百分点，达到新的SOTA；可视化显示注入文本后热力图显著聚焦于描述关键词对应的地物，且正则化项使框外相似目标的响应降低约18%。消融实验表明语言注入与注意力正则分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单源光学影像上验证，未涉及多光谱、SAR或时序数据；注意力正则依赖额外的难负采样启发式规则，对超参数K敏感；方法仍基于离线提取的文本特征，尚未探索端到端的大语言模型协同。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究将多模态大模型作为统一编码器，实现遥感影像-文本-地理知识联合推理；并引入自监督预训练以利用海量未标注遥感-文本对，提升模型对新颖地物描述的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感理解、轻量化融合设计或细粒度目标定位，该文提供的“即插即用”文本注入单元与背景-区分正则化策略可直接迁移至遥感字幕生成、变化描述检测等任务，减少重新设计复杂融合模块的成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PVF-DectNet++：基于透视体素的自适应多模态融合三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ke Wang，Weilin Gao，Kai Chen，Tianyi Shao，Liyang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3650671</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机融合中深度缺失与固定权重导致的语义提取不足、适应性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>透视体素投影对齐特征，RGB-I 稠密插值提取全局多级图像特征，可学习融合模块自适应加权</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI 上 mAP 提升 3.56%，nuScenes mAP/NDS 增 3.8%/2.6%，Waymo 行人/骑行者 AP 显著提高</p>
                <p><span class="font-medium text-accent">创新点：</span>提出透视体素对齐与 RGB-I 多通道表示，并设计可学习通道自适应融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶 3D 检测提供更精准的多模态融合方案，尤其改善小目标与复杂环境表现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶对3D目标检测的精度要求极高，单一LiDAR或相机模态均难以同时提供稠密几何与丰富语义。现有LiDAR-相机融合方法普遍依赖图像深度估计误差大且融合权重固定，导致在行人、骑行者等小目标上召回率不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PVF-DectNet++提出透视体素投影(Perspective Voxel Projection)，将点云与图像对齐到统一视角空间，避免显式深度估计误差传播。其自适应图像语义提取模块把RGB与点云反射强度插值为稠密RGB-I多通道张量，通过全局上下文网络捕获多级语义。融合阶段引入可学习通道-空间权重网络，根据外观、颜色及环境变化动态调整LiDAR与图像特征的贡献比例。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI基准上，PVF-DectNet++对行人、骑行者、车辆的AP分别达到66.3%、78.8%、86.8%，mAP较基线PVF-DectNet提升3.56%。nuScenes上mAP与NDS分别再涨3.8%和2.6%，Waymo上小目标AP提升更为显著，证明其对多数据集、多天气、多密度的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>透视体素投影依赖高精度外参标定，标定误差会直接扭曲跨模态对齐；动态权重模块增加参数量与显存，对车载嵌入式GPU的实时性提出更高要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定或自标定的柔性对齐机制，并将可学习融合压缩为轻量化二进制掩码以提升边缘端推理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态3D感知、小目标检测或动态融合策略，本文提供的透视体素对齐与自适应权重思路可直接迁移到其它LiDAR-视觉框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Excluding the Interference for Open-Vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">开放词汇语义分割中的干扰排除研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Shao，Shiyuan Zhao，Rui Xu，Yan Wang，Baodi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650803</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 在开放词汇语义分割中易将新类误判为常见类，需排除干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>EXIST-Net 两阶段框架：MPN 生成掩码，MFC 与 MRC 分别计算包含/排除概率，PCor 校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上显著提升新类分割精度，MRC 块模型无关且资源占用低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“排除干扰”反向分类器 MRC，用排除概率修正识别，实现两阶段精细分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇语义分割提供即插即用低耗模块，助力 CLIP 类模型抑制类别偏差。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)需在测试时识别训练阶段从未见过的类别，而主流CLIP-based方法因类别不平衡与训练数据仅含已知类，易将新类误判为常见类，产生“干扰项”问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EXIST-Net，将单步识别拆为两阶段：先用Mask Proposal Network生成类无关掩码，再由Mask Forward Classifier给出掩码属于某类的包含概率；核心为Mask Reverse Classifier，并行计算掩码“不属于”每类的排除概率，最后Probability Corrector用排除概率修正包含概率，实现“先排除干扰、再精细分类”。MRC模块仅增加轻量反向分类头，可与任何CLIP-based分割框架即插即用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个主流OVSS数据集上，EXIST-Net显著提升了新类识别精度并降低干扰误判，平均mIoU优于ELSE-Net及最新方法约2–4个百分点；消融实验表明仅引入MRC即可为现有方法带来稳定增益，且显存与推理时间增幅&lt;5%，验证其模型无关性与低资源消耗。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在RGB图像基准上验证，未探讨复杂场景(强光照、跨域)下排除概率的稳定性；MRC依赖预定义类别名，若开放词汇极度扩展，反向分类头计算量线性增长，可能削弱效率优势。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言蒸馏将MRC压缩为动态提示，或利用不确定性估计自适应决定是否启用排除机制，以进一步扩展词汇规模并提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇/零样本分割、CLIP偏差修正或即插即用模块设计，本文提供的“排除-再确认”范式与轻量MRC块可直接迁移至相关框架，加速新类识别性能的提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00562v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cascaded Information Interaction Network for Precise Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">级联信息交互网络用于精确图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hewen Xiao，Jie Mei，Guangfu Ma，Weiren Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00562v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂、模糊或杂乱场景中实现更鲁棒的图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出级联CNN并嵌入全局信息引导模块，跨层融合低层纹理与高层语义特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准数据集上精度优于现有SOTA，显著提升模糊与杂乱环境下的分割效果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入全局信息引导模块，实现多尺度特征级联融合，突破单尺度提取局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉感知机器人提供高精度分割方案，可替代昂贵多传感器系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉感知是自主系统的核心，但复杂场景下的鲁棒分割仍受单尺度特征提取限制，导致在杂乱或模糊环境中精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出级联CNN，并在其中插入Global Information Guidance Module，以逐层递归方式将低层纹理与高层语义进行全局融合。该模块采用多尺度池化与通道-空间双重注意机制，实现跨层信息交互与动态权重分配。网络以级联结构逐步细化掩膜，每级接收前级预测作为先验，并结合原始图像再训练，降低累积误差。训练策略结合在线难例挖掘与多损失加权，使边缘与细小结构获得额外监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL VOC 2012、Cityscapes与模糊版BSDS上的mIoU分别提升2.3、1.9和3.7个百分点，边缘F-score提高4.1%，显著超越同期arXiv最优模型。消融实验表明，移除全局引导模块后mIoU下降2%，验证其对跨层融合的关键作用。模型在NVIDIA Jetson Xavier上达到28 FPS，满足实时机器人部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告arXiv版本，尚未经过同行评审；实验未涵盖医学或高光谱图像，泛化能力待验证。级联结构带来额外参数与内存开销，对资源受限嵌入式设备仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化全局引导模块以适配移动端，并引入自监督预训练进一步提升在少样本场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多尺度特征融合、实时分割或机器人视觉，该文提供的级联-全局交互框架可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越LLaVA-HD：深入探索高分辨率多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YiFan Zhang，Qingsong Wen，Chaoyou Fu，Kun Wang，Xue Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650761</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不暴增计算量的前提下，让多模态大语言模型真正利用高分辨率图像进行精细视觉推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全局-局部双分支：混合适配器提炼全局语义，可学习查询压缩局部块并用相似度筛选关键令牌，交替训练优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>更少但更信息丰富的局部令牌即可提升性能，交替训练平衡全局-局部学习，仅用200万数据达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合适配器全局专家、查询式局部令牌压缩与相似度选择、交替训练策略集成于高分辨率MLLM框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高分辨率视觉-语言理解提供新范式，显著降低计算与数据成本，对多模态模型研究与部署具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视觉输入对多模态大语言模型(MLLM)的视觉感知与推理至关重要，但现有LLaVA-HD类方法简单地把切片局部图块放大到与全局分支同分辨率，导致随分辨率提升计算量激增，且局部token过多会淹没全局上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SliME框架：全局分支用混合适配器(Mixture of Adapters)挖掘不同任务擅长的专家特征；局部分支引入可学习查询嵌入先压缩图块token，再用基于相似度的选择器保留与问题最相关的少数token；训练上采用全局挖掘块与局部压缩块交替训练而非同时端到端，以平衡两者学习；并构建了一个对细节要求极高的新数据集强化局部压缩层训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验在仅200万训练数据下取得多个基准领先成绩，验证“少即是多”现象：用更少但高信息量的局部token反而提升性能；交替训练策略显著优于联合端到端训练；新数据集有效增强模型对细粒度细节的捕捉能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖手工设计的适配器组合与查询数量，可能难以泛化到所有视觉任务；相似度选择器基于余弦相似度，可能忽略低相似但关键的细节token；整体流程增加训练与推理的工程复杂度，实际部署成本需进一步评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应适配器网络结构搜索与动态查询数预测，并研究无相似度选择器的可微分token稀疏化机制以实现端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统剖析了高分辨率MLLM的效率瓶颈，提出可插拔的局部token压缩与全局专家混合策略，为研究高分辨率视觉-语言模型、高效多模态推理或视觉token选择的研究者提供可直接对比与扩展的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LRANet++：用于精准高效文本定位的低秩近似网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchen Su，Zhineng Chen，Yongkun Du，Zuxuan Wu，Hongtao Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650769</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一、精准且高效地检测与识别任意形状文本</p>
                <p><span class="font-medium text-accent">研究方法：</span>低秩近似参数化文本轮廓+三重分配检测头+轻量识别分支</p>
                <p><span class="font-medium text-accent">主要发现：</span>LRANet++在多项基准上精度与速度均优于现有端到端方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用数据驱动的低秩基向量表示文本形状并解耦训练与推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时场景下的任意形状文本检测识别提供了高效可行的新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端文本检测-识别联合优化已成为场景文字理解的主流范式，但现有方法在任意形状文本上仍面临检测精度与推理速度难以兼得的瓶颈。作者指出，核心障碍是缺乏一种既可靠又高效的文本检测表示，使得整个 spotting 框架在复杂形状下要么精度下降，要么计算开销激增。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出基于低秩近似（Low-Rank Approximation, LRA）的参数化文本轮廓表示：先利用训练集所有标注边界构造形状矩阵，通过ℓ1范数最小化提取一组正交基向量，仅用少量基即可线性重构任意弯曲文本轮廓，从而对标注噪声鲁棒且精度高。检测头采用“三重分配”策略——深而稀疏的辅助分支在训练阶段提供难例监督，超轻量推理分支只负责最终输出，另设密集分支给出丰富中间监督，实现训练复杂度与推理速度解耦。将上述检测模块与轻量级识别分支拼接，得到端到端框架 LRANet++，全流程共享特征图，兼顾任意形状文本的精确定位与快速识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Total-Text、SCUT-CTW1500、ICDAR2015 等基准上，LRANet++ 的 F-score 与端到端识别准确率均优于同期 SOTA，而 FPS 提升 1.4×–2.2×；低秩基仅需 8–16 个向量即可将 100+ 点轮廓误差控制在 1.2 px 以内，验证了对复杂形状的紧凑表达能力。消融实验显示，去掉 LRA 表示或三重分配后，精度分别下降 3.1% 与 2.4%，推理时间增加 27%，证明两项设计对精度-效率平衡至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>低秩子空间需依赖训练集形状统计，若测试场景出现全新文本拓扑（如极端艺术字体）可能重构失效；三重分配虽加速推理，却引入额外超参（稀疏阈值、分支权重）需经验调优。此外，论文未在移动端芯片上做量化或 TensorRT 级部署验证，实际硬件加速效果尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线更新低秩基的自适应机制，以应对开放域新形状；并将框架蒸馏为纯单分支网络，进一步消除训练-推理结构差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究任意形状文本检测、端到端 spotting、或低秩近似在视觉任务中应用的研究者，该文提供了可复现的代码与严谨实验，可直接作为基线或灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132597" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MarineSeg: A CNN–Transformer Hybrid Architecture with Feature Voting Decoder for Robust Semantic Segmentation in USV-Captured Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MarineSeg：一种CNN–Transformer混合架构，配备特征投票解码器，用于USV图像的鲁棒语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyang Gu，Baoyuan Deng，Yunze He，Yongjie Zhang，Liang Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132597" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132597</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation is essential for scene perception in unmanned surface vehicles (USVs), particularly in complex and dynamic navigation environments. Although recent Transformer models have outperformed convolutional neural networks (CNNs) in segmentation tasks, their high computational costs and large training data requirements limit their applicability to real-time navigation scenarios. To address these challenges in complex waterway environments, MarineSeg is proposed, a CNN-Transformer hybrid semantic segmentation architecture that achieves high performance while maintaining competitive efficiency. MarineSeg’s backbone integrates sparse self-attention and multi-scale pooling into CoAtNet, which stacks convolutional blocks for capturing local features and attention blocks for modeling global features. To enhance the integration of local convolutional and global attention features, a feature voting module with ensemble mechanism is introduced, facilitating adaptive fusion based on semantic importance. By extracting and integrating multi-level features, MarineSeg achieves state-of-the-art performance among pure image models on the YZ-PLUS, GBA, MODS and MODD2 datasets across key metrics while maintaining competitive real-time capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂水域环境中为无人船实时图像语义分割兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CNN-Transformer混合架构MarineSeg，用稀疏自注意力CoAtNet骨干并引入特征投票解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在YZ-PLUS等四数据集上取得纯图像模型SOTA精度，同时保持实时推理速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稀疏自注意力与多尺度池化结合于CoAtNet，并设计基于语义重要性的特征投票融合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的无人船提供高精度实时分割方案，推动水上自主导航研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人水面艇(USV)在复杂动态水域航行时，需要实时语义分割来感知场景，但纯Transformer模型计算量大、数据需求高，难以满足实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MarineSeg以CoAtNet为骨干，交替堆叠卷积块捕获局部特征与稀疏自注意块建模全局特征，并在解码端引入特征投票模块，通过集成机制按语义重要性自适应融合多尺度卷积与注意特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在YZ-PLUS、GBA、MODS、MODD2四个公开数据集上，MarineSeg仅用图像输入即取得SOTA精度，同时保持实时推理速度，显著优于现有纯CNN与纯Transformer方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论夜间、雨雾等低能见度条件下的鲁棒性；稀疏注意仍依赖GPU，极端嵌入式平台的能耗与内存占用未量化；缺乏与多模态(如LiDAR、雷达)方法的直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或红外数据构建多模态轻量化框架，并设计自适应稀疏度机制以进一步降低边缘设备上的计算与功耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在水面场景实现实时语义分割的研究者提供了一种兼顾精度与效率的CNN-Transformer混合范式，其特征投票解码思想亦可迁移到其他资源受限的机器人感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650947" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DPS-Net: Direction-Aware Pseudo-Stereo Network for Accurate Road Surface Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DPS-Net：方向感知伪立体网络，用于精确的路面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiyuan Han，Yidan Pei，Rui Wang，Tong Zhang，C. L. Philip Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650947" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650947</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决透视投影导致远处区域几何失真、长尾分布的道路表面重建难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DPS-Net，集成方向感知特征增强与伪立体融合两轻量级即插即用模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSRD数据集上实现高精度低复杂度重建，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入方向上下文增强稀疏特征，并用伪立体融合捕获全局依赖，保持轻量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供准确高效的路面几何，助力感知与规划性能提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>道路表面的几何形状直接影响自动驾驶系统的定位、路径规划与安全性，但单目图像在远距离区域因透视投影产生几何畸变与长尾分布，使现有深度估计或重建方法精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DPS-Net 以伪双目框架替代真双目，先利用 Direction-Aware Feature Enhancement (DFE) 模块在极坐标方向轴上聚合上下文，强化稀疏且几何不变特征；再通过 Pseudo-Stereo Fusion (PSF) 模块在伪左右特征图之间执行跨视角、跨通道的全局依赖建模，实现低计算量的全局-局部信息耦合；两模块均以 1×1/3×3 深度可分离卷积构建，可即插即用到任何单目深度网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 RSRD 数据集上，DPS-Net 将 REL 降低 18.7%，RMSE 降低 15.2%，帧率达 48 fps（RTX-3080），显著优于 MonoDepth2、BTS、FAL-Net 等基线；消融实验表明 DFE 与 PSF 分别贡献 7.4% 与 6.9% 的 REL 下降，且参数量仅增加 0.38 M。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖假设路面为连续可微表面，对坑洼、减速带等突变区域存在过度平滑；伪双目生成阶段未显式利用时序信息，导致高速场景下动态目标深度抖动；此外，RSRD 数据集场景以平直公路为主，复杂城市路口与坡道泛化性能尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序伪双目序列与自监督光度一致性，提升高速动态场景鲁棒性，并构建含陡坡、井盖等几何突变的新基准以进一步验证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注单目深度估计、自动驾驶感知、轻量级网络设计或伪双目/自监督三维重建的研究者，该文提供了可插拔的方向感知模块与在真实道路数据集上的详尽实验，可直接迁移或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">子图像重叠预测：面向遥感影像语义分割的任务对齐自监督预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lakshay Sharma，Alex Marin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01781v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少预训练影像下为遥感语义分割获得高质量自监督表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出子图重叠预测任务：模型预测被裁剪子图在原图语义掩码中的位置</p>
                <p><span class="font-medium text-accent">主要发现：</span>相同预训练数据量下mIoU更高，且减少标注样本时收敛更快、性能优势更大</p>
                <p><span class="font-medium text-accent">创新点：</span>首个利用子图-原图空间对应关系进行自监督预训练，显著降低数据需求</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供低数据成本、高迁移性的预训练策略，可即插即用到多种分割网络与数据集</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割依赖大量标注数据，而标签获取成本高昂；自监督预训练虽可缓解标注压力，但主流对比或掩码方法需海量无标签影像，难以在数据稀缺场景中落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“子图重叠预测”(Subimage Overlap Prediction) 预训练任务：从原图随机裁剪子图，让网络输出该子图在原图中的像素级二值掩码，从而强制编码器学习空间-语义对应关系；整个流程仅使用少量无标签遥感影像，预训练完成后丢弃解码头，将编码器接入 DeepLabV3+、U-Net 等分割框架进行微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DeepGlobe、Potsdam、Vaihingen 三个下游数据集上，用 5%–20% 的常规 SSL 预训练数据量即可使 mIoU 持平或提升 1.3–3.2 个百分点；当下游标注图像减少至 10% 时，收敛速度提升约 2×，mIoU 优势扩大至 4–6 个百分点；效果在 ResNet-50、EfficientNet-B3、Swin-T 三种骨干上均稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>任务设计依赖遥感影像的刚性几何与显著纹理，若场景缺乏可辨识结构（如云层、水域）则预训练信号可能退化；目前仅在 0.1–0.5 m 分辨率光学影像验证，未测试 SAR、多光谱或时序数据；与对比式 SSL 的混合增益及理论可迁移下界尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将子图重叠预测扩展为多尺度、多时相版本以利用高分辨率时序遥感数据，并结合对比约束进一步压缩所需预训练数据量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感语义分割、轻量级预训练或空间定位代理任务设计，该文提供了可复现的新基准与代码，可直接嵌入现有分割框架并减少数据采集成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650952" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A SAM Fine-Tuning Framework with Frequency-Domain Interactive LoRA for Remote Sensing Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种结合频域交互LoRA的SAM微调框架，用于遥感变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junqing Huang，Shucheng Ji，Yapeng Wang，Min Xia，Xiaochen Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650952" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650952</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Achieving high-accuracy remote sensing change detection (RSCD) algorithms requires high-quality semantic feature extraction from remote sensing images (RSIs). Due to its powerful general-purpose feature extraction capability, the Segment Anything Model (SAM) has found wide application across diverse fields. However, SAM may not be optimally suited for RSIs. To address this limitation, we propose a Frequency-domain Interactive LoRA Fine-tuning Architecture (FILFArch) to enhance the performance of SAM in RSCD tasks. Based on FILFArch, we then develop two task-specific algorithms, the FILFBCD for Binary Change Detection (BCD), and the FILFSCD for Semantic Change Detection (SCD). To enhance the capability of SAM in capturing bi-temporal RSIs feature relationship, the Bi-temporal Feature Interactive LoRA (BIF-LoRA) is designed with Siamese architecture. Within BIF-LoRA, Frequency-Domain Feature Interaction (FDFI) utilizes Fast Fourier Transform Block (FFTB) to fuse bi-temporal frequency-domain features. This enables cross-temporal frequency-domain interaction, effectively discriminating spatio-temporal feature differences. Additionally, we use a shared BCD Decoder to serves as the binary change detector for both FILFBCD and FILFSCD. The BCD Decoder first applies a Coarse Difference Feature Extraction (CDFE) to coarsely fuse deep semantic features, yielding a coarse-grained change feature map. Subsequently, a Frequency-Domain Feature Enhancement (FDFE) refines these abstract features to generate a fine-grained change map. In FILFSCD, FDFE is further utilized to recover semantic change information of each temporal RSIs. Experimental results demonstrate that FILFBCD achieves the highest F1 scores of 83.53%, 66.75%, and 83.79% on BCD datasets MLCD, S2Looking, and SYSU-CD, respectively. Meanwhile, FILFSCD achieves the highest F1 scores of 64.05% and 87.02% on SCD datasets SECOND, and DSCD, respectively. These results demonstrate the effectiveness and versatility of the propos...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用SAM模型高效适应遥感影像变化检测任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域交互式LoRA微调框架FILFArch，结合双时相频域特征交互与共享解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FILFBCD/SCD在五个基准数据集上均取得最高F1，验证频域LoRA显著提升检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LoRA与FFT频域融合引入SAM微调，实现轻量级双时相特征交互与差异增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供即插即用的SAM适配方案，降低计算成本并刷新多项公开纪录。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测(RSCD)的核心瓶颈在于对高分辨率、多光谱影像进行鲁棒且可迁移的语义特征提取。Segment Anything Model(SAM)虽具备通用视觉先验，但其原生编码器针对自然图像训练，缺乏对遥感地物尺度-光谱特性以及双时相差异的建模能力，直接微调易导致灾难性遗忘与域偏移。因此，亟需一种轻量级、可解释且能显式利用时-频耦合信息的SAM适配范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Frequency-domain Interactive LoRA Fine-tuning Architecture(FILFArch)，在SAM的ViT编码器中插入并行BIF-LoRA支路，仅训练低秩矩阵而冻结主干，实现参数高效迁移。BIF-LoRA采用权重共享的Siamese结构，双时相特征在FFT块内变换至频域后做逐频带交互，通过可学习掩码抑制共有分量、增强差异分量，再逆变换回空域，从而显式放大时空差异。随后，统一BCD Decoder先以CDFE模块对深层语义做粗粒度差分，再由FDFE模块在频域二次滤波，生成精细变化概率图；FILFSCD进一步复用FDFE解码分支，逐时相恢复语义变化掩膜，实现二值与语义变化检测的一体化框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个主流BCD数据集(MLCD、S2Looking、SYSU-CD)上，FILFBCD的F1分别达到83.53%、66.75%、83.79%，显著优于此前最佳方法2.1-4.3个百分点；在SECOND与DSCD两个SCD基准上，FILFSCD亦以64.05%与87.02%的F1刷新纪录。消融实验表明，BIF-LoRA与FDFI分别贡献约40%与35%的性能增益，且仅引入1.8%可训练参数，验证了频域交互对差异特征增强的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在五个公开数据集上验证，尚未覆盖高光谱、SAR或多源异构影像，对大幅几何配准误差与季节辐射差异的鲁棒性未深入讨论；此外，FFT-based频域交互对输入尺寸要求2的幂次填充，导致显存占用随分辨率二次增长，可能限制大场景在线应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索小波或DCT等局部-频域混合变换以降低计算负荷，并将BIF-LoRA扩展为时序一致的多时相版本，支持长序列土地覆盖动态监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注参数高效微调、基础模型在遥感下游任务的适配，或希望利用频域先验提升变化检测精度，该文提供了可即插即用的LoRA-Frequency范式与完整代码基线，可直接迁移至建筑物提取、灾害评估等遥感语义分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MADTP++：弥合Token剪枝与权重剪枝差距，加速VLTs</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianjian Cao，Chong Yu，Peng Ye，Tao Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650545</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时压缩视觉-语言 Transformer 的输入 token 与模型权重并维持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MADTP++：多模态对齐引导+动态 token 剪枝、硬件感知细粒度权重剪枝与协同优化蒸馏训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个 VLT 模型与数据集上显著降低参数量与 GFLOPs，同时保持与原始模型竞争的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 token 与权重剪枝统一于同一框架，引入 MAG 对齐、DTP 动态剪枝及 HWP 硬件细粒度稀疏核心利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大规模多模态模型提供可扩展的联合压缩方案，推动 VLT 在资源受限场景的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Transformers (VLTs) have become the de-facto architecture for cross-modal tasks, but their quadratic token complexity and hundreds of millions of parameters make deployment on edge devices prohibitive. Prior compression works treat visual token pruning and weight pruning as isolated problems, overlooking the semantic mis-alignment between modalities and the layer-wise heterogeneity in redundancy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MADTP++ couples two new modules—Multi-modality Alignment Guidance (MAG) that computes cross-modal attention entropy to identify semantically mis-aligned tokens, and Dynamic Token Pruning (DTP) that learns input-specific keep-ratios per layer with a lightweight LSTM-based controller—into a single token pruning stream. Parallel to this, a Hardware-aware Weight Pruning (HWP) module uses a differentiable sparse-mask guided by the Tensor-Core granularity of the target GPU to yield 2:4 fine-grained sparsity without extra sparse kernels. A Cooperative Optimization Training Strategy pre-allocates the desired GFLOPs/Params reduction to each branch via a bi-level knapsack solver and then co-distills the compressed model with the full model using bidirectional KL divergence to recover accuracy.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On NLVR2, MADTP++ removes 48 % of tokens and 60 % of weights while dropping only 0.7 % accuracy, outperforming previous best token-only (ToMe) and weight-only (S2FP) methods by 2.3 % and 3.1 % absolute points respectively. Similar trends hold for Flickr30K and VQA v2, where the method achieves 2.1× measured wall-clock speed-up on an RTX-4090 with Tensor-Core sparsity enabled. Ablation shows that removing either MAG or the cooperative distillation leads to &gt;1.5 % accuracy loss, confirming the necessity of joint optimization.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The HWP module is currently limited to NVIDIA GPUs that support 2:4 sparsity, leaving other accelerators and CPUs un-optimized; the DTP controller adds 0.3 M extra parameters and 5 % training overhead; and the approach still requires the full teacher during distillation, increasing memory footprint during compression.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend HWP to arbitrary block-sparse patterns via compiler-aware super-net search, and integrate quantization-aware training to enable simultaneous INT8 + sparsity co-compression.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal inference, dynamic neural network pruning, or hardware-aware compression can directly adopt the MAG alignment metric and the cooperative distillation schedule as plug-ins for any transformer-based vision-language model.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义感知代价聚合与空间传播的多视角全向深度估计增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Li，Xuejiao Hu，Zihang Gao，Sidan Du，Yang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651056</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多鱼眼相机环绕配置下提升360°深度估计在弱纹理、遮挡等复杂区域的精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多视图特征方差与均值构建语义-几何联合代价体，360°语义上下文聚合后再用语义感知空间传播细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集与实车数据均达SOTA，深度图与全景联合预测精度高，对相机布局变化鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将360°语义上下文引入代价聚合，并提出语义感知空间传播模块，实现深度-全景多任务协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶环视感知提供高精度360°深度，推动无盲区三维环境理解与多传感器标定研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>360°全景深度估计对自动驾驶、AR/VR等应用至关重要，但现有方法在环绕鱼眼阵列中常因视角差异、弱纹理、遮挡及非重叠区而难以引入语义线索，导致边界与细节精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义感知代价聚合网络：先对多视角鱼眼特征计算方差-均值融合匹配代价，在360°球面代价体内并行嵌入几何与语义约束；随后以多尺度语义上下文聚合模块生成全景分割与深度联合初始解；最后设计语义感知空间传播模块，在球面格网上沿语义一致方向迭代扩散深度，实现边界保持的细粒度优化；整体采用多任务学习同步监督深度与全景分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上该方法将360°深度MAE降低12%，边界误差下降20%，并在真实车载四鱼眼阵列中取得厘米级精度；消融实验显示语义代价聚合与传播模块分别贡献7%与5%的MAE下降，不同相机布局（3-6目）下性能稳定，验证通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖同步且已标定的多鱼眼输入，对动态物体仍可能出现语义-深度不一致；语义分支需要额外标注的全景分割数据，在弱标注场景下性能下降；计算开销约为基线2.3×，尚难实时运行于车载低功耗平台。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督语义-深度联合训练以减少标注依赖，并结合球面Transformer进一步压缩模型；引入时序信息提升动态区域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多视角几何、360°感知或语义-深度耦合任务，该文提供了将语义线索嵌入球面匹配代价与空间传播的新范式，可直接迁移至其他全景视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向广义零样本视频分类的多模态知识驱动方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyao Hong，Xinfeng Zhang，Guorong Li，Qingming Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02584-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决仅靠类别名称学习导致的广义零样本视频分类性能受限问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多模态知识补全与生成模型，融合文本概念和视觉基元合成近真实分布特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准数据集上显著超越现有GZSVC方法，提升 unseen 类识别准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入多模态知识驱动框架，通过跨模态知识补全与语义强化约束弥补视觉缺失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频零样本学习提供可扩展的知识融合范式，推动开放域视频理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>广义零样本视频分类(GZSVC)仅依赖类别名称难以捕捉新视频的本质特征，限制了模型泛化。作者观察到人类会借助文本概念与视觉基础来构建对新事物的认知，受此启发提出引入多模态知识以突破传统GZSVC的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出多模态知识驱动框架：首先为每类自动检索并补充文本与视觉知识，弥补先验知识对新视频覆盖不足；随后设计多模态生成模型，将文本、视觉原型与语义嵌入融合，合成逼近真实分布的视频特征；训练阶段因缺乏不可见类真实样本，框架强化语义约束，使生成特征保持高语义一致性并用于分类器学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开视频数据集上的GZSVC实验显示，该方法在可见/不可见类调和平均准确率上均优于现有最佳方法，尤其在不可见类提升约4-7%，验证了知识补充与语义强化策略的有效性。消融实验表明，去除知识补充或语义约束后性能显著下降，证明各模块对泛化至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库的规模与质量，若检索知识存在噪声或偏差，可能引入错误关联；多模态生成模型计算开销大，对长视频或大规模类别扩展时训练与推理成本显著增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自监督知识挖掘以减少对外部库的依赖，并引入轻量级生成架构提升效率；同时研究知识动态更新机制，适应开放环境的新概念漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本学习、跨模态理解或视频语义生成，该文提供的知识驱动与语义强化思路可直接迁移至图像ZSL、动作识别或跨模态检索任务，为多模态知识融合提供可复用的范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcyb.2025.3634359" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parameter-Aware Mamba Model for Multitask Dense Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多任务密集预测的参数感知 Mamba 模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Cybernetics">
                IEEE Transactions on Cybernetics
                
                  <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinzhuo Yu，Yunzhi Zhuge，Sitong Gong，Lu Zhang，Pingping Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcyb.2025.3634359" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcyb.2025.3634359</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding the inter-relations and interactions between tasks is crucial for multitask dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, parameter-aware Mamba model (PAMM), specifically designed for dense prediction in multitask learning (MTL) setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state-space models (SSMs) to enhance task interconnectivity. It features dual state-space parameter experts (PEs) that integrate and set task-specific parameter priors (PPs), capturing the intrinsic properties of each task. This approach not only facilitates precise multitask interactions but also allows for the global integration of task priors through the structured state-space sequence (S4) model. Furthermore, we employ the multidirectional Hilbert scanning (MDHS) method to construct multiangle feature sequences, thereby enhancing the sequence model’s perceptual capabilities for 2-D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多任务密集预测中有效建模任务间关系并提升整体性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出参数感知 Mamba 解码器，用双状态空间参数专家整合任务先验并配合多维 Hilbert 扫描序列建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 NYUD-v2 与 PASCAL-Context 上显著优于现有方法，验证任务交互与参数共享的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型参数作为任务先验嵌入解码器，实现任务感知参数动态分配与全局序列融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉多任务学习提供高效轻量新架构，拓展状态空间模型在密集预测中的应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多任务稠密预测要求网络同时估计深度、语义、边缘等像素级标签，核心难点在于刻画任务间互利或竞争关系。现有方法主要依赖卷积或自注意力在特征空间显式建模任务交互，计算量随任务数二次增长且难以捕捉长程依赖。作者观察到状态空间模型(SSM)以线性复杂度即可建立全局感受野，遂提出用其参数空间而非特征空间来刻画任务关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PAMM在解码端为每个任务维护一对State-Space Parameter Experts(PE)，它们分别输出该任务的参数先验PP，并作为SSM的时变矩阵A/B/C/D注入S4层，实现“任务-参数-序列”三元耦合。为适配2-D图像，作者设计Multi-Directional Hilbert Scanning(MDHS)将特征图按希尔伯特曲线展开成多角度1-D序列，再送入参数条件化的双向S4，兼顾局部连续性与全局上下文。整体框架仅含轻量级SSM层与PE小网络，无需沉重Transformer即可在O(L)复杂度完成跨任务信息交换。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYUD-v2(3任务)与PASCAL-Context(5任务)上，PAMM将平均IoU/mIoU/RMSE同时提升1.8-2.4个百分点，参数仅增加0.9M，推理速度比基于Transformer的MTL方法快1.7×。消融实验显示，移除PE或MDHS任一分支，所有任务性能同步下降，验证参数级交互与多角扫描均不可或缺。可视化表明PP能自动学到任务冲突(深度vs边缘)与协同(语义vs部件)区域，为网络提供可解释的调节信号。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个室内/室外基准测试，尚不清楚在更高分辨率或更多任务场景下的可扩展性；SSM的连续微分方程假设对强纹理或重复结构区域可能欠拟合。此外，MDHS需离线预计算希尔伯特路径，对动态尺寸输入不够灵活，且额外引入约8%的内存拷贝开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索任务-参数先验的自监督预训练，使PE模块快速迁移到新任务；亦或把PAMM的思想推广到视频MTL，利用时间维度进一步压缩参数与计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多任务学习、高效视觉Transformer替代或状态空间模型在CV中的应用，PAMM提供了“用参数空间而非特征空间做任务交互”的新视角，其轻量、线性复杂度与开源代码可直接作为强基线或插件模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01024v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ITSELF：面向视觉-语言检索的注意力引导细粒度对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tien-Huy Nguyen，Huu-Loc Tran，Thanh Duc Ngo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01024v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model&#39;s own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本行人检索中局部对齐易受捷径学习与伪相关干扰导致的图文细粒度失配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ITSELF框架，用GRAB把模型自身注意力建成高显著token库并施加局部目标，辅以MARS跨层去冗余选token及ATS渐进式保留预算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个TBPS基准上达到新SOTA，跨库泛化强，无需额外先验监督。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用早期注意力自建高显著token库实现隐式局部对齐，并设计跨层聚合与渐进预算调度保证可靠且非冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度跨模态对齐的检索任务提供无需外部标注的注意力驱动新范式，可提升模型鲁棒性与泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have proven powerful for text-based person search, yet prior local-alignment strategies easily fall into shortcut learning and spurious correlations, causing region–phrase mis-matches. Injecting external prior knowledge to correct alignment often warps the intra-modal feature structure, so the authors ask whether the model’s own attention can provide cleaner, self-contained guidance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ITSELF performs implicit local alignment by harvesting the model’s early-layer attention to build an Attentive Bank (GRAB) of high-saliency tokens and applies contrastive objectives only on this bank, eliminating extra supervision. MARS aggregates attention scores across multiple transformer layers and performs diversity-aware top-k sampling to keep informative yet non-redundant tokens. An Adaptive Token Scheduler (ATS) gradually shrinks the retention budget from coarse context to fine-grained details as training proceeds, preventing over-focus on background regions too early.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CUHK-PEDES, ICFG-PEDES and RSTPReid the method sets new state-of-the-art R@1 scores while exhibiting superior cross-dataset generalization, e.g., +3.7 pp R@1 gain when CUHK-PEDES → ICFG-PEDES. Ablation shows GRAB alone contributes ≈60 % of the boost and MARS/ATS jointly remove 28 % of prior false-positive token matches. Attention visualizations confirm that selected tokens tightly overlap human-annotated foreground parts, indicating the framework truly learns fine-grained correspondences rather than dataset shortcuts.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach is tested only on person-search benchmarks; scalability to general image–text retrieval or video remains unverified. Attention-based selection may still inherit biases present in the initial pretrained VLM, and the coarse-to-fine scheduling hyper-parameters require dataset-specific tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ITSELF to video-text retrieval by incorporating temporal attention aggregation, and explore bank-driven alignment for other fine-grained tasks such as product or vehicle search.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal alignment, self-supervised fine-grained representation learning, or debiasing vision-language models can directly adapt the attention-guided bank paradigm without extra annotations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651017" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Human Feature Refinement for Weakly Supervised Group Activity Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向弱监督群体行为识别的高效人体特征精炼</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihui Zhou，Hao Chen，Zhanzhou Feng，Shiliang Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651017" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651017</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised group activity recognition (WSGAR) aims to identify the joint activity of a group of people without relying on hand-annotated human bounding boxes. Existing WSGAR methods typically acquire coarse human-level features by pooling from detected bounding boxes or applying human queries with cross attentions. These approaches focus on learning human relations from the acquired features. However, discriminative person-specific clues might be confused with irrelevant backgrounds, hindering the effectiveness of downstream human relation learning. To address this limitation, we propose a Human Feature Refinement framework that enhances human-level information with graph convolutional networks and self-attention. We define in-box regions as tokens and learn their spatial correspondence through GCN and self-attention. By explicitly extracting in-box details and suppressing irrelevant regions, our method acquires more discriminative human-level features for relation learning and group activity prediction. We further propose a Graph-based Token Merging algorithm to reduce the computation cost of Human Feature Refinement, while minimizing information loss and overfitting risk. Experiments show that our method outperforms previous WSGAR methods on Volleyball, NBA and JRDB-PAR benchmarks, with reduced computation cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无人工框标注条件下抑制背景噪声、提纯群体视频中的人物特征以提升弱监督群体行为识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以检测框内区域为token，用图卷积+自注意力精炼特征，并引入图式token合并降低计算量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Volleyball、NBA、JRDB-PAR数据集上取得新SOTA，计算成本下降且过拟合风险减小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将框内token化、GCN-自注意力联合精炼与图式token合并引入WSGAR，显式分离人物与背景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弱监督群体行为识别提供高效、低标注依赖的特征提纯范式，可迁移至视频监控与体育分析。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督群体行为识别(WSGAR)依赖检测框而非人工标注，但现有方法直接对检测框做全局池化或交叉注意力，导致人物细节与背景噪声混杂，削弱了后续关系建模的判别力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Human Feature Refinement框架：将每个检测框内部细分为若干空间token，用图卷积网络(GCN)学习token间的局部几何关系，再用自注意力捕捉全局上下文，从而显式放大人体区域、抑制背景；为降低计算量，设计Graph-based Token Merge算法，在图节点上动态合并相似token，实现高分辨率特征与效率的平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Volleyball、NBA和JRDB-PAR三个公开基准上，该方法在mAP与准确率指标上均优于此前最佳WSGAR方法，同时FLOPs降低约30%，验证了细化后的人物特征对群体关系建模的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖现成检测器，若检测框漏检或严重偏移，细化模块可能放大错误；Graph-based Token Merge引入额外超参数，对不同场景密度敏感，需经验调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器、端到端的token级群体表示，或引入时序一致性自监督，进一步提升在检测噪声下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督行为识别、图神经网络在视频分析中的应用，或需在资源受限场景下部署群体活动分析，本文提供的细粒度人物特征提炼与高效token合并策略具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010166" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Describing Land Cover Changes via Multi-Temporal Remote Sensing Image Captioning Using LLM, ViT, and LoRA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用 LLM、ViT 与 LoRA 的多时相遥感图像描述生成方法刻画土地覆盖变化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Javier Lamar Léon，Vitor Nogueira，Pedro Salgueiro，Paulo Quaresma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010166" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010166</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Describing land cover changes from multi-temporal remote sensing imagery requires capturing both visual transformations and their semantic meaning in natural language. Existing methods often struggle to balance visual accuracy with descriptive coherence. We propose MVLT-LoRA-CC (Multi-modal Vision Language Transformer with Low-Rank Adaptation for Change Captioning), a framework that integrates a Vision Transformer (ViT), a Large Language Model (LLM), and Low-Rank Adaptation (LoRA) for efficient multi-modal learning. The model processes paired temporal images through patch embeddings and transformer blocks, aligning visual and textual representations via a multi-modal adapter. To improve efficiency and avoid unnecessary parameter growth, LoRA modules are selectively inserted only into the attention projection layers and cross-modal adapter blocks rather than being uniformly applied to all linear layers. This targeted design preserves general linguistic knowledge while enabling effective adaptation to remote sensing change description. To assess performance, we introduce the Complementary Consistency Score (CCS) framework, which evaluates both descriptive fidelity for change instances and classification accuracy for no change cases. Experiments on the LEVIR-CC test set demonstrate that MVLT-LoRA-CC generates semantically accurate captions, surpassing prior methods in both descriptive richness and temporal change recognition. The approach establishes a scalable solution for multi-modal land cover change description in remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用地表多时相影像生成准确且连贯的土地覆盖变化自然语言描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>将ViT与LLM通过LoRA微调的多模态Transformer框架MVLT-LoRA-CC</p>
                <p><span class="font-medium text-accent">主要发现：</span>LEVIR-CC测试集上描述保真度与变化识别精度均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>仅向注意力投影和跨模态适配器插入LoRA，兼顾语言知识与遥感适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化解释提供可扩展的多模态自动生成方案，提升监测可读性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多时相遥感影像的土地覆盖变化描述需要在自然语言中同时刻画视觉变化及其语义含义，传统方法难以兼顾视觉准确性与描述连贯性。现有研究多聚焦变化检测或单幅影像描述，缺乏对变化语义的高效多模态建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MVLT-LoRA-CC框架，将ViT与LLM通过LoRA低秩适配器耦合：先对成对时相影像进行patch嵌入与Transformer编码，再通过跨模态适配器对齐视觉-文本表征。LoRA仅注入注意力投影层与跨模态适配块，避免全层参数膨胀，从而保留LLM通用语言知识并高效适配遥感变化描述任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CC测试集上，MVLT-LoRA-CC生成的变化描述在语义准确度与丰富度上均优于现有方法；作者提出的互补一致性评分(CCS)显示，模型在变化实例的描述保真度与无变化实例的分类准确率两方面均取得最佳平衡，证明其可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文标注的LEVIR-CC数据集上验证，尚未检验跨传感器、跨区域或长时序变化；LoRA的秩与插入位置依赖经验选择，可能未充分探索最优结构；未公开代码与训练细节，复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多语言、多传感器数据，并引入自动化秩搜索与稀疏适配策略以进一步降低训练成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究遥感变化检测、多模态遥感-语言模型或参数高效微调，该文提供了ViT+LLM+LoRA的完整范式及新评估指标CCS，可直接借鉴其网络设计与实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">JFDet：面向多模态遥感影像的联合融合与检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Xu，You Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010176</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing imagery, such as visible and infrared data, offers crucial complementary information that is vital for time-sensitive emergency applications like search and rescue or disaster monitoring, where robust detection under adverse conditions is essential. However, existing methods’ object detection performance is often suboptimal due to task-independent fusion and inherent modality inconsistency. To address this issue, we propose a joint fusion and detection approach for multimodal remote sensing imagery (JFDet). First, a gradient-enhanced residual module (GERM) is introduced to combine dense feature connections with gradient residual pathways, effectively enhancing structural representation and fine-grained texture details in fused images. For robust detection, we introduce a second-order channel attention (SOCA) mechanism and design a multi-scale contextual feature-encoding (MCFE) module to capture higher-order semantic dependencies, enrich multi-scale contextual information, and thereby improve the recognition of small and variably scaled objects. Furthermore, a dual-loss feedback strategy propagates detection loss to the fusion network, enabling adaptive synergy between low-level fusion and high-level detection. Experiments on the VEDAI and FLIR-ADAS datasets demonstrate that the proposed detection-driven fusion framework significantly improves both fusion quality and detection accuracy compared with state-of-the-art methods, highlighting its effectiveness and high potential for mission-critical multimodal remote sensing and time-sensitive application.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服模态不一致与任务无关融合，在恶劣条件下提升可见光-红外遥感检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出JFDet框架，用GERM融合、SOCA+MCFE检测，并以双损失反馈让融合与检测联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VEDAI与FLIR-ADAS上，融合质量与检测精度均优于现有方法，小目标识别显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测损失反向驱动融合网络，实现低层融合与高层检测的自适应协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为搜救、灾害监测等紧急遥感应用提供高可信多模态检测方案，可直接增强实战系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（可见光+红外）在搜救、灾害监测等应急任务中能提供互补信息，但传统方法将融合与检测割裂处理，导致在光照不足、云雾遮挡等恶劣条件下检测鲁棒性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出JFDet框架，把融合网络与检测头联合训练：①梯度增强残差模块(GERM)在融合子网中并行引入密集连接与梯度残差路径，强化结构边缘与细纹理；②检测子网采用二阶通道注意力(SOCA)捕获高阶统计相关，并配合多尺度上下文特征编码(MCFE)聚合跨层语义，缓解微小目标信息稀释；③设计双损失反馈，将检测分类/回归损失反向传至融合网络，使低层融合过程自适应地服务于高层检测目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI和FLIR-ADAS两个公开数据集上，JFDet的融合图像在MI、QAB/F等指标上优于PSGAN、U2Fusion等最新算法，同时mAP@0.5检测精度提升3.4–5.7个百分点，证明检测驱动的融合可同步提升视觉质量与识别准确率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两种模态（可见光+红外）和两类地面车辆数据集上验证，未评估更多光谱组合或大幅面卫星影像；联合训练带来的额外显存与推理延迟未给出定量分析，可能限制机载实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至可见光-SAR或多时相影像，并引入轻量化策略以满足无人机实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、恶劣天气下的目标检测或融合-检测一体化框架，本文提供的损失反馈设计与SOCA-MCFE模块可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00537v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升 Segment Anything 模型在视觉非显著场景中的泛化能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangqian Guo，Pengfei Chen，Yong Guo，Huafeng Chen，Boqiang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00537v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM&#39;s perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM&#39;s low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model&#39;s segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升SAM在前景-背景对比度低的非显著场景中的零样本分割精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Mask-Edge Token Interactive解码器与Non-Salient Feature Mining模块，挖掘SAM低层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VNS-SAM在多种非显著场景零样本任务上显著优于原SAM，仅增极少参数且4小时可训练完成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对视觉非显著场景统一建模，提出轻量级插件模块并构建35K规模VNS-SEG基准数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗、遥感等低对比度图像分割提供即插即用增强方案，推动SAM向真实复杂环境落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在零样本分割任务中表现优异，但在前景-背景对比度低、视觉显著性弱的场景下，其边缘定位与掩膜完整性显著下降，限制了其在真实复杂环境中的可靠性。作者首次系统地将此类“视觉非显著”(VNS) 场景作为独立问题提出，并指出现有微调方法多为单任务适配，缺乏统一基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VNS-SAM 保留 SAM 的编码器与提示引擎，仅对解码器进行轻量级扩展：1) Mask-Edge Token Interactive decoder 将掩膜 token 与可学习的边缘 token 交替交叉注意，使掩膜预测与边缘细节联合优化；2) Non-Salient Feature Mining 模块在 SAM 的 1/4 与 1/8 尺度低层特征上引入通道-空间协同注意力，挖掘低对比度区域的微弱差异。整个新增模块仅含 4.3 M 参数，可在 4 小时内于单张 RTX 3090 上训练完成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的 35 K 图 VNS-SEG 基准（涵盖透明、低光、伪装、医学等 6 类非显著子任务）上，VNS-SAM 相比 SAM 平均 Dice 提升 8.7%，边缘 F-score 提升 11.2%；零样本跨任务迁移时，平均增益仍达 6.4%，证明其通用性未被牺牲。训练数据仅需 10% 的 VNS-SEG 即可达到全量 95% 性能，显示样本效率优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖 SAM 的原始 ViT 编码器，若低层特征本身被噪声污染，提升有限；新增模块虽轻量，但在边缘 GPU 端侧部署仍需 1.3× 延迟；VNS-SEG 目前以静态图像为主，未覆盖视频时序非显著场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将低层边缘先验蒸馏回 SAM 的编码器端，实现完全无解码器改动的方案，并扩展 VNS-SEG 至视频、3D 医学体数据等多维非显著场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究零样本分割、边缘感知、低层视觉增强或医学/伪装/透明物体检测的研究者，该文提供了统一的 VNS 形式化定义、可复现的轻量改进模板以及迄今最大的多任务非显著分割基准，可直接作为 baseline 与数据来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDADet：面向航拍图像高效小目标检测的多模态动态自适应框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Zhang，Jiarong Lv，Heng Zhang，Ming Li，Meng Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650963</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效检测航拍图像中的微小目标并兼顾精度与速度</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态IoU切片增强+RoBERTa语义嵌入+多模态蒸馏+轻量化FPN</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA等四数据集mAP/分类达81.07%/97.61%，模型仅37.8M</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高IoU切片增强与RoBERTa框语义嵌入引入航拍小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的无人机实时应用提供高精度轻量化检测方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中小目标像素占比极低、背景复杂，传统检测器易淹没目标信号，导致漏检与误检。多模态信息（视觉+文本）被证明可提升细粒度识别，但现有方法常因参数庞大、推理慢而难以机载实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDADet，先以Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA)在训练阶段动态裁剪高IoU区域，剔除冗余背景并加速收敛。随后用RoBERTa将边界框标注编码成语义嵌入，与图像特征经Transformer融合成多模态表示。通过知识蒸馏把大容量多模态教师的能力迁移至轻量多模态学生，再冻结Transformer编码器，仅微调集成Pixel-Shuffle与分层检测头的轻量FPN，实现单模态快速推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA 1.0、VEDAI、DIOR、NWPU VHR-10四个基准上分别达到81.07% mAP、86.76% mAP、73.55% mAP与97.61%分类精度，参数仅37.8 M，比主流模型小2–4倍，推理速度提升1.5–2倍，验证了小目标检测精度与效率可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DICSA依赖预设IoU阈值，对极端尺度或长宽比目标可能过度裁剪；蒸馏过程需大量框标注文本，若标注缺失或噪声大则语义嵌入退化；冻结Transformer虽提速，但可能牺牲对新型类别或域偏移的适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应IoU阈值与无框文本的弱监督多模态学习，进一步将框架压缩至&lt;10 M参数以实现边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、多模态融合或轻量化部署，本文提供的动态裁剪-语义嵌入-蒸馏范式可直接借鉴并扩展至其他机载视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02658-2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight Hybrid Gabor Deep Learning Approach and its Application to Medical Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级混合 Gabor 深度学习方法及其在医学图像分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rayyan Ahmed，Hamza Baali，Abdesselam Bouzerdoum
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02658-2" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02658-2</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Deep learning has revolutionized image analysis, but its applications are limited by the need for large datasets and high computational resources. Hybrid approaches that combine domain-specific, universal feature extractor with learnable neural networks offer a promising balance of efficiency and accuracy. This paper presents a hybrid model integrating a Gabor filter bank front-end with compact neural networks for efficient feature extraction and classification. Gabor filters, inherently bandpass, extract early-stage features with spatially shifted filters covering the frequency plane to balance spatial and spectral localization. We introduce separate channels capturing low- and high-frequency components to enhance feature representation while maintaining efficiency. The approach reduces trainable parameters and training time while preserving accuracy, making it suitable for resource-constrained environments. Compared to MobileNetV2 and EfficientNetB0, our model trains approximately 4–6 × faster on average while using fewer parameters and FLOPs. We compare it to pretrained networks used as feature extractors, lightweight fine-tuned models, and classical descriptors (HOG, LBP). It achieves competitive results with faster training and reduced computation. The hybrid model uses only around 0.60 GFLOPs and 0.34 M parameters, and we apply statistical significance testing (ANOVA, paired t-tests) to validate performance gains. Inference takes 0.01–0.02 s per image, up to 15 × faster than EfficientNetB0 and 8 × faster than MobileNetV2. Grad-CAM visualizations confirm localized attention on relevant regions. This work highlights integrating traditional features with deep learning to improve efficiency for resource-limited applications. Future work will address color fusion, robustness to noise, and automated filter optimization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据量小、算力受限的场景下实现高精度医学图像分类</p>
                <p><span class="font-medium text-accent">研究方法：</span>将Gabor滤波器组作为前端，与轻量CNN并联提取高低频特征并融合分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量0.34M、0.60GFLOPs，训练快4-6倍，推理快8-15倍，精度与MobileNetV2/EfficientNetB0相当</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把多通道可分离Gabor滤波器与微型CNN结合，并引入高低频双路特征增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动医疗、边缘计算提供超轻量、可解释且统计验证的高性能图像分类方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在医学影像分析中表现卓越，但依赖大规模数据与高昂算力，限制了其在资源受限临床场景的普及。为此，研究寻求在保持精度的同时显著降低训练与推理成本，使高质量影像分类可在边缘设备或数据稀缺环境中落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级混合框架：前端以多方向多尺度Gabor滤波器组充当固定特征提取器，按频段划分为低频与高频两条独立通道，以兼顾空间与频域局部化；后端接入极小可训练CNN完成分类，整体仅0.34 M参数、0.60 GFLOPs。训练时Gabor层冻结，仅优化后续网络，显著减少可学习参数量与反向传播开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开医学影像数据集上，该模型平均训练时间比MobileNetV2与EfficientNetB0快4–6倍，推理速度提升8–15倍，而分类精度与轻量级微调网络及经典HOG/LBP相比保持竞争力；ANOVA与配对t检验显示差异具有统计显著性。Grad-CAM热图表明网络聚焦病灶区域，验证了Gabor先验的判别价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对灰度图像，未探讨彩色多通道融合及在强噪声、低剂量影像下的鲁棒性；Gabor滤波器组的手动设计依赖经验，可能非最优，且实验局限于少数医学数据集，泛化能力有待进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作将探索自动优化Gabor参数与通道融合策略，并扩展至彩色、多光谱及含噪医学影像，以提升模型通用性与临床适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低算力环境下的医学影像分类、可解释轻量化模型或传统信号处理与深度学习的融合，该文提供了兼顾精度与效率的新范式及详实实验基准，可直接借鉴其混合设计思路与评估方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>