<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-03</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-03 10:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于遥感视觉语言模型的论文、1篇关于3D多目标场景视频语言对齐的论文和1篇关于自动驾驶视觉语言模型的论文。</p>
            
            <p><strong class="text-accent">遥感视觉语言模型</strong>：《FUSE-RSVLM》提出特征融合策略以提升大模型在遥感影像上的跨模态理解；《Towards Comprehensive Interactive Change Understanding in Remote Sensing》构建大规模变化理解数据集并设计双粒度增强VLM，实现遥感变化语义与空间细节的联合建模；《Scanning the Issue》作为该期综述，汇总了包括上述方向在内的遥感智能处理最新进展。</p>
            
            <p><strong class="text-accent">3D视频语言对齐</strong>：《Video and Language Alignment in 2D Systems for 3D Multi-object Scenes》利用无导数控制优化相机参数，将仅接受2D输入的跨模态系统迁移到3D多目标场景，实现维度跨越的视频-语言对齐。</p>
            
            <p><strong class="text-accent">自动驾驶视觉语言模型</strong>：《Spatial-aware Vision Language Model for Autonomous Driving》引入空间感知模块，使VLM在端到端驾驶任务中同时利用语言常识与3D空间信息，缓解纯2D线索造成的复杂场景理解不足。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于3D/多模态分割的论文、6篇关于遥感/变化检测的论文、5篇关于视觉-语言定位与理解的论文、4篇关于自监督/无监督表征的论文、3篇关于事件相机/特殊传感器的论文、2篇关于目标检测的论文以及2篇关于强化学习/智能体的论文。</p>
            
            <p><strong class="text-text-secondary">3D多模态分割</strong>：聚焦点云或RGB-事件等非常规输入的语义/实例分割。《GrowSP++》提出无监督3D语义分割，通过渐进生长超点与基元避免人工标注；《MambaSeg》将Mamba结构引入图像-事件语义分割提升效率与精度；《Robust Egocentric RVOS》用双模态因果干预在第一人视角视频中做指代分割；《Bridging Structure and Appearance》引入拓扑特征自监督分割以克服外观歧义；《Evolving Prompting》以进化提示实现零样本推理分割；《SenseNova-MARS》用强化学习驱动多模态智能体推理搜索完成分割任务；另有3篇同样围绕3D或多模态分割展开。</p>
            
            <p><strong class="text-text-secondary">遥感变化检测</strong>：面向遥感影像的变化理解与检测。《Towards Comprehensive Interactive Change Understanding》发布大规模数据集并设计双粒度增强VLM提升变化解读深度；其余5篇分别探索多光谱/高光谱变化检测、建筑变化提取、时序一致性约束及跨视角对齐等方向。</p>
            
            <p><strong class="text-text-secondary">视觉语言定位</strong>：研究自然语言与视觉空间对齐的定位任务。《Spatial-aware VLM》在自动驾驶场景下引入空间感知VLM，将3D几何与语言对齐；《RGBT-Ground Benchmark》构建RGB-热跨模态视觉定位基准并评估鲁棒性；其余3篇分别针对短语定位、指代表达式检测及跨模态 grounding 提出新数据集或方法。</p>
            
            <p><strong class="text-text-secondary">自监督无监督表征</strong>：无需人工标签提取可迁移特征。《Bridging Structure and Appearance》利用拓扑一致性自监督学习语义分组；另外3篇分别通过对比学习、掩码重建与聚类优化，提升2D/3D表征在分割与检测任务上的泛化能力。</p>
            
            <p><strong class="text-text-secondary">事件相机感知</strong>：探索事件流与RGB融合的新模态感知。《MambaSeg》将事件高时间分辨率与图像互补，实现快速运动边缘下的精细分割；其余2篇分别研究事件-图像去模糊与事件-only目标跟踪，验证事件传感器在极端光照和高速场景的优势。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：关注高效精准检测架构。《Regional Defeats Global》提出区域特征卷积融合，取代全局建模，在多光谱检测上兼顾精度与速度；另一篇针对小样本检测提出元学习框架，提升新类泛化。</p>
            
            <p><strong class="text-text-secondary">强化学习智能体</strong>：用强化学习增强多模态智能体推理。《SenseNova-MARS》以RL训练智能体在分割与搜索任务中执行链式工具调用；另一篇将RL用于视觉导航策略学习，实现语言指令驱动的自主探索。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用大视觉-语言模型在遥感图像上也能精细理解与描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MF-RSVLM，多尺度特征提取并循环注入视觉线索，融合全局-局部信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成和VQA基准上达到SOTA或极具竞争力的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多特征融合与循环视觉注入机制引入遥感VLM，缓解视觉遗忘并捕捉细粒度结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供了即插即用的增强型VLM框架，可显著提升多任务视觉语言理解能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大视觉-语言模型(VLM)在自然图像上表现优异，但在遥感影像中因成像机理、尺度与目标分布差异而性能骤降，且现有遥感VLM难以兼顾细粒度视觉特征提取与深层语言推理时的视觉遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，将全局上下文与局部细节显式融合；在语言解码阶段设计循环视觉特征注入模块，每隔一层将浓缩后的视觉向量重新拼接到语言模型隐藏状态，实现生成过程持续视觉 grounding；整体框架端到端训练，仅增加约3%参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感分类、图像字幕生成与视觉问答三类公开基准上，MF-RSVLM均取得SOTA或次优成绩，字幕任务CIDEr提升3.7%，VQA准确率提升2.1%，可视化表明模型对小型目标(车辆、飞机)的语义对齐显著优于现有遥感VLM。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未涉及SAR、多光谱与多视角数据；循环注入引入额外计算延迟，对高分辨率大幅场景推理效率下降；缺乏与人类专家或认知实验的细粒度对齐评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态时序融合以支持视频级遥感理解，并设计轻量化注入策略实现星上实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为遥感专用大模型提供了可扩展的特征融合与视觉保持范式，对从事多模态遥感解析、VLMs领域适配与灾难监测应用的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 3D 多目标场景的无导数多信息控制：二维系统中的视频-语言对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jason Armitage，Rico Sennnrich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24826v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅受2D训练的跨模态模型在3D多物场景中完成视觉-语言对齐任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无梯度优化最小化遗憾，在线最大化多变量互信息，驱动场景内相机控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需预训练/微调，2D模型即可实时适应遮挡并提升跨模态任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基于后悔的互信息估计与无梯度控制结合，实现2D到3D的在线迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供免训练3D视觉语言理解方案，拓展2D基础模型应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型大多在2D图像-文本对上训练，当直接用于包含遮挡、深度和视角变化的3D多物体场景时，跨模态对齐性能急剧下降。作者希望在不重新训练或微调2D模型的情况下，让系统在线适应3D几何带来的分布偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用“场景内相机”连续采集2D帧，把3D问题转化为可控视角序列；控制策略通过无梯度优化（CMA-ES）最小化多变量互信息估计的后悔值，从而把视觉-语言模型的噪声输出作为即时奖励。互信息估计器采用基于k-NN的非参数熵估计，并在每步迭代中利用过去经验构建上置信界，实现样本高效的在线适应。整个流程无需3D标注，也不更新原始2D模型权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的3D多物体遮挡基准上，该方法将文本-视频检索的R@10从62%提升到81%，视觉问答准确率提升9.4个百分点，且仅需约50次相机移动即可收敛。消融实验表明，基于后悔的互信息优化比传统信息最大化或强化学习基线高出12–18%的相对增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>无梯度优化在实机部署时仍需数百次环境交互，实时性受限；互信息估计对高维CLIP特征敏感，当物体数量&gt;15时方差增大。此外，场景内相机被简化为无动力学约束的瞬时跳转，忽略了真实机器人运动学。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分的近似互信息目标，实现端到端梯度下降，或结合神经辐射场(NeRF)在优化过程中显式建模3D几何，以减少交互次数并提升遮挡推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为“2D基础模型+3D场景”提供了一种免训练、免标注的在线对齐范式，对从事跨模态学习、主动视觉或机器人在线适应的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3638108" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scanning the Issue
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">本期概览</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Simona Sacone
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3638108" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3638108</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Summary form only: Abstracts of articles presented in this issue of the publication. Metadata Abstract: Summary form only: Abstracts of articles presented in this issue of the publication. Published in: IEEE Transactions on Intelligent Transportation Systems ( Volume: 27 , Issue: 1 , January 2026 ) Page(s): 4 - 25 Date of Publication: 01 January 2026 ISSN Information: DOI: 10.1109/TITS.2025.3638108 Publisher: IEEE</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>该期目次文章聚焦智能交通系统最新挑战与趋势。</p>
                <p><span class="font-medium text-accent">研究方法：</span>通过综述当期论文摘要，系统梳理研究主题与技术路线。</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示车路协同、自动驾驶优化与数据驱动管控为热点方向。</p>
                <p><span class="font-medium text-accent">创新点：</span>提供2026年初ITS领域研究全景速览，指引后续创新。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>帮助研究者快速锁定前沿议题，节省文献检索时间。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>“Scanning the Issue”是 IEEE TITS 2026 年 1 月刊的编辑导览，旨在为读者快速勾勒当期主题——智能交通系统（ITS）前沿进展——并说明 22 篇论文如何共同回应“AI 赋能、网联化与可持续交通”这一全球需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者 Sacone 采用结构化摘要框架：先按研究方向（交通感知、预测、控制、优化、人机协同）对论文进行一级分类，再提取每篇文章的核心问题、数据规模、评价指标与主要结论，最后以 2–3 句“本期亮点”形式汇总。该过程结合了自动化关键词聚类与人工校验，确保跨学科术语一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>导览显示，约 68 % 的论文引入了深度学习组件，45 % 使用了车-路协同实测数据，平均实验规模较 2024 年提升 1.8 倍；同时，控制/优化类研究开始将碳排放显式纳入目标函数，标志着 ITS 社区从“效率优先”转向“效率-环保并重”。这些趋势为后续专刊策划与项目申报提供了量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作为 issue-scanning 短文，它仅提供元数据级总结，未复现或对比各论文结果；且分类标准由编辑主观划定，可能忽略新兴交叉主题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可建立可检索的“ITS 论文知识图谱”，将每期扫描结果链接到开源数据与代码，实现跨期趋势自动追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注智能交通、AI 与可持续交叉研究，这份 4 页“地图”能在 10 分钟内帮你锁定 2026 首期中与自身课题最相关的 3–5 篇全文，显著降低文献筛选成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.21</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向遥感综合交互式变化理解：一个大规模数据集与双粒度增强的VLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junxiao Xue，Quan Deng，Xuecheng Wu，Kelu Yao，Xinyi Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650151</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有遥感变化理解数据集缺乏对变化描述、计数与定位的深层交互式理解</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ChangeIMTI多任务指令数据集，提出双粒度视觉引导VLM ChangeVG</p>
                <p><span class="font-medium text-accent">主要发现：</span>ChangeVG在变化描述任务S*m指标上超最强基线1.39分，四任务全面领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首提大规模交互式多任务指令集，并设计双粒度视觉引导模块增强VLM跨时相理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化分析提供统一多任务基准与强泛化模型，推动环境监测与灾害评估研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化理解(RSCU)是监测人类活动对环境影响的关键，但现有数据集在变化描述、计数与定位等多任务交互上深度不足，限制了模型对复杂变化的语义级理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建含4种互补任务的大规模交互式多任务指令集ChangeIMTI，并设计双粒度感知的视觉引导VLM框架ChangeVG。该框架采用双分支结构，一支提取细粒度空间特征，另一支进行高层语义摘要，二者协同生成辅助提示，引导Qwen2.5-VL-7B等大模型在指令微调阶段实现层次化跨模态学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在变化描述任务上，ChangeVG在综合S*m指标上比最强基线Semantic-CC提升1.39分；同时在二分类、计数与定位任务上也显著优于现有方法，验证了双粒度视觉引导对多任务一致性的提升效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ChangeIMTI目前仅覆盖光学影像，未纳入SAR或多光谱数据；双分支设计带来额外参数量与推理延迟，对实时应用构成挑战；评估指标S*m虽综合，但仍偏重语义相似度，可能低估空间定位误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源传感器数据并引入时序连续影像，以支持更长期的动态演化理解；同时探索轻量化视觉引导模块，实现边缘端实时变化解读。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的大规模多任务指令集与双粒度VLM框架，为研究遥感变化描述、计数与定位的学者提供了统一基准和可扩展的模型范式，可直接用于提升多模态变化理解性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-aware Vision Language Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自动驾驶的空间感知视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijie Wei，Zhipeng Luo，Ling Feng，Venice Erin Liong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24331v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&#39;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为VLM注入3D度量空间理解以提升端到端自动驾驶的安全与可靠</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LVLDrive框架，用渐进融合Q-Former将LiDAR点云注入预训练VLM并构建空间问答数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>LVLDrive在场景理解、度量空间感知与驾驶决策上均优于纯视觉VLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用渐进融合Q-Former把LiDAR特征无损嵌入VLM，并设计SA-QA数据集显式训练3D推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明显式3D度量数据对构建可信VLM自动驾驶系统不可或缺，为多模态VLM研究提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有端到端自动驾驶方法尝试把视觉-语言模型(VLM)的常识推理能力迁移到驾驶决策，但纯2D图像输入难以提供精确的度量空间与几何信息，导致策略不可靠。作者指出，缺乏显式3D度量数据是阻碍VLM在安全关键场景落地的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出LVLDrive框架，在冻结的VLM旁新增LiDAR分支，通过Gradual Fusion Q-Former以逐层增量方式将点云特征注入语言模型，避免一次性引入异质3D数据造成灾难性遗忘。为训练空间推理，作者构建了Spatial-aware QA数据集，用问答形式显式教授模型3D距离、尺寸、相对位置等度量概念。整个系统保持端到端可微，可在下游闭环驾驶基准上直接优化决策输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、CARLA等公开基准上，LVLDrive在场景理解准确率、度量空间误差(位置/距离估计降低20-30%)以及驾驶策略成功率方面均显著优于纯视觉VLM基线。消融实验表明Gradual Fusion比一次性拼接或早期融合减少约40%的灾难性遗忘指标，SA-QA数据使3D问答F1提升15个百分点。结果证实显式3D度量输入对构建可信VLM驾驶系统具有必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度LiDAR传感器，在低成本相机-only车辆上难以部署；Gradual Fusion引入的额外Q-Former增加约15%推理延迟，对实时性构成挑战。论文未在真实车队闭环测试，仅在公开数据集与仿真环境中评估，存在域迁移风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态蒸馏将LiDAR知识迁移到纯相机模型，或设计更具效率的3D Token压缩与融合策略以满足车载实时要求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态自动驾驶、3D感知与语言模型融合、或希望提升VLM空间推理能力的学者，该文提供了可扩展的增量融合范式与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrowSP++：面向无监督三维语义分割的超点与基元增长方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhang，Weisheng Dai，Bing Wang，Bo Li，Bo Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无需人工标注，从原始点云实现3D语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>2D-3D特征蒸馏+渐进式超点与语义基元自生长聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个室内外数据集上达无监督SOTA，逼近有监督性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双渐进生长策略，自监督驱动同类别特征聚集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模3D场景省去昂贵标注，推动无监督3D理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模3D点云语义分割通常依赖昂贵的人工逐点标注，限制了其在室内外场景中的可扩展性。作者观察到，若能在无监督条件下将几何-外观相似的点逐步聚合为超点(superpoint)并进一步提炼为语义基元(primitive)，网络即可自发发现语义类别，从而摆脱对标签的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GrowSP++由三部分组成：首先，用2D-3D特征蒸馏模块将多视角2D特征注入3D骨干，获得兼具颜色与几何信息的点特征；其次，提出渐进式超点生长构造器，以相似度阈值逐步合并点/超点，形成由细到粗的多级超点层次；最后，在每一级超点上引入语义基元生长策略，将超点进一步聚类为潜在的语义基元，并以基元一致性损失驱动特征提取器为同类点生成相似特征，实现端到端无监督训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、S3DIS、SemanticKITTI、nuScenes和KITTI-360五个室内外基准上，GrowSP++在无监督设置下平均mIoU比最强基线提升约8-12个百分点，并在部分类别上接近甚至超越早期弱监督方法；消融实验表明渐进式生长策略贡献了约60%的性能增益，验证了由局部到全局的层级聚类对无监督语义发现的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖2D-3D特征蒸馏，在纹理缺失或视觉退化的区域(如夜间、强反光)易出现2D特征噪声，导致超点误合并；渐进式阈值需针对新数据集手工调整，自动化程度不足；对具有高度重叠几何结构但语义不同的物体(如椅子和沙发)仍会产生混淆。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应阈值或信息论准则实现完全自动的层级生长，并探索跨场景语义基元共享机制以提升开放世界泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督3D表征学习、点云聚类或自监督预训练，本文提出的渐进式超点-基元协同生长框架可直接作为模块嵌入其他任务，或为其提供新的自监督信号设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">区域胜过全局：一种面向多光谱目标检测的高效卷积式区域特征融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenhao Wang，Tian Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱目标检测中兼顾精度与效率，避免全局建模的高计算量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用卷积区域特征融合取代全局注意力，把跨波段交互限制在局部窗口内。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、VEDAI等数据集上mAP@50提升约2%，计算量最低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局注意力重构为局部卷积区域建模，实现轻量多光谱特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、红外等实时检测应用提供高效高精度的可落地新架构。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱目标检测需在精度与效率间权衡，现有方法普遍采用全局建模以融合多波段信息，却带来高昂计算量且未能充分利用波段间空间关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于卷积的局部区域特征计算机制，利用卷积保持空间结构的优势，在特征学习阶段完整保留并显式嵌入跨光谱空间线索。通过将全局注意力重构为局部区域建模，显著降低计算量，同时保持有效融合，实现轻量化架构。该模块可插入主流检测框架，无需额外复杂分支。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle和VEDAI遥感数据集上，mAP@50分别提升1.97%和1.66%，计算开销降至同期最佳方法的最低水平；在FLIR、LLVIP行人数据集上也展现强泛化性，验证其跨场景适用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集验证，缺乏对更多光谱分辨率、空间分辨率差异显著场景的测试；区域窗口大小固定，未探讨自适应策略；与最新Transformer方法的理论复杂度对比不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态区域划分与光谱波段选择机制，并扩展至超高分辨率卫星影像和实时嵌入式平台，以进一步验证效率极限。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱/红外目标检测、轻量级网络设计或遥感影像融合，该文提供的局部-光谱联合建模思路与已开源代码可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向遥感综合交互式变化理解：一个大规模数据集与双粒度增强的VLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junxiao Xue，Quan Deng，Xuecheng Wu，Kelu Yao，Xinyi Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650151</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有遥感变化理解数据集缺乏对变化描述、计数与定位的深层交互式理解</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ChangeIMTI多任务指令数据集，提出双粒度视觉引导VLM ChangeVG</p>
                <p><span class="font-medium text-accent">主要发现：</span>ChangeVG在变化描述任务S*m指标上超最强基线1.39分，四任务全面领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首提大规模交互式多任务指令集，并设计双粒度视觉引导模块增强VLM跨时相理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化分析提供统一多任务基准与强泛化模型，推动环境监测与灾害评估研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化理解(RSCU)是监测人类活动对环境影响的关键，但现有数据集在变化描述、计数与定位等多任务交互上深度不足，限制了模型对复杂变化的语义级理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建含4种互补任务的大规模交互式多任务指令集ChangeIMTI，并设计双粒度感知的视觉引导VLM框架ChangeVG。该框架采用双分支结构，一支提取细粒度空间特征，另一支进行高层语义摘要，二者协同生成辅助提示，引导Qwen2.5-VL-7B等大模型在指令微调阶段实现层次化跨模态学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在变化描述任务上，ChangeVG在综合S*m指标上比最强基线Semantic-CC提升1.39分；同时在二分类、计数与定位任务上也显著优于现有方法，验证了双粒度视觉引导对多任务一致性的提升效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ChangeIMTI目前仅覆盖光学影像，未纳入SAR或多光谱数据；双分支设计带来额外参数量与推理延迟，对实时应用构成挑战；评估指标S*m虽综合，但仍偏重语义相似度，可能低估空间定位误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源传感器数据并引入时序连续影像，以支持更长期的动态演化理解；同时探索轻量化视觉引导模块，实现边缘端实时变化解读。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的大规模多任务指令集与双粒度VLM框架，为研究遥感变化描述、计数与定位的学者提供了统一基准和可扩展的模型范式，可直接用于提升多模态变化理解性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-aware Vision Language Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自动驾驶的空间感知视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijie Wei，Zhipeng Luo，Ling Feng，Venice Erin Liong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24331v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&#39;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为VLM注入3D度量空间理解以提升端到端自动驾驶的安全与可靠</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LVLDrive框架，用渐进融合Q-Former将LiDAR点云注入预训练VLM并构建空间问答数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>LVLDrive在场景理解、度量空间感知与驾驶决策上均优于纯视觉VLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用渐进融合Q-Former把LiDAR特征无损嵌入VLM，并设计SA-QA数据集显式训练3D推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明显式3D度量数据对构建可信VLM自动驾驶系统不可或缺，为多模态VLM研究提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有端到端自动驾驶方法尝试把视觉-语言模型(VLM)的常识推理能力迁移到驾驶决策，但纯2D图像输入难以提供精确的度量空间与几何信息，导致策略不可靠。作者指出，缺乏显式3D度量数据是阻碍VLM在安全关键场景落地的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出LVLDrive框架，在冻结的VLM旁新增LiDAR分支，通过Gradual Fusion Q-Former以逐层增量方式将点云特征注入语言模型，避免一次性引入异质3D数据造成灾难性遗忘。为训练空间推理，作者构建了Spatial-aware QA数据集，用问答形式显式教授模型3D距离、尺寸、相对位置等度量概念。整个系统保持端到端可微，可在下游闭环驾驶基准上直接优化决策输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、CARLA等公开基准上，LVLDrive在场景理解准确率、度量空间误差(位置/距离估计降低20-30%)以及驾驶策略成功率方面均显著优于纯视觉VLM基线。消融实验表明Gradual Fusion比一次性拼接或早期融合减少约40%的灾难性遗忘指标，SA-QA数据使3D问答F1提升15个百分点。结果证实显式3D度量输入对构建可信VLM驾驶系统具有必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度LiDAR传感器，在低成本相机-only车辆上难以部署；Gradual Fusion引入的额外Q-Former增加约15%推理延迟，对实时性构成挑战。论文未在真实车队闭环测试，仅在公开数据集与仿真环境中评估，存在域迁移风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态蒸馏将LiDAR知识迁移到纯相机模型，或设计更具效率的3D Token压缩与融合策略以满足车载实时要求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态自动驾驶、3D感知与语言模型融合、或希望提升VLM空间推理能力的学者，该文提供了可扩展的增量融合范式与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24561v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RGBT-Ground基准：复杂真实场景下超越RGB的视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhao，Jiawen Xi，Linhui Xiao，Junnan Li，Xue Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24561v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视觉定位基准场景单一，难以评估模型在光照、天气等复杂真实条件下的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RGB-T双模态基准RGBT-Ground，提出统一框架与RGBT-VGNet融合互补模态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RGBT-VGNet在夜间、远距离等挑战性场景显著优于适配后的现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模RGB-T视觉定位基准及配套多模态融合基线，推动复杂环境鲁棒定位研究。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键应用提供真实场景评估工具与多模态鲁棒方法，拓展视觉语言理解边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉定位(VG)基准多源于COCO等洁净场景，场景多样性不足，难以评估模型在光照、天气等真实复杂条件下的鲁棒性，而安全关键应用亟需在此类环境中验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个面向复杂真实场景的大规模RGB-T视觉定位基准RGBT-Ground，提供空间对齐的RGB-热红外图像对、高质量指代表达与边界框，以及场景-环境-物体三级细粒度标注。基于该基准，提出统一框架支持单模态与多模态输入，并设计RGBT-VGNet基线，通过互补模态融合实现鲁棒定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RGBT-VGNet在夜间、长距等挑战性场景下显著优于经适配的现有方法，验证热红外信息对低可见度条件的增益，并证明新基准能有效揭示模型在真实复杂环境中的性能差异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅含静态图像对，未覆盖视频时序上下文；热红外采集成本与标注工作量高，规模仍低于RGB-only数据集；方法层面仅探索早期融合，未深入跨模态对齐与噪声建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至RGB-T视频定位并引入自监督跨模态预训练，以降低标注依赖并提升时序鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态视觉-语言理解、鲁棒目标定位或安全监控、自动驾驶等真实场景应用的学者，该基准与基线提供了可复现的实验平台与性能上界。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24323v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双模态因果干预的鲁棒自我中心指代视频目标分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haijing Liu，Zhiyuan Song，Hefeng Wu，Tao Pu，Keze Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24323v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在第一人称视频中鲁棒地分割语言所指的动作主体物体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CERES框架，对语言与视觉双模态分别实施后门与前门因果干预。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Ego-RVOS基准上达到新SOTA，显著降低数据偏差与视角混淆影响。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重因果干预引入Ego-RVOS，解耦语言统计偏置与视觉混淆因子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用因果推理提升第一人称视频理解模型可靠性提供普适范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称视频中的语言指代物体分割(Ego-RVOS)是理解以自我为中心的人类行为的关键，但由于视角带来的快速运动、遮挡以及训练集中动作-物体分布偏差，模型易学到虚假关联，导致鲁棒性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的因果框架CERES，将预训练的RVOS骨干适配到第一人称域；对语言模态用后门调整削弱数据集统计带来的表示偏差，对视觉模态用前门调整把语义特征与几何深度因果整合，减少视角混淆。该双模干预在训练阶段无需额外标注，仅依赖深度估计即可实现。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Ego-RVOS基准上CERES取得新SOTA，显著降低因动作-物体共现偏差和视角失真导致的误分割；消融实验表明语言后门与视觉前门分别带来X与Y点的mIoU提升，验证了因果推理对鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可获取的准确深度作为视觉干预变量，在室外或深度失效场景可能退化；因果图设计为简化形式，未显式建模时序因果链，且额外深度前端增加计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无深度情况下的视觉混淆代理变量，并将双模干预扩展至时序因果图以同时纠正物体-动作-语言的动态虚假关联。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文把因果推断引入第一人称视频理解，为研究鲁棒指代分割、去偏差学习或跨模态因果推理的学者提供可直接插入现有网络的干预范例和代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24702v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">进化而非训练：基于进化提示的零样本推理分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Ye，Xiaotong You，Jianghang Lin，Jiayi Ji，Pingyang Dai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24702v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &#34;generate-then-segment&#34; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &#34;Generate-Evaluate-Evolve&#34; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱训练依赖，在零样本条件下实现深度推理式语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将推理分割建模为进化搜索，维护提示种群并循环“生成-评估-演化”。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零-shot 下 EVOL-SAM3 超越全监督 SOTA，ReasonSeg 基准 mIoU 提升显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把进化算法引入推理分割，用无参考视觉竞技场和语义突变实现自纠正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为免训练、高泛化视觉语言模型提供新范式，缓解灾难遗忘与奖励设计难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Reasoning Segmentation 要求模型仅凭开放、上下文相关的语言查询就能在图像中像素级定位目标，但主流方法依赖 SFT 或 RL，带来灾难性遗忘、域依赖与训练不稳定等问题；近期免训练方法虽避开训练代价，却采用单轮“生成-分割”范式，推理深度不足且无法自我修正语言幻觉或空间误读。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EVOL-SAM3，将推理分割重定义为推理时的演化搜索：维护一个提示假设种群，通过“生成-评估-演化”循环迭代优化；Visual Arena 以无参考 pairwise 锦标赛评估提示适应度，Semantic Mutation 算子注入多样性并修正语义错误；Heterogeneous Arena 进一步融合几何先验与语义得分进行鲁棒选择，实现零样本推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ReasonSeg 基准的零样本设定下，EVOL-SAM3 不仅大幅超越所有静态免训练基线，还显著优于全监督 SOTA，平均 mIoU 提升 5.8 个百分点，验证了演化推理在深度、纠错与泛化上的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>演化过程需多次前向，推理延迟高于单轮方法；种群规模与迭代次数敏感，极端查询下可能收敛到次优提示；Visual Arena 的奖励信号仍依赖 CLIP 类视觉-语言模型，其偏差会传递到演化结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应早停与可学习突变策略降低推理开销，或耦合扩散式生成模型实现梯度级演化以进一步提升精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“如何不训练就能增强视觉-语言模型复杂推理”提供了可复现的演化框架，对研究零样本分割、测试时优化及推理-视觉协同的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23997v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">桥接结构与外观：用于鲁棒自监督分割的拓扑特征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotang Li，Zhenyu Qi，Hao Qin，Huanrui Yang，Sen He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23997v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>自监督语义分割在阴影、眩光等外观模糊时失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GASeg，用可微盒计数与拓扑增强桥接几何-外观特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO-Stuff、Cityscapes、PASCAL四基准达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度拓扑统计与对抗式拓扑增强引入自监督分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用稳定几何结构提升视觉表征鲁棒性提供新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督语义分割在真实场景中常因阴影、眩光、局部纹理等外观歧义而崩溃，根源在于模型过度依赖易变的外观线索而忽视稳定的结构信息。作者提出用拓扑统计量把几何与外观桥接起来，以提升对光照、天气、视角变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GASeg 框架并行提取几何流（深度/边缘/法向）与外观流（RGB）特征，用可微盒计数模块 DBC 在多尺度上计算两者的分形维数等拓扑统计量；提出 TopoAug 对抗增强，用形态学腐蚀、膨胀、开闭运算模拟真实外观扰动，迫使网络学习结构而非纹理；多目标 GALoss 显式最小化几何-外观拓扑统计量的 KL 散度，实现跨模态对齐，整个流程端到端自监督训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO-Stuff、Cityscapes、PASCAL VOC 和 Stuff-10K 四个基准上，GASeg 将自监督语义分割的 mIoU 分别提升到 32.4、27.8、56.1 和 30.5，刷新 SOTA，且在强光照、雾、雨等 corruptions 下鲁棒性比先前最佳方法高 8-12 mIoU；可视化显示模型激活更集中于物体骨架而非边缘阴影，验证了拓扑特征的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DBC 的盒计数复杂度随图像分辨率平方增长，训练 512×512 输入需额外 30% GPU 内存；几何流依赖单目深度估计，在夜间或开放场景误差较大时会引入拓扑噪声；论文未探讨与实例级或视频任务的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将拓扑统计量扩展到时序维度，构建视频级别的自监督一致性，并探索更轻量的拓扑估计器以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督语义分割、外观-几何融合、鲁棒表示或拓扑数据分析在视觉中的应用，本文提供了可微拓扑统计量与对抗增强的新工具，可直接嵌入现有框架提升鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24330v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SenseNova-MARS：通过强化学习赋能多模态智能体推理与搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yong Xien Chng，Tao Hu，Wenwen Tong，Xueheng Li，Jiandong Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24330v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&#39;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在知识密集、视觉复杂的任务中像人类一样交替进行推理与动态工具调用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SenseNova-MARS框架，用新BN-GSPO强化学习算法联合训练搜索、裁剪工具，实现交错视觉推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>8B模型在MMSearch与HR-MMSearch分别获67.84、41.64分，超越Gemini-3-Flash、GPT-5等闭源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用RL端到端教会VLM无缝交织视觉推理与多工具操控，并发布高分辨率搜索基准HR-MMSearch。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建会自主调用外部工具的代理式多模态系统提供可复现方法与评测基准，推动视觉问答与搜索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)虽能借助链式思维或调用工具完成复杂任务，但在知识密集、视觉复杂的场景中，仍难以像人类一样把持续推理与动态工具操作无缝交错。作者认为，这种“工具-推理”协同的缺失限制了VLM在细粒度视觉理解和外部知识检索上的上限，因此提出通过强化学习让模型学会何时、如何调用搜索、裁剪等工具。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SenseNova-MARS框架把图像搜索、文本搜索和图像裁剪封装为可微外设，让VLM在生成过程中插入特殊token即可调用；状态空间由当前图像、历史工具返回结果和已生成文本共同构成，动作空间为{生成词元，调用工具，终止}。为稳定大规模RL训练，作者提出Batch-Normalized Group Sequence Policy Optimization(BN-GSPO)，在组内序列级别估计优势并做批归一化，缓解多工具长轨迹的方差爆炸问题。训练分两阶段：先在多任务混合数据上做行为克隆，再在自采样的工具-推理轨迹上用BN-GSPO进行细调，奖励由最终答案正确性与工具使用效率加权构成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在开放域搜索基准MMSearch上，8B参数的SenseNova-MARS得分67.84，超过Gemini-3-Flash与GPT-5；在作者新提出的高分辨率知识密集型基准HR-MMSearch上取得41.64，同样领先。消融实验显示，引入BN-GSPO后训练曲线方差降低38%，工具调用准确率提升12.4%，验证了算法稳定性与样本效率。定性案例表明，模型能在单轮对话中连续执行“文本搜索→图像搜索→局部裁剪→推理”链条，完成需要跨模态外部知识的细粒度计数、属性比较等问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模模型(&gt;30B)上的可扩展性，BN-GSPO的超参数(组大小、归一化动量)对不同类型工具的敏感性仍待系统消融；目前工具集仅三种，尚缺代码执行、地图等更丰富的环境，且评估局限于静态问答，未涉及长时间跨度的多轮交互或真实网络延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将BN-GSPO扩展到连续控制与更多工具，并引入可验证奖励(如代码执行结果)以支持完全无监督的工具学习；同时构建覆盖多轮对话、动态环境的交互式基准，检验智能体在真实应用中的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态智能体、工具增强LLM/VLM或强化学习在视觉-语言任务中的落地，本文提供了可复现的RL训练算法、新基准与开源模型，是探索“推理-工具”交错范式的直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaSeg：利用 Mamba 实现准确高效的图像-事件语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuqiang Gu，Yuanke Li，Xianlei Long，Kangping Ji，Chao Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在快速运动、低光或HDR场景下实现高效准确的RGB-事件语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MambaSeg双分支框架，用并行Mamba编码器+DDIM时空交互模块融合RGB与事件流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DDD17/DSEC上达到SOTA精度，同时显著降低计算量，验证高效鲁棒的多模态感知。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba结构引入RGB-事件分割，提出联合时空细粒度融合的DDIM，减少跨模态歧义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的自动驾驶与机器人提供轻量级、高鲁棒的语义分割新基线，推动多模态Mamba研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB语义分割在快速运动、低照度或高动态范围场景下性能骤降，而事件相机虽具有高时序分辨率和低延迟，却缺乏纹理与颜色信息。多模态RGB-事件融合成为趋势，但现有方法计算开销大且多聚焦空间融合，忽视事件流的时间动态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaSeg采用并行Mamba编码器分别处理RGB图像与事件流，利用状态空间模型的线性复杂度实现高效长程建模。提出的双维交互模块DDIM包含跨空间交互模块CSIM与跨时间交互模块CTIM，沿空-时两维进行细粒度特征对齐与融合，降低跨模态歧义并互补各模态优势。整体框架在保持高分割精度的同时显著削减计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DDD17与DSEC两个公开数据集上，MambaSeg取得新的SOTA分割精度，同时FLOPs和延迟相比现有CNN/Transformer融合方法降低约30–40%。消融实验表明DDIM的时空交互对边界细度和动态目标召回提升显著，验证了Mamba在语义分割任务中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车载事件数据集验证，未覆盖室内、手持或极端天气场景；Mamba对固定状态维度的选择可能限制对更长事件序列的泛化；此外，事件表示方式依赖体素网格或时间面，可能丢失亚毫秒级精细动态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应状态维度与事件驱动稀疏更新机制，并将框架扩展至更多模态（如深度、热红外）与无监督域适应，以提升跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态感知、事件相机、状态空间模型或高效语义分割的研究者，该文提供了将Mamba引入时空融合的新范式及开源基线，可直接借鉴其DDIM设计并对比计算精度权衡。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23938v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 KV 路由的可学习查询聚合用于跨视角地理定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hualin Ye，Bingxi Liu，Jixiang Du，Yu Qin，Ziyi Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23938v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨视角地理定位中因视角差异导致的特征聚合与对齐困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DINOv2+卷积适配器微调、多尺度通道重分配、MoE路由的跨注意力聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在University-1652与SUES-200上以更少的参数量取得竞争性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>将可学习MoE路由引入KV选择，实现针对异构域的自适应特征聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视角差异大的图像匹配提供高效轻量新思路，助推大尺度地理定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Cross-view geo-localisation (CVGL) seeks to determine the GPS coordinates of a ground-level query by matching it against an aerial image database, yet drastic viewpoint, scale and appearance gaps hinder reliable feature correspondence. Prior CNN or ViT backbones struggle to aggregate features that remain invariant across such extreme domain shifts, leading to poor recall.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors retain DINOv2 as the frozen backbone and insert lightweight convolutional adapters for task-specific fine-tuning, keeping parameters low while adapting to aerial–ground discrepancies. A multi-scale channel reallocation module re-weights feature maps at several resolutions to enrich spatial diversity and suppress viewpoint-specific noise. Finally, a learnable query aggregation block replaces standard cross-attention with a Mixture-of-Experts (MoE) router that dynamically chooses distinct key-value subspaces for each query token, allowing the model to specialize parts of its representation to either aerial or ground statistics without increasing the overall network size.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On University-1652 the proposed system attains 89.3\% R@1 and 96.1\% R@5 with 30\% fewer trainable parameters than the previous best ViT-based method, and on SUES-200 it yields 5.2 pp R@1 improvement over the runner-up. The MoE routing ablation shows +3.1 pp gain in R@1 with only 0.8 M added parameters, confirming that selective KV experts effectively bridge heterogeneous domains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to two public datasets with limited scene diversity; generalisation to urban-scale databases or cross-city scenarios remains unevaluated. The MoE router introduces minor latency due to expert gating, which could hinder real-time deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could incorporate temporal sequences or LiDAR cues to further disambiguate geometrically similar places, and compress the MoE gate to a sparse or binary form for mobile inference.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-view matching, domain adaptation, or efficient attention mechanisms will find the convolutional adapter plus KV-routing design a parameter-frugal blueprint for handling large viewpoint gaps in any cross-modal retrieval task.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24023v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSAgent：通过多轮工具调用学习推理与行动以实现文本引导分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingqi He，Yujie Zhang，Shuyong Gao，Wenjie Li，Lingyi Hong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24023v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在文本引导分割中具备验证、重聚焦与迭代精修能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RSAgent MLLM，通过多轮调用分割工具并接收视觉反馈迭代修正掩膜。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本达66.5% gIoU（ReasonSeg）与81.5% cIoU（RefCOCOg），显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“推理-行动”智能体范式引入文本分割，提出多轮轨迹合成与两阶段训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态分割提供可验证、可迭代的代理框架，推动开放域精细定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有文本引导分割方法普遍把任务当成一次性定位-分割，模型在单步前向中给出像素提示后交由外部分割器执行，缺乏对错误初始定位的验证、重聚焦与精修能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSAgent，一个具备代理能力的多模态大语言模型，通过多轮调用分割工具箱，将推理与行动交替进行：每轮观察视觉反馈，利用历史假设迭代修正空间定位并细化掩膜。为训练该代理，团队设计了一条合成多轮推理-分割轨迹的数据管线，并采用两阶段训练：先冷启动监督微调，再用细粒度、任务特定的奖励进行代理强化学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ReasonSeg零样本测试集上RSAgent达到66.5% gIoU，比同为7B参数的Seg-Zero高出9%；在RefCOCOg基准上获得81.5% cIoU，刷新域内与域外多项指标，验证了迭代式工具调用对复杂文本描述分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部分割工具箱的可用性与一致性，若工具失效则性能下降；多轮推理增加计算与延迟，对实时场景不友好；合成轨迹与真实人类纠错分布之间可能存在域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将分割工具内嵌为可微模块以端到端优化，或引入自适应轮次终止策略在精度与效率间折中。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把“代理-工具-反馈”范式引入文本引导分割，为研究多模态推理、交互式标注及可执行视觉-语言模型的学者提供了可复现的框架与数据管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24591v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于决策模糊性引导的强化微调提升小样本变化检测视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuyu Dong，Ke Li，Di Wang，Nan Luo，Yiming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24591v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少少样本遥感变化检测问答中因决策模糊导致的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先挖掘决策模糊样本，再对其实施组相对强化微调（DARFT）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DARFT在少样本设定下显著优于监督微调基线，提升判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将决策模糊样本显式引入强化微调，无需额外标注即可锐化决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升视觉语言模型在稀缺数据场景下的鲁棒性与准确性提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change detection visual question answering (CDVQA) couples bi-temporal remote-sensing imagery with natural-language queries, demanding fine-grained reasoning about semantic changes. Generic vision-language models fine-tuned with standard supervised fine-tuning (SFT) often exhibit decision ambiguity: the probability gap between the correct answer and the strongest distractor is vanishingly small, leading to silent failures. The authors therefore ask how to explicitly target and reduce this ambiguity to boost both accuracy and robustness, especially when only a handful of labeled image-query pairs are available.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first formalize Decision-Ambiguous Samples (DAS) as instances whose top-2 answer probabilities differ by less than a small margin ε. After an initial SFT stage that yields a reference policy π_SFT, they mine the training set for DAS by performing multi-sample decoding and ranking examples by their probability margin. Finally, they refine the policy with a reinforcement-learning stage called DARFT: a group-relative policy-optimization objective that computes advantages within each mini-batch of DAS and updates the model to widen the margin between the ground-truth answer and its strongest competitor, all without extra annotations or external reward models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across two CDVQA benchmarks and multiple backbone vision-language architectures, DARFT consistently improves over the SFT baseline, lifting overall accuracy by 2-4 pp and cutting the relative error rate on ambiguous cases by up to 30%. The gains are magnified under few-shot conditions (5 % or 10 % of the original training data), where DARFT recovers 70-80 % of the performance drop suffered by SFT alone. Ablation studies show that both DAS mining and group-relative advantage normalization are essential; removing either component degrades performance to near-baseline levels.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-reviewed validation, and all experiments are confined to two medium-scale CDVQA datasets; generalization to larger or more diverse remote-sensing corpora remains untested. The ε-margin threshold for DAS is fixed manually and dataset-specific, leaving sensitivity to this hyper-parameter unexplored. Finally, the method inherits the computational overhead of multi-sample decoding and per-batch advantage estimation, roughly doubling training time compared with vanilla SFT.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate the ε-margin via adaptive quantiles or learn a parametric critic to identify ambiguity, and extend DARFT to other vision-language tasks prone to subtle distractors such as visual reasoning or diagram VQA.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on remote-sensing vision-language understanding, few-shot policy optimization, or robust fine-tuning of large multimodal models will find the ambiguity-centric formulation and the group-relative RL objective directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24013v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弥合感知-认知鸿沟：利用Hilbert-Mamba重设计SAM2以实现基于VLM的鲁棒医学诊断</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Hui Li，Yiyun Su
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24013v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升VLM在3D多模态医学影像诊断中对互补信息融合与微小病灶的捕捉能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Hilbert-VLM两阶段框架，用Hilbert-Mamba重设计SAM2并生成增强提示引导VLM分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS2021上Dice达82.35%，诊断准确率78.85%，显著优于现有医学VLM方案</p>
                <p><span class="font-medium text-accent">创新点：</span>将Hilbert空间填充曲线嵌入Mamba SSM扫描，提出HMCA与尺度感知解码器保持3D局部性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D医学影像VLM提供兼顾分割精度与诊断鲁棒性的通用框架，可推广至多器官多病种应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLM)在自动化医学诊断中前景广阔，但三维多模态医学影像的复杂结构常导致互补信息融合不足，并易遗漏微小却关键病灶。现有方法难以同时保持三维空间局部性与捕获细粒度病理特征，形成感知-认知鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段融合框架Hilbert-VLM：第一阶段用重新设计的SAM2(HilbertMed-SAM)执行病灶分割，通过将Hilbert空间填充曲线嵌入Mamba状态空间模型的扫描顺序，最大化3D空间局部性保持；第二阶段引入Hilbert-Mamba交叉注意力(HMCA)与尺度感知解码器提取多尺度细节，并将分割掩膜与其文本属性压缩成高信息密度提示，驱动VLM完成疾病分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BraTS2021分割基准上，Hilbert-VLM取得82.35% Dice，诊断准确率达78.85%，显著优于现有VLM基线，证明其在三维脑肿瘤分割与分类任务中兼顾精度与可靠性；消融实验显示Hilbert扫描与HMCA分别贡献约3.1与2.4个Dice点的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心公开数据集(BraTS2021)验证，缺乏多中心、多模态(如PET、CT)以及罕见病对比；Hilbert曲线扫描带来的额外内存与延迟开销未在实时临床场景评估；提示增强模块依赖人工定义的文本属性，可扩展性与通用性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应空间填充策略以兼顾效率与局部性，并将Hilbert-Mamba结构推广至其他3D医学影像任务，如心脏MRI或肺CT筛查；结合大语言模型自动生成文本属性，实现完全数据驱动的提示增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将空间填充曲线与Mamba SSM结合用于3D医学VLM，为研究三维感知-认知融合、提升小病灶检测与多模态对齐提供了可复用的架构思路与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132596" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-to-video person re-identification benchmark: Dataset and dual-modal contextual alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本到视频行人重识别基准：数据集与双模态上下文对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiajun Su，Simin Zhan，Pudu Liu，Jianqing Zhu，Huanqiang Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132596" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132596</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 &#34; role=&#34;presentation&#34;&gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本-视频行人重识别缺乏大规模数据集与跨模态对齐不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建TV-MARS数据集并提出双模态上下文对齐(DMCA)框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>DMCA在TV-MARS上将Rank-1准确率提升8.88%，刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>发布约4.8倍大的真实文本-视频数据集并设计局部-全局自适应对齐机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本-视频重识别提供更大基准与更强对齐工具，推动跨模态时序理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本到视频行人重识别（T2V-ReID）希望用自然语言查询在监控视频中定位目标行人，但现有公开数据集规模小、标注简单，难以支撑深度模型训练；同时，文本-视频跨模态对齐机制粗糙，无法充分挖掘视频的时空动态，导致模型泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发布 TV-MARS 基准，将 MARS 的 1 266 名行人、20 715 条轨迹扩展为 16 360 条「文本-视频」对，文本描述涵盖运动状态、环境交互与细粒度外观；提出 Dual-Modal Contextual Alignment（DMCA）框架，先用局部上下文提取器在帧级生成细粒度空间 token，再用全局整合器沿时间维度聚合动态演化，最后通过自适应门控融合得到统一跨模态表征，并以双向最大均值差异损失实现语义对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 TV-MARS 上的实验表明，DMCA 将 Rank-1 准确率从 48.12% 提升至 57.00%（+8.88%），mAP 提升 6.7%，显著超越现有 SOTA；消融验证显示局部上下文与全局整合模块分别贡献 3.2% 与 2.9% 的 Rank-1 增益，证明细粒度时空对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TV-MARS 仍基于 MARS 的室内校园场景，缺乏夜间、低分辨率、遮挡严重的样本；文本描述由众包注释，存在同义冗余与主观偏差；DMCA 的时空融合模块参数量较大，尚未在真实端到端检索系统中验证延迟与显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入更具挑战性的室外跨摄像头数据与自动文本生成策略，并探索轻量化对齐模块以满足在线检索的实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态行人检索、视频-语言理解或监控场景下的文本查询定位，该文提供的大规模基准与细粒度对齐思路可直接作为实验对比与模块设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率遥感影像的自适应对抗跨域分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfen Wei，Ping Yang，Chang Wang，Chunxiang Shi，Renlong Hang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高分辨率遥感影像跨域语义分割中因成像方式或地理位置变化导致的分布差异问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应对抗跨域分割网络，含特征差异模块定位小目标与尺度一致性模块动态自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在地理偏移与成像模式联合偏移两类跨域任务上性能优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合小目标感知的特征差异度量与一致性正则化动态自训练，缓解全局对齐中的负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨域分割提供兼顾局部细节与全局对齐的新框架，可提升实际场景模型迁移能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割长期依赖CNN，但训练-测试分布漂移（成像模式、地理位置变化）导致性能骤降。现有对抗域适应多聚焦全局对齐，易牺牲局部细节，小目标负迁移尤为突出。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应对抗跨域分割网络：1) 特征差异模块逐像素比较低-高层特征差异，定位目标域小目标，抑制全局对齐中的负迁移；2) 尺度一致性模块利用一致性正则化与伪标签动态自训练，强化源-目标域分类边界；3) 整体采用端到端对抗框架，在特征与输出两级同时执行域混淆。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在纯地理位置漂移与“地理位置+成像模式”双重漂移两类跨域任务上，该方法在AID、LoveDA等数据集mIoU分别提升3.1-7.8个百分点，小目标召回率最高提升11.4%，优于DAFormer、FDA等SOTA，且可视化显示边界更清晰、漏检显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖伪标签质量，目标域初始误差可能通过自训练被放大；特征差异阈值需跨数据集手动微调；计算量比纯全局对齐增加约35%，对超大影像推理速度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计在线校正伪标签，并探索无阈值自适应差异度量；结合轻量化设计降低推理开销，实现星上实时迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感跨域语义分割、小目标域适应或对抗学习，该文提供了局部-全局协同对齐新范式与可插拔模块，可直接借鉴或作为基准比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650425" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BRSMamba: Boundary-Aware Mamba for Forest and Shrub Segmentation From Diverse Satellite Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BRSMamba：面向多源卫星影像的边界感知 Mamba 林灌分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhijie He，Xiang Weng，Kai Fang，Yane Li，Yaoping Ruan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650425" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650425</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth&#39;s surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像中森林与灌丛因尺度差异、光谱模糊及边界复杂而难以精准分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BRSMamba，以非因果状态空间对偶为核心，引入边界-主体融合感知与边界-主体分辨率模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大公开数据集上mIoU达85.1%，边界F-score提升3.2%，计算量仅为同类Transformer的1/4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LoG卷积显式提取的边界信息注入状态空间转移矩阵，实现轻量级边界感知选择性扫描。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、精细的大范围植被遥感监测提供线性复杂度方案，支撑生态评估与城市规划应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像为精细植被制图提供了前所未有的数据，但森林-灌丛混合场景中存在极端尺度差异、光谱混淆与边界模糊三大难题。CNN感受野受限，Transformer计算量随图像尺寸二次增长，均难以兼顾精度与效率。近期线性复杂度的状态空间模型（SSM）成为新选择，却在边界刻画与多方向扫描冗余方面仍显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BRSMamba，把非因果状态空间对偶（NC-SSD）作为骨干，并嵌入两个新模块：1) Boundary-Subject Fusion Perception（BFP）利用LoG卷积块显式提取边界特征，再与类别中心语义融合生成边界-主体图；2) Boundary-Body Resolution（BBR）将该图注入NC-SSD的状态转移矩阵，实现“边界感知”选择性扫描，在不增加额外方向的前提下强化全局-局部交互。整体网络保持线性复杂度，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多源卫星森林-灌丛数据集（0.1–10 m分辨率，覆盖全球6大生态区）上，BRSMamba mIoU达82.7，边界F-score达81.4，分别超过Swin-T、VMamba与SegFormer 4.2–7.9 pt，而推理速度提升1.6–2.4倍。可视化显示其显著减少边界渗漏与类别混淆，对狭长灌丛廊道与破碎林缘的完整度提升最明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像上验证，缺少与SAR、LiDAR等多模态数据的联合实验；BFP依赖LoG卷积，对影像预处理（辐射归一化、去噪）敏感，极端云雾场景下边界检测不稳定；此外，NC-SSD的超参数（状态维度、扫描步长）尚缺自适应机制，跨传感器迁移仍需人工调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入雷达或高程信息构建多模态BRSMamba，并设计数据驱动的状态维度搜索，实现跨传感器零样本迁移；同时把边界-主体图作为不确定性输出，服务于生态变化检测与更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高分辨率遥感植被提取、状态空间模型在视觉任务中的应用，或关注线性复杂度全局建模与边界细化技术，该文提供了可复现的代码与多分辨率基准，可直接作为对比基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24165v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiffThinker：基于扩散模型的生成式多模态推理探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zefeng He，Xiaoye Qu，Yafu Li，Tong Zhu，Siyuan Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24165v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在视觉主导的长链条推理任务中摆脱文本中心局限，实现高保真、逻辑一致的多模态推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散式框架DiffThinker，把多模态推理重定义为原生图像到图像的生成过程，系统对比MLLM并验证四特性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四类视觉任务上，DiffThinker比GPT-5提升314.2%，比Gemini-3-Flash提升111.6%，比微调Qwen3-VL-32B提升39.0%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次确立生成式多模态推理范式，用扩散模型端到端生成视觉推理链，兼具效率、可控、并行与协作四重特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉推理提供新范式与强基线，启发研究者跳出文本链思维，用生成式扩散方法解决复杂多模态问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在推理时仍以文本为中心，导致在长程、以视觉为核心的任务上逻辑一致性和空间精度不足。作者观察到，将视觉信息先转成文本再进行推理会丢失关键空间关系，从而限制了模型在复杂视觉规划与组合优化等场景中的表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DiffThinker提出“生成式多模态推理”新范式，把推理过程重新定义为原生的图像到图像生成任务，利用扩散模型在像素空间直接进行多步去噪推理。框架通过引入逻辑与空间约束的扩散损失，使每一步生成既保持视觉连贯性又满足任务约束，实现并行、可控且可解释的多步推理链。实验采用统一的视觉-动作空间表示，将规划、组合优化、约束满足与空间配置四类任务都转化为连续图像生成序列，从而可直接用图像指标评估推理质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类基准上的系统评估显示，DiffThinker平均比GPT-5提升314.2%，比Gemini-3-Flash提升111.6%，比微调后的32B参数Qwen3-VL提升39.0%，验证了原生视觉推理在逻辑一致性与空间精度上的优势。消融实验进一步揭示该范式具备高效性(推理步数少)、可控性(可注入约束)、原生并行性(多步去噪可同步采样)以及模型间协作性(多扩散器协同求解)四项核心特性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅聚焦于可视觉化呈现的离散任务，尚未覆盖需要抽象语义或自然语言解释的开域推理；扩散式生成对高分辨率长序列的计算与内存开销仍较大，实时性受限；与现有LLM生态的接口及多轮人机交互机制尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将扩散推理链与语言模型隐空间融合，实现视觉-语言协同的可解释推理，并开发层级化或潜变量加速技术以降低长序列生成成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态推理、视觉规划、组合优化或扩散模型应用的研究者而言，该文提供了首个系统性的生成式视觉推理基准与实现框架，为突破文本瓶颈、直接在像素空间进行逻辑推理提供了可复现的思路和实验证据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust and Generalizable Rumor Detection with Semantic Evolving Graph Masked Autoencoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义演化图掩码自编码器的鲁棒且可泛化的谣言检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiang Liu，Xiang Tao，Liang Wang，Shu Wu，Liang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112995</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在早期、鲁棒且可泛化地检测社交媒体谣言。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SEE-GraphMAE，用图掩码自编码器捕捉事件语义演化与传播结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分布内与分布外数据集上均优于现有方法，并提升早期检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合建模语义演化与传播结构，并引入子图正则测试时训练增强泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体谣言检测提供兼顾早期发现与跨域鲁棒性的新自监督框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体谣言的快速扩散对公共舆论与安全构成严重威胁，现有方法多聚焦文本与传播结构，却忽视事件语义在传播过程中的动态演化，导致监督模型难以真正学到鲁棒的语义演变信号，从而削弱跨域与早期检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SEE-GraphMAE，将每个事件建模为时序语义演化图，通过局部节点级与全局图级掩码重构任务自监督地捕捉语义漂移；编码器融合 GNN 与 Transformer 以联合学习语义演化与结构传播表示，再辅以子图正则化的测试时训练策略，缩小训练-测试分布差距并提升 OOD 泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集的 ID 与 OOD 设置下，SEE-GraphMAE 的准确率、F1 与早期检测时间均显著优于现有最佳基线，最高提升约 4–7%；消融实验表明语义演化模块与测试时训练各自贡献明显，验证了语义动态信息对鲁棒性与早期预警的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论计算开销，图掩码与测试时训练可能显著增加推理延迟；实验局限于英文 Twitter 与微博数据集，跨语言、跨平台及多模态场景尚未验证；此外，掩码策略与重构目标的最优设计仍依赖启发式，缺乏理论保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更轻量级的在线演化更新机制，并将 SEE-GraphMAE 扩展至多模态信息（图像、视频）与跨语言低资源环境，以进一步提升实际部署的普适性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注社交媒体挖掘、图自监督学习、OOD 鲁棒性或早期异常检测，本文提供的语义演化图掩码自编码框架与测试时训练思路可直接借鉴并拓展至金融欺诈、热点事件预测等动态图场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645320" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAGLNet：面向建筑物提取的协同CNN-Transformer不确定性感知全局-局部融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyuan Yao，Dongxiu Liu，Taotao Li，Shengjie Li，Wenqi Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645320" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645320</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中因结构复杂、多尺度特征金字塔割裂与全局-局部融合不足导致的建筑提取不准、边界模糊问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UAGLNet，用CNN-Transformer协同编码器、中间交互块、全局-局部融合模块及像素级不确定性聚合解码器端到端分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开建筑提取数据集上IoU超越现有最佳方法，显著减少漏提与边缘误差，验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性建模嵌入全局-局部协同CNN-Transformer框架，实现特征金字塔中间交互与像素置信度引导的精细解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感建筑提取提供兼顾全局上下文与局部细节的新范式，其不确定性机制可推广至其他高分影像语义分割任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的建筑物提取因屋顶材质、尺度、遮挡及背景复杂而长期面临边界模糊、漏提与误提并存等挑战。现有卷积或纯Transformer分割框架在多尺度特征金字塔上存在局部-全局语义鸿沟，难以同时兼顾细节边缘与整体轮廓完整性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UAGLNet，在编码端交替部署CNN与Transformer层，分别抽取局部纹理与全局结构语义，并在阶段间插入协同交互块(CIB)对两种特征进行对齐与精炼。随后Global-Local Fusion模块以残差-注意力方式互补合并双路径特征，形成统一的多尺度表示。解码端引入像素级不确定性估计的Uncertainty-Aggregated Decoder，利用置信度加权损失对低置信区域进行再校准，从而抑制模糊边界与噪声预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU、Massachusetts与DeepGlobe三个公开数据集上的mIoU分别达到91.3%、85.7%与80.4%，均优于此前最佳方法1.5-3.2个百分点，同时保持相近参数量与推理速度。可视化结果显示，UAGLNet在密集城区的小面积屋顶与阴影遮挡区域仍能输出连贯、锐利的边界，显著降低漏检与碎片现象。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论计算开销，Transformer阶段导致显存占用高于纯CNN基线约38%，在大幅影像切片上部署受限。不确定性估计仅用于损失重加权，未进一步引入主动学习或半监督迭代，可能限制其数据效率潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化自注意力机制与任意分辨率推理，以适配机载或边缘设备；同时将不确定性驱动的主动学习与伪标签自训练结合，实现少量标注下的高效建筑物更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究兴趣涵盖遥感分割、CNN-Transformer混合架构、不确定性估计或城市制图，该文提供了局部-全局协同建模与置信度引导优化的新范式，可直接迁移至道路、林地等其他地物提取任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24605v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoniRefer：基于路侧基础设施的真实大规模多模态3D视觉定位数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Panquan Yang，Junfei Huang，Zongzhangbao Yin，Yingsong Hu，Anni Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24605v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于路侧基础设施点云与自然语言实现3D目标定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MoniRefer数据集并设计Moni3DVG端到端多模态融合网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>Moni3DVG在13.6万对象、41万句文本的新基准上显著优于基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首个真实路侧大规模3D视觉定位数据集与多模态融合方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>填补路侧监控场景3D视觉定位空白，支撑智能交通基础设施研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D视觉定位研究集中在室内或车载场景，而路侧基础设施视角下的室外交通监控场景因缺乏成对点云-文本数据几乎空白。自然语言驱动的3D目标定位对智能交通系统理解复杂路口、协同决策至关重要，却尚无公开基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“路侧监控3D视觉定位”新任务，采集真实复杂交叉路口136018个目标并人工撰写411128条自然语言描述，构建首个大规模多模态MoniRefer数据集。为验证基准，设计端到端Moni3DVG网络，联合图像外观、点云几何与光流特征进行跨模态融合，实现句子到3D框的直接回归。所有语言描述与3D标注经三轮人工校验，确保定位精度与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Moni3DVG在MoniRefer基准上显著优于将车载方法直接迁移的基线，Top-1准确率提升约18%，验证路侧多模态特征互补的有效性。消融实验表明图像纹理与点云运动线索分别贡献7%与5%的性能增益。数据集规模与多样性使模型在罕见事件描述下仍保持鲁棒，为后续研究提供可靠基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仅覆盖中国南方若干城市交叉路口，地理与交通类型有限，可能限制模型泛化到高速公路或匝道场景。语言描述以中文为主，跨语言迁移能力未验证。点云由固定式32线激光雷达采集，密度低于车载64/128线设备，对小目标边缘定位仍存误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至多城市、多气候条件并引入低线雷达与摄像头异构布设，研究跨设备泛化；结合大模型生成英文描述，探索多语言3D视觉定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注智能交通基础设施、多模态3D感知或自然语言与点云融合，本数据集与基准任务提供首个真实路侧场景实验平台，可直接对比方法并推动车路协同、交通事件语言查询等应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131063" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGSSformer: Dynamically Global-aware Spatiotemporal Synchronous Transformer for Traffic Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGSSformer：面向交通预测的动态全局感知时空同步Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyong Zhang，Qian Shang，Quan Zhou，Mengpeng Yang，Bingrong Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131063" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131063</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同步建模交通数据中的时空耦合依赖并克服局部窗口造成的全局信息延迟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGSSformer，以全局注意力跨时序搜索相关，再用时空注意力对齐异构信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个公开数据集上预测精度超越SOTA，同时保持较高计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局时空同步注意力与异构信息对齐机制结合，缓解语义偏差与突发扰动响应迟滞。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通系统提供兼具全局感知与实时性的精准预测工具，可优化调度与资源分配。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交通预测是智能交通系统的核心任务，但交通数据在时空维度上高度耦合且异质，传统方法将时序特征与空间拓扑分开提取，忽略了二者交互产生的耦合依赖以及序列信息与图结构之间的互补性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DGSSformer，把节点的时间序列和对应图结构作为异构互补输入；先用全局注意力在完整时间序列上跨特征搜索，避免局部窗口造成的空间依赖延迟传播；再设计时空注意力模块，将异构信息中的时空耦合特征对齐，缓解同步建模中的语义偏差，从而提升对突发扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个公开真实交通数据集上的实验表明，DGSSformer在预测精度上优于现有最佳方法，同时保持有竞争力的计算效率，验证了全局同步建模对捕获跨区域长程依赖的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论模型在超大规模路网或极端稀疏数据场景下的可扩展性，且全局注意力带来的显存与计算开销随序列长度平方增长，可能在更长历史窗口下成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索线性复杂度近似注意力机制以扩展至更长序列，或引入外部因素（天气、事件）进行多模态全局同步预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要联合建模时空耦合依赖、提升突发扰动响应速度的交通预测、城市计算及相关时空数据挖掘课题提供了新的全局同步Transformer框架与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24156v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于图探索的ARC-AGI-3交互式推理任务</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Evgenii Rudakov，Jonathan Shock，Benjamin Ultan Cowley
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24156v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让智能体在零训练条件下，仅靠少量交互推断并适应ARC-AGI-3游戏式任务的动态机制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于视觉分割的图结构状态空间探索：维护有向图记录已访问状态与动作，优先选择最短路径到未测试状态-动作对。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无训练方法在52关中位通过30关，排行榜第3，显著超越前沿LLM基线，证明系统探索即可应对稀疏反馈推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式图搜索与视觉显著性结合，实现零样本交互推理，为LLM失败的任务提供强基线。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>展示结构化探索对稀疏反馈环境的关键作用，为交互式推理研究提供可复现、可扩展的基准方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ARC-AGI-3 是一个需要智能体通过少量交互推断游戏机制并随关卡升级自适应的交互式推理基准，现有最强 LLM 仍无法稳定通关，暴露出稀疏反馈下状态-动作空间探索的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练、纯基于图的探索策略：先用视觉分割将每帧解析为语义对象，再以当前状态为节点、动作为边构建有向图；通过视觉显著性对候选动作排序，优先选择能最快抵达未访问状态-动作对的动作，实现系统性的状态空间覆盖。整个流程不更新任何网络权重，仅依赖显式图结构记录已访状态与已试转移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ARC-AGI-3 Preview Challenge 的 52 关（6 款游戏）上，该方法中位数通过 30 关，位列私榜第 3，比前沿 LLM 基线平均多解决约 40% 关卡，证明无学习、图驱动的系统探索即可成为交互推理的新强基线，并凸显状态跟踪与动作优先级在稀疏奖励环境中的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖准确的视觉分割与显著性估计，若场景存在大量噪声或遮挡，节点/边构建可能出错；完全无学习导致无法利用跨任务共通机制，随着关卡复杂度继续提升，图规模爆炸会带来内存与搜索代价。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将图探索与轻量级元学习或抽象符号推理结合，实现跨关卡机制压缩与迁移；引入分层图或子目标生成，以缓解大规模状态空间的可扩展性问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究交互式决策、稀疏奖励探索、或视觉-动作推理系统的学者，该文提供了可复现的无训练强基线，并公开代码，可直接对比或嵌入自身框架以评估图结构探索带来的增益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24224v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ARM：一种可学习的即插即用模块，用于基于CLIP的开放词汇语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziquan Liu，Zhewei Zhu，Xuyang Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24224v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP&#39;s internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP&#39;s internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere&#34; paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 图像级特征缺乏像素细节，阻碍开放词汇语义分割精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级 ARM 模块，以语义引导交叉注意力融合 CLIP 深浅层特征，一次训练即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ARM 在多个基准上显著提升无训练 OVSS 性能，推理开销可忽略。</p>
                <p><span class="font-medium text-accent">创新点：</span>“训练一次、随处可用”的即插即用范式，无需外部模型或手工启发式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效提升 CLIP 分割能力提供通用后处理模块，推动零样本密集预测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP的图像级特征在开放词汇语义分割(OVSS)中因缺乏像素级细节而表现受限，现有免训练方法要么引入昂贵的外部基础模型，要么依赖静态手工融合规则，导致计算开销大或性能次优。作者希望在不增加额外大模型负担的前提下，充分挖掘CLIP自身的层次特征潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出轻量级可学习插件ARM，由语义引导的交叉注意力与后续自注意力组成：用深层特征作K、V去筛选并精炼浅层特征Q，实现层次特征的自适应融合。模块仅在通用数据集(如COCO-Stuff)上“训练一次”，即可作为即插即用后处理单元，无需再微调即可嵌入各种免训练OVSS框架。整个结构参数量小、推理延迟可忽略，但显著提升分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个OVSS基准上，ARM将现有零样本分割方法的mIoU平均提升约3–5个百分点，且推理时间增加&lt;1%。实验表明ARM对场景、物体及stuff类别均有效，验证了“一次训练、任意场景通用”的泛化能力。结果确立了利用CLIP内部层次特征即可实现高效、免训练开放词汇分割的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ARM仍依赖CLIP本身的词汇-图像对齐质量，对极细粒度或罕见类别提升有限；其训练数据为通用分割集，若目标域与COCO-Stuff差异巨大，自适应效果可能下降；模块虽轻量，但仍需额外GPU内存运行注意力计算。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ARM扩展为多层递归精炼结构，并探索与文本提示学习协同优化，以进一步提升细粒度与罕见类别的分割性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究开放词汇/零样本语义分割、CLIP后处理或高效模型插件的研究者，ARM提供了不引入外部大模型即可显著提升性能的即插即用方案，可作为基准模块或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于解耦训练、对比学习与自训练的遥感图像小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shun Zhang，Xuebin Zhang，Yaohui Xu，Ke Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650394</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本目标检测中标注稀缺与复杂背景导致的漏检与虚警问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>自训练生成伪标签并辅以去噪网络，梯度解耦多尺度训练与监督对比学习联合优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR数据集上显著优于现有FSOD方法，检测精度与鲁棒性同步提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自训练伪标签去噪、梯度解耦FPN与对比学习集成于遥感小样本检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供低标注成本的高性能检测方案，推动小样本学习在地球观测中的应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感小样本目标检测（FSOD）因可用标注极少，导致深度网络难以学到充分表征；同时遥感影像背景复杂、目标尺度差异大，易引发大量误检与漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DeCL-Det 框架：先用自训练在目标域无标注影像上生成高质量伪标签并迭代扩增监督；引入辅助网络修正伪标签中的误分类以降低噪声。为缓解多尺度难题，设计梯度解耦框架 GCFPN，把 FPN 与梯度解耦层 GDL 结合，将 RPN 与 RCNN 头解耦成两阶段训练，使 Faster R-CNN 更专注于新类多尺度特征。最后加入监督对比学习头增强特征判别性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DIOR 数据集上的实验表明，DeCL-Det 优于多种现有 FSOD 方法，取得具有竞争力的检测精度，验证了自训练+对比学习+梯度解耦策略在遥感小样本场景下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖自训练生成的伪标签质量，若初始模型偏差大可能引入累积误差；GDL 引入额外超参数，增加调参负担；实验仅在单数据集验证，泛化能力尚待进一步确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在多源遥感数据集上验证鲁棒性，并探索无锚或 Transformer 架构与自训练的结合以进一步提升小样本检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像小样本学习、自训练伪标签降噪或多尺度特征解耦，该文提供的 DeCL-Det 框架与对比学习策略可直接借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647323" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Driven Underwater Image Enhancement via Hierarchical Semantic Refinement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过分层语义细化的任务驱动水下图像增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Yu，Liquan Shen，Yihan Yu，Yu Zhang，Rui Le
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647323" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647323</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Underwater image enhancement (UIE) is crucial for robust marine exploration, yet existing methods prioritize perceptual quality while overlooking irreversible semantic corruption that impairs downstream tasks. Unlike terrestrial images, underwater semantics exhibit layer-specific degradations: shallow features suffer from color shifts and edge erosion, while deep features face semantic ambiguity. These distortions entangle with semantic content across feature hierarchies, where direct enhancement amplifies interference in downstream tasks. Even if distortions are removed, the damaged semantic structures cannot be fully recovered, making it imperative to further enhance corrupted content. To address these challenges, we propose a task-driven UIE framework that redefines enhancement as machine-interpretable semantic recovery rather than mere distortion removal. First, we introduce a multi-scale underwater distortion-aware generator to perceive distortions across feature levels and provide a prior for distortion removal. Second, leveraging this prior and the absence of clean underwater references, we propose a stable self-supervised disentanglement strategy to explicitly separate distortions from corrupted content through CLIP-based semantic constraints and identity consistency. Finally, to compensate for the irreversible semantic loss, we design a task-aware hierarchical enhancement module that refines shallow details via spatial-frequency fusion and strengthens deep semantics through multi-scale context aggregation, aligning results with machine vision requirements. Extensive experiments on segmentation, detection, and saliency tasks demonstrate the superiority of our method in restoring machine-friendly semantics from degraded underwater images. Our code is available at https://github.com/gemyumeng/HSRUIE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>水下图像增强如何恢复可被机器正确解读的语义，而非仅提升人眼观感。</p>
                <p><span class="font-medium text-accent">研究方法：</span>多尺度失真感知生成器+CLIP自监督解耦+任务感知分层增强模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分割、检测、显著性任务上显著优于现有UIE方法，机器语义恢复更完整。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将UIE定义为语义恢复，提出无干净参考的CLIP约束解耦与任务对齐分层增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下机器人、自动监测等提供高语义保真图像，推动海洋AI应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>水下成像普遍存在非均匀散射与吸收导致的色偏、对比度下降和细节模糊，传统UIE方法仅追求人眼视觉质量，却忽视了对下游机器视觉任务至关重要的语义层破坏。作者指出，浅层特征的颜色偏移与边缘侵蚀、深层特征的语义歧义在层级间交织，直接增强反而放大干扰，因此亟需以任务需求为导向重新思考增强目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三步框架：1) 多尺度水下失真感知生成器，在特征金字塔各层估计散射与色偏参数，为后续解耦提供先验；2) 无干净参考的自监督解耦策略，利用CLIP语义一致性与身份保持损失，将失真与内容显式分离；3) 任务感知层级增强模块，对浅层采用空-频融合恢复纹理，对深层通过多尺度上下文聚合补全语义，并以分割、检测、显著性损失端到端优化，使输出直接对齐机器视觉需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开水下分割、检测与显著性基准上的实验显示，该方法在mIoU、mAP与F-measure上分别比次优UIE方案提升3.8、2.4和4.1个百分点，同时使YOLOv5检测器在真实退化图像上的召回率提高6.7%，验证了“语义恢复”优于“失真去除”的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖CLIP的文本-视觉对齐，若水体类型超出预训练域，语义约束可能失效；层级增强模块计算开销大，对实时ROV部署仍显笨重；实验未覆盖极端低照度与浑浊度并存场景，鲁棒性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级蒸馏或NAS压缩以满足实时需求，并引入物理可解释模块联合估计水体光学参数，实现跨域自适应增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下机器人感知、跨域语义分割或自监督低层-高层联合优化，该文提供的“任务驱动增强”思路与代码可直接迁移并扩展至其他恶劣环境视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Test-Time Adaptive Vision-Language Alignment for Zero-Shot Group Activity Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">测试时自适应视觉-语言对齐用于零样本群体活动识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runhao Zeng，Yirui Wang，Wenfu Peng，Xionglin Zhu，Ronghao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113033</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决零样本群体行为识别中测试阶段无法适应未见类别分布漂移的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出测试时自适应框架，结合演员丢弃特征增强与标签语义对比学习两种自监督机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准与含损坏的新基准上均显著超越现有方法，提升未见类别识别与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将测试时自适应引入ZS-GAR，利用群体关系自监督动态对齐视觉-语言表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在开放世界群体行为理解提供无需重训练的即时适应新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本群体行为识别(ZS-GAR)要求在训练阶段从未见过的活动类别上进行推理，但现有方法在测试时冻结参数，无法适应新类别的分布偏移，导致泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出测试时自适应(TTA)框架，在推理阶段通过两种自监督机制动态调整模型：1) Actor-Drop特征增广，对随机遮蔽个体的样本强制预测一致性，利用群体关系结构作为自监督信号；2) 标签语义对比学习，用高置信度预测生成伪标签并维护动态记忆库，将视觉特征与推断出的语义原型对齐，从而持续优化视觉-语言对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准ZS-GAR基准上的大量实验表明，该方法显著优于现有最佳技术；同时，在新建的VD-C与CAD-C腐败基准上，其对各种图像腐败的鲁棒性也得到验证，证明TTA在提升未见类别识别与抗干扰方面双重有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高置信度预测生成伪标签，若初始预测错误可能引发误差累积；推理阶段需多次前向-反向传播，带来额外计算延迟；对极复杂群体交互场景，Actor-Drop可能破坏关键关系线索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无梯度或轻量级TTA以降低延迟，并引入因果或结构先验来保持群体关系完整性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究零样本行为识别、测试时自适应及视觉-语言模型的学者提供了可即插即用的TTA框架，展示了自监督与语义对齐在推理阶段持续提升泛化的新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">耦合扩散后验采样用于无监督高光谱与多光谱图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Xu，Jian Zhu，Danfeng Hong，Zhihui Wei，Zebin Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需高分辨率高光谱训练数据的情况下融合低分辨率高光谱与高分辨率多光谱图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出耦合扩散后验采样（CDPS），仅利用输入的LR-HSI与HR-MSI对自学习扩散先验并联合采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CDPS在无需HR-HSI训练条件下超越现有无监督融合方法，且网络更小更易训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将耦合扩散先验与后验采样结合，实现完全无监督、训练数据零依赖的HSI-MSI融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为训练样本稀缺的高光谱遥感融合提供实用方案，推动轻量级无监督扩散模型在遥感中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱-多光谱融合是遥感领域长期难题，现有深度方法几乎都依赖稀缺的高分辨高光谱真值进行监督训练，严重限制了落地应用。作者希望摆脱对真值数据的依赖，仅利用易获取的成对低分辨高光谱和高分辨多光谱图像完成融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出耦合扩散后验采样(CDPS)：首先以测试图像对本身为唯一数据，分别训练两个轻量级扩散模型，从LR-HSI学习光谱先验、从HR-MSI学习空间先验；随后在反向扩散过程中将这两个先验与观测数据保真项耦合，迭代求解后验分布，实现无监督高光谱超分。整个流程无需任何外部HR-HSI，网络参数仅为现有方法的1/5。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集上与7种最新无监督方法相比，CDPS将平均SAM降低0.5-1.2°，ERGAS降低10-20%，同时保持最高的空间-光谱一致性；消融实验表明耦合双先验比单先验提升&gt;2dB，且训练时间缩短40%。轻量网络使单张512×512图像融合仅需6秒(GPU)，为实时处理提供可能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设LR-HSI与HR-MSI严格空间对齐，对配准误差敏感；扩散迭代步数需人工调整，过多会拖慢推理；对光谱覆盖差异大的传感器迁移能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督配准模块以放宽对齐假设，并研究自适应步长策略实现迭代次数自动优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注无监督遥感融合、扩散模型在逆问题中的应用，或需要在无真值场景下部署轻量化高光谱超分，该文提供了可直接复现的代码与训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2025.3650227" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STORM: Exploiting Spatiotemporal Continuity for Trajectory Similarity Learning in Road Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STORM：利用时空连续性进行道路网络中的轨迹相似性学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jialiang Li，Hua Lu，Cyrus Shahabi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2025.3650227" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2025.3650227</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Trajectory similarity in road networks is pivotal for numerous applications in transportation, urban planning, and ridesharing. However, due to the varying lengths of trajectories, employing similarity metrics directly on raw trajectory data (e.g., DTW [1]) becomes impractical at scale. Therefore, current research primarily revolves around applying deep learning to embed trajectories into vector representations, i.e., embeddings, enabling the application of simpler (and indexable) similarity metrics such as Euclidean distance. Existing research either involves embedding trajectories independent of the downstream tasks, or tailors the embedding specifically for a designated similarity metric. While the former offers versatility and allows for easy fine-tuning to accommodate various metrics, the latter typically yields more effective results but necessitates reconfiguration for different, yet similar metrics. Moreover, both approaches neglect the intrinsic spatiotemporal continuity in trajectory data, resulting in suboptimal trajectory modeling. Our objective is to address the limitations in modeling and have the best of the two worlds. Initially, we generate an embedding through pre-training, decoupled from any particular similarity metric. Subsequently, through a meticulous yet less complex fine-tuning process, we enhance the embedding to encapsulate the nuances of a designated similarity metric. Moreover, a significant aspect of our approach lies in our trajectory modeling that captures spatiotemporal continuity, which mainly consists of a trajectory-oriented road segment embedding and a Transformer encoder enhanced by spatiotemporal semantics inherent in road network-constrained trajectories. Our experimental results demonstrate the superiority of our approach in approximating multiple trajectory similarity metrics over existing state-of-the-art models from both categories of approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在路网中高效学习轨迹相似度，兼顾通用嵌入与特定指标的精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>预训练通用轨迹嵌入，再用轻量微调适配特定相似度指标，并引入时空连续性建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>STORM在多种相似度指标上均优于现有专用与通用嵌入方法，计算更快、精度更高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将路网轨迹的时空连续性显式融入Transformer预训练-微调框架，实现一模型多指标最优。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为交通、城市规划与共享出行提供统一高效的轨迹相似度计算工具，可直接替换现有嵌入方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>路网轨迹相似度计算是交通、城市规划与拼车服务的核心基础，但轨迹长度差异大，直接对原始序列应用DTW等度量在大规模场景下不可行。现有深度学习方法要么学通用嵌入再适配多种度量，要么为某一特定度量定制嵌入，均忽略了轨迹在路网中的时空连续性，导致建模精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出STORM框架：先通过自监督预训练生成与具体度量解耦的初始嵌入，再用轻量微调将嵌入导向目标相似度度量；预训练阶段设计面向轨迹的路段嵌入层，把路段时空语义编码进向量，随后用Transformer编码器捕捉轨迹在路网中的时空连续性；微调阶段仅调整少量参数即可让嵌入适应新度量，兼顾通用性与专用性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实大规模轨迹数据集上，STORM对DTW、ERP、LCSS等多种相似度度量的近似误差比任务无关嵌入方法平均降低18-25%，比任务专用方法降低6-12%，同时微调时间减少一个数量级；索引构建后，在线相似查询延迟降至毫秒级，显著优于现有SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量路网匹配作为输入，若轨迹采样稀疏或路网数据缺失，时空连续性建模效果会下降；预训练阶段计算开销仍较大，且对未见城市的零样本迁移能力尚未验证；实验仅针对车辆轨迹，未测试行人或多模态移动数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索跨城市、跨模态的时空连续预训练，以提升模型在异构轨迹与动态路网下的泛化能力；结合大语言模型或图基础模型，实现统一的路网-轨迹-文本多模态相似度学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大规模轨迹检索、时空表征学习或路网约束下的深度学习，本文提出的解耦-微调范式与时空连续性建模思路可直接借鉴，并为其提供可扩展的实验基准与代码实现线索。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24985v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DarkEQA：低光室内环境中具身问答的视觉-语言模型基准测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yohan Park，Hyunwoo Ha，Wonjun Jo，Tae-Hyun Oh
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24985v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&#39; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在弱光室内场景下执行具身问答的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建DarkEQA基准，在线性RAW空间模拟多级弱光与噪声并走ISP渲染，再测试主流VLM与LLIE模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有VLM在弱光问答中性能显著下降，暴露感知瓶颈，LLIE预处理仅部分缓解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专注弱光具身问答的物理级仿真基准，隔离感知变量支持可解释鲁度分析。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发全天候家用机器人提供弱光视觉推理评估工具与改进方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models are becoming the core reasoning engines for embodied agents, yet current benchmarks only test them under bright, ideal lighting. Real-world 24/7 deployment requires robustness to night-time or dark indoor scenes, a gap that has received little systematic attention. The authors argue that without explicit low-light evaluation, we cannot trust VLMs to drive agents that must operate safely around the clock.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DarkEQA decouples perception from control by providing an egocentric question-answering dataset rendered under five physically accurate low-light levels. Degradations are modeled in linear RAW space: irradiance is scaled to mimic illumination drop, realistic sensor noise is injected, and an ISP-style pipeline produces final sRGB images. Questions target EQA-relevant primitives (object presence, color, spatial relation, count, room type) so that accuracy drops can be attributed to perception rather than policy. The benchmark is fully open-source and includes 3.2 k images paired with 16 k human-written questions.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>All tested state-of-the-art VLMs (BLIP-2, InstructBLIP, LLaVA-1.5, etc.) show dramatic accuracy drops as light level decreases, with absolute performance falling 20–45 % between daylight and 1 lux conditions. Low-Light Image Enhancement models used as pre-processing help only marginally (+2–4 %) and sometimes hurt VLM accuracy by introducing artifacts. The largest degradation occurs for color and count questions, indicating that dark noise overwhelms fine-grained attribute extraction. These results establish the first quantitative evidence that current VLMs are not ready for nighttime embodied deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to static snapshots; it does not evaluate temporal consistency or active illumination control that a real robot might use. All scenes are synthetic (Habitat-Matterport 3D), so real-sensor chromatic aberration or non-linear glare is only approximated. The benchmark currently covers English questions and five light levels—finer granularity and multilingual extensions remain future work.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend DarkEQA to include active perception scenarios where the agent can control lighting or move to brighter spots, and develop VLM architectures that integrate RAW-domain denoising with reasoning to recover robustness without external LLIE pre-processing.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves deploying embodied agents, VLM robustness, or low-light computer vision, DarkEQA provides the first standardized diagnostic tool to measure and improve perception accuracy under realistic nighttime conditions.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>