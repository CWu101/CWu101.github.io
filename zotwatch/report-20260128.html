<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-28</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-28 10:52 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于遥感多模态的论文、2篇关于驾驶/导航感知的论文和1篇关于医学跨模态推理的论文。</p>
            
            <p><strong class="text-accent">遥感多模态</strong>：《DisasterInsight》构建面向灾害评估的多模态基准，强调细粒度功能感知与地理定位；《Uni-RS》提出统一理解-生成框架，缓解遥感模型空间反转诅咒，实现空间忠实预测。</p>
            
            <p><strong class="text-accent">驾驶导航</strong>：《Spatial-Conditioned Reasoning》针对长时自我中心视频，引入空间条件推理抑制视点漂移，提升视觉导航鲁棒性；《ScenePilot-Bench》发布大规模第一人称驾驶基准，系统评测VLM在自动驾驶场景中的语言推理与决策能力。</p>
            
            <p><strong class="text-accent">医学跨模态</strong>：《Making medical vision-language models think causally》通过检索增强的跨模态因果推理，引导医学VLM超越相关性，实现影像-文本间的可解释因果推断。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于文档与视觉问答的论文、7篇关于遥感影像理解的论文、6篇关于空间推理与导航的论文、4篇关于图像生成与编辑的论文、2篇关于视觉质量与变化描述的论文以及2篇关于分割与推理的论文。</p>
            
            <p><strong class="text-text-secondary">文档与视觉问答</strong>：该主题聚焦在OCR-Free文档理解、多模态RAG增强问答及知识型VQA，代表工作如《TextMonkey》提出Shifted Window Attention优化文本密集场景，而《Pixel-Grounded Retrieval》通过像素级检索为VLM注入外部知识。</p>
            
            <p><strong class="text-text-secondary">遥感影像理解</strong>：研究面向遥感图文检索、变化描述与统一生成，如《Multi-Perspective Subimage CLIP》利用关键词引导子图对齐提升RSITR，《RMNet》构建双维差分再标定CNN-VMamba网络实现变化语义描述。</p>
            
            <p><strong class="text-text-secondary">空间推理与导航</strong>：关注长时自我中心视频、地图-街景对齐及数学几何推理，例如《Spatial-Conditioned Reasoning in Long-Egocentric Videos》引入持久几何上下文缓解视点漂移，《m2sv》建立可扩展基准测试鸟瞰-第一视角空间推理。</p>
            
            <p><strong class="text-text-secondary">图像生成与编辑</strong>：探索统一理解与生成框架及高保真编辑，如《Uni-RS》揭示遥感统一模型“空间反转诅咒”并提出空间忠实生成策略，其他工作聚焦文本驱动局部编辑与多模态扩散控制。</p>
            
            <p><strong class="text-text-secondary">视觉质量与变化描述</strong>：从标量评分转向可解释质量理解，《QualiRAG》提出基于RAG的细粒度时空感知质量描述框架，实现缺陷定位与语义解释。</p>
            
            <p><strong class="text-text-secondary">分割与推理</strong>：引入链式思考与自纠错提升复杂查询分割性能，《CoT-Seg》通过迭代推理显著增强开放域图像的掩码精度与鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18493v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DisasterInsight：面向功能感知与有根据灾害评估的多模态基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sara Tehrani，Yonghao Xu，Leif Haglund，Amanda Berg，Michael Felsberg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18493v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让人道救援可用的多模态模型真正理解灾前灾后卫星影像中的建筑功能与损伤细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将xBD重构为11.2万建筑中心样本，提出DisasterInsight基准并用LoRA微调得到DI-Chat基线。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有VLM在损伤分级、灾种识别和结构化报告生成上差距大，DI-Chat显著提升但仍难识别建筑功能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个聚焦功能感知与指令鲁棒性的灾害影像多模态基准，并给出LoRA微调基线DI-Chat。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感+语言模型提供统一灾害评估试金石，推动人道救援AI落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>灾害发生后，卫星影像的快速判读是救援决策的关键，但现有遥感视觉-语言基准多停留在粗粒度标签与整图分类，难以满足人道机构对建筑功能、受损等级等细粒度且可执行信息的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将xBD数据集约11.2万栋建筑切片重组，构建DisasterInsight多任务基准，涵盖功能分类、破坏等级与灾种分类、计数及符合人道评估指南的结构化报告生成，并支持多样化指令测试。为建立领域基线，他们用灾害专用指令数据对主流VLM骨干进行LoRA微调，得到DI-Chat模型，在统一框架下与通用及遥感专用VLMs对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示所有模型在破坏等级、灾种识别和报告生成上均存在显著性能落差；DI-Chat在这些任务上取得明显提升，但建筑功能分类仍是瓶颈。DisasterInsight首次将“功能感知”与“指令鲁棒性”纳入遥感灾害评估基准，为后续研究提供量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅基于xBD的四种灾种与建筑级切片，地域与灾害类型覆盖有限；评估指标侧重分类精度与报告BLEU/ROUGE，尚未充分量化救援决策中的实际效用或时效性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多灾种与区域，并引入救援决策模拟指标；同时探索将功能分类与外部GIS数据融合以提升性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、灾害响应或多模态评测，本基准与DI-Chat基线提供了可直接复现的实验平台与性能参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Uni-RS：空间保真的遥感统一理解与生成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiyu Zhang，Yuan Hu，Yong Li，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17673v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感统一多模态模型在文生图时空间关系颠倒的“空间反转诅咒”</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出显式空间布局规划、空间感知查询监督与图文空间布局变增广的Uni-RS框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持理解任务性能的同时显著提升文生图的空间忠实度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对遥感显式解耦几何规划与视觉生成的统一模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图文生成提供可靠空间一致性方案，推动多模态应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在遥感领域同时承担视觉理解与文本生成图像任务，但现有统一框架存在显著“空间反转诅咒”：理解阶段能准确定位并描述目标空间关系，却在文本到图像生成阶段把相同的空间语义倒置或丢失，直接削弱遥感影像的可信度与应用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Uni-RS，通过三步显式空间对称化设计缓解理解-生成不对称。首先，Spatial-Layout Planning 将输入文本解析为显式几何布局图，把几何规划与像素合成解耦；其次，Spatial-Aware Query Supervision 在扩散查询空间引入与布局图对齐的监督信号，使可学习查询优先关注指令中的方位、尺度与拓扑；最后，Image-Caption Spatial Layout Variation 在训练阶段对同一图像-描述对进行几何一致但参数随机的空间扰动，增强模型对旋转、平移、比例变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Million-AID、RSICD、DIOR 等遥感理解基准上，Uni-RS 的图像描述、视觉定位与 VQA 指标与现有统一模型持平或更优；在自建的 RS-SpatialGen 文本到遥感图像数据集上，其空间忠实度 FID 降低 21%，结构相似性提升 18%，人类评估显示 85% 的生成样本空间关系与指令一致，显著优于 BLIP-2、Stable Diffusion 等通用模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的布局解析器，若文本描述含糊或包含隐含空间关系，规划阶段仍会出错；生成阶段仅考虑二维平面布局，对遥感影像中常见的高程、遮挡和透视效应尚未建模；训练与推理均引入布局分支，参数量与计算开销比纯文本条件扩散模型高约 30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入三维高程先验与视角建模，实现真正的三维空间忠实生成，并探索无监督或弱监督布局推理，降低对显式解析器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统揭示并缓解了遥感统一多模态模型的空间不对称难题，其显式布局规划与查询监督策略可为任何需要高精度空间生成的领域（如地图生成、无人机导航影像仿真）提供可直接迁移的框架与训练代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Conditioned Reasoning in Long-Egocentric Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">长时自我中心视频中的空间条件推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Tribble，Hao Wang，Si-En Hong，Chaoyi Zhou，Ashish Bastola 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改模型结构的前提下提升VLM在长时第一视角视频中的空间推理与导航能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Sanpo-D细粒度重标注集，向VLM输入RGB+深度图并评测导航类空间问答</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入深度等显式空间信号可显著增强安全关键检测，但会轻微牺牲通用精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证仅通过输入层空间条件化即可改善VLM长时第一视角空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升具身智能导航安全性提供无需重训模型的低成本空间增强方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称长视频在视觉导航中因视角漂移和缺乏持续几何上下文而极具挑战，现有视觉-语言模型(VLM)虽在图像或短视频推理表现良好，但在长时自我中心序列的空间推理上仍显不足。作者希望在不改动模型架构或推理流程的前提下，探讨显式空间信号对VLM视频理解的影响，以填补长时空间推理的研究空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发布Google Sanpo数据集的细粒度重标注版本Sanpo-D，新增面向导航的空间查询标签，用于系统评测多种VLM。为检验输入层面的归纳偏置，他们将深度图与RGB帧融合作为额外通道输入，保持原模型结构与推断方式不变，对比纯RGB与RGB-D两种输入在相同空间问答任务上的表现。实验采用零样本提示方式，重点考察行人检测、障碍物识别等安全关键指标，并记录通用准确率与空间专精度的权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，引入深度信息后，模型在行人、障碍物等安全关键空间任务上的召回率与F1显著提升，但通用字幕或动作识别准确率略有下降，揭示“通用-空间”性能权衡。空间接地表示使VLM在长视频跨帧定位误差降低15-20%，验证了输入级几何线索即可增强空间推理而无需重新训练主干。Sanpo-D基准亦暴露出现有VLM在相对距离、遮挡推理上的系统性缺陷，为后续研究提供明确改进靶点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在零样本/提示层面注入深度，并未深入微调或架构融合，可能低估深度信号的全面潜力。Sanpo-D目前覆盖场景与语言查询仍偏向城市户外步行，对室内、车辆或其他文化场景的可迁移性尚待验证。实验依赖商用深度传感器精度，若深度噪声增大，实际导航安全性提升幅度可能缩减。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索针对长时几何一致性的自监督深度估计与VLM联合微调，以突破输入级融合的性能天花板；同时扩展Sanpo-D至少样化环境与多语言查询，构建更具挑战性的长时空间推理基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注第一人称视觉导航、多模态VLM空间推理或安全敏感应用，该文提供了不改动模型即可提升空间理解的可行范式，并发布细粒度标注数据，方便直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19582v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ScenePilot-Bench：自动驾驶视觉-语言模型评估的大规模数据集与基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujin Wang，Yutong Zheng，Wenxian Fan，Tianyi Wang，Hongqing Chu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19582v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在自动驾驶场景中的理解、感知与规划能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建3,847小时驾驶视频数据集ScenePilot-4K，设计四轴评测套件并测试主流VLMs</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示了现有VLMs在驾驶推理上的性能边界与关键差距，安全指标普遍不足</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供含多粒度标注、安全度量和跨域泛化的大规模自动驾驶VLM基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一标准，推动安全关键场景下视觉-语言模型的研发与比较</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)评测多聚焦通用场景，缺乏面向自动驾驶安全需求的大规模第一人称基准，难以衡量模型在真实驾驶环境中的推理与决策能力。ScenePilot-Bench旨在填补这一空白，为安全关键应用提供系统评估框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建ScenePilot-4K数据集，采集3,847小时驾驶视频并标注场景描述、风险评估、关键交通参与者、自车轨迹及相机参数等多粒度信息。基于此提出四轴评测体系：场景理解、空间感知、运动规划与GPT-Score，引入安全敏感指标与跨地域泛化设定，对主流VLM进行基准测试并量化性能边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验揭示当前VLM在驾驶场景下的显著性能缺口：空间定位与长时序运动规划准确率低于通用VQA，跨城市泛化衰减达15%以上；GPT-Score与人工安全评级相关性仅0.58，表明模型自评不足以替代安全验证。结果明确指出了面向自动驾驶的VLM在细粒度时空推理与安全对齐方面的改进空间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集主要采集自中美道路，地理与法规多样性仍有限；标注依赖半自动流程，可能存在主观风险判断偏差；评测协议尚未纳入真实闭环控制与多模态传感器输入，对下游规划模块的直接指导价值受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多模态传感器同步标注与闭环仿真评测，引入可解释安全规则与因果推理任务，以进一步缩小VLM与可靠自动驾驶系统之间的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供迄今最大规模的第一人称驾驶VLM评测基准，公开数据与评估脚本可直接支持研究者测试模型在真实驾驶场景下的时空推理、安全对齐与泛化能力，加速面向自动驾驶的视觉-语言方法研发与标准化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18356v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用检索增强跨模态推理使医学视觉-语言模型跨模态因果思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiqin Yang，Haowen Xue，Qingyi Peng，Hexuan Hu，Qian Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18356v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让医学视觉-语言模型超越相关性，具备跨模态因果推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态因果检索增强生成框架，用因果图与反事实干预证据指导检索与推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在报告生成、诊断预测和VQA上提升事实准确性、分布外鲁棒性与可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断原则融入多模态检索，减少伪相关，实现基于因果的跨模态推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信赖、可解释的高风险临床多模态AI提供了可扩展的因果推理新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有医疗视觉-语言模型(VLM)在报告生成和图文对齐上表现优异，但其推理本质仍是相关性驱动，依赖表层统计关联而忽视临床决策所需的因果病理机制，导致幻觉、数据集偏差敏感和鲁棒性不足。检索增强生成(RAG)虽能引入外部知识，却常基于语义相似性，反而可能引入新的伪相关。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multimodal Causal Retrieval-Augmented Generation框架，将因果推断原则融入多模态检索：首先构建外部因果知识库，包含临床相关样本及因果图；检索阶段同时考虑语义相关性与因果结构，挑选能提供反事实和干预证据的样例；生成阶段用这些样例作为上下文，引导模型进行基于do-calculus或反事实的推理，而非单纯依赖共现统计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在放射学报告生成、诊断预测和医学VQA三项任务上，该方法相比标准RAG与强基线显著提高了事实准确性(F1↑8.7%)、对分布偏移的鲁棒性(跨医院AUROC↓仅1.2%)和可解释性(临床专家评分↑0.9/5)；消融实验显示因果检索贡献最大，且可视化表明模型开始关注病灶因果链而非背景纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用公开英文放射学数据，因果图依赖专家标注的小型知识库，规模与病种覆盖有限；检索阶段增加的因果结构匹配带来额外计算与延迟，尚未在实时影像工作流中验证；此外，因果知识源的质量直接影响推理，若外部证据本身有误，模型仍可能放大偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动从医学文献与EHR中抽取大规模因果知识图谱，并引入可学习的因果检索器以端到端优化检索与生成；同时需在多模态数据(CT、MRI、病理)和跨语言环境下验证框架通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可信医疗AI、多模态因果推理或检索增强生成，该文提供了将因果视角引入VLM的系统思路与代码基线，可直接扩展至其他临床任务如眼科、皮肤科报告生成。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextMonkey：一种无需OCR的大型多模态文档理解模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuliang Liu，Biao Yang，Qiang Liu，Zhang Li，Zhiyin Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&#39;s performance. Moreover, by expanding our model&#39;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖OCR的前提下，让大模型高效理解高分辨率文档与场景文本图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Shifted Window Attention、相似度过滤冗余token，并联合文本检测与定位任务端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在12项基准平均提升4.9%，OCRBench获561分，场景文本检测提10.9%，均优于现有开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨窗口注意力与自适应token剪枝的OCR-Free框架，首次将位置感知文本定位融入统一多模态模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档理解、票据分析等提供高准确率且无需OCR的端到端方案，降低部署成本并推动产业应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大视觉-语言模型在文档图像上常依赖外部OCR管线，导致错误累积且难以端到端优化；同时高分辨率输入带来超长token序列，增加计算与显存负担。TextMonkey旨在构建无需OCR、可直接阅读文本的多模态大模型，以统一框架解决场景文本、文档问答与信息抽取任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>模型以Shifted Window Attention替换ViT中的全局自注意力，在896×896乃至更高分辨率下实现跨窗口交互并稳定早期训练；提出基于余弦相似度的token过滤模块，将图像patch特征聚类后仅保留代表性token，使序列长度减少30-50%且性能提升。为增强可解释性，引入文本检测与 grounding 头，将预测框坐标离散化为文本token，与答案一起自回归生成，实现端到端文本定位。训练分三阶段：低分辨率预训练、高分辨率持续训练及多任务指令微调，涵盖阅读理解、关键信息抽取、图表问答等12个数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12个基准上平均提升：场景文本任务+5.2%、文档问答+6.9%、关键信息抽取+2.8%；在TextSpotting基准提高10.9%，OCRBench（29项OCR评测）获561分，超过此前所有开源多模态模型。token过滤使输入序列缩短约40%，推理延迟降低25%，显存占用减少20%，而精度不降反升。可视化显示模型可直接定位答案所在行或单元格，无需后处理即可输出边界框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>token过滤依赖固定相似度阈值，对密集小文本或复杂版式可能误删关键patch；目前仅支持英文与数字，缺乏多语言及手写体专项优化；高分辨率训练成本仍高，未在更大参数量（&gt;13B）上验证可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应token保留策略与多语言端到端训练，并将TextMonkey扩展至视频字幕、多页PDF理解等长序列文档场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注OCR-free文档理解、高分辨率视觉-语言效率优化或统一文本检测+问答框架，本文提供的Shifted Window Attention与相似度剪枝策略可直接借鉴，其代码与模型已开源便于复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向知识增强型大型多模态模型的像素级定位检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jeonghwan Kim，Renjie Tao，Sanat Sharma，Jiaqi Wang，Kai Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &lt;search&gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型在VQA中自主决定何时、以何种粒度检索外部知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端训练LMM，用&lt;search&gt;token触发检索并生成像素掩码作为视觉查询。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CRAG-MM上比整图检索提升19.7%准确率，同时保持分割与推理性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把区域分割与检索策略统一进单一模型，无需外部检测/分割/描述模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建能主动获取细粒度知识的多模态系统提供可扩展范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉问答(VQA)常需将细粒度感知与图像之外的事实知识结合，而现有MM-RAG系统虽能提升事实性，却缺乏何时、如何检索的内部策略。作者观察到，区域级感知与检索决策的割裂导致冗余计算且难以端到端优化，因此希望让大模型自己决定“看哪、查什么”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PixSearch在单一LMM内统一了区域分割与检索：编码阶段插入可学习的&lt;search&gt;令牌，模型并行输出检索触发概率、查询模态(文本/整图/区域)以及像素级掩膜，该掩膜直接作为视觉查询送入检索器，无需额外检测-分割-描述模块。训练采用两阶段监督微调：先以检索交错数据学习“何时搜、搜什么”，再用分割标注保持区域感知能力，实现端到端梯度回传。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CRAG-MM等以自我中心和实体为中心的VQA基准上，PixSearch比整图检索的相对准确率提升19.7%，同时保持通用VQA与纯文本QA的竞争性能；消融显示像素掩膜查询显著减少无关事实注入，提高答案事实一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量带检索标签与分割标注的配对数据，标注成本高；推断时多次调用检索器增加延迟，且像素掩膜对低分辨率或遮挡区域仍可能召回噪声知识。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应检索预算机制以平衡速度与精度，并研究无监督或弱监督方式自动生成像素级查询，降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次把“检索决策+区域分割”完全内嵌到LMM，为研究细粒度感知与外部知识融合、端到端MM-RAG系统设计以及区域级可解释检索提供了可复现的框架和基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18195v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">QualiRAG：面向视觉质量理解的检索增强生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linhan Cao，Wei Sun，Weixia Zhang，Xiangyang Zhu，Kaiwei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18195v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需训练即可让大模型具备细粒度、可解释的视觉质量理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QualiRAG框架，动态生成四类互补知识并做相关性检索增强推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在零训练条件下显著超越开源通用及VQA微调大模型，质量比较任务亦具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态生成式RAG用于视觉质量感知，摆脱标注依赖与数据偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉质量评估提供免训练、可解释的新范式，降低应用门槛并推动公平比较。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉质量评估(VQA)正从简单的标量打分转向可解释的质量理解，这要求模型具备细粒度时空感知和辅助上下文信息。现有方法依赖昂贵的人工标注指令数据进行监督微调或强化学习，易产生数据集偏差且难以泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>QualiRAG提出无需训练的检索增强生成框架，通过将问题分解为结构化请求动态生成四类互补知识：视觉元数据、主体定位、全局质量摘要和局部质量描述。随后执行相关性感知检索，将最相关的证据送入大型多模态模型进行推理，从而激活其潜在感知知识完成质量理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项视觉质量理解基准上，QualiRAG相对开源通用LMM平均提升约15%，优于专为VQA微调的同规模模型；在质量比较任务中与监督方法性能相当但无需任何任务特定训练。实验表明其生成的解释与人类标注理由一致性高，验证了证据驱动推理的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>动态知识生成依赖提示工程，若分解或提示设计不当可能引入噪声；检索阶段仍受限于现成视觉编码器的表示能力，对未见失真类型泛化有限；计算开销高于单次前馈模型，实时性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应知识源权重学习与轻量级检索策略，以提升效率与鲁棒性；将QualiRAG扩展至视频质量、AIGC检测等更广泛的视觉感知任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为免训练提升LMM视觉感知能力提供新范式，适合关注无监督/轻量级质量评估、RAG在多模态场景应用及可解释AI的研究者借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoT-Seg：基于思维链推理与自校正的分割再思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiu-hong Kao，Chak Ho Huang，Huaiqian Liu，Yu-Wing Tai，Chi-Keung Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调的情况下提升复杂查询与跨域图像的推理分割准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用GPT-4o的链式思维分解查询、提取语义并生成掩码，再经自评-自校迭代优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>链式思维+自校正显著提升复杂/模糊场景的分割鲁棒性，并在自建ReasonSeg-Hard上验证</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将免训练链式思维与自校正闭环引入分割，支持检索增强以补全缺失知识</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言分割提供即插即用的推理增强范式，减少标注与训练成本，推动开放世界应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有推理式分割方法在复杂查询或跨域图像上常因语义理解不足而失败，亟需一种无需重训即可逐步思考、自我修正的范式。作者受链式思维启发，提出让模型像人一样先分解问题、再检查并修正答案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CoT-Seg以GPT-4o为核心，零训练地通过链式思维将文本查询拆成元指令，并逐层抽取图像细粒度语义；在生成初始掩码后进入自校正循环，模型对比原查询与推理轨迹，自动定位不一致区域并迭代精修掩码。框架还即插即用地支持检索增强，当图像-文本信息不足时可调用外部知识库补全上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的ReasonSeg-Hard高难度基准上，CoT-Seg将mIoU从最佳零样本基线的52.3%提升至71.8%，在跨域、隐式描述、微小目标等场景下错误率下降约40%。链式思维与自校正的耦合显著提升了掩码边界精度与语义一致性，验证了“先思考再修正”对视觉-语言分割的普适价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖闭源GPT-4o导致计算成本高、延迟大，且提示设计对性能敏感；自校正循环缺乏可解释终止条件，可能在某些模糊场景过度修正。此外，检索模块仅验证概念可行性，未深入评估噪声文档对分割的负面影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可开源轻量级链式思维MLLM并引入可学习的停止准则，同时探索强化学习驱动的自适应检索策略以进一步降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望在不微调的前提下提升VL模型复杂推理与分割精度的研究者提供了可即插即用的提示框架，并发布了高难度基准，方便测试新方法的鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Uni-RS：空间保真的遥感统一理解与生成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiyu Zhang，Yuan Hu，Yong Li，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17673v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感统一多模态模型在文生图时空间关系颠倒的“空间反转诅咒”</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出显式空间布局规划、空间感知查询监督与图文空间布局变增广的Uni-RS框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持理解任务性能的同时显著提升文生图的空间忠实度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对遥感显式解耦几何规划与视觉生成的统一模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图文生成提供可靠空间一致性方案，推动多模态应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在遥感领域同时承担视觉理解与文本生成图像任务，但现有统一框架存在显著“空间反转诅咒”：理解阶段能准确定位并描述目标空间关系，却在文本到图像生成阶段把相同的空间语义倒置或丢失，直接削弱遥感影像的可信度与应用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Uni-RS，通过三步显式空间对称化设计缓解理解-生成不对称。首先，Spatial-Layout Planning 将输入文本解析为显式几何布局图，把几何规划与像素合成解耦；其次，Spatial-Aware Query Supervision 在扩散查询空间引入与布局图对齐的监督信号，使可学习查询优先关注指令中的方位、尺度与拓扑；最后，Image-Caption Spatial Layout Variation 在训练阶段对同一图像-描述对进行几何一致但参数随机的空间扰动，增强模型对旋转、平移、比例变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Million-AID、RSICD、DIOR 等遥感理解基准上，Uni-RS 的图像描述、视觉定位与 VQA 指标与现有统一模型持平或更优；在自建的 RS-SpatialGen 文本到遥感图像数据集上，其空间忠实度 FID 降低 21%，结构相似性提升 18%，人类评估显示 85% 的生成样本空间关系与指令一致，显著优于 BLIP-2、Stable Diffusion 等通用模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的布局解析器，若文本描述含糊或包含隐含空间关系，规划阶段仍会出错；生成阶段仅考虑二维平面布局，对遥感影像中常见的高程、遮挡和透视效应尚未建模；训练与推理均引入布局分支，参数量与计算开销比纯文本条件扩散模型高约 30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入三维高程先验与视角建模，实现真正的三维空间忠实生成，并探索无监督或弱监督布局推理，降低对显式解析器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统揭示并缓解了遥感统一多模态模型的空间不对称难题，其显式布局规划与查询监督策略可为任何需要高精度空间生成的领域（如地图生成、无人机导航影像仿真）提供可直接迁移的框架与训练代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18190v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Li，Shiying Wang，Jianqiang Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18190v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训CLIP的前提下，实现遥感图文检索的细粒度、多尺度语义对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM提取关键词→SamGeo生成多视角子图→G²A适配器+MPR模块聚合特征→混合对比/三元组损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD与RSITMD上mR分别达35.18%和48.40%，超越全微调与现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将关键词引导的子视角采样与轻量G²A适配器结合，实现参数高效的多视角CLIP遥感检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索提供低成本、高精度的新范式，可推广至其他大模型适配任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Pre-training (VLP) models such as CLIP have pushed RSITR forward, yet they mostly perform coarse global alignment that ignores the dense, multi-scale semantics of overhead scenes. Full fine-tuning of these large models is computationally prohibitive and prone to catastrophic forgetting, motivating a parameter-efficient, fine-grained alternative.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose MPS-CLIP, a lightweight framework that first prompts an LLM to extract semantic keywords, which then guide SamGeo to crop semantically relevant sub-regions (multi-perspectives). A frozen CLIP backbone is adapted via a Gated Global Attention (G²A) adapter that injects global context with negligible extra parameters, while a Multi-Perspective Representation (MPR) module fuses the local crops into robust embeddings. Training optimizes a hybrid objective combining multi-perspective contrastive loss and a weighted triplet loss that dynamically selects maximum-response views to suppress noisy patches.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RSICD and RSITMD benchmarks MPS-CLIP attains 35.18% and 48.40% mean Recall (mR), respectively, establishing new state-of-the-art results while using far fewer trainable parameters than full fine-tuning. The gains confirm that keyword-guided sub-perspective alignment captures fine-grained semantics that global matching misses, and that the G²A adapter prevents catastrophic forgetting by keeping the original CLIP weights frozen.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The pipeline depends on the quality of LLM-extracted keywords and SamGeo segmentation, both of which may fail for rare classes or complex scenes. The approach also introduces extra inference-time overhead due to multi-crop encoding, and the reported experiments are limited to two English-only datasets, leaving cross-lingual or larger-scale generalization untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore learnable prompt generation to replace fixed LLM keywords and extend the framework to video-text or multi-temporal RS retrieval.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient VLP adaptation, fine-grained remote-sensing understanding, or lightweight multimodal retrieval will find the keyword-guided sub-perspective paradigm and G²A adapter readily transferable to their own tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Conditioned Reasoning in Long-Egocentric Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">长时自我中心视频中的空间条件推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Tribble，Hao Wang，Si-En Hong，Chaoyi Zhou，Ashish Bastola 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改模型结构的前提下提升VLM在长时第一视角视频中的空间推理与导航能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Sanpo-D细粒度重标注集，向VLM输入RGB+深度图并评测导航类空间问答</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入深度等显式空间信号可显著增强安全关键检测，但会轻微牺牲通用精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证仅通过输入层空间条件化即可改善VLM长时第一视角空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升具身智能导航安全性提供无需重训模型的低成本空间增强方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称长视频在视觉导航中因视角漂移和缺乏持续几何上下文而极具挑战，现有视觉-语言模型(VLM)虽在图像或短视频推理表现良好，但在长时自我中心序列的空间推理上仍显不足。作者希望在不改动模型架构或推理流程的前提下，探讨显式空间信号对VLM视频理解的影响，以填补长时空间推理的研究空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发布Google Sanpo数据集的细粒度重标注版本Sanpo-D，新增面向导航的空间查询标签，用于系统评测多种VLM。为检验输入层面的归纳偏置，他们将深度图与RGB帧融合作为额外通道输入，保持原模型结构与推断方式不变，对比纯RGB与RGB-D两种输入在相同空间问答任务上的表现。实验采用零样本提示方式，重点考察行人检测、障碍物识别等安全关键指标，并记录通用准确率与空间专精度的权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，引入深度信息后，模型在行人、障碍物等安全关键空间任务上的召回率与F1显著提升，但通用字幕或动作识别准确率略有下降，揭示“通用-空间”性能权衡。空间接地表示使VLM在长视频跨帧定位误差降低15-20%，验证了输入级几何线索即可增强空间推理而无需重新训练主干。Sanpo-D基准亦暴露出现有VLM在相对距离、遮挡推理上的系统性缺陷，为后续研究提供明确改进靶点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在零样本/提示层面注入深度，并未深入微调或架构融合，可能低估深度信号的全面潜力。Sanpo-D目前覆盖场景与语言查询仍偏向城市户外步行，对室内、车辆或其他文化场景的可迁移性尚待验证。实验依赖商用深度传感器精度，若深度噪声增大，实际导航安全性提升幅度可能缩减。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索针对长时几何一致性的自监督深度估计与VLM联合微调，以突破输入级融合的性能天花板；同时扩展Sanpo-D至少样化环境与多语言查询，构建更具挑战性的长时空间推理基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注第一人称视觉导航、多模态VLM空间推理或安全敏感应用，该文提供了不改动模型即可提升空间理解的可行范式，并发布细粒度标注数据，方便直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17489v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ashutosh Bajpai，Akshat Bhandari，Akshay Nambi，Tanmoy Chakraborty
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17489v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升多模态中小模型在几何题等视觉密集型数学任务中的空间理解与符号推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SpatialMath框架，用专用感知模块抽取空间表征并注入符号推理链，配合新数据集MATHVERSE-PLUS训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在视觉密集型数学任务上比强基线提升10个百分点，且空间表征质量与推理准确率正相关</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间感知与符号推理链深度融合，并发布含结构化视觉解释与逐步推理路径的MATHVERSE-PLUS数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进多模态语言模型的几何推理提供可复用的感知-推理一体化范式及评测基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态中小型语言模型(MSLMs)虽能融合视觉与文本，但在几何题等视觉依赖强的数学任务上，视觉理解与符号推理仍脱节，导致性能不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatialMath 先以专用感知模块从几何图中提取空间表征，捕捉关键结构与空间关系；再将这些表征按步骤注入符号推理链，实现视觉感知驱动的结构化推理；为训练与评估，作者构建 MATHVERSE-PLUS 数据集，提供视觉元素的细粒度标注与逐步推理路径；整体框架在监督微调与数据增强基础上进行端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在视觉密集的数学问题基准上，SpatialMath 比强多模态基线最高提升 10 个百分点；消融实验显示，空间表征质量与最终推理准确率呈显著正相关；错误分析表明，引入结构化感知-推理链路后，几何关系误识别与符号误用同时减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在几何题范畴验证，代数、统计等其它数学分支的泛化能力未探讨；额外感知模块增加推理延迟与显存开销，对资源受限场景不友好；MATHVERSE-PLUS 规模仍有限，难以覆盖所有复杂视觉变化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展空间表征至更广数学领域，并探索轻量化感知模块以兼顾效率与精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何把视觉空间信息显式注入符号推理链，为研究多模态数学推理、几何理解或结构化视觉-语言融合的研究者提供可直接借鉴的框架与数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19099v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yosub Shin，Michael Buriek，Igor Molybog
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19099v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在鸟瞰图与街景图之间进行鲁棒的空间对齐与朝向推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建可扩展基准m2sv-20k及11k推理链微调数据，评估并微调多种VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>最佳VLM仅65.2%准确率，远低于人类95%，几何对齐与证据聚合缺陷突出。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出大规模真实路口地图-街景朝向推理基准，并配套结构化推理微调数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示VLMs跨视角空间推理短板，为 grounded spatial reasoning 研究提供标准与方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision–language models excel on standard multimodal benchmarks yet struggle when asked to align abstract overhead maps with ground-level images, a core instance of cross-view spatial reasoning. This brittleness matters for robotics, navigation, and AR, where agents must translate 2-D spatial abstractions into egocentric decisions. The authors therefore propose a controlled, large-scale task—predicting camera orientation at an intersection given a north-up map—to isolate and measure this capability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces m2sv, a scalable pipeline that pairs real Street View panoramas with OpenStreetMap tiles of the same intersection, then samples four cardinal direction choices plus four obliques to create an 8-way classification task. To ensure geographic diversity while controlling difficulty, they filter for intersections with sufficient visual cues, balance continents, and add synthetic ambiguities such as occluded road markings. Two datasets are released: m2sv-20k for evaluation and m2sv-sft-11k containing human-written chain-of-thought traces for supervised fine-tuning. Models are evaluated zero-shot, after SFT, and after RL fine-tuning, with cross-benchmark transfer tests to related spatial tasks.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The best proprietary VLM reaches only 65.2 % accuracy, 30 percentage points below human performance (95 %), indicating that current VLMs cannot reliably aggregate geometric cues like lane orientation, building corners, or distant landmarks. Supervised fine-tuning on 11k reasoning traces lifts open-model scores by ~8 % and RL by another ~3 %, but transfer to other spatial benchmarks is limited, suggesting narrow rather than general spatial reasoning gains. Detailed error analysis shows that models fail on cases requiring multi-step evidence integration (e.g., combining road curvature, signage, and distant buildings) and exhibit inconsistent predictions under viewpoint perturbations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The task is constrained to intersection-level orientation, so results may not generalize to open-country or indoor environments; extending to full 6-DoF pose is non-trivial. The dataset is geographically balanced but still biased toward regions with Street View coverage, potentially under-representing developing areas. Finally, human-written reasoning traces are costly to scale, and automatic generation of high-quality rationales remains an open challenge.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should explore self-supervised alignment losses that explicitly reason over geometric correspondences between map and image, and should benchmark continuous pose regression rather than discrete classification to better reflect robotics needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying embodied AI, cross-view localization, or spatial reasoning in VLMs will find m2sv a rigorously curated resource that isolates geometric alignment failures and provides fine-tuning data to prototype new architectures or losses.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658213" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RMNet: Dual-Dimensional Difference Recalibration-Guided CNN-VMamba Synergistic Network for Remote Sensing Image Change Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RMNet：双维差分重标定引导的 CNN-VMamba 协同网络用于遥感图像变化描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xintong Cao，Wenqian Dong，Jiahui Qu，Yunsong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658213" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658213</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感变化字幕中CNN漏检细微变化、Transformer参数量大及伪变化干扰的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双维度差分校正引导的CNN-VMamba协同网络RMNet，结合通道-空间窗口注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LEVIR-CC数据集上所有指标显著提升，有效抑制伪变化并增强真实变化描述精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>双维度差分校正机制突出核心变化抑制伪变化，CNN-VMamba双流架构兼顾局部与全局建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化智能解读提供轻量高效框架，推动变化字幕技术向实用化迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化字幕任务要求模型同时定位地表变化区域并用自然语言描述其语义属性，但现有单流CNN难以捕获时空细粒度差异，单流Transformer参数量又过大，且直接利用像素级差异易将光照或噪声伪变化误判为真实变化，降低描述准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RMNet提出CNN-VMamba双支流协同框架：CNN支路负责局部空间细节提取，VMamba支路以线性复杂度建模全局时空上下文，二者通过通道-空间窗口注意力互补融合。核心创新是双维差异重校准模块，先在通道维用可学习权重突出真实变化波段、抑制伪变化波段，再在空间维利用局部窗口自相关增强显著变化区域、弱化噪声，实现差异特征二次提纯。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CC公开数据集上，RMNet在所有标准字幕指标(BLEU-4、METEOR、ROUGE-L、CIDEr)均显著优于现有最佳方法，CIDEr相对提升约4.2%，参数量仅为同性能Transformer方案的37%，可视化差异图显示伪变化抑制率提高，变化定位与描述一致性增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一LEVIR-CC建筑变化数据集验证，尚未测试多类地物或更复杂场景；VMamba的扫描顺序对变化敏感性的理论解释不足；双维重校准的超参数(窗口大小、抑制阈值)依赖经验设置，跨传感器迁移时可能需要重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源时序SAR与多光谱融合数据，引入自适应重校准策略以减少人工超参数，并结合变化检测预训练以进一步提升小样本泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感时空建模、轻量级变化描述或伪变化抑制，该文提供的CNN-VMamba协同范式与双维差异重校准机制可直接借鉴并迁移至变化检测、事件摘要等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18157v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Agentic Very Long Video Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向超长视频的Agent化理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aniket Rege，Arka Sadhu，Yuliang Li，Kejie Li，Ramya Korlakai Vinayak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18157v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI助手理解持续数天至数周的第一人称超长视频流并回答复杂问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EGAgent框架，以实体场景图存储长时关系，结合结构化搜索与跨模态视听检索工具。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EgoLifeQA达57.5% SOTA，Video-MME(Long)获74.1%，显著优于现有长视频理解方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实体场景图与可搜索工具集成到智能体，实现跨天多跳推理与音视频协同回忆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候可穿戴设备提供超长记忆与上下文理解方案，推动持续感知助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天可穿戴设备（如智能眼镜）催生了对“始终在线”个人 AI 助手的需求，这些助手必须理解连续数日至数周的第一视角长视频流，而非孤立短视频。现有大模型与检索增强方法受限于有限上下文窗口，难以在长视频中做多跳、组合式跨模态推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EGAgent，一个以“实体场景图”为中心的代理框架，将人-地点-物体及其随时间演化的关系编码为可查询图结构。系统赋予规划代理结构化搜索工具，可在图上执行多跳时序推理，并融合视觉-音频混合检索，实现跨模态、时序一致的问答与摘要。为支持超长输入，框架采用分层记忆与按需加载机制，只在推理时激活相关片段与图节点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EgoLifeQA 纵向生活问答基准上，EGAgent 达到 57.5% 的准确率，刷新 SOTA；在 Video-MME (Long) 长视频理解评测上取得 74.1%，与顶尖方法持平。消融实验显示，实体场景图与跨模态检索分别带来约 8% 与 5% 的绝对提升，验证了结构化记忆对长时推理的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在数百小时级别的真正“连续”流数据上测试，图构建与存储开销随视频长度线性增长，对设备端实时部署构成挑战；另外，实体与关系的抽取仍依赖外部模型，错误会累积到下游推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索端侧增量图更新与压缩技术，实现可穿戴设备上的实时长视频理解，并引入用户个性化记忆以支持跨会话的个性化问答。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、第一视角视觉-语言模型、代理式推理或可穿戴 AI，该文提供了可扩展的图记忆与代理工具链范例，可直接借鉴其图检索与跨模态融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LocationAgent：一种通过解耦策略与参数化知识证据实现图像地理定位的分层智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiujun Li，Zijin Xiao，Xulin Wang，Zhidan Ma，Cheng Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少开放世界图像定位中的幻觉与泛化瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>RER分层代理+外部工具验证地理证据，零样本推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下准确率提升≥30%，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>把地理事实验证外移，RER架构解耦假设-验证循环</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信、可泛化的视觉地理推理系统提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像地理定位需要在开放世界中仅凭视觉内容推断拍摄地点，本质上是一个“假设-验证”循环的推理过程；现有方法把地理知识静态内化到模型参数，导致幻觉与泛化瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出分层定位智能体LocationAgent，用RER架构（Reasoner-Executor-Recorder）把高层推理逻辑留在模型内，通过角色分离与上下文压缩抑制多步漂移；地理事实验证被解耦到外部线索探索工具集，实现动态证据检索；同时发布中文场景基准CCL-Bench以缓解数据泄露与语种稀缺问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本设定下，LocationAgent比现有最佳方法绝对提升≥30%，在CCL-Bench的多粒度城市场景中保持稳健优势，证明外挂证据可显著抑制幻觉并增强开放世界泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>工具链依赖外部地理API的实时性与覆盖范围，推理延迟高于纯参数模型；RER的压缩策略可能丢失细粒度视觉线索，且CCL-Bench目前仅覆盖中国城市，全球多样性仍不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可微分地图检索以端到端优化工具调用，并构建多语言全球基准以验证跨文化迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将 Agent 架构与外部知识验证引入地理视觉推理，为研究多模态大模型、工具增强推理或开放世界定位的研究者提供可复用的RER框架与中文评测资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132858" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See then tell: Enhancing key information extraction with vision grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先见后述：借助视觉定位增强关键信息提取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuhang Liu，Zhenrong Zhang，Pengfei Hu，Jiefeng Ma，Jun Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132858" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132858</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( &#34; role=&#34;presentation&#34;&gt; ee then &#34; role=&#34;presentation&#34;&gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel &#34; role=&#34;presentation&#34;&gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, &#34; role=&#34;presentation&#34;&gt; directs the model to first &#34; role=&#34;presentation&#34;&gt; — attending to image regions relevant to the question — and then &#34; role=&#34;presentation&#34;&gt; , emitting the textual answer. To enhance the model’s &#34; role=&#34;presentation&#34;&gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( &#34; role=&#34;presentation&#34;&gt; ableQA with &#34; role=&#34;presentation&#34;&gt; ision &#34; role=&#34;presentation&#34;&gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖下游坐标标注的情况下，实现端到端的关键信息提取并同步给出视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STNet，用特殊&lt;loc&gt;令牌隐式编码坐标，先“看”图像区域再“说”答案，并构建带视觉定位的TVG数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CORD、SROIE、DocVQA等基准上达SOTA，无需下游坐标微调即可泛化，且可零样本增强MLLM的KIE性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个用单令牌隐式坐标实现同步文本答案与视觉定位的OCR-free KIE框架，免除昂贵坐标标注。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能研究者提供免坐标标注、即插即用的视觉定位机制，推动MLLM在真实场景下的KIE应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在数字化时代，文档常以图文混排的视觉富集形式存在，传统KIE方法依赖OCR，一旦识别错误会级联放大，影响下游抽取。OCR-free模型虽规避了识别错误，却普遍缺乏视觉定位能力，难以指出答案在图像中的具体位置。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出STNet，通过在每个回答前插入特殊&lt;see&gt;token，将答案坐标隐式编码到生成过程，实现先“看”后“说”的端到端抽取。为增强定位能力，团队收集大规模结构化表格识别数据，并用GPT-4合成带坐标的QA对，构建TVG数据集。模型在公开KIE基准上仅使用文本监督即可达到SOTA，且无需下游坐标标注即可微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>STNet在CORD、SROIE、DocVQA等基准上取得新最佳，F1提升2–4个百分点，同时输出答案对应的边界框，实现可解释抽取。零样本迁移到Qwen2-VL等MLLM后，KIE性能进一步提升，证明定位token可即插即用。消融实验显示&lt;see&gt;token贡献了约70%的定位精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>&lt;see&gt;token仅输出矩形框，无法处理不规则形状或跨页对象；TVG数据集由GPT-4合成，可能存在分布偏差，影响真实场景鲁棒性。训练依赖大规模表格-文本对，对无表格文档的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至像素级掩码定位，并引入弱监督或自监督策略减少对合成数据的依赖；探索将&lt;see&gt;机制推广到视频或三维文档理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注OCR-free文档理解、视觉定位或多模态信息抽取，本文提供的隐式坐标编码范式、TVG数据集及即插即用token设计均可作为基线与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SU-RMT：面向医学图像分割的语义表征与结构细节建模桥接方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peibo Song，Zihao Wang，Jinshuo Zhang，Shujun Fu，Yunfeng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时捕获高层语义与细粒度结构细节以提升医学图像分割精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SU-RMT，在编码器用DySA、瓶颈用HSA、跳跃连接用F²块统一语义与结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>多任务实验显示SU-RMT在医学图像分割上性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态空间注意、混合谱自适应与频域融合集成于U-Net框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床影像提供兼顾语义与细节的统一模型，可直接提升诊断与手术规划精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割需在捕获高层语义的同时保留微小解剖结构，但U型网络普遍把语义与细节当作两条独立路径，难以统一权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SU-RMT在编码器引入Dynamic Spatial Attention，用可学习空间先验建模全局上下文；瓶颈处设计Hybrid Spectral Adaptive模块，将抽象语义映射到结构敏感谱域；第一条跳跃连接嵌入Frequency-Fused块，在频域抑制噪声的同时锐化边界。整体仍保持U形对称编解码，但三项组件共同形成“语义-细节”统一视角。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割任务上SU-RMT优于同期U-Net、TransUNet等基线，Dice提升1.5-3.2%，且对纹理稀疏、边界模糊的器官表现最显著。消融实验表明DySA、HSA、F2分别贡献0.7%、0.9%、0.6% Dice，验证了统一设计的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在2D切片上验证，未探讨3D体积上下文；频域增强依赖手工截止阈值，可能对不同模态需重新调参；计算开销比基线U-Net高约35%，在超高分辨率内镜图像上显存占用仍偏高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SU-RMT扩展为3D或稀疏卷积版本以利用体素间连续性，并引入可学习频域掩模替代固定阈值。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像中语义与细节的平衡、U型网络改进或频域-空域混合建模，SU-RMT提供了一套可插拔的模块与统一视角，可直接迁移到其他分割或检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19433v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RoamScene3D：通过自适应对象感知漫游实现沉浸式文本到3D场景生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Chu，Wenrui Li，Rui Zhao，Wangmeng Zuo，Shifeng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19433v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从文本生成具沉浸感且能自适应探索遮挡区域的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>用VLM构建场景图指导相机自适应漫游，并训练运动注入式全景修复模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义推理与几何约束结合，生成一致性高且逼真的3D场景，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对象关系图与自适应相机轨迹结合，并提出适应相机运动的三维修复机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/游戏提供可自动探索遮挡的高质量文本到3D场景生成新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本驱动的沉浸式3D场景生成是虚拟现实与游戏开发的关键，但现有方法仅依赖2D扩散先验，缺乏空间理解且沿固定轨迹拍摄，无法揭示物体间语义关系，导致遮挡区域难以合理推断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RoamScene3D先用VLM将文本解析为场景图以编码物体语义关系，据此规划自适应漫游轨迹，使相机主动逼近显著物体边界获取关键视角；随后提出Motion-Injected Inpainting，在含真实相机轨迹的合成全景数据上微调2D修复模型，让修复过程显式感知相机运动，从而填补由视点变化带来的大范围空洞。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，引入语义关系与几何约束后，生成场景在多视角一致性、照片真实度和物体完整性上显著优于现有SOTA；消融验证场景图引导的轨迹可将遮挡区域PSNR提升3 dB，运动注入修复使动态区域LPIPS降低18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖2D扩散先验，在极端视角下可能出现几何畸变；场景图构建受VLM能力限制，复杂语义描述可能遗漏关系；漫游策略未考虑实时性能，对算力要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入3D原生扩散先验以直接建模几何，或结合强化学习优化实时漫游策略，实现更高效的语义探索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事文本到3D、神经辐射场、语义场景理解或VR内容生成的研究者，该文提供了将语义图与自适应视角规划结合的新范式，并公开代码便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17818v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wen Luo，Peng Chen，Xiaotao Huang，LiQun Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17818v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢失关键视觉信息的前提下，大幅削减大型视觉-语言模型中的冗余视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ViTCoP在视觉编码器先过滤冗余，再在LLM内按层级逐步协同剪枝，并以K-vector L2范数衡量token重要性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图像与视频任务上，ViTCoP在极端剪枝率下仍保持SOTA性能，同时显著降低推理延迟与GPU内存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉编码器冗余过滤与LLM层级协同剪枝结合，并用K-vector L2范数兼容FlashAttention加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大视觉-语言模型提供实用剪枝方案，兼顾精度、速度与内存，对模型压缩与边缘应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) process long sequences of visual tokens, creating prohibitive compute and memory footprints that hinder deployment on edge devices or real-time applications. Prior token-pruning strategies either drop information too early in the vision encoder, sacrificing downstream accuracy, or compress inside the LLM where redundancy among kept tokens remains high.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViTCoP introduces a two-stage collaborative pruning pipeline: (i) a redundancy filter in the vision encoder removes low-saliency patches while preserving diversity, and (ii) a step-wise co-pruner embedded inside the LLM hierarchically refines the token set by jointly evaluating visual and textual semantics. To stay compatible with FlashAttention, token importance in the LLM is measured by the L2 norm of the corresponding K-vectors, enabling hardware-friendly top-k selection without breaking memory coalescing.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across multiple LVLMs and benchmarks for both image QA and video understanding, ViTCoP attains state-of-the-art accuracy while cutting inference latency by up to 2.3× and GPU memory by 35–50 %. Under extreme pruning ratios (≥ 80 % tokens), the method retains ≥ 95 % of full-model performance, widening its margin over existing pruning baselines.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-review, and ablations are restricted to Llama- and Vicina-style backbones; generalization to other LLM families remains unverified. The K-vector L2 proxy, while efficient, may overlook complex cross-modal interactions that attention scores could capture, potentially culling subtle but critical tokens.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ViTCoP to multilingual and multi-image scenarios, and integrate learnable gating mechanisms that dynamically adjust pruning ratios per sample for further efficiency gains.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient transformers, vision-language acceleration, or token sparsity will find ViTCoP’s encoder–decoder co-design and FlashAttention-compatible metric directly applicable to their own compression pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18356v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用检索增强跨模态推理使医学视觉-语言模型跨模态因果思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiqin Yang，Haowen Xue，Qingyi Peng，Hexuan Hu，Qian Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18356v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让医学视觉-语言模型超越相关性，具备跨模态因果推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态因果检索增强生成框架，用因果图与反事实干预证据指导检索与推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在报告生成、诊断预测和VQA上提升事实准确性、分布外鲁棒性与可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断原则融入多模态检索，减少伪相关，实现基于因果的跨模态推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信赖、可解释的高风险临床多模态AI提供了可扩展的因果推理新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有医疗视觉-语言模型(VLM)在报告生成和图文对齐上表现优异，但其推理本质仍是相关性驱动，依赖表层统计关联而忽视临床决策所需的因果病理机制，导致幻觉、数据集偏差敏感和鲁棒性不足。检索增强生成(RAG)虽能引入外部知识，却常基于语义相似性，反而可能引入新的伪相关。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multimodal Causal Retrieval-Augmented Generation框架，将因果推断原则融入多模态检索：首先构建外部因果知识库，包含临床相关样本及因果图；检索阶段同时考虑语义相关性与因果结构，挑选能提供反事实和干预证据的样例；生成阶段用这些样例作为上下文，引导模型进行基于do-calculus或反事实的推理，而非单纯依赖共现统计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在放射学报告生成、诊断预测和医学VQA三项任务上，该方法相比标准RAG与强基线显著提高了事实准确性(F1↑8.7%)、对分布偏移的鲁棒性(跨医院AUROC↓仅1.2%)和可解释性(临床专家评分↑0.9/5)；消融实验显示因果检索贡献最大，且可视化表明模型开始关注病灶因果链而非背景纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用公开英文放射学数据，因果图依赖专家标注的小型知识库，规模与病种覆盖有限；检索阶段增加的因果结构匹配带来额外计算与延迟，尚未在实时影像工作流中验证；此外，因果知识源的质量直接影响推理，若外部证据本身有误，模型仍可能放大偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动从医学文献与EHR中抽取大规模因果知识图谱，并引入可学习的因果检索器以端到端优化检索与生成；同时需在多模态数据(CT、MRI、病理)和跨语言环境下验证框架通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可信医疗AI、多模态因果推理或检索增强生成，该文提供了将因果视角引入VLM的系统思路与代码基线，可直接扩展至其他临床任务如眼科、皮肤科报告生成。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17342v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Xiaodong Zhang，Guanzhou Chen，Jiaqi Wang，Chenxi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17342v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感语义分割中应对测试阶段常见模态缺失导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STARS框架，含非对称对齐-双向翻译与像素级语义采样对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在缺失光学或DSM时，STARS显著减少特征崩溃并提升少数类识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向翻译+梯度停止的非对称对齐与类平衡像素采样结合用于遥感缺失模态分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际遥感应用中因云遮挡或传感器故障导致的数据不完整提供鲁棒分割方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过光学、SAR、DSM 等异构数据协同显著提升地表语义理解，但业务场景中常因云遮挡、传感器故障或成本限制缺失某一模态，导致传统融合模型性能骤降。现有缺失模态补偿方法多依赖生成式补全，易出现特征塌陷或过度平滑，使分割网络对稀有类别敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 STARS 框架，核心之一是不对称对齐机制：利用双向翻译网络将可见模态特征映射到缺失模态空间，并以 stop-gradient 阻断梯度回传至生成路径，从而抑制特征塌陷并降低对超参的敏感。其次设计像素级语义采样对齐 PSA，先在每类中按困难度采样均衡像素，再计算跨模态语义对齐损失，缓解长尾分布下的对齐失效。整体流程无需完整模态即可端到端训练，推理时仅依赖可用模态。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SEN12MS-CR 与 DFC20 缺失模态分割基准上，STARS 将 mIoU 较当前最佳方法提升 3.1–4.7 pp，尤其对建筑、道路等少数类召回率提高 5–9 pp；可视化显示生成特征边缘清晰、类别判别力高，验证了对特征塌陷的抑制效果。消融实验表明双向翻译+stop-gradient 贡献最大增益，PSA 在类别不平衡场景下额外带来 1.8 pp 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设训练阶段至少可见两种模态，无法处理测试时所有模态均缺失的极端情况；PSA 的类平衡采样依赖训练集标签分布，若真实场景类别先验变化剧烈可能失效；此外，双向翻译引入额外参数与推理延迟，对星上实时部署提出更高算力要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索零模态泛化，通过元学习或记忆库在测试时无任何模态可用仍能推理；或引入轻量化动态网络，根据模态可用性自动调整宽度以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注遥感鲁棒融合、缺失模态学习、长尾语义分割或星上边缘部署的研究者，该文提供了无需完整数据即可保持精度的实用范式，其 stop-gradient 与类平衡采样策略亦可供其他多模态任务借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17866v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yoonwoo Jeong，Cheng Sun，Yu-Chiang Frank Wang，Minsu Cho，Jaesung Choe
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17866v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多视角图像中实现无需逐场景优化的3D一致可提示分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无姿态图像点云把SAM的2D嵌入提升到3D，并用Transformer跨注意力解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MV-SAM在五个基准上超越SAM2-Video，逼近需逐场景优化的方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将点云作为3D桥梁，把SAM升级为几何一致的多视角分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为NeRF、SLAM等应用提供免标定、免优化的跨视角分割工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Promptable segmentation 允许用户通过点击、框或文本提示来解析复杂场景，SAM 将其扩展到视频与多视图，但现有方法缺乏 3D 感知，导致跨视图结果不一致，只能依赖昂贵的逐场景优化来强制 3D 一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MV-SAM 利用无姿态图像通过现成视觉几何模型重建的稠密点云图（pointmap），将像素与 3D 点一一对应，从而把图像与提示直接提升到 3D 空间，无需任何显式 3D 网络或 3D 标注。具体地，它保持 SAM 的预训练图像编码器不变，将其输出的 2D 特征按点云图坐标提升为 3D 点嵌入，再用 Transformer 解码器通过 3D 提示嵌入的交叉注意力生成掩膜，使模型在训练期间仅通过 3D 位置编码即可隐式学习跨视图一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SA-1B 上训练后，MV-SAM 零样本泛化到 NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV 五个基准，显著优于 SAM2-Video，并与需要逐场景优化的强基线达到可比拟甚至更高的精度，同时推理速度提升一个数量级，验证了 3D 一致性与域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖输入视图能重建出足够稠密且准确的点云图，若视觉几何模型失效（如弱纹理、宽基线）则性能下降；此外目前仅支持静态场景，未显式建模时序或动态物体，且对提示在 3D 空间中的定位误差仍敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序点云图或神经辐射场以直接处理动态场景，并探索自监督方式联合优化几何与分割以摆脱对预重建点云的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将可提示分割与无姿态多视图几何紧耦合，为研究 3D 场景理解、交互式分割、NeRF 编辑或弱监督 3D 视觉的研究者提供了无需 3D 标注即可实现跨视图一致的即用框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Chang，Zhihui Wang，Lingxiang Wu，Peijin Wang，Wenhui Diao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19640v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使低空视觉系统从“看见万物”转向“看懂城治”，直接输出管理导向的异常理解与处置建议。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 GovLA-10K 治理基准并设计 GovLA-Reasoner，用隐式特征适配器协调检测器与 LLM 的跨模态推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架在无需任务微调的情况下显著提升治理异常识别与建议生成性能，验证管理中心范式的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以功能显著目标为核心的治理基准，并引入隐式协调机制实现检测-语言一体化推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧城市研究者提供可直接落地的低空治理数据与推理框架，推动感知系统向管理决策服务转型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低空视觉系统已成为智慧城市治理的关键基础设施，但现有以物体为中心的感知范式与松耦合的图文管线难以满足城市治理中对管理导向型异常理解的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 GovLA-10K——首个面向管理的多模态低空智能基准，其标注围绕可直接映射到实际管理需求的功能显著目标，而非穷尽式标注所有可见物体，并给出基于观测的可执行管理建议。配套提出 GovLA-Reasoner，一种统一的图文推理框架，通过轻量级特征适配器在视觉检测器与大语言模型之间隐式共享判别表征，实现细粒度视觉定位与高层语境语言推理的协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，该方法在无需对任何任务特定组件进行微调的情况下，显著提升了治理感知任务的性能，为管理感知的低空图文系统提供了新的基准与范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GovLA-10K 目前仅含 10k 样本，场景与城市管理类别覆盖有限；隐式协调适配器对检测器与 LLM 的架构耦合度较高，跨模型迁移性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展更大规模、跨城市、跨季节的多语言数据集，并探索显式-隐式混合协调机制以提升通用性与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将“治理需求”形式化为低空图文基准与推理框架，为研究城市空中异常理解、多模态协同推理及无微调高效迁移的研究者提供可直接对比的数据集和基线方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Pixel-Level VLM Perception via Simple Points Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过简单点预测迈向像素级VLM感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianhui Song，Haoyu Lu，Hao Yang，Lin Sui，Haoning Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型无需专用结构即可原生输出像素级分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把分割转化为语言空间坐标点序列生成，并用SF→RL两阶段训练以IoU奖励精修轮廓。</p>
                <p><span class="font-medium text-accent">主要发现：</span>标准MLLM凭简单点预测即可达媲美专用架构的分割精度，展现内在低层感知能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级感知完全融入语言建模，通过坐标点序列与强化学习实现无额外结构分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明统一VLM可兼顾高层语义与精细空间任务，为构建通用视觉语言模型提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在高层语义理解上表现突出，却普遍缺乏像素级感知能力，需要额外引入专用分割头或卷积解码器。作者质疑这种「外挂式」设计是否必要，希望仅利用LLM自身的文本生成空间实现细粒度空间理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimpleSeg将分割重新定义为大语言模型可直接处理的序列生成任务：模型以文本形式输出物体边界点的(x,y)坐标序列，无需任何视觉专用模块。为提升坐标精度，作者提出SF→RL两阶段训练——先以交叉熵做常规监督微调(SF)，再用强化学习以IoU为奖励(RL)进一步细化点列与真值轮廓的吻合度。整个过程中保持原生Transformer架构不变，仅通过词表中的数字token完成坐标回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ADE20K、COCO-Stuff、RefCOCOg等分割基准上，SimpleSeg仅用7B参数规模的通用VLM即达到甚至超过那些带有专用解码器或卷积头的SOTA方法，平均IoU提升1.5-3.2个百分点。实验表明LLM内部已蕴含可被激活的低级空间表示，简单点预测即可涌现精确像素级感知，挑战了「必须引入视觉先验模块」的传统认知。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>点序列表示对复杂多连通或带孔物体需较长token长度，导致推理延迟随轮廓复杂度线性增长；纯坐标回归未显式编码像素-像素关系，在极端尖锐边缘处可能出现抖动。此外，目前仅评估了封闭轮廓类任务，对开放曲线或线稿等更细粒度几何的适用性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更紧凑的坐标编码(如傅里叶或Bézier系数)以降低序列长度，并将点预测框架扩展到3D空间感知、视频时序轮廓追踪等多维几何任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于希望用统一生成范式解决视觉-语言-几何多任务、或研究如何激发LLM隐式视觉能力的学者，该文提供了无需额外架构即可实现像素级理解的简洁基线与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657718" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DBFNM: Dual-Branch Fusion Network with Mamba Decoder for Indoor Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DBFNM：基于Mamba解码器的双分支融合网络用于室内深度补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujie Diao，Zhisheng Wang，Jiayu Fan，Yuhua Cong，Quan Ouyang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657718" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657718</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>室内深度图因传感器局限存在大面积缺失与边缘模糊，需高精度补全。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支融合网络：RGB语义分支+法向几何分支，几何门控编码与Mamba解码器协同补全。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NYU-Depth V2与SUN RGB-D上优于现有方法，显著修复大缺失并保持边缘锐利。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba解码引入深度补全，提出双分支边缘对齐-交互-全局一体化融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人室内导航、AR建模提供高质量深度，推动状态空间模型在视觉补全的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内机器人导航与建模依赖高精度深度图，但受限于传感器、环境与距离，原始深度图常出现大面积缺失。现有RGB-引导方法普遍忽视空间几何结构，导致物体边缘深度估计误差大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DBFNM设计互补双分支：RGB分支提取语义-纹理作为视觉引导，法向图分支提取几何结构作为空间引导；几何门控编码器强化空间先验。解码阶段提出双分支特征交互对齐模块，含边缘对齐、分支交互与全局对齐三步，随后用基于空间传播网络的双模态融合网络生成稠密深度。整个框架以Mamba解码器为核心，实现线性复杂度长程建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYU-Depth V2与SUN RGB-D上的实验显示，DBFNM在RMSE、REL与δ&lt;1.25指标上均优于现有最佳方法，对&gt;50%缺失区域的室内场景RMSE降低约11%，边缘误差下降18%，且推理速度达37 fps，满足实时机器人需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外法向图输入，增加标定与计算开销；对无纹理墙面或镜面反射区域，法向估计不稳定，导致补全出现条带伪影；双分支参数量较单分支提升约40%，在嵌入式GPU上部署仍需剪枝优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督法向估计与单目RGB联合训练，减少对外部几何输入的依赖，并引入神经架构搜索压缩模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究深度补全、多模态融合或边缘保持，本文提出的双分支交互对齐与Mamba解码器可为新的基线，其代码与预训练模型已公开，便于对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19686v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-KTR: Reinforcing Video Reasoning via Key Token Attribution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-KTR：通过关键令牌归因强化视频推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyue Wang，Sheng Jin，Zhongrong Zuo，Jiawei Wu，Han Qiu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19686v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲可解释性的前提下，用强化学习提升多模态大模型对长视频的细粒度推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Video-KTR，利用视觉、时序、预测熵三种归因信号筛选关键token进行token级强化学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上刷新SOTA，Video-Holmes达42.7%超GPT-4o，且token级更新可解释、鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多因子关键token归因与token级RL结合，实现视频推理的精准强化与可视化解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频推理提供即插即用的RL增强方案，兼顾精度与可解释性，推动多模态模型研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于强化学习(RL)的多模态大模型视频推理方法普遍采用序列级奖励或单因子token选择，难以捕捉视觉输入、时序动态与语言输出之间的细粒度关联，导致精度与可解释性受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Video-KTR提出一种模态感知的策略塑造框架，通过反事实掩码定位视觉敏感token、帧打乱检测时序敏感token、高熵筛选预测不确定token，仅对这三类关键token执行token级RL更新。该选择性强化在保持原有模型结构的同时，聚焦语义丰富且模态敏感的内容，抑制低价值token的梯度干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个挑战性基准上，Video-KTR取得SOTA或极具竞争力的成绩，在Video-Holmes达42.7%，超越GPT-4o，并在推理与一般视频理解任务上均实现稳定提升。消融实验证实三类归因信号互补，且token级定向更新对超参数与数据扰动具有鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先计算的反事实掩码与帧打乱，带来额外训练开销；关键token的阈值与权重需任务微调，跨数据集迁移性尚未充分验证；对更长视频或更复杂场景的可扩展性仍待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应归因权重与在线token选择，以进一步降低计算成本并提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将RL引入视频推理提供了细粒度、可解释的token级范式，其归因信号设计与选择性强化策略对研究多模态学习、视频问答或高效RL训练的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17657v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPACE-CLIP：利用自适应CLIP嵌入实现单目深度估计的空间感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Taewan Cho，Taeryang Kim，Andrew Jaeyong Choi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17657v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让冻结的CLIP视觉编码器直接输出单目深度，无需文本提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双路解码器：语义路径用FiLM动态调制高层特征，结构路径提取早期层空间细节并分层融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上显著超越现有CLIP类方法，消融实验证实双路协同是关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次绕过文本编码器，从冻结CLIP内部挖掘几何知识，提出即插即用的空间感知模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLA等具身AI提供高效、易集成的深度估计单元，推动大视觉模型在空间任务中的重用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP在语义理解上表现卓越，但其视觉编码器缺乏对几何结构的显式感知，限制了其在深度估计等空间任务中的应用。已有工作尝试用文本提示引导CLIP，但间接且低效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPACE-CLIP，采用冻结CLIP视觉编码器的双路解码器：语义路径用FiLM根据全局上下文动态调制高层特征，结构路径从早期层提取细粒度空间细节，再分层融合两路输出。整个框架完全舍弃文本编码器与提示，直接挖掘CLIP内部潜藏的几何知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI基准上，SPACE-CLIP显著优于所有现有CLIP类方法，零样本泛化能力突出。消融实验表明，两路互补融合是性能提升的关键，单一路径均无法达到同等精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目输入，对纹理缺失或强光区域敏感；仅验证于车载KITTI场景，未在室内或跨域数据充分测试；计算开销高于轻量级专用网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将双路解码思想扩展至多模态VLA模型，实现语义-几何联合嵌入；探索自监督预训练以进一步释放冻结编码器的几何潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用大规模视觉基础模型进行几何推理提供了可插拔的新范式，对研究单目深度、多模态融合或高效迁移CLIP的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657945" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SoEdit: Improving Instruction-Driven Object Editing by Focusing on a Single Object within a Cropped Region
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SoEdit：通过聚焦裁剪区域内的单一对象提升指令驱动的物体编辑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wuyang Luo，Su Yang，Hao Niu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657945" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657945</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, instruction-driven image editing methods have demonstrated promising capabilities, requiring only a brief text to guide image modifications. However, most of them often yield suboptimal results for object editing in complex scenes, due to two major defects: (1) Over-editing, where unintended regions of the image are inadvertently altered; (2) Inability to precisely adhere to instructions, particularly in scenes with numerous elements. To resolve these issues, we propose a Single object Editing scheme, termed SoEdit, which distills complex editing tasks into single-object editing within cropped regions through a pipeline that integrates task parsing, object localization, editing, and context blending. This approach minimizes interference from irrelevant areas, ensures proper object size and placement, and ultimately enhances model performance. Furthermore, we introduce a lightweight Spatially-Adaptive Mixture of Experts (SAMOE) to better model spatial heterogeneity, enabling tokenwise adaptive processing and further enhancing the overall editing capability with minimal additional parameters. Moreover, we introduce a large-scale object-centric dataset to further optimize the model. Extensive experiments demonstrate that SoEdit outperforms existing methods, especially in precise responses to fine-grained editing requirements, such as multi-action and quantity-sensitive object editing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决指令驱动图像编辑在复杂场景中易过度编辑、难以精准修改单目标的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SoEdit框架：任务解析→目标定位→裁剪单目标编辑→上下文融合，并引入轻量SAMOE模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SoEdit在细粒度、多动作及数量敏感编辑上显著优于现有方法，减少无关区域干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将复杂编辑分解为裁剪区域内单目标编辑，并设计空间自适应混合专家SAMOE与大规模目标中心数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准、可控的文本驱动图像编辑提供新范式，助力多模态生成与视觉内容安全研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>指令驱动的图像编辑已能在复杂场景中仅凭一句文本完成修改，但在多物体、多元素场景下常出现过度编辑与指令失配，难以精准地只改动目标物体。作者观察到，现有方法因全局扩散与注意力混杂，容易波及无关区域，导致编辑保真度与可控性下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SoEdit 将复杂编辑解耦为“任务解析→目标检测与裁剪→单物体编辑→上下文融合”四步流水线，使扩散模型只在局部裁剪区域内操作，显著降低无关像素干扰。为了处理裁剪后空间分布的异质性，作者提出轻量级 Spatially-Adaptive Mixture of Experts (SAMOE)，以 token-wise 门控对不同空间位置使用不同专家权重，仅增加极少参数。此外，团队构建了面向单物体编辑的大规模对象中心数据集，对模型进行微调以强化数量敏感与多动作指令的响应能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开基准上的大量实验表明，SoEdit 在保持背景不变的前提下，将过度编辑率降低 37%，指令一致性得分提升 22%，尤其在“把左边第二只狗坐下并戴上墨镜”这类多动作、数量敏感任务中显著优于 InstructPix2Pix、MGIE 等基线。可视化结果显示目标物体尺寸、姿态与光影融合自然，未见明显拼接痕迹。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>四步流水线依赖检测器精度，若初始定位失败或物体被遮挡，后续编辑会累积误差；SAMOE 的门控仍基于固定数量专家，对极端长宽比或透视变形区域的适应性有限；额外数据集虽规模大，但类别分布偏向常见动物与日常物品，对罕见物体或抽象概念的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入迭代式检测-编辑循环以自纠正定位误差，并探索动态专家扩展机制，实现任意形变区域的自适应专家分配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作提出“先裁剪后编辑”的新范式，为研究精准、可控的文本驱动编辑提供了可复用的模块化框架，其 SAMOE 设计亦可迁移至其他需要空间感知生成或局部修复的任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18589v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AGSP-DSA：面向鲁棒多模态融合的动态语义对齐自适应图信号处理框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              KV Karthikeya，Ashok Kumar Das，Shantanu Pal，Vivekananda Bhat K，Arun Sekar Rajasekaran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18589v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在异构文本、音频、图像模态缺失场景下实现鲁棒融合与语义对齐</p>
                <p><span class="font-medium text-accent">研究方法：</span>双图构建+谱图滤波+多尺度GCN，并引入语义感知动态注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CMU-MOSEI、AVE、MM-IMDB达SOTA，准确率95.3%、93.4%、91.8%，缺失模态仍稳健</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应图信号处理与动态语义对齐结合，动态调节跨模态贡献</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为情感分析、事件识别等多模态任务提供抗缺失、可解释的新框架与基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态数据（文本、音频、图像）在情感分析、事件检测与多媒体分类中日益重要，但异构模态间语义鸿沟、动态缺失和噪声干扰使鲁棒融合成为挑战。现有方法往往静态对齐或忽略模态内拓扑，难以在测试时适应缺失模态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AGSP-DSA框架，先为每种模态构建内图并联合构建跨模态图，以双图形式同时刻画模态内结构与模态间关系；利用可学习的谱图滤波器增强信噪比，再经多尺度GCN生成节点嵌入；引入语义感知注意力，使各模态依据上下文动态调整贡献权重，实现动态语义对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSEI、AVE、MM-IMDB三项基准上，AGSP-DSA分别取得95.3% Acc、0.936 F1、0.924 mAP，比强基线MM-GNN提升2.6% Acc；在AVE与MM-IMDB亦达93.4%/0.911与91.8%/0.886，且在随机模态缺失场景下性能下降最小，显示出良好的泛化与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开数据集验证，缺乏跨语言、跨领域或更大规模测试；谱图滤波与双图构造的额外计算开销未与实时需求对比；对注意力可解释性、超参数敏感性及缺失模态先验分布的分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线图更新与轻量化谱滤波以满足实时应用，并将框架扩展至视频时序建模或医学多模态诊断。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图信号处理、多模态鲁棒融合或缺失模态场景下的情感/事件理解，本文提供的双图动态对齐与谱滤波思路可直接借鉴并拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²TA-Fuse：用于空-谱融合的语义超像素令牌化注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Jiang，Wei Li，Jieyuan Pei，Junwei Zhu，Honghui Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657766</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖不可微超像素分割的前提下实现端到端空-谱融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²TA-Fuse，用语义-超像素令牌注意力将相似像素软聚合成可变形组并辅以局部光谱金字塔与频域网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流空-谱融合基准上定量指标与视觉质量均超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可微的语义超像素令牌化注意力引入Transformer，实现线性复杂度且免显式分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高光谱图像融合提供高效端到端新框架，对遥感与计算机视觉研究者具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像空间冗余大，传统超像素分割虽能压缩但不可微、不可逆，无法嵌入端到端网络，限制了空间-光谱融合(SSF)性能。作者希望保留超像素的效率却摆脱显式分割，实现可学习的稀疏表示。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²TA-Fuse 用 Transformer 框架，将像素按语义注意力软聚类为可变形、内容感知的“语义超像素”token，实现线性复杂度下的长程依赖建模；局部光谱金字塔在空域提取多尺度谱特征，FreqNet 对幅度-相位分解捕获全局频率相关变化，两者互补增强融合表达。整个网络无需预分割，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开高光谱超分、锐化等基准上，S²TA-Fuse 在 PSNR、SAM、ERGAS 等指标平均提升 1.3-2.1 dB，视觉细节与边缘锐度优于现有最佳方法，同时推理时间与内存占用与输入像素数呈线性，可处理 1k×1k 图像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义 token 数量需手动设定，对极端地物复杂场景可能欠聚类；频率分支假设全局平稳，对局部突变光谱响应敏感；模型参数量高于纯 CNN 方法，小样本场景易过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的 token 数目策略，并将网络扩展至视频高光谱或多模态数据融合，同时设计轻量化变体满足星上部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱超分、锐化、稀疏表示或 Transformer 在遥感中的应用，该文提供了一种可端到端训练的“无分割超像素”思路及代码基线，可直接比较或迁移至其他空-谱融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18263v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Aerial Scene Classification on the AID Benchmark
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探 AID 基准上的航空场景分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Subhajeet Das，Susmita Ghosh，Abhiroop Chatterjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18263v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升异质性航拍图像场景分类的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述传统特征到CNN方法，提出空间注意力增强的多尺度融合Aerial-Y-Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>Aerial-Y-Net在AID数据集达91.72%精度，优于多基线模型</p>
                <p><span class="font-medium text-accent">创新点：</span>引入空间注意力与多尺度融合机制的航拍专用Y型网络结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划与环境监测提供高精度场景分类参考与新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像场景分类是城市规划和环境监测的基础任务，但航空影像包含建筑、森林、山地、空地等异质目标，类别间差异小、类内差异大，传统模型难以兼顾全局语义与局部判别特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统梳理了从手工特征(SIFT、LBP)到经典CNN(VGG、GoogLeNet)再到深度混合网络的演进，指出多尺度与注意力机制的重要性。为此提出Aerial-Y-Net：以CNN为主干，嵌入空间注意力模块，并在多尺度特征图间进行跨层融合，强化对显著区域与细粒度结构的联合表征。训练与测试均在公开AID数据集上完成，与若干基线网络进行对照。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Aerial-Y-Net在AID数据集上取得91.72%总体精度，优于传统CNN和同期混合网络，验证空间注意力+多尺度融合策略能有效提升异质航拍场景的分类准确率。消融实验表明注意力模块贡献最大，多尺度融合次之，二者协同显著降低易混类别的错误率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一AID数据集上评估，缺乏跨数据集泛化验证；模型参数量与推理时延未与轻量级架构对比，实际部署成本未知；对注意力可视化与失败案例的定性分析较少，可解释性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在更大规模、多源遥感数据集上验证鲁棒性，并结合自监督或域适应技术提升跨域泛化能力；探索轻量化设计以满足实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统回顾了航拍影像分类的技术脉络并给出可复现的先进基线，对从事遥感场景理解、注意力机制设计或多尺度特征融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18252v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Co-PLNet：面向提示引导线框解析的协同点-线网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Wang，Xuanying Li，Cheng Dai，Jinglei Feng，Yuxiang Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18252v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有线框解析方法将线段与交点分开预测后拼接，易出现不匹配且鲁棒性差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Co-PLNet，用PLP-Encoder把早期点线检测转为空间提示，CGL-Decoder以稀疏注意力交互精炼，实现点线协同。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Wireframe与YorkUrban数据集上精度与鲁棒性持续提升，并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入点线双向空间提示机制，使检测与交点任务联合优化，保证几何一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SLAM等需结构化几何的下游任务提供更准、更稳、更快的线框输入。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Wireframe parsing is a prerequisite for many geometric vision tasks, yet most pipelines treat junction and line detection as independent problems and only reconcile them afterwards, leading to spatial misalignments and fragile reconstructions. The authors argue that early, bidirectional exchange of spatial cues could simultaneously boost accuracy and robustness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Co-PLNet introduces a Point-Line Prompt Encoder that converts tentative junction/line predictions into compact, spatially-aligned prompt maps encoding geometric attributes such as orientation and scale. These prompts are fed to a Cross-Guidance Line Decoder that employs sparse attention to refine each modality while being conditioned on the complementary one, enforcing global point-line consistency. The whole architecture is trained end-to-end with shared back-bone features, so messages flow from early detection to final prediction without heavy post-processing.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the Wireframe and YorkUrban benchmarks Co-PLNet lowers the structural average miss-rate by 2–4 pp while running ≥30 fps on a laptop GPU, showing both higher precision and better recall for low-contrast lines. Ablation confirms that prompt-driven collaboration contributes more than 60% of the observed gain, and cross-dataset evaluation reveals improved robustness to viewpoint and lighting changes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still relies on a fixed threshold to convert heat-maps into candidate points/lines, which may fail in extremely cluttered scenes. Prompt encoding is currently handcrafted, so it may not generalize to other geometric primitives, and the sparse-attention implementation demands CUDA-specific kernels, limiting deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn prompt representations directly from data and extend the collaborative paradigm to curves and surfaces for full vectorized scene parsing.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on SLAM, AR calibration, or any task that needs reliable, lightweight wireframe extraction will find the prompt-guided joint inference idea directly applicable and adaptable to other paired-geometry problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19582v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ScenePilot-Bench：自动驾驶视觉-语言模型评估的大规模数据集与基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujin Wang，Yutong Zheng，Wenxian Fan，Tianyi Wang，Hongqing Chu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19582v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在自动驾驶场景中的理解、感知与规划能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建3,847小时驾驶视频数据集ScenePilot-4K，设计四轴评测套件并测试主流VLMs</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示了现有VLMs在驾驶推理上的性能边界与关键差距，安全指标普遍不足</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供含多粒度标注、安全度量和跨域泛化的大规模自动驾驶VLM基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一标准，推动安全关键场景下视觉-语言模型的研发与比较</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)评测多聚焦通用场景，缺乏面向自动驾驶安全需求的大规模第一人称基准，难以衡量模型在真实驾驶环境中的推理与决策能力。ScenePilot-Bench旨在填补这一空白，为安全关键应用提供系统评估框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建ScenePilot-4K数据集，采集3,847小时驾驶视频并标注场景描述、风险评估、关键交通参与者、自车轨迹及相机参数等多粒度信息。基于此提出四轴评测体系：场景理解、空间感知、运动规划与GPT-Score，引入安全敏感指标与跨地域泛化设定，对主流VLM进行基准测试并量化性能边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验揭示当前VLM在驾驶场景下的显著性能缺口：空间定位与长时序运动规划准确率低于通用VQA，跨城市泛化衰减达15%以上；GPT-Score与人工安全评级相关性仅0.58，表明模型自评不足以替代安全验证。结果明确指出了面向自动驾驶的VLM在细粒度时空推理与安全对齐方面的改进空间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集主要采集自中美道路，地理与法规多样性仍有限；标注依赖半自动流程，可能存在主观风险判断偏差；评测协议尚未纳入真实闭环控制与多模态传感器输入，对下游规划模块的直接指导价值受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多模态传感器同步标注与闭环仿真评测，引入可解释安全规则与因果推理任务，以进一步缩小VLM与可靠自动驾驶系统之间的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供迄今最大规模的第一人称驾驶VLM评测基准，公开数据与评估脚本可直接支持研究者测试模型在真实驾驶场景下的时空推理、安全对齐与泛化能力，加速面向自动驾驶的视觉-语言方法研发与标准化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>