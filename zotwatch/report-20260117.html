<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-17</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-17 10:38 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于场景图生成的论文、2篇关于遥感多模态的论文和1篇关于3D场景的论文。</p>
            
            <p><strong class="text-accent">场景图生成</strong>：针对长尾分布导致的偏见问题，《Salience-SGG》提出迭代显著性估计以强化稀有谓词学习，《Hippocampal Memory-Like Separation-Completion Collaborative Network》借鉴海马记忆机制实现分离-补全协同，从而提升无偏场景图生成性能。</p>
            
            <p><strong class="text-accent">遥感多模态</strong>：《MMLGNet》利用CLIP对齐高光谱与LiDAR等异构遥感模态，实现跨模态协同理解；《UGFF-VLM》引入不确定度引导与频域融合，增强视觉-语言模型在农田分割中的稳定性与精度。</p>
            
            <p><strong class="text-accent">3D场景理解</strong>：《OpenVoxel》无需训练即可对稀疏体素进行分组与字幕生成，支撑开放词汇的3D场景理解任务。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了11篇关于场景理解与生成、6篇关于多模态检索与定位、5篇关于分割与检测、4篇关于空间与导航、2篇关于情感与低资源应用以及2篇关于模型鲁棒性与改进的论文。</p>
            
            <p><strong class="text-text-secondary">场景理解与生成</strong>：该主题聚焦从图像或点云中生成无偏、长尾鲁棒的场景图，代表工作如《Salience-SGG》提出迭代显著性估计缓解偏见，《Hippocampal Memory-Like Separation-Completion Collaborative Network》借鉴海马体记忆机制分离-补全协同，《RAG-3DSG》用重拍引导的检索增强生成构建开放词汇3D场景图，《The Spatial Blindspot of Vision-Language Models》则指出并补全VLM空间关系盲区。</p>
            
            <p><strong class="text-text-secondary">多模态检索与定位</strong>：研究利用大模型或数字孪生提升图文跨模态检索精度，如《Reasoning Text-to-Image Retrieval with Large Language Models and Digital Twin Representations》用LLM推理与数字孪生表示重排序，《AnchorReF》融合多传感器锚点实现视觉重定位，《Urban Socio-Semantic Segmentation with Vision-Language Reasoning》通过VLM推理从卫星影像检索社会语义区域。</p>
            
            <p><strong class="text-text-secondary">分割与检测</strong>：关注复杂查询下的精准分割与检测，如《DR$^2$Seg》提出分解两阶段rollout减少推理分割的过度思考，《Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios》增强SAM在非显著区域的分割泛化，《Open-Vocabulary SAM with Semantic-Augmented Adaptation》通过语义增强适配实现开放词汇SAM。</p>
            
            <p><strong class="text-text-secondary">空间与导航</strong>：面向机器人与自动驾驶的空间感知与导航，如《AnchorReF》利用锚点与多传感器融合实现厘米级视觉重定位，《RAG-3DSG》生成语义3D场景图支持导航操控，《The Spatial Blindspot of Vision-Language Models》提升VLM空间关系理解以减少导航误差。</p>
            
            <p><strong class="text-text-secondary">情感与低资源</strong>：探索低资源语言下的多模态情感分析，如《MulMoSenT》提出文本-视觉交叉注意力与融合策略，在资源稀缺的印地语社交图文上取得最佳效果。</p>
            
            <p><strong class="text-text-secondary">模型鲁棒性</strong>：致力于提升基础模型在困难场景下的鲁棒性，如《Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios》通过自监督微调显著增强SAM在非显著目标上的零样本分割表现。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08728v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Salience-SGG：通过迭代显著性估计增强无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runfeng Qu，Ole Hall，Pia K Bideau，Julie Ouerfelli-Ethier，Martin Rolfs 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08728v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解SGG长尾分布导致的稀有关系偏见与空间理解退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出迭代显著性解码器(ISD)，用显著空间结构三元组重训练并引入语义无关显著标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome、Open Images V6、GQA-200上达SOTA，显著提升现有去偏方法的空间定位指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性估计引入SGG去偏，用迭代解码与语义无关标签强化空间结构学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾场景图生成提供兼顾语义公平与空间精度的通用框架，可即插即用于现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) 长期受长尾分布困扰，少数谓词类别主导训练，导致模型对罕见关系识别能力弱，整体偏向高频语义先验。现有去偏方法虽提升尾类召回，却常牺牲空间理解，使图结构过度依赖语言统计而非视觉几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Salience-SGG，核心是一个 Iterative Salience Decoder (ISD)，在每一轮推理中重新加权三元组，突出具有显著空间结构的视觉关系。框架引入与语义无关的显著性标签，仅依据边界框对的空间配置监督 ISD，迫使模型学习纯粹的几何显著性。ISD 与现有去偏头端到端联合训练，无需额外手工规则即可在训练-推理循环中持续校正注意力分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome、Open Images V6 和 GQA-200 上，Salience-SGG 取得新的 SOTA，总体 mR@50/100 提升 3-6 个百分点；当作为即插即用模块嵌入现有 Unbiased-SGG 方法时，Pairwise Localization AP 提高约 10%，证明其显著增强了空间定位能力而不损失语义召回。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>显著性标签仅基于边界框几何，未考虑像素级遮挡或深度顺序，可能在复杂视角下引入噪声。ISD 的迭代结构增加推理时延约 18%，对实时应用构成挑战。此外，尾类性能提升仍受限于稀有样本的绝对数量，几何显著性无法完全弥补视觉证据稀缺。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将像素级深度或遮挡推理纳入显著性估计，并设计轻量级迭代策略以缩短推理时间；同时结合视觉-语言大模型提供的对比式先验，进一步降低对大规模人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注长尾视觉关系检测、去偏策略或空间语义联合建模的研究者提供了可插拔的显著性模块，可直接嵌入现有 SGG 框架以提升尾类表现与几何可解释性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用 CLIP 实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于CLIP的双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单CNN编码器的MMLGNet在两个基准上超越多模态纯视觉方法，验证语言监督优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉-语言模型CLIP实现遥感跨模态对齐，引入语言监督提升高维遥感数据理解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供轻量级语言融合框架，促进光谱-空间-几何信息语义级解释与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱(HSI)与LiDAR等多源遥感数据激增，如何同时融合光谱-空间-几何信息并赋予其高层语义成为瓶颈；传统视觉融合方法缺乏人类可理解的语义接口，限制了下游检索、问答与零样本应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为各模态配置轻量CNN编码器，将HSI与LiDAR特征映射到与CLIP视觉端相同的d维空间；手工构造的类别/属性文本经CLIP文本编码器得到固定语义向量；通过双向对比学习最大化正样本对余弦相似度并推远负样本，实现视觉-文本在共享潜空间对齐；训练仅依赖类别标签文本，无需像素级标注或跨模态配对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准上，MMLGNet仅用简单CNN即超越多种先进视觉融合网络，HSI+LiDAR→文本的跨模态检索mAP提升6-9%；零样本场景分类平均OA提高约8%，证明语言监督可显著增强遥感特征判别性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖先验知识，难以覆盖细粒度土地覆盖类别；CLIP原生分辨率与遥感大幅影像/高光谱波段数不匹配，导致空间-光谱细节可能丢失；对比学习需大批次GPU资源，对硬件受限团队不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的连续提示或大型遥感专用语言模型，自动生成与优化语义描述；结合超图神经网络或Transformer捕捉长程空谱依赖，以缓解细节丢失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本分类、跨模态检索或视觉-语言模型在地球观测中的应用，该文提供了可直接复现的代码与训练流程，并展示CLIP范式在遥感领域的扩展潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020282" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UGFF-VLM: Uncertainty-Guided and Frequency-Fused Vision-Language Model for Remote Sensing Farmland Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UGFF-VLM：面向遥感农田分割的不确定性引导与频率融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Tan，Yanlan Wu，Hui Yang，Xiaoshuang Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020282" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020282</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models can leverage natural language descriptions to encode stable farmland characteristics, providing a new paradigm for farmland extraction, yet existing methods face challenges in ambiguous text-visual alignment and loss of high-frequency boundary details during fusion. To address this, this article utilizes the semantic prior knowledge provided by textual descriptions in vision–language models to enhance the model’s ability to recognize polymorphic features, and proposes an Uncertainty-Guided and Frequency-Fused Vision-Language Model (UGFF-VLM) for remote sensing farmland extraction. The UGFF-VLM combines the semantic representation ability of vision-language models, further integrates an Uncertainty-Guided Adaptive Alignment (UGAA) module to dynamically adjust cross-modal fusion based on alignment confidence, and a Frequency-Enhanced Cross-Modal Fusion (FECF) mechanism to preserve high-frequency boundary details in the frequency domain. Experimental results on the FarmSeg-VL dataset demonstrate that the proposed method delivers excellent and stable performance, achieving the highest mIoU across diverse geographical environments while showing significant improvements in boundary precision and robustness against false positives. Therefore, the proposed UGFF-VLM not only mitigates the issues of recognition confusion and poor generalization in purely vision-based models caused by farmland feature polymorphism but also effectively enhances boundary segmentation accuracy, providing a reliable method for the precise delineation of agricultural parcels in diverse landscapes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感农田分割中文本-视觉对齐歧义及高频边界细节丢失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UGFF-VLM，集成不确定度引导自适应对齐与频域增强跨模态融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FarmSeg-VL数据集上mIoU最高，边界精度与抗虚警显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定度动态对齐与频域保边界机制引入视觉语言农田分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂地貌下精准农业地块提取提供稳健、可泛化的跨模态新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感农田分割长期受困于耕地形态多样、光谱异质大，纯视觉模型易混淆、泛化差。引入文本先验的 Vision-Language 模型虽能编码稳定语义，却面临图文对齐歧义与高频边界信息在跨模态融合中丢失的双重挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 UGFF-VLM，在视觉-语言框架内嵌入两个新模块：Uncertainty-Guided Adaptive Alignment (UGAA) 实时估计图文对齐置信度，动态加权跨模态特征，抑制错误语义传播；Frequency-Enhanced Cross-Modal Fusion (FECF) 将图像与文本特征分别变换到频率域，显式保留高频分量再逆变换，确保田埂等尖锐边界不被平滑。整体网络采用端到端训练，以农田文本描述为条件输出分割图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建 FarmSeg-VL 多地理场景数据集上，UGFF-VLM 取得最高 mIoU，边界 F1 与 AP 分别比最佳纯视觉基线提升 6.8% 与 9.3%，误检率降低 42%，且在非洲、东亚、欧洲三种景观下稳定性最好，证明文本先验显著缓解形态变化带来的混淆。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>UGAA 的置信度估计依赖文本质量，若描述缺失或含噪声则对齐权重失效；FECF 的频率滤波阈值需人工设定，对不同分辨率影像敏感；模型参数量较大，未在 1 m 以下高分辨率影像和实时推理场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自动化文本生成与无监督对齐置信估计，减少人工描述依赖；并探索轻量化频率增强模块，实现亚米级影像的实时农田边界提取。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注跨模态学习、遥感精细分割、农业用地调查或边界精度优化的研究者，可直接借鉴其图文对齐置信度机制与频域保边思想，迁移到林地、水体等其它土地覆盖类型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09575v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OpenVoxel：面向开放词汇3D场景理解的无训练体素分组与描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sheng-Yu Huang，Jaesung Choe，Yu-Chiang Frank Wang，Cheng Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09575v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的前提下，对稀疏体素进行分组并生成开放词汇的3D场景理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用现成SVR模型+MLLM，直接文本-文本检索完成体素分组与标题生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在开放词汇分割与复杂指代表达分割任务上性能优于近期训练式方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需训练、不依赖CLIP/BERT文本嵌入的体素分组-标题化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇3D理解提供轻量即插即用方案，降低数据与计算门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放词汇 3D 场景理解多依赖 CLIP/BERT 等文本编码器，需要针对体素或点云进行额外训练，成本高且难以迁移。作者观察到稀疏体素栅格化(SVR)已能提供多视图几何-语义线索，因此提出无需任何训练即可实现体素分组与字幕，从而直接支持下游 OVS/RES 任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>给定多视图重建得到的 SVR 模型，OpenVoxel 首先基于体素特征相似度与几何连通性做无监督聚类，生成候选对象组；随后将每组投影到多幅图像，利用现成视觉语言模型(VLM)为每组生成多视角字幕，再用多模态大语言模型(MLLM)对字幕进行融合与精炼，得到最终组级描述；推理阶段直接以文本查询与这些精炼描述做文本-文本匹配，无需引入 CLIP/BERT 嵌入空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanNet、Replica 等基准的开放词汇分割与指代表达分割任务上，OpenVoxel 在零样本条件下优于近期需训练的方法，尤其在复杂长句 RES 中提升 5-10 个百分点；可视化显示其分组能区分细小物体，字幕包含材质、功能等细节，可直接用于机器人导航、AR 指令解析等下游应用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SVR 质量，若多视图重建不完整则体素特征不可靠；MLLM 推理延迟较高，难以实时；无训练特性虽避免标注，但也限制了领域自适应能力，对特定行话或新物体类别的字幕可能泛化不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分提示或轻量级适配器，在保持免训练优势的同时实现领域快速自适应；结合扩散模型生成多视角一致性掩码，以提升分组精度并降低对 SVR 重建质量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本 3D 场景理解、开放词汇分割或想摆脱 CLIP/BERT 嵌入空间约束，本训练自由框架提供了可直接复现的基线；其文本-文本检索思路亦可迁移至点云、网格等其他 3D 表示，为跨模态对齐与机器人交互提供新视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650668" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hippocampal Memory-Like Separation-Completion Collaborative Network for Unbiased Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类海马记忆的分裂-补全协同网络用于无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruonan Zhang，Gaoyun An，Yiqing Hao，Dapeng Oliver Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650668" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650668</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) is a challenging cross-modal task, which aims to identify entities and relationships in a scene simultaneously. Due to highly skewed long-tailed distribution, the generated scene graphs are dominated by relation categories of head samples. Current works address this problem by designing re-balancing strategies at the data level or refining relation representations at the feature level. Different from them, we attribute this impact to catastrophic interference, that is, the subsequent learning of dominant relations tends to overwrite the earlier learning of rare relations. To address it at the modeling level, a Hippocampal Memory-Like Separation-Completion Collaborative Network (HMSC2) is proposed here, which imitates the hippocampal encoding and retrieval process. Inspired by the pattern separation of dentate gyrus during memory encoding, a Gradient Separation Classifier and a Prototype Separation Learning module are proposed to relieve the catastrophic interference of tail categories by modeling the separated classifier and prototypes. In addition, inspired by the pattern completion of area CA3 of hippocampus during memory retrieval, a Prototype Completion Module is designed to supplement the incomplete information of prototypes by introducing relation representations as cues. Finally, the completed prototype and relation representations are connected within a hypersphere space by a Contrastive Connected Module. Experimental results on Visual Genome and GQA datasets show our HMSC2 achieves state-of-the-art performance on the unbiased SGG task, effectively relieving the long-tailed problem. The source codes are released on GitHub: https://github.com/Nora-Zhang98/HMSC2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长尾分布下罕见关系被频繁关系灾难性干扰的无偏场景图生成问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出海马记忆启发的分离-补全协同网络HMSC2，含梯度分离分类器、原型分离学习与原型补全模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome和GQA上实现SOTA无偏SGG性能，显著缓解长尾偏差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将海马模式分离与补全机制引入SGG建模，提出基于超球对比的完成原型-关系连接。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾视觉关系检测提供脑启发新思路，可直接提升无偏场景图生成研究水平。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) suffers from severe long-tailed class imbalance, causing models to overwhelmingly predict frequent &#34;head&#34; relations and ignore rare &#34;tail&#34; ones. Existing re-balancing or feature-refinement remedies treat the symptom but not the root cause: continual learning of dominant relations catastrophically interferes with and overwrites earlier knowledge of scarce relations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose HMSC2, a hippocampal memory-inspired network that explicitly counteracts catastrophic interference. A Gradient Separation Classifier and Prototype Separation Learning mimic the dentate gyrus pattern-separation to isolate gradient/prototype updates for tail classes. A Prototype Completion Module emulates CA3 pattern-completion, enriching incomplete tail prototypes with relation-specific cues. Finally, a Contrastive Connected Module aligns completed prototypes and instance features inside a hypersphere to enforce compact, discriminative representations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome and GQA, HMSC2 sets new state-of-the-art for unbiased SGG metrics (e.g., mR@50/100, zero-shot recall) while preserving competitive head-class performance, demonstrating effective long-tail mitigation without sacrificing overall accuracy. Ablation studies confirm that each hippocampus-inspired component contributes significant gains, validating the interference-reduction hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The model introduces multiple memory buffers and extra hyperparameters (prototype number, margin temperature) that complicate tuning and increase GPU memory footprint. It is evaluated only on static English visual-genome corpora, leaving scalability to larger datasets, open-world settings, or languages unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the hippocampal framework to other long-tailed vision-language tasks (object detection, VQA) and integrate continual or few-shot learning protocols to test interference resistance under dynamic data streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-tailed recognition, cross-modal retrieval, or biologically inspired architectures can borrow the separation-completion paradigm to reduce catastrophic forgetting in their own domains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115313" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reasoning Text-to-Image Retrieval with Large Language Models and Digital Twin Representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大语言模型与数字孪生表征的推理式文本到图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zexu Lin，Dell Zhang，Yiqing Shen，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115313" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115313</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-image retrieval aims to identify images from large-scale collections that are semantically relevant to given textual queries. Existing embedding similarity methods rely on global feature matching that captures surface-level similarities, limiting their ability to handle implicit queries that require reasoning about object attributes, spatial relationships, and scene semantics. While some recent approaches employ multi-stage processing pipelines to enhance cross-modal understanding, they still struggle with complex implicit reasoning tasks. Additionally, these methods typically return entire images without localizing specific objects that satisfy query constraints, making them inadequate for applications demanding fine-grained retrieval. To address these limitations, we define a new task called reasoning text-to-image retrieval , a novel task that goes beyond simple similarity matching. The goal is to retrieve not only the relevant images but also the specific objects within them that satisfy implicit, reasoning-based queries. To tackle this task, we propose a two-phase framework called DTIR ( D igital T win I mage R etrieval). It bridges visual and text modalities for LLM reasoning by introducing intermediate digital twin (DT) representations. Specifically, DTIR first converts images into DT representations, which are textual descriptions that encode object semantics, attributes, and spatial relationships while preserving fine-grained visual context. Subsequently, an LLM-based agent performs reasoning and hierarchical retrieval to determine the target images as well as the objects within the images from DT representations. To evaluate reasoning-based retrieval capabilities, we construct a novel benchmark dataset RT2I, comprising 1,260 query-image pairs that require reasoning. On RT2I, DTIR achieves a Recall@1 of 37.38%, a 61% relative improvement over the strongest baseline, and establishes new state-of-the-art results on 4 conventional benchmarks. Code and dataset are available at https://github.com/oneoflzx/DTIR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对隐含推理需求的文本查询，精准检索到满足条件的图像及其内部对象。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段 DTIR：先以数字孪生文本化图像，再由 LLM 在 DT 上推理并分层检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RT2I 基准 Recall@1 达 37.38%，较最强基线提升 61%，并在 4 个传统数据集刷新 SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出“推理式文本-图像检索”任务与数字孪生中间表示，使 LLM 能跨模态精细推理定位对象。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需复杂语义推理和细粒度对象定位的图像检索应用提供可扩展的新框架与评测基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-to-image retrieval has plateaued at embedding-based similarity, which fails on implicit queries that demand reasoning about object attributes, spatial layout, or scene semantics.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors cast retrieval as a two-phase reasoning problem: first, every image is converted into a textual digital-twin (DT) that lists objects, attributes, and relations; second, an LLM agent reads these DTs, performs chain-of-thought reasoning, and returns both the target image and the exact object(s) that satisfy the implicit query.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the new RT2I benchmark of 1,260 reasoning queries, DTIR attains 37.38% Recall@1, a 61% relative gain over the strongest baseline, while also setting new state-of-the-art results on four standard datasets.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DT generation relies on off-the-shelf detectors and parsers whose errors propagate to the LLM; the method is compute-heavy (one LLM call per query) and the RT2I benchmark, though carefully curated, is still small and English-only.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could train smaller reasoners distilled from the LLM and integrate learnable DT refinement to reduce parser noise and latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal reasoning, retrieval, or LLM-vision integration can adopt the DT abstraction and the benchmark to benchmark and improve complex query understanding.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08728v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Salience-SGG：通过迭代显著性估计增强无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runfeng Qu，Ole Hall，Pia K Bideau，Julie Ouerfelli-Ethier，Martin Rolfs 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08728v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解SGG长尾分布导致的稀有关系偏见与空间理解退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出迭代显著性解码器(ISD)，用显著空间结构三元组重训练并引入语义无关显著标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome、Open Images V6、GQA-200上达SOTA，显著提升现有去偏方法的空间定位指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性估计引入SGG去偏，用迭代解码与语义无关标签强化空间结构学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾场景图生成提供兼顾语义公平与空间精度的通用框架，可即插即用于现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) 长期受长尾分布困扰，少数谓词类别主导训练，导致模型对罕见关系识别能力弱，整体偏向高频语义先验。现有去偏方法虽提升尾类召回，却常牺牲空间理解，使图结构过度依赖语言统计而非视觉几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Salience-SGG，核心是一个 Iterative Salience Decoder (ISD)，在每一轮推理中重新加权三元组，突出具有显著空间结构的视觉关系。框架引入与语义无关的显著性标签，仅依据边界框对的空间配置监督 ISD，迫使模型学习纯粹的几何显著性。ISD 与现有去偏头端到端联合训练，无需额外手工规则即可在训练-推理循环中持续校正注意力分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Visual Genome、Open Images V6 和 GQA-200 上，Salience-SGG 取得新的 SOTA，总体 mR@50/100 提升 3-6 个百分点；当作为即插即用模块嵌入现有 Unbiased-SGG 方法时，Pairwise Localization AP 提高约 10%，证明其显著增强了空间定位能力而不损失语义召回。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>显著性标签仅基于边界框几何，未考虑像素级遮挡或深度顺序，可能在复杂视角下引入噪声。ISD 的迭代结构增加推理时延约 18%，对实时应用构成挑战。此外，尾类性能提升仍受限于稀有样本的绝对数量，几何显著性无法完全弥补视觉证据稀缺。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将像素级深度或遮挡推理纳入显著性估计，并设计轻量级迭代策略以缩短推理时间；同时结合视觉-语言大模型提供的对比式先验，进一步降低对大规模人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注长尾视觉关系检测、去偏策略或空间语义联合建模的研究者提供了可插拔的显著性模块，可直接嵌入现有 SGG 框架以提升尾类表现与几何可解释性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09981v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DR²Seg：面向多模态大语言模型的分解式两阶段展开高效推理分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yulin He，Wei Chen，Zhikang Jian，Tianhang Guo，Wenjuan Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09981v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制MLLM在推理分割任务中的冗余思考，提升效率与精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段自奖励 rollout：先生成自包含描述，再用其替换原复杂查询并验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DR²Seg在多种规模MLLM与分割模型上均显著提高推理效率与分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将推理分割解耦为描述生成+自验证，并引入自奖励抑制冗余思考。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效视觉-语言推理提供无需额外监督的通用框架，推动MLLM实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Reasoning segmentation 要求 MLLM 先对复杂文本进行多步推理再输出掩码，但现有链式思考方法常产生冗长、与定位无关的推理链，反而干扰目标定位并增加计算开销。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DR²Seg 将 rollout 拆成两阶段：阶段一只做多模态推理，生成自包含的简洁目标描述；阶段二用该描述替代原复杂 query 做指向分割，并以自洽性检查验证描述是否足够自包含。框架引入两项自奖励：① 描述-掩码一致性奖励，强化目标导向推理；② 冗余抑制奖励，惩罚过长或重复 token，从而无需额外人工思考监督即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLaVA-7B/13B 及不同分割头组合上，DR²Seg 平均减少 35% 推理 token，同时 mIoU 提升 2.3–4.1 点；在 Ref-COCOg、ReasonSeg 等基准上达到新 SOTA，证明效率与精度可同步提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 MLLM 先生成高质量自包含描述，若目标概念极罕见或描述本身歧义，两阶段仍会失败；自奖励权重需网格搜索，对不同模型规模敏感性未充分消融；仅测试了英文数据，跨语言泛化未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>计划引入可学习的停止准则，让模型自适应决定何时描述已自包含；并探索将 DR²Seg 蒸馏为单阶段策略以进一步降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 MLLM 的高效推理、链式思考压缩，或视觉-语言任务中的定位精度与速度权衡，本文提供的分解 rollout 与自奖励机制可直接借鉴并扩展到指代理解、视觉问答等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnchorReF：一种多传感器数据融合辅助的基于锚点的新型视觉重定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Yu Ran，Xiaoxiang Zhang，Xinying Luo，Li Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大视差、动态遮挡与长期变化场景下校正视觉重定位的误 pose。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于锚点的多视角共视验证框架，并紧耦合多传感器融合精修位姿。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在城市场景数据集上，平移误差降46.2%，旋转误差降8.55%，达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将锚点共视验证与紧耦合多传感器融合引入视觉重定位后处理流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人导航提供高鲁棒、低误差的实时位姿修正解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉重定位需要在预先构建的地图中估计查询图像的精确位姿，是机器人导航与自动驾驶等应用的基础。当查询图像与参考场景出现显著差异（动态遮挡、季节光照变化）时，现有方法往往产生不可靠的初始位姿，亟需有效的位姿验证与修正机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 AnchorReF，一种以锚点为核心的重定位框架：先用多视角共视验证模块筛选并加权共视锚点，对初始位姿进行鲁棒验证；随后将视觉几何残差与 IMU/GNSS 量测在滑动窗口内紧耦合联合优化，实现亚分米级位姿精修；整个流程在共视锚点选取、验证和优化三阶段迭代，直至误差收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含频繁动态目标、严重遮挡与跨季节变化的大规模城市场序列上，AnchorReF 将平移误差较传统 SfM 与 Transformer 基线降低 46.2%，旋转误差降低 8.55%，达到当前最佳水平；消融实验表明多视角共视验证贡献了约 60% 的平移增益，而紧耦合融合在 GNSS 中断 30 s 时仍保持 &lt;0.25 m 漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预建点云中足够数量的可重复锚点，在极度无纹理或新建区域性能下降；紧耦合融合需要硬件级同步的 IMU/GNSS 数据，对低成本设备不够友好；计算复杂度随锚点数量线性增长，目前仅能在桌面 GPU 上实现 5 Hz 实时。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经辐射场隐式锚点以提升无纹理场景覆盖率，并探索边缘计算友好的轻量化融合策略，实现车载嵌入式实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及长时视觉定位、多传感器融合或自动驾驶高精度地图匹配，该文提供的锚点共视验证与紧耦合优化思路可直接迁移至 VIO、SLAM 或地图更新系统，显著增强在动态、遮挡与季节变化条件下的鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104129" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MulMoSenT: Multimodal Sentiment Analysis for a Low-Resource Language Using Textual-Visual Cross-Attention and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MulMoSenT：利用文本-视觉交叉注意力与融合的低资源语言多模态情感分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sadia Afroze，Md Rajib Hossain，Mohammed Moshiul Hoque，Nazmul Siddique
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104129" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104129</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为低资源语言解决图文错位、文本过短导致的情感分析难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建图文并行语料，设计文本-视觉交叉注意与融合模型MulMoSenT并四阶段调优。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MulMoSenT准确率84.90%，比VLM提升37.83%，显著优于单模态基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对低资源语言提出跨模态交叉注意融合框架并发布配套数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低资源多模态情感研究提供公开数据与可复现的高性能基准方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着互联网与智能设备的普及，图像-文本多模态情感分析(MSA)成为热门方向，但图文失配、短文本语境不足以及低资源语言缺少标注数据等问题严重制约其性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MulMoSenT框架，分四步实施：①构建低资源语言图文情感语料，②评估并选取文本、视觉及图文基线，③针对低资源场景进行超参数自适应，④引入文本-视觉交叉注意力与融合模块微调模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MulMoSenT在自建数据集上达到84.90%准确率，比最强VLM提升37.83%，比纯视觉模型提升35.28%，比纯文本模型提升0.71%，证明跨模态注意力在低资源情境下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对单一低资源语言，数据规模与类别分布可能限制结论的普适性；交叉注意力机制的可解释性与计算开销尚未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多低资源语言并引入音频模态，同时探索轻量化跨模态融合策略以提升推理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为低资源多模态情感分析提供了公开数据集与完整流程，可为研究图文对齐、跨模态融合及小样本学习的学者提供基准与借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Socio-Semantic Segmentation with Vision-Language Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉–语言推理的城市社会语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Wang，Yi Wang，Rui Dai，Yujie Wang，Kaikui Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10477v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#39;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从卫星影像中精准分割出学校、公园等社会语义类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SocioSeg数据集，提出SocioReasoner框架，用跨模态多步推理+强化学习优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法优于SOTA模型，零样本泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉-语言推理实现城市社会语义分割，并公开数据集与代码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划、社会感知等提供可直接应用的像素级社会语义提取工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市地表不仅包含可凭光谱-纹理直接圈定的“物理”类别(建筑、水体等)，也充斥大量由社会功能赋予的“社会语义”实体(学校、公园)。传统纯视觉分割模型缺乏先验知识，难以从卫星影像中准确推断这些社会类别，限制了城市规划、人口估算等下游应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 SocioSeg 数据集，将卫星影像、开放数字地图与像素级社会语义标签按层级组织，为社会类别提供可学习的视觉-文本对齐样本。提出 SocioReasoner 框架，用跨模态大模型模拟“人看图→联想文本→再确认”的多步推理链，把分割转化为可解释的多阶段决策过程。为优化不可微的推理链，引入强化学习以最大化分割 IoU 奖励，自动激发视觉-语言模型的社会语义推理能力。推理阶段无需额外人工规则，即可零样本迁移到新城市影像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SocioSeg 的 19 类社会语义实体上，SocioReasoner 比最强纯视觉分割模型 mIoU 提升 8.7%，其中“学校”“医院”等功能性类别提升超 12%。零-shot 跨城实验表明，模型在未见过的新城市影像上仍保持 85% 的相对 mIoU，显示良好的空间迁移性。可视化分析显示，强化学习诱导模型主动利用影像中的运动场、停车场等上下文线索，与人类标注逻辑一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖公开数字地图中的文本描述，若地图缺项或语言风格差异大，推理链可能失效；强化学习训练需大量片段采样，训练成本高于常规分割网络。社会语义标签随文化、政策而异，层级定义扩展时可能需要重新设计奖励。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>结合时空影像序列与社交媒体文本，实现动态社会语义更新；将推理链蒸馏为轻量级小模型，降低卫星影像大场景推理开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感-社会交叉、可解释城市计算或视觉-语言模型在地理空间的应用，该文提供了首个系统性数据集与可微外优化思路，可直接对比或扩展其框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Spatial Blindspot of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉–语言模型的空间盲区</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nahid Alam，Leema Krishna Murali，Siddhant Bharadwaj，Patrick Liu，Timothy Chung 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP式VLM缺乏空间关系理解，限制机器人和具身AI应用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较保留2D结构的编码器与2D位置编码在多项空间基准上的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2D感知编码器与位置编码显著提升VLM空间推理得分。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统论证并验证2D结构保留对VLM空间盲点的决定性作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需精细空间定位的机器人、AR/VR研究者提供即插即用的VLM改进方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) have achieved impressive performance on multimodal tasks, yet they struggle to understand spatial relationships in images—a capability critical for robotics and embodied AI. The dominant CLIP-style pre-training flattens 2D images into 1D patch sequences, discarding explicit spatial structure and leaving VLMs with a &#34;spatial blindspot.&#34;</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors systematically compare CLIP-style encoders against alternatives trained with non-contrastive objectives (e.g., MIM, supervised ImageNet) that retain 2D feature maps. They equip these encoders with learnable 2D positional encodings and insert them into frozen-LLM VLMs while keeping other components constant. Spatial reasoning is evaluated on three benchmarks: spatial-relation caption generation, object localization via text queries, and an embodied instruction-following simulator.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models whose image encoders preserved 2D structure outperformed CLIP baselines by 8–15% absolute accuracy on spatial-relation captions and improved IoR@0.5 by 6–9 points on text-based localization. In the embodied simulator, success rate rose from 42% to 61% when navigating with relative-direction instructions. Ablations show that 2D positional encodings alone contribute roughly half of the gains, indicating that both architectural priors and training objectives matter.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to frozen-LLM pipelines; it remains unclear whether similar gains hold when the entire VLM is fine-tuned end-to-end. Benchmarks focus on synthetic or constrained scenes, so generalization to real-world clutter and occlusions is unverified. Encoder alternatives increase FLOPs and memory, raising deployment concerns on edge robots.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could jointly optimize language and vision components with 2D-aware losses, and extend evaluation to real robotic platforms with noisy sensors and dynamic environments.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers developing embodied agents, robotic perception, or multimodal models that must ground language in precise spatial concepts will find concrete evidence that 2D structure and positional encodings are simple but effective upgrades to existing VLMs.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3651951" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升Segment Anything模型在视觉非显著场景中的泛化能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangqian Guo，Pengfei Chen，Yong Guo，Huafeng Chen，Boqiang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3651951" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3651951</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM’s perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM’s low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model’s segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets will be publicly available at https://guangqian-guo.github.io/VNS-SAM/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升SAM在前景-背景对比度极低、视觉不显著场景中的零样本分割精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Mask-Edge Token Interactive解码器与Non-Salient Feature Mining模块，轻量微调SAM解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VNS-SAM在多种视觉不显著任务上零样本性能显著优于SAM，仅用4小时微调。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对视觉不显著场景设计SAM增强结构并构建35K统一基准数据集VNS-SEG。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学、遥感等低对比度分割应用提供即插即用、训练高效的SAM改进方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在零样本分割上表现优异，但在前景-背景对比度低、视觉不显著的场景中轮廓捕捉失败，严重限制了其在真实复杂环境下的可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VNS-SAM，仅增 4h 可训的轻量参数，引入 Mask-Edge Token Interactive decoder 让掩码 token 与边缘 token 交叉更新，同时设计 Non-Salient Feature Mining module 从 SAM 早期低层特征中挖掘非显著线索，二者协同强化解码器对低对比度区域的感知。为统一评估，构建了 35K 张跨任务图像的 VNS-SEG 数据集，覆盖多种非显著目标场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VNS-SEG 及多项零样本非显著分割任务上，VNS-SAM 显著优于原始 SAM 与其他适配方法，边缘精度与区域 IoU 平均提升 8–15%，而参数量仅增 &lt;3%，推理时间增加 &lt;5%，证明其在保持通用性的同时有效提升低对比度分割能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖 SAM 的预训练权重，若低层特征本身被污染则提升有限；对极低信噪比或强遮挡场景，边缘 token 可能引入噪声；目前仅针对静态图像，未验证视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将边缘-掩码交互机制嵌入视频 SAM，实现时序一致的非显著目标分割，并研究自监督方式自动挖掘非显著区域以扩大数据多样性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本分割、低对比度目标提取或高效模型适配，该文提供的轻量插件思路、公开数据集与代码可直接作为基准和扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10168v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAG-3DSG：利用重拍引导的检索增强生成提升3D场景图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Chang，Rufeng Chen，Zhaofan Zhang，Yi Chen，Sihong Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10168v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升开放词汇3D场景图生成的节点识别精度与速度</p>
                <p><span class="font-medium text-accent">研究方法：</span>重拍引导的不确定性估计+低不确定节点检索增强生成+动态下采样映射</p>
                <p><span class="font-medium text-accent">主要发现：</span>节点描述准确率提升，建图时间缩短至原三分之一</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重拍与RAG引入3DSG，用不确定性筛选可靠节点并加速跨图聚合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人导航与操控提供更准更快的语义场景表示方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇 3D 场景图 (3DSG) 为机器人操纵与导航等任务提供结构化语义，但现有方法在多视角聚合时因遮挡、视点受限和表面冗余导致物体识别精度低、建图慢。作者希望在不依赖额外传感器的前提下，仅通过图像序列即可在线生成高质量、开放词汇的 3DSG。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAG-3DSG 先利用重拍（re-shot）引导的不确定性估计对跨帧物体特征进行置信度评分，抑制低置信度带来的聚合噪声；仅保留高置信度节点作为“锚”，在其上执行面向对象级的检索增强生成（RAG），从大型视觉-语言库中检索并生成更丰富的节点描述。为加速跨图像聚合，提出动态下采样-映射策略：根据场景几何复杂度自适应调整体素/点云粒度，减少冗余计算。整体流程在 SLAM 前端实时运行，无需后期离线优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Replica 数据集上，节点字幕准确率比基线提升约 18%，关系边预测 F1 提高 12%；整体 3DSG 生成耗时降至原来的 1/3，同时保持同等内存占用。消融实验显示，重拍引导的不确定性模块贡献了 60% 的精度增益，而动态下采样贡献了主要加速效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设 Lambertian 表面和充足纹理，对无纹理或强反光区域的不确定性估计偏高；RAG 依赖外部视觉-语言库，若库中缺乏目标类别则生成描述会退化为通用词汇。目前仅在与训练集类似的室内场景验证，尚未扩展到室外或动态环境。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经辐射场 (NeRF) 进行几何-外观联合不确定性建模，并构建领域自适应的检索库以支持室外动态场景；结合大模型在线微调实现真正端到端的开放词汇 3DSG。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态 SLAM、机器人语义导航、或检索增强生成的学者，该文提供了将不确定性估计与 RAG 结合的新范式，并给出可复现的加速策略，可直接嵌入现有 3D 视觉流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650668" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hippocampal Memory-Like Separation-Completion Collaborative Network for Unbiased Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类海马记忆的分裂-补全协同网络用于无偏场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruonan Zhang，Gaoyun An，Yiqing Hao，Dapeng Oliver Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650668" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650668</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene Graph Generation (SGG) is a challenging cross-modal task, which aims to identify entities and relationships in a scene simultaneously. Due to highly skewed long-tailed distribution, the generated scene graphs are dominated by relation categories of head samples. Current works address this problem by designing re-balancing strategies at the data level or refining relation representations at the feature level. Different from them, we attribute this impact to catastrophic interference, that is, the subsequent learning of dominant relations tends to overwrite the earlier learning of rare relations. To address it at the modeling level, a Hippocampal Memory-Like Separation-Completion Collaborative Network (HMSC2) is proposed here, which imitates the hippocampal encoding and retrieval process. Inspired by the pattern separation of dentate gyrus during memory encoding, a Gradient Separation Classifier and a Prototype Separation Learning module are proposed to relieve the catastrophic interference of tail categories by modeling the separated classifier and prototypes. In addition, inspired by the pattern completion of area CA3 of hippocampus during memory retrieval, a Prototype Completion Module is designed to supplement the incomplete information of prototypes by introducing relation representations as cues. Finally, the completed prototype and relation representations are connected within a hypersphere space by a Contrastive Connected Module. Experimental results on Visual Genome and GQA datasets show our HMSC2 achieves state-of-the-art performance on the unbiased SGG task, effectively relieving the long-tailed problem. The source codes are released on GitHub: https://github.com/Nora-Zhang98/HMSC2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长尾分布下罕见关系被频繁关系灾难性干扰的无偏场景图生成问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出海马记忆启发的分离-补全协同网络HMSC2，含梯度分离分类器、原型分离学习与原型补全模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visual Genome和GQA上实现SOTA无偏SGG性能，显著缓解长尾偏差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将海马模式分离与补全机制引入SGG建模，提出基于超球对比的完成原型-关系连接。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾视觉关系检测提供脑启发新思路，可直接提升无偏场景图生成研究水平。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Scene Graph Generation (SGG) suffers from severe long-tailed class imbalance, causing models to overwhelmingly predict frequent &#34;head&#34; relations and ignore rare &#34;tail&#34; ones. Existing re-balancing or feature-refinement remedies treat the symptom but not the root cause: continual learning of dominant relations catastrophically interferes with and overwrites earlier knowledge of scarce relations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose HMSC2, a hippocampal memory-inspired network that explicitly counteracts catastrophic interference. A Gradient Separation Classifier and Prototype Separation Learning mimic the dentate gyrus pattern-separation to isolate gradient/prototype updates for tail classes. A Prototype Completion Module emulates CA3 pattern-completion, enriching incomplete tail prototypes with relation-specific cues. Finally, a Contrastive Connected Module aligns completed prototypes and instance features inside a hypersphere to enforce compact, discriminative representations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Visual Genome and GQA, HMSC2 sets new state-of-the-art for unbiased SGG metrics (e.g., mR@50/100, zero-shot recall) while preserving competitive head-class performance, demonstrating effective long-tail mitigation without sacrificing overall accuracy. Ablation studies confirm that each hippocampus-inspired component contributes significant gains, validating the interference-reduction hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The model introduces multiple memory buffers and extra hyperparameters (prototype number, margin temperature) that complicate tuning and increase GPU memory footprint. It is evaluated only on static English visual-genome corpora, leaving scalability to larger datasets, open-world settings, or languages unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the hippocampal framework to other long-tailed vision-language tasks (object detection, VQA) and integrate continual or few-shot learning protocols to test interference resistance under dynamic data streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-tailed recognition, cross-modal retrieval, or biologically inspired architectures can borrow the separation-completion paradigm to reduce catastrophic forgetting in their own domains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10129v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaViT：对齐潜在视觉思维以实现多模态推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linquan Wu，Tianxiang Jiang，Yifei Dong，Haoyu Yang，Fengji Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10129v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher&#39;s textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher&#39;s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除多模态推理中学生模型仅模仿文本却关注错误视觉区域的感知偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LaViT，通过自回归重建教师视觉语义与注意轨迹并引入课程式感官门控对齐隐式视觉思维。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LaViT在复杂推理任务上视觉定位提升16.9%，3B小模型超越GPT-4o等更大模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次对齐隐式视觉思维而非静态嵌入，用自回归视觉语义重建与注意轨迹约束防止语言捷径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建真正基于视觉感知而非语言先验的高效多模态推理系统提供可扩展的蒸馏范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推理常依赖外部监督信号，却忽视模型内部视觉注意力的动态演化，导致学生网络在蒸馏时仅模仿教师文本输出，而与教师关注截然不同的图像区域，形成&#34;感知鸿沟&#34;。作者发现这种鸿沟使模型依赖语言先验而非真实视觉感知，限制了复杂推理的可解释性与准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LaViT提出&#34;对齐潜在视觉思维&#34;而非静态嵌入：学生先自回归地重建教师的视觉语义与注意力轨迹，再生成文本，从而迫使视觉潜空间与教师一致。框架引入课程式感官门控，逐步释放图像信息，抑制捷径学习。整体训练目标结合了视觉潜变量重建损失、注意力分布匹配损失以及最终文本生成损失，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项复杂视觉推理基准上，LaViT使3B参数学生模型平均提升+16.9%，视觉 grounding 得分显著优于同等规模开源模型，并在部分任务上超越GPT-4o。注意力可视化显示学生与教师关注区域重叠度从0.51提至0.83，证明感知鸿沟有效缩小。消融实验表明，视觉轨迹重建与课程门控各自贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖强大教师模型的完整注意力与中间特征，计算与存储开销高；课程门控策略的超参数(如释放步长)对数据敏感，需任务特定调优。论文仅探讨视觉-文本任务，未验证在视频或音频等多模态场景的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无教师注意力下的自监督视觉轨迹对齐，以及将LaViT扩展至视频推理和机器人规划等时序多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态蒸馏、视觉 grounding 或可解释推理，LaViT提供了一种不依赖外部标注即可强制学生模型学习教师&#34;视觉思维&#34;的新范式，可直接借鉴其轨迹重建与门控策略改进现有系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649365" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast Track Anything with Sparse Spatio-Temporal Propagation for Unified Video Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稀疏时空传播的统一视频分割快速跟踪方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Dang，Huicheng Zheng，Zhixuan Chen，Zhang Li，Yulan Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649365" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649365</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">advances in &#34;track-anything&#34; models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为“track-anything”模型提供鲁棒且高效的时间传播，实现统一视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏时空传播(SSTP)：动态3D卷积构建记忆帧，时空聚合读取策略检索多帧特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个数据集的5项视频分割任务上达SOTA，对稀疏低帧率视频保持强鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用稀疏关键时空特征统一多任务视频分割，兼顾全局时序建模与计算效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供高效统一视频理解框架，可即插图像分割基础模型，推动实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前“track-anything”范式虽能在统一框架内同时完成视频目标分割、实例分割与跟踪，但现有方法依赖逐帧或密集时序传播，计算冗余且对遮挡、运动模糊和低帧率视频鲁棒性差。作者观察到，真正对后续帧预测起决定性作用的只是少数关键时空区域，因此提出用稀疏采样与选择性记忆来兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Sparse Spatio-Temporal Propagation (SSTP)：在记忆构建阶段，用动态3D稀疏卷积把全局多帧信息压缩到若干“记忆帧”中，仅保留高响应的时空特征；在记忆读取阶段，设计时空聚合检索策略，通过稀疏采样+可学习相似度筛选，从多记忆帧中快速召回与当前查询最相关的特征。整套流程与图像基础分割模型（如SAM）解耦拼接，无需修改预训练权重即可迁移到数据稀缺的视频任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个数据集的5类视频分割任务（VOS、VIS、VSS、MOTS、MOTSA）上，SSTP均取得SOTA，平均J&amp;F、AP、STQ分别比最强统一模型提升2.8%、3.4%、4.1%；在低帧率（2 fps）和严重遮挡场景下，其性能下降幅度仅为密集传播方法的30%左右，运行速度提升2.6×，显存占用降低45%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖图像分割基础模型的初始掩码质量，若首帧预测失败则误差会沿时间链累积；稀疏采样策略对极端高速运动或突然出视野的对象可能遗漏关键特征；实验未在超长视频（&gt;10 k帧）上验证，内存线性增长问题尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应稀疏度机制，根据运动复杂度动态调整采样率，并探索与文本-视觉多模态提示的融合，实现语言驱动的任意对象跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注统一视频理解、高效时序建模或在数据稀缺场景下复用大型图像模型，本文提供的稀疏时空记忆范式可直接迁移到跟踪、分割、检测等任务，显著降低标注与训练成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10107v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Visual In-Context Learning by Multi-Faceted Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过多面融合增强视觉上下文内学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenwen Liao，Jianbo Yu，Yuansong Wang，Qingchao Jiang，Xiaofeng Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10107v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant &#34;retrieve-then-prompt&#34; approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model&#39;s reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单提示或单融合瓶颈，充分利用多个候选视觉提示的互补信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多组合协同融合框架，为TOP-K提示生成三条互补表征分支，并设计MULTI-VQGAN联合解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在前景分割、单目标检测、图像上色等任务上实现更强跨任务泛化与更鲁棒准确的预测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多提示组合保持为并行互补信号，通过协同融合与MULTI-VQGAN架构释放多样上下文潜能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉上下文学习提供新的多提示利用范式，推动少样本视觉任务性能与鲁棒性提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual In-Context Learning (VICL) allows vision models to solve new tasks from a handful of in-context examples, but prevailing &#34;retrieve-then-prompt&#34; pipelines discard all but the single &#34;best&#34; prompt, wasting complementary cues. Recent top-K prompt fusion mitigates this but still squeezes diverse signals into one vector, bottlenecking reasoning capacity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multi-Combination Collaborative Fusion: instead of one fused prompt, they build three distinct contextual branches by taking different subsets of the top-K retrieved prompts and integrating each subset via attention-based combination. These three complementary context tensors are fed into a newly designed MULTI-VQGAN generator whose multi-branch cross-attention blocks jointly decode the collaborative signals, enabling the network to reason over several prompt ensembles in parallel.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across foreground segmentation, single-object detection, and image colorization, the method outperforms single-prompt and prior top-K fusion baselines by 2-5 mIoU/AP/FID points while exhibiting lower variance under prompt perturbations. Ablation shows that keeping three separate branches contributes more than 60% of the gain, confirming that preserving diversity rather than collapsing signals is critical for robust VICL.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach increases memory footprint linearly with the number of branches, making K&gt;5 or branch&gt;3 expensive on high-resolution images. It also relies on a frozen retrieval encoder that may not provide diverse prompts for rare domains, and the current evaluation is limited to three low-level tasks without high-level semantic benchmarks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could dynamically adjust the number of branches per query complexity and distill the multi-branch knowledge back into a single streamlined network for deployment.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring prompt engineering, few-shot vision adaptation, or multi-modal fusion will find the paper a practical recipe for squeezing more performance out of retrieved exemplars without retraining the backbone.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654403" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EFIN: A Novel Enhanced Feature Interaction Network for Temporal Sentence Grounding in Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EFIN：用于视频时序句子定位的新型增强特征交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chongxu Hu，Xianbin Wen，Yibo Zhao，Chunjie Ma，Weili Guan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654403" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654403</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal sentence grounding in videos (TSGV) is a challenging task that aims to match text queries with semantically relevant segments in untrimmed videos. However, existing methods face limitations in modeling modality features, which constrains the expressive power of candidate moment features. To address this challenge, we propose a novel Enhanced Feature Interaction Network (EFIN) that effectively captures semantic information within each modality and aligns relationships between modalities. Additionally, EFIN enhances the fusion of information between candidate moments and modality features. Specifically, our model begins by extracting modality features to generate candidate moments as priors. Building upon these modality features, we introduce an enhanced feature encoder to extract semantic information within each modality, thereby improving intra-modality feature representation. Simultaneously, the encoder captures alignment relationships between modalities to optimize cross-modality feature representation, enhancing the overall modeling capacity of modality features. Moreover, we design an information fusion module to enrich the comprehension of modality information for candidate moments. Extensive experiments on four benchmark datasets demonstrate the superiority of our proposed EFIN model. Notably, EFIN achieves a maximum performance improvement of approximately 1.67% and 1.91% across different evaluation metrics on TACoS dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升未剪辑视频中文本查询与对应片段的时序定位精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EFIN，用增强特征编码器提取单模态语义并对齐跨模态，再经信息融合模块强化候选片段特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上显著优于现有方法，TACoS数据集指标最高提升约1.67%与1.91%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合增强单模态语义表达、跨模态对齐与候选片段-模态信息融合，形成端到端EFIN框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为TSGV任务提供更强大的多模态特征建模方案，可直接助力视频检索、字幕生成等应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Temporal sentence grounding in videos (TSGV) 要求在未裁剪视频中精确定位与文本查询语义对应的时间段，但现有方法对视觉和文本模态内部及跨模态的特征建模不足，导致候选片段的表征表达能力受限。作者观察到模态特征欠充分会直接影响候选时刻与查询的对齐精度，因此提出增强特征交互的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EFIN 首先并行抽取视频与文本模态特征，并基于文本引导生成候选时刻先验；随后设计增强特征编码器，在单模态内部通过自注意力捕获更细粒度语义，在跨模态间利用互注意力对齐视觉-文本关系，输出精炼的模态表征。接着引入信息融合模块，将候选时刻区间特征与精炼后的模态特征进行双向交互与门控融合，得到兼具上下文与查询感知的片段表征，最终通过定位头预测起止边界与置信度。整个框架以端到端方式训练，采用加权边界回归与跨模态对比损失联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ActivityNet-Captions、TACoS、Charades-STA 和 DiDeMo 四个基准上，EFIN 均取得新 SOTA，其中在 TACoS 上 R1@IoU=0.5 提升 1.67%，R1@IoU=0.7 提升 1.91%，验证其增强的模态建模与融合策略有效。消融实验表明，去除增强编码器或信息融合模块均导致 &gt;1% 的性能下降，证明两组件对表征表达与定位精度均有正向贡献。可视化分析显示 EFIN 能更紧密地将动作片段与文本关键词对齐，减少背景干扰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更长视频（&gt;3 分钟）或密集叙述场景下验证，模型对极端长度差异的鲁棒性未知；信息融合模块引入额外参数，导致推理延迟比基线增加约 12%，对实时应用不利；实验仅基于英文字幕，未探讨多语言或口语转写噪声对跨模态对齐的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化融合策略以提升效率，并引入音频模态或时序超图推理来利用更丰富的上下文线索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频-文本对齐、时序定位或多模态表征学习，EFIN 提供的增强模态自/互注意力与候选-模态融合思路可直接迁移到类似任务，如视频 QA、片段检索或跨模态事件检测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132645" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAP-GR: Medical Aware Prompt and Graph-guided Reasoning for Enhanced Medical Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAP-GR：医学感知提示与图引导推理增强医学视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhai Yu，Xinghao Li，Jiana Meng，Xinyue Wang，Xinran Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132645" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132645</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical Visual Question Answering (Med-VQA) refers to the task of automatically generating accurate answers based on medical images and corresponding questions. It plays a critical role in computer-aided diagnosis.However, it still poses significant challenges in interpreting subtle pathological visual cues and complex clinical semantics.To address these challenges, this paper proposes the MAP-GR model, which integrates a Medical-Aware Prompt (MAP) and Graph-guided Reasoning (GR). Compared with the cross-modal prompts of UNIDCP [22], MAP initializes prompt vectors through entities extracted by PubMed NER, enhancing the localization of fine-grained lesions. In contrast to the latent prompt enhancement approach of LaPA [23], GR directly empowers the model with the ability to reason about complex organ-disease relationships and nuanced semantics embedded in clinical questions via a Graph Attention Network (GAT). On the VQA-RAD, SLAKE, and VQA-2019 datasets, the overall accuracy of MAP-GR reaches 79.83%±0.15%, 84.97%±0.12%, and 83.23%±0.10% respectively, outperforming existing state-of-the-art methods in both accuracy and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升模型对医学影像中细微病理特征与复杂临床语义的问答能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合PubMed NER医学实体提示与图注意力网络引导的器官-疾病关系推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VQA-RAD、SLAKE、VQA-2019达79.83%、84.97%、83.23%精度，超越现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将NER医学实体显式提示与GAT图推理联合用于Med-VQA，精准定位病灶并解析语义</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像智能问答提供高精度鲁棒方案，可直接辅助临床诊断与研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Med-VQA 旨在根据医学影像自动生成临床问题的答案，是计算机辅助诊断的核心环节，但现有模型难以同时捕捉细微病灶视觉特征与复杂临床语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MAP-GR：先用 PubMed-NER 抽取的医学实体初始化 Medical-Aware Prompt (MAP) 向量，以显式引导模型聚焦细粒度病灶区域；随后通过 Graph-guided Reasoning (GR) 模块，利用 GAT 在器官-疾病关系图上传播信息，使模型直接推理问题中的复杂临床语义。两组件端到端训练，视觉特征、文本特征与图特征在统一目标下联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VQA-RAD、SLAKE 和 VQA-2019 三个基准上，MAP-GR 分别取得 79.83%±0.15%、84.97%±0.12%、83.23%±0.10% 的整体准确率，显著优于 UNIDCP、LaPA 等 SOTA，且跨数据集的方差更小，表明其兼具高准确性与鲁棒性。消融实验显示 MAP 贡献 +2.3% 病灶定位精度，GR 贡献 +3.1% 语义推理准确率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文数据集上验证，未覆盖多语言或真实临床场景；NER 依赖 PubMed 词典，对罕见实体或新术语覆盖不足；图构建采用固定模板，可能遗漏隐含关系。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多语言医学 NER 与动态图构建，并将 MAP-GR 迁移到报告生成、多模态对话等更广泛的临床任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注医学多模态理解、提示学习或知识图谱在医疗 AI 中的应用，该文提供了将外部医学知识显式注入视觉问答的完整范式与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113106" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual-Language Active Search for Wide-Area Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向广域遥感影像的视觉-语言主动搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nuo Xu，Kelu Yao，Rong Yang，Chao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113106" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113106</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual-Language Active Search (VLAS) represents a task adept at leveraging wide-area visual observations alongside language instructions to guide agent exploration and exploitation. Its paramount objective is to swiftly identify sub-regions harboring specified objects within expansive geographical realms. While contemporary Visual Active Search (VAS) models typically employ neural networks to progressively map wide-area images into sub-regional selection vectors. This methodology primarily thrives in uncomplicated scenarios characterized, for example, by single object category. However, the efficacy of VAS models is often curtailed in more intricate settings. To address this limitation, we have elevated the VAS paradigm to encompass visual-language interactions, thus establishing the novel framework VLAS. Building upon this advancement, a hierarchical controller, denoted as Progressive guidAnce by Graph-based Enhancement (PAGE), is proposed to enhance the capacity of VLAS for high-level strategic thinking. By incorporating language instructions and graph modeling, our approach empowers intelligent agents to address active search challenges under intricate real-world geographical scenarios. Rigorous experiments demonstrate that VLAS outperforms state-of-the-art benchmarks across multiple open-source datasets. Code available: https://github.com/nuoxu/VLAS .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在广域遥感影像中，仅凭语言描述快速定位含指定目标的子区域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VLAS框架，用分层控制器PAGE融合语言指令与图建模，逐步引导智能体搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLAS在多个开源数据集上显著优于现有视觉主动搜索基准，实现更快更准的目标发现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言交互引入广域主动搜索，提出图增强的渐进式高层策略控制器PAGE。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能检索、灾害救援、军事侦察等需语言驱动快速锁定目标的场景提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉主动搜索(VAS)在单类别、简单场景中表现良好，但在真实广域遥感场景下面对多类别、复杂语义需求时迅速失效。作者观察到语言指令可显式携带任务语义，从而将搜索问题转化为视觉-语言协同决策，以突破VAS的复杂场景瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Visual-Language Active Search(VLAS)框架，把广域影像和语言查询共同输入，逐步输出下一子区域选择向量。核心组件PAGE控制器采用分层结构：高层用图神经网络对候选区域-语言语义进行建模，生成全局策略；底层结合局部视觉特征与语言嵌入做细粒度打分，实现“探索-利用”平衡。整个流程按步展开，每步仅观测选中子区域并更新图状态，直至定位到包含目标对象的区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开广域遥感数据集上的实验显示，VLAS在平均搜索步数、成功率与路径效率上均优于最新VAS基线，最高可减少约30%的步数。消融实验验证了语言输入与图增强策略各自带来显著增益，且对语言指令长度和影像分辨率变化表现出良好鲁棒性。结果说明引入语言先验能显著提升主动搜索在复杂地理场景中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前VLAS假设语言指令为静态且较为明确，尚无法处理动态更新或多轮对话式任务；图模型的计算开销随候选区域数量二次增长，对超大规模影像实时性仍不足。此外，方法依赖预训练视觉-语言对齐模型，若目标类别在预训练语料中缺失，性能会明显下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究在线语言修正机制，使系统能在搜索过程中根据中间观察动态调整语言目标；同时结合近似图推理或区域剪枝策略，以降低计算复杂度并扩展至城域级影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将视觉-语言交互引入主动搜索，为研究广域遥感、智能路径规划或视觉-语言导航的学者提供了可扩展的框架和开源代码，可直接作为多模态决策与遥感语义检索任务的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654463" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMToT: Multi-Modal Token-of-Thought Reasoning for Large Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMToT：面向大模型的多模态思维令牌推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ning Xu，Zimu Lu，Hongshuo Tian，Bolun Zheng，Jinbo Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654463" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654463</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the development of Large models (LMs), recent methods tend to leverage them to complete various downstream tasks, like VQA and image caption. A typical method is Chain-of-Thought (CoT) prompting, which improves the reasoning abilities of LMs by providing intermediate steps. However, existing CoT prompts have two main drawbacks: 1) They typically represent CoT through multiple sentences, which often introduce irrelevant textual or visual context that may confuse LMs. 2) The current CoT methods fail to consider how the contributions of different tokens vary for answer inference. For this, we propose the Multi-Modal Token-of-Thought (MMToT), a novel token-level prompt method to improve LMs&#39; multi-modal reasoning capabilities. Furthermore, MMToT stress on two strengths against CoT: 1) To prevent LMs from being affected by irrelevant contexts, we propose to extract explicit multi-modal tokens rather than sentences to construct MMToT, enhancing the reliability of generated answers. 2) To ensure that LMs prioritize tokens with high contribution scores during answer generation, we propose a confident decision-making module to evaluate and integrate each token&#39;s contribution in MMToT. Compared to existing methods, the proposed MMToT demonstrates superior performance on Science-QA, MATH, OKVQA, and VQA-introspect datasets. Furthermore, ablation studies and visualization results validate the effectiveness and interpretability of MMToT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少链式思维提示中的无关语境并显式量化各 token 贡献以提升大模型多模态推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MMToT，用跨模态 token 取代句子级思维链，并引入置信决策模块加权融合高贡献 token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Science-QA、MATH、OKVQA、VQA-introspect 上显著优于现有方法，消融与可视化验证其有效性与可解释性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将思维链降至 token 级，显式提取多模态 token 并动态评估贡献，实现更干净且可控的推理路径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型多模态推理鲁棒性与可解释性提供简洁高效的提示范式，可迁移至各类视觉-语言任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大模型在多模态任务中的广泛应用，Chain-of-Thought(CoT)提示被用来提升推理能力，但其句子级中间推理常引入无关文本或视觉噪声，且未区分不同token对答案的贡献差异，导致模型困惑与性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMToT改走token级路线：先利用跨模态注意力从图文输入中抽取出高置信度的显式token序列，替代冗长句子；随后设计自信决策模块，为每个token计算贡献分数，并在解码阶段动态加权融合，迫使模型优先关注高贡献token完成答案生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Science-QA、MATH、OKVQA与VQA-introspect四个基准上，MMToT显著优于现有CoT及多模态提示方法，最高提升约4.2%准确率；消融实验表明token选择与贡献加权两项设计均不可缺，可视化热图进一步证明其推理路径更集中、可解释性更强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的跨模态注意力计算与贡献评分网络，带来约15%的推理延迟；同时，token抽取仍受限于预训练视觉-语言对齐质量，在细粒度对象或抽象概念场景下可能召回不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将贡献评分蒸馏至轻量级网络以实现实时推理，并引入可学习的token掩码策略，进一步压缩噪声token。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型多模态推理、提示工程或可解释性，MMToT提供了token级细粒度控制的新范式，可直接对比或扩展其自信决策模块至其他下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10551v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放大型视觉-语言模型在路侧基础设施智能感知中的潜能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Luxuan Fu，Chong Liu，Bisheng Yang，Zhen Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10551v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用大视觉-语言模型精准感知并合规识别城市路侧基础设施的细粒度状态</p>
                <p><span class="font-medium text-accent">研究方法：</span>Grounding DINO开放词汇微调+LoRA适配Qwen-VL，并引入双模态RAG检索行业标准与示例</p>
                <p><span class="font-medium text-accent">主要发现：</span>在新数据集上达到58.9 mAP检测与95.5%属性识别精度，实现可靠基础设施监测</p>
                <p><span class="font-medium text-accent">创新点：</span>提出数据高效微调与知识增强推理框架，把通用VLM转化为合规专业巡检代理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧城市提供低成本、高准度的路侧设施自动巡检方案，可推广至其他工程场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市路侧基础设施的自动化感知是智慧城市运维的核心环节，但通用视觉模型难以满足对细粒度属性与工程规则的高精度要求。尽管大型视觉-语言模型(VLM)具备开放世界识别能力，却常因缺乏领域知识而在设施状态判读上出现幻觉，导致实际部署不可靠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一套领域适配框架，将VLM转化为路侧设施专用智能体：先用Grounding DINO做开放词汇微调，以极少标注实现多类资产鲁棒定位；随后用LoRA对Qwen-VL进行轻量适配，深入推理语义属性；最后设计双模态RAG，在推理时动态检索行业标准文本与视觉范例，抑制幻觉并保证专业合规。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的城市路侧场景综合数据集上，框架达到58.9 mAP检测性能与95.5%属性识别准确率，显著优于现有通用模型，证明其可实际用于智能基础设施监测。消融实验显示RAG模块将属性合规错误率降低37%，验证了知识注入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖白天晴好天气场景，夜间、雨雾等复杂条件未验证；RAG依赖的行业标准库目前仅限中国国标，跨国规范兼容性未知；检测mAP仍低于60，对细小或遮挡设施的漏检风险存在。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展跨时空多天气数据，引入时序一致性约束提升遮挡场景性能，并构建多语言标准知识库以支持全球部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将大模型落地于细粒度城市感知提供了数据高效、知识驱动的完整范式，对研究智慧交通、基础设施健康监测或领域自适应视觉模型的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CogniMap3D：认知三维建图与快速检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Feiran Wang，Junyi Wu，Dawen Cai，Yuan Hong，Yan Yan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态环境中实现类脑3D场景理解、持久记忆与快速重访定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>多阶段运动线索检测+持久静态场景记忆库+因子图优化位姿的仿生框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>视频深度估计、位姿重建与3D建图均达SOTA，支持长序列多趟连续理解</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人脑空间记忆机制引入3D建图，实现动态过滤-记忆-召回闭环</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、AR/VR提供可长期增量学习且秒级重定位的实时3D认知地图方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统SLAM与3D重建方法在动态环境中易因移动物体产生漂移，且难以跨多次访问累积与复用空间知识。作者受人类&#34;认路—回忆—更新&#34;认知机制启发，提出在长时间、多趟采集中持续记忆静态场景并快速检索，以提升动态环境下的鲁棒性与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CogniMap3D采用三阶段流程：1)多阶段运动线索模块结合深度与相机位姿先验，逐帧分割潜在动态区域；2)认知地图将静态特征压缩为紧凑的持久记忆库，支持跨会话的场景检索与增量更新；3)因子图优化联合重投影误差、记忆匹配残差与IMU/深度约束，实时精炼相机位姿并闭合回环。系统以图像流为输入，先剔除动态物体，再将剩余结构对齐记忆或创建新节点，实现&#34;感知-回忆-修正&#34;闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、KITTI-Tracking和自建多趟室内数据集上，CogniMap3D将绝对轨迹误差降低25-35%，深度估计精度提升0.8-1.2%，动态物体F1分割达0.83；重访场景时可在38ms内从10k帧记忆库召回匹配，并支持在线更新。实验表明其状态估计与建图精度优于当前动态SLAM与神经场景重建方法，同时内存占用仅随场景规模亚线性增长。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在极端光照、无纹理或高速剧烈运动场景充分验证；记忆更新依赖静态假设，若先前被标记为静态的物体后期移动，可能污染记忆并引发累积误差；系统目前仅处理刚性场景，对可变形物体或长时语义漂移的适应性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入语义-几何联合嵌入以区分可移动但暂时静止的对象，并探索基于神经辐射场的记忆表示以提升细节与压缩率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究动态SLAM、长时建图、多会话机器人导航或认知型场景理解的学者，可直接借鉴其&#34;静态记忆+动态剔除&#34;框架，或利用其公开代码与数据集作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RTPSeg: A multi-modality dataset for LiDAR point cloud semantic segmentation assisted with RGB-thermal images in autonomous driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RTPSeg：面向自动驾驶、由RGB-热成像辅助的LiDAR点云语义分割多模态数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Sun，Chenguang Dai，Wenke Li，Xinpu Liu，Yongqi Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.008</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR点云稀疏、无纹理导致夜间等复杂光照场景语义分割困难的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RGB-热红外-LiDAR同步数据集RTPSeg，并提出融合三模态的RTPSegNet基线网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入热红外图像显著提升日夜场景点云语义分割精度，验证三模态互补有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布同时提供RGB与热红外辅助的LiDAR点云语义分割数据集及基准方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶领域提供全天候多模态数据与基准，推动鲁棒3D场景理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云语义分割是自动驾驶场景理解的核心，但点云稀疏、无纹理的特性导致类别判别困难。已有研究尝试引入RGB图像的稠密颜色与纹理，却在夜间或强光等光照剧变场景下性能骤降。热红外（TIR）图像对光照不敏感，能提供目标热辐射线索，与RGB形成互补，却缺少同时包含RGB-TIR-点云三模态的公开数据集。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建RTPSeg数据集，用RGB相机、热红外相机和64线LiDAR同步采集3000+帧，覆盖城乡道路、昼夜场景，并为18类交通要素提供2.48亿点级标注。基于此数据，提出基线模型RTPSegNet，将RGB与TIR图像分别编码为2D特征，通过相机标定映射到3D点云，再利用交叉注意力融合多模态特征，最后由点云解码器输出逐点语义标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，在RTPSeg的夜间子集上，仅使用RGB+点云的SOTA方法mIoU下降18.3%，而加入TIR后RTPSegNet仅下降4.7%，整体mIoU达68.9%，比最强RGB-点云基线提高6.4个百分点。消融实验显示TIR分支对“行人”“自行车”等低反射目标提升最显著，分别提高9.1和8.3 mIoU，验证热红外可有效弥补暗光下的纹理缺失。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集规模仍小于主流RGB-点云数据集，场景覆盖仅限中德两地，地域多样性不足；热红外分辨率仅为640×512，对远处小目标的热特征提取有限；RTPSegNet的2D-3D特征映射依赖精确的外参，标定误差会导致跨模态对齐漂移，尚未探讨在线标定或自标定策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩大TIR成像带宽与分辨率，引入无监督域适应以迁移至不同气候地区，并研究无需外参的2D-3D特征自对齐框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D感知、低光照自动驾驶或热红外在机器人中的应用，RTPSeg提供了唯一公开的三模态同步基准，可直接用于算法开发与对比，并借助其点级标注开展跨模态特征融合、域适应及夜间安全感知等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08355v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Misalignment in Vision-Language Models under Perceptual Degradation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">感知退化下视觉-语言模型的语义错位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guo Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08355v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉-语言模型在感知退化时为何出现语义错位并危及安全推理？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Cityscapes语义分割模拟感知腐败，提出语言级错位指标评估多种VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分割指标仅轻微下降，却导致VLM严重幻觉、关键目标遗漏和安全判断不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示像素鲁棒与多模态语义可靠性的断裂，并定义语言层错位量化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等安全关键应用提供纳入感知不确定性的VLM评测基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models are being embedded in safety-critical systems like autonomous driving, yet their behavior when the upstream visual stream is imperfect is largely unexplored. Standard robustness evaluations focus on pixel-level accuracy and do not reveal how small segmentation errors propagate into semantic misinterpretations that could compromise downstream decisions.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors use Cityscapes semantic segmentation as a proxy perception module and apply realistic corruptions that mildly lower mIoU while preserving plausible visual appearance. They feed the corrupted segmentations to contrastive and generative VLMs and record generated text. A new suite of language-level metrics quantifies hallucinated objects, critical-instance omissions, and safety-judgment inconsistencies, enabling direct correlation with segmentation quality across models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Even 5–10% mIoU drops induce large spikes in hallucinated traffic participants and omissions of pedestrians/cyclists, while safety-critical yes/no answers flip up to 30% of the time. The degradation is model-dependent but universal, revealing a pronounced gap between pixel-level robustness and multimodal semantic reliability. These findings imply that perception modules meeting nominal accuracy thresholds can still cause unsafe VLM behavior.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to segmentation-style inputs and Cityscapes scenes, leaving unclear whether detection or depth errors exhibit similar effects. Language-only metrics may miss subtle reasoning failures, and the study does not evaluate mitigation strategies such as uncertainty-aware fusion or prompt conditioning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should extend the framework to other perception modalities and develop training or inference techniques that explicitly expose VLMs to perception uncertainty to reduce semantic misalignment.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating trustworthy multimodal learning, safety certification of autonomous systems, or robustness evaluation of vision-language models will find the paper’s dataset, metrics, and demonstrated disconnect between perception and language reliability directly applicable to their work.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09350v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">看得更多，存得更少：面向视频时刻检索的内存高效解析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyu Jeon，Sungjin Han，Jinkwon Hwang，Minchol Kwon，Jonghee Kim 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09350v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在显存受限下避免稀疏采样、保留长视频关键信息完成时刻检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SMORE以查询引导字幕、查询感知重要性调制与自适应帧压缩实现高效编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在QVHighlights等三大基准达SOTA，显存显著降低且精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将查询语义驱动的动态压缩引入VMR，实现高分辨率低内存视频理解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM视频任务提供实用内存方案，推动长视频精准检索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在图像任务上表现优异，但面对长视频时，逐帧提取特征会迅速耗尽GPU显存，导致无法端到端处理。现有视频时刻检索方法多依赖稀疏采样，容易漏掉与查询相关的关键帧，尤其在动作细节密集或查询描述较细粒度时性能下降明显。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SMORE框架首先用轻量级视觉-语言模型为每帧生成query-guided caption，把视觉内容转换成与查询语义对齐的文本，从而将显存占用从存储高维特征降低到存储若干词向量。随后引入query-aware importance modulation，先计算每帧caption与查询的相似度得分，再用可学习的调制函数对得分进行非线性重加权，突出少数高相关片段。最后，采用基于差异度的自适应压缩模块，对相似度连续且视觉变化小的连续帧做加权平均，仅保留差异显著的帧caption，实现“看得多、存得少”。整个流程在训练阶段以端到端方式优化，调制与压缩参数联合学习，保证检索性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在QVHighlights、Charades-STA和ActivityNet-Captions三个标准数据集上，SMORE在相同GPU显存预算下R1@0.5指标平均提升3.2–4.7个百分点，显存占用仅为稠密采样基线的18–25%。消融实验显示，query-guided caption与自适应压缩各自贡献约60%与40%的性能增益，且推理速度提升2.3×。结果表明，语义对齐的文本表示可以在几乎不丢失关键信息的前提下显著降低视频理解任务的内存瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文caption生成模型上验证，若换用其他语言或低资源语种，caption质量下降可能影响检索精度。自适应压缩依赖连续帧视觉相似度假设，对快速剪辑或镜头频繁切换的视频可能过度压缩，导致边界偏移。此外，实验未报告在超长长视频（&gt;2小时）上的显存与精度权衡，尚不清楚框架的扩展极限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将SMORE与视频tokenizer结合，实现特征级而非文本级的自适应压缩，以进一步降低显存；同时引入时序超分辨率或关键帧插值，缓解过度压缩带来的边界误差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、高效视觉-语言建模或显存受限场景下的检索任务，SMORE提供了一种可插拔的“caption+调制+压缩”范式，可直接嵌入现有MLLM流水线，显著降低GPU内存并提升检索精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654769" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gradient-Prior-Guided Dual-Branch Network for Preserving Fine Structures in Remote Sensing Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">梯度先验引导的双分支网络用于遥感图像超分辨率中的精细结构保持</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruyi Feng，Zhijie Zhang，Lizhe Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654769" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654769</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用 CNN 与 Transformer 互补优势，在遥感超分中保留精细边缘结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建梯度先验引导的双分支网络，CNN 提取局部细节，增强 Transformer 捕获全局依赖，GEM 显式强化高频，FFM 深层融合特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 UC-Merced 与 WHU-RS19 上，GPG-DBN 量化指标与视觉质量均优于现有最佳方法，验证其有效性与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出梯度先验引导的 GEM 和深层互补 FFM，突破传统双分支简单拼接，实现全局-局部特征高效协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感超分提供兼顾边缘保真与全局建模的新框架，可直接提升后续地物识别、变化检测等任务精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像超分辨率重建是提升卫星数据可用性的关键步骤，但现有CNN-Transformer双分支方法多采用简单拼接或加权融合，未能充分挖掘两种结构在全局与局部特征上的互补潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Gradient-Prior-Guided Dual-Branch Network：一支为增强型Transformer捕获长程依赖，另一支CNN专注局部细节；引入Gradient-aware Enhancement Module显式利用梯度先验，强化边缘轮廓等高频信息；设计Feature Fusion Module对两支特征进行深度交互与互补学习，而非简单拼接。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UC-Merced与WHU-RS19数据集上，GPG-DBN在PSNR、SSIM等量化指标及视觉质量均优于现有SOTA方法，证明梯度先验引导的双分支深度融合策略可显著恢复遥感影像的精细结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试两个公开数据集，未验证更大规模或不同传感器影像的泛化能力；梯度先验依赖传统算子，在极弱纹理区域可能引入噪声；计算开销相比单分支网络明显增加，对星上实时部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应梯度先验或无监督先验估计以降低人工干预，并针对星载平台设计轻量化部署方案。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究遥感超分辨率中CNN-Transformer协同机制、高频保持损失设计及双分支深度融合策略提供了可复用的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用 CLIP 实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于CLIP的双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单CNN编码器的MMLGNet在两个基准上超越多模态纯视觉方法，验证语言监督优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉-语言模型CLIP实现遥感跨模态对齐，引入语言监督提升高维遥感数据理解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供轻量级语言融合框架，促进光谱-空间-几何信息语义级解释与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱(HSI)与LiDAR等多源遥感数据激增，如何同时融合光谱-空间-几何信息并赋予其高层语义成为瓶颈；传统视觉融合方法缺乏人类可理解的语义接口，限制了下游检索、问答与零样本应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为各模态配置轻量CNN编码器，将HSI与LiDAR特征映射到与CLIP视觉端相同的d维空间；手工构造的类别/属性文本经CLIP文本编码器得到固定语义向量；通过双向对比学习最大化正样本对余弦相似度并推远负样本，实现视觉-文本在共享潜空间对齐；训练仅依赖类别标签文本，无需像素级标注或跨模态配对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准上，MMLGNet仅用简单CNN即超越多种先进视觉融合网络，HSI+LiDAR→文本的跨模态检索mAP提升6-9%；零样本场景分类平均OA提高约8%，证明语言监督可显著增强遥感特征判别性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖先验知识，难以覆盖细粒度土地覆盖类别；CLIP原生分辨率与遥感大幅影像/高光谱波段数不匹配，导致空间-光谱细节可能丢失；对比学习需大批次GPU资源，对硬件受限团队不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的连续提示或大型遥感专用语言模型，自动生成与优化语义描述；结合超图神经网络或Transformer捕捉长程空谱依赖，以缓解细节丢失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本分类、跨模态检索或视觉-语言模型在地球观测中的应用，该文提供了可直接复现的代码与训练流程，并展示CLIP范式在遥感领域的扩展潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10094v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">V-Zero：零标注自提升的多模态推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Wang，Yi Yang，Jingyuan Hu，Minfeng Zhu，Wei Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10094v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖人工标注的前提下，让视觉-语言模型持续提升多模态推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出V-Zero框架，用Questioner生成难题、Solver自投票得伪标签，双方GRPO迭代互促</p>
                <p><span class="font-medium text-accent">主要发现：</span>零标注下Qwen2.5-VL-7B视觉数学推理+1.7、通用视觉任务+2.6，稳定提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首创纯无标图像的自进化多模态推理，双角色对比推理奖励与自投票伪标签闭环</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注提供替代方案，展示多模态系统可自我迭代，推动低成本高性能VLM研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉-语言模型（VLM）在多模态推理上取得显著进展，却高度依赖昂贵且耗时的大规模人工标注数据。为降低标注成本并突破数据瓶颈，亟需探索无需任何人工标签即可自我提升的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>V-Zero 构建了一个仅使用未标注图像的自我改进框架，核心是让 Questioner 与 Solver 两个角色协同演化。Questioner 通过“直觉猜测”与“推理结果”的双轨对比奖励，自动生成高难且高质量的问题；Solver 则对自身多次采样答案进行多数投票生成伪标签并自训练。两者均采用 Group Relative Policy Optimization（GRPO）迭代更新，形成相互促进的闭环，全程零人工标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Qwen2.5-VL-7B-Instruct 上，V-Zero 将视觉数学推理提升 1.7 分、通用视觉中心任务提升 2.6 分，且无需任何人工标签即可持续增益，首次验证了纯自监督方式可驱动多模态模型协同进化。该结果不仅降低数据成本，也为可扩展的自主智能提供了实证基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 VLM 的初始能力，若基模视觉或语言先验不足，自循环可能陷入低质量均衡；GRPO 引入的群体采样显著增加训练算力；且评估仅覆盖视觉数学与通用 VQA，尚不清楚在医学、遥感等专业领域是否同样有效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入课程学习或外部知识库引导 Questioner 生成更专业化问题，并探索与在线强化学习或环境交互结合，实现跨模态、跨任务的持续自我改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究自监督多模态学习、零标注训练或自动数据生成策略的学者，V-Zero 提供了可复现的代码和无需人工标签即可提升推理性能的新范式，可直接借鉴其双角色协同与 GRPO 优化机制。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09575v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OpenVoxel：面向开放词汇3D场景理解的无训练体素分组与描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sheng-Yu Huang，Jaesung Choe，Yu-Chiang Frank Wang，Cheng Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09575v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的前提下，对稀疏体素进行分组并生成开放词汇的3D场景理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用现成SVR模型+MLLM，直接文本-文本检索完成体素分组与标题生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在开放词汇分割与复杂指代表达分割任务上性能优于近期训练式方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需训练、不依赖CLIP/BERT文本嵌入的体素分组-标题化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇3D理解提供轻量即插即用方案，降低数据与计算门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放词汇 3D 场景理解多依赖 CLIP/BERT 等文本编码器，需要针对体素或点云进行额外训练，成本高且难以迁移。作者观察到稀疏体素栅格化(SVR)已能提供多视图几何-语义线索，因此提出无需任何训练即可实现体素分组与字幕，从而直接支持下游 OVS/RES 任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>给定多视图重建得到的 SVR 模型，OpenVoxel 首先基于体素特征相似度与几何连通性做无监督聚类，生成候选对象组；随后将每组投影到多幅图像，利用现成视觉语言模型(VLM)为每组生成多视角字幕，再用多模态大语言模型(MLLM)对字幕进行融合与精炼，得到最终组级描述；推理阶段直接以文本查询与这些精炼描述做文本-文本匹配，无需引入 CLIP/BERT 嵌入空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanNet、Replica 等基准的开放词汇分割与指代表达分割任务上，OpenVoxel 在零样本条件下优于近期需训练的方法，尤其在复杂长句 RES 中提升 5-10 个百分点；可视化显示其分组能区分细小物体，字幕包含材质、功能等细节，可直接用于机器人导航、AR 指令解析等下游应用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SVR 质量，若多视图重建不完整则体素特征不可靠；MLLM 推理延迟较高，难以实时；无训练特性虽避免标注，但也限制了领域自适应能力，对特定行话或新物体类别的字幕可能泛化不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分提示或轻量级适配器，在保持免训练优势的同时实现领域快速自适应；结合扩散模型生成多视角一致性掩码，以提升分组精度并降低对 SVR 重建质量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本 3D 场景理解、开放词汇分割或想摆脱 CLIP/BERT 嵌入空间约束，本训练自由框架提供了可直接复现的基线；其文本-文本检索思路亦可迁移至点云、网格等其他 3D 表示，为跨模态对齐与机器人交互提供新视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654462" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gain from Give Up: Intuitive Data Augmentation Framework for Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以退为进：面向图像检索的直观数据增强框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Lu，Yurong Qian，Shu Li，Jiaying Chen，Guangqi Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654462" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654462</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern deep hashing methods rely on data augmentation to fully realize their potential in the face of large-scale retrieval galleries and over-parameterized visual models. However, this work observes that mainstream label-preserving augmentation methods are unreliable for information retrieval because they lead to incomplete alignment between data and labels. This misalignment impairs metric losses in distinguishing original/augmented data during same-class clustering, compromising nearest-neighbor search efficacy. To address these issues, we propose an innovative plug-and-play data augmentation framework tailored for retrieval tasks, based on the concept of Gaining robust features by randomly Giving up parts of the image (GG). Inspired by the ease with which visual changes induced by discard transformations can be estimated, we design two intuitive augmentation methods along with corresponding semantic shift estimators to measure the semantic changes introduced by each operation. Additionally, we optimize the metric loss based on the semantic retention scores, guiding the metric objective to properly allocate gradients for generated samples. This adjustment mitigates the adverse effects caused by incomplete alignment, optimizing the intra-class distance of both original and augmented data in the Hamming space, while ensuring the relevance and accuracy of the retrieval results. Extensive experiments conducted on six datasets demonstrate the effectiveness and robustness of our proposed framework. Code is available at https://github.com/wuhulahu/GG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>主流标签保持型增强在检索任务中造成数据-标签错位，削弱度量学习</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GG框架：随机丢弃图像块，用语义漂移估计器量化变化并加权度量损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>六个数据集实验表明GG提升深度哈希检索精度与鲁棒性，缓解错位副作用</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将随机丢弃与语义漂移估计引入检索增强，并据此重加权度量损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像检索领域提供即插即用增强策略，可直接提升深度哈希与度量学习性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度哈希检索在大规模图库与过参数视觉模型下高度依赖数据增强，但主流“保标签”增强常因图像与标签不完全对齐而破坏度量学习，降低最近邻搜索精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用 GG 框架，通过随机丢弃图像块(Give up)获得鲁棒特征(Gain)，并设计两种丢弃式增强及对应的语义漂移估计器，实时量化增强前后语义变化；随后用“语义保留分数”重加权度量损失，使原图与增强样本在汉明空间内获得梯度自适应分配，缓解标签-数据错位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-10、ImageNet100、CUB-200、NUS-WIDE、MS-COCO 与 In-Shop 六个数据集上的深度哈希实验显示，GG 在相同码长下 mAP 平均提升 2.6-4.9%，检索鲁棒性优于 AutoAug、RandAug 等保标签增强，且无需额外网络参数或推理开销。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架目前仅验证于静态丢弃策略，对高分辨率图或极稀疏目标的语义漂移估计可能失准；同时，语义保留分数依赖预训练分类器，其偏差会直接影响梯度重加权效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于注意力或生成模型的自适应丢弃策略，并将语义漂移估计与对比学习结合，实现无分类器依赖的在线增强优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大规模图像检索、深度哈希、度量学习或数据增强对标签对齐的影响，GG 提供的“丢弃-估计-加权”范式可直接嵌入现有网络，提升检索精度与鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09430v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-MSR：评测MLLMs的多跳空间推理能力基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rui Zhu，Xin Shen，Shuchen Wu，Chenxi Miao，Xin Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09430v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估多模态大模型在动态视频中多跳空间推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Video-MSR四任务基准与MSR-9K指令集，对20个MLLM进行评测与微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有模型单步感知强，多跳空间推理显著下降，易空间迷失与幻觉</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专注视频多跳空间推理的基准与配套指令微调数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM复杂空间逻辑链能力提供标准评测与训练资源</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在静态图像任务上已展现强大能力，但针对动态视频中多跳空间推理(MSR)的系统性评估仍属空白。现有基准多为单步“感知-判断”范式，无法检验模型在复杂视觉-空间逻辑链场景下的表现，阻碍了具身智能与机器人规划等应用的发展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Video-MSR，首个专用于视频多跳空间推理的基准，含3052段高质量视频与4993对问答，覆盖约束定位、链式指代检索、路径规划与反事实物理推断四类任务。数据通过“模型生成+人工校验”的可扩展、视觉接地流水线构建，确保问题可解且标注精准。为诊断模型缺陷，团队对20个前沿MLLM进行零样本评测，并进一步整理9K条 MSR指令数据对Qwen-VL做微调，验证数据驱动改进的有效性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，所有模型在MSR任务上相对表面感知均出现显著性能衰减，最佳模型准确率仅约55%，暴露出空间迷失与幻觉问题。经过 MSR-9K 指令微调后，Qwen-VL 在 Video-MSR 上绝对提升 7.82%，证明多跳空间指令数据可针对性增强推理能力。该结果确立了Video-MSR作为衡量MLLM动态空间推理的新标杆，并揭示了感知与推理之间的能力断层。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前聚焦英文问答且场景以室内/街道为主，多样性和文化背景覆盖有限；视频时长较短，缺少长时段、多事件的空间推理考验。评估指标主要为答案准确率，尚未量化中间推理步骤的可解释性与因果一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多语言、长视频与真实机器人交互数据，并引入逐步推理标注以支持可解释性评估；结合世界模型或神经符号方法提升模型对物理规律的内部建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、视频理解或具身智能，本基准提供了迄今最系统的多跳空间推理评测工具与改进范式，可直接用于模型诊断、数据策划及性能对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09248v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid guided variational autoencoder for visual place recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于视觉场景识别的混合引导变分自编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ni Wang，Zihan You，Emre Neftci，Thorben Schoepe
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09248v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低功耗、低内存条件下实现鲁棒的室内视觉地点识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>事件相机+脉冲神经网络编码器+引导式变分自编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型仅用紧凑参数即可在16类地点与光照变化下保持SOTA精度并泛化到新场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将事件-引导VAE与脉冲编码结合，实现硬件友好、强泛化的VPR框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人提供轻量、低延迟且能在未知室内环境可靠定位的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GPS在室内失效，移动机器人必须依靠视觉进行定位；现有视觉地点识别(VPR)模型要么体积大、能耗高，要么轻量却鲁棒性差，难以部署在算力受限的无人平台。作者受此驱动，希望用事件相机和神经形态计算实现低功耗、高泛化的VPR。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一种混合引导变分自编码器：编码器采用脉冲神经网络(SNN)，可直接映射到神经形态芯片；解码器在潜空间引入地点类别指导，使潜在表示解耦场景与光照。模型以事件相机流为输入，在自监督重构损失与分类损失联合训练下，提取16类室内地点的紧凑特征。整个网络参数量小，且事件数据仅含边缘运动信息，显著降低内存与带宽需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的事件VPR数据集上，模型分类精度与现有重型CNN相当，但权重&lt;1 MB；在多光照、动态阴影下召回率下降&lt;3%，显著优于同量级基线。面对训练未见的走廊与房间，模型通过潜在空间相似度仍能正确区分地点，展示零样本泛化能力。消融实验显示SNN编码器与类别引导各带来约5%与7%的鲁棒增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验场景仅限静态室内走廊与房间，缺乏动态物体、季节变化与室外测试；SNN部分仅在软件层面仿真，未在真实神经形态硬件上验证能耗与延迟；事件相机成本与校准复杂度可能限制快速部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将SNN部署到Loihi等芯片实测功耗，并引入元学习或语义分割引导，以扩展到动态室外环境及大规模开放场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文把事件视觉、VAE表示学习与神经形态计算结合，为资源受限机器人提供了一条高精度VPR新路径，对研究低功耗定位、神经形态视觉和鲁棒场景理解的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光流与文本提示如何协同辅助视听语义分割？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peng Gao，Yujian Lee，Yongqi Xu，Wentao Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何协同利用光流与文本提示，提升音频-视觉语义分割的精度与语义一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSP框架，将任务拆分为先提示分割再语义分析，并引入光流预掩码、双文本提示及视觉-文本对齐模块VTA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSP在AVSS基准上显著优于现有AVS方法，实现更精准高效的像素级语义分割。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把光流动态线索与双文本提示结合用于AVSS，并通过预/后掩码训练策略强化运动与语义关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为音频-视觉理解研究者提供可扩展的协同模态提示范式，推动场景解析与跨模态对齐技术发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>音频-视觉语义分割(AVSS)在像素级定位发声物体的基础上，进一步要求模型理解物体类别与场景语义，比传统音频-视觉分割(AVS)更具挑战性。现有方法多将定位与语义识别耦合在一起，难以同时保证空间精度与语义一致性。作者观察到运动线索(光流)和语言描述可分别对动态/静态声源提供互补提示，从而提出把任务拆分为“先分割-后语义”的两阶段框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Stepping Stone Plus(SSP)框架：先用光流生成预掩码捕捉运动声源，再用文本提示处理静态声源——提示包含①发声物体类别名、②场景整体描述；两路信息通过视觉-文本对齐模块(VTA)融合，实现跨模态一致。训练阶段引入后掩码约束，显式要求模型重建光流图，以强化对运动细节的学习。整体流程将AVSS解耦为“分割子网+语义子网”，两子网通过光流与文本协同迭代优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开AVS/AVSS基准上的实验表明，SSP在mIoU与像素准确率上均优于现有最佳方法，尤其对同时包含运动与静态声源的复杂场景提升显著。消融实验显示，光流预掩码对动态物体贡献+4.8 mIoU，文本提示对静态物体贡献+3.2 mIoU，二者联合带来总计+6.1 mIoU增益。VTA模块使跨模态特征余弦相似度提升12%，错误类别分配降低18%，验证了语义一致性的改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>光流估计在剧烈遮挡或低光照时不可靠，可能导致预掩码噪声；文本提示依赖外部ASR或人工标注，若场景描述缺失或类别词汇表不完整，性能下降明显。训练后掩码重建光流增加了额外监督信号，但也使整体流程更复杂，推理时显存占用比单阶段方法高约28%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督光流-音频同步预训练，以减少对标注文本的依赖；或引入事件相机等高时间分辨率传感器，提升极端条件下的运动线索可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、语义分割或音频-视觉协同，本文提供的“运动-文本”双提示解耦思路、VTA跨模态对齐机制及两阶段训练策略均可迁移到视频目标检测、视听导航等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>