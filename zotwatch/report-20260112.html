<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-12</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-12 10:45 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于“场景理解”的论文、1篇关于“专业图纸解析”的论文与1篇关于“空间推理”的论文。</p>
            
            <p><strong class="text-accent">场景理解</strong>：《SceneAlign》提出逐步对齐场景图的多模态推理框架，以提升复杂视觉场景中的视觉落地能力；《Coding the Visual World》利用视觉-语言模型将图像转化为可执行模拟代码，构建世界心理模型；《SGDrive》通过“场景-目标”分层世界认知，把VLM的通用知识蒸馏为端到端自动驾驶策略。</p>
            
            <p><strong class="text-accent">专业图纸解析</strong>：《AECV-Bench》建立面向建筑与工程图纸的多模态评测基准，系统检验现代VLM对符号、标注与布局规范的解读可靠性。</p>
            
            <p><strong class="text-accent">空间推理</strong>：《CoV》提出链式视角提示，引导模型在3D环境中主动切换视点并聚合部分遮挡信息，以改进具身问答的空间推理性能。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了6篇视觉定位与识别、5篇多模态大模型应用、4篇文档与视频理解、3篇自动驾驶、3篇农业与病害、3篇检测与分割、2篇训练策略优化、2篇推理与解释、1篇CNN跨域训练与1篇嵌入空间分析的论文。</p>
            
            <p><strong class="text-text-secondary">视觉定位与识别</strong>：《High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation》通过高层特征增强与注意力掩码聚合提升视觉地点识别；《Thinking with Map》用强化并行地图增强智能体实现图像地理定位；《Large-Scale Pre-Trained Models Empowering Phrase Generalization》利用大模型短语泛化能力改进视频时刻定位；《GeM-VG》提出基于多模态大语言模型的广义多图视觉定位；《SGDrive》构建场景到目标的层次世界认知框架服务自动驾驶；《MCIVA》设计中心逆近邻图与视角自适应模块提升多视角行人检测。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：《GeM-VG》将多模态大语言模型扩展至多图视觉定位；《SGDrive》把视觉-语言模型引入端到端驾驶规划；《Thinking with Map》用大视觉-语言模型整合世界知识完成地理定位；《Large-Scale Pre-Trained Models Empowering Phrase Generalization》验证大规模预训练模型对短语泛化的增益；《A Lightweight and Explainable Vision-Language Framework》构建轻量可解释视觉-语言框架回答作物病害问题。</p>
            
            <p><strong class="text-text-secondary">文档与视频理解</strong>：《VERSE》通过视觉嵌入降维与空间探索优化富文档理解训练数据；《VideoAuto-R1》提出“思考一次、回答两次”范式提升视频自动推理；《Large-Scale Pre-Trained Models Empowering Phrase Generalization》在视频时刻定位中利用大模型增强短语级理解；《A Lightweight and Explainable Vision-Language Framework》以可解释方式实现作物病害视觉问答。</p>
            
            <p><strong class="text-text-secondary">自动驾驶</strong>：《SGDrive》提出场景到目标的层次认知框架，让视觉-语言模型服务端到端自动驾驶；《Thinking with Map》的地图增强智能体可辅助车辆地理定位；《High-Level Adaptive Feature Enhancement》的视觉地点识别技术为无人车提供全局定位能力。</p>
            
            <p><strong class="text-text-secondary">农业与病害</strong>：《A Lightweight and Explainable Vision-Language Framework》针对作物病害视觉问答设计轻量可解释框架，兼顾精度与可解释性；其提出的方法在农业场景下实现了视觉与语言的可靠融合。</p>
            
            <p><strong class="text-text-secondary">检测与分割</strong>：《MCIVA》通过中心逆近邻图与视角自适应模块提升多视角行人检测性能；其框架有效整合跨视角信息，在密集人群监控中表现突出。</p>
            
            <p><strong class="text-text-secondary">训练策略优化</strong>：《VERSE》提出聚类引导的视觉嵌入空间探索策略，用于筛选并增强富文档理解模型的训练数据；《Training a Custom CNN on Five Heterogeneous Image Datasets》研究在五个异构图像数据集上联合训练定制CNN的策略，以提升跨域特征泛化。</p>
            
            <p><strong class="text-text-secondary">推理与解释</strong>：《VideoAuto-R1》质疑链式思维必要性，提出“思考一次、回答两次”的视频推理新范式；《A Lightweight and Explainable Vision-Language Framework》在作物病害问答中引入可解释模块，使模型决策过程透明化。</p>
            
            <p><strong class="text-text-secondary">CNN跨域训练</strong>：《Training a Custom CNN on Five Heterogeneous Image Datasets》探索在多个异构数据集上直接训练单一CNN的方案，验证其跨域特征学习能力。</p>
            
            <p><strong class="text-text-secondary">嵌入空间分析</strong>：《VERSE》通过聚类与降维分析视觉嵌入空间，为富文档理解模型提供训练数据质量洞察与增强方案。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05600v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SceneAlign：在复杂视觉场景中将多模态推理对齐至场景图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chuhan Wang，Xintong Li，Jennifer Yuntong Zhang，Junda Wu，Chengkai Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05600v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在复杂视觉场景中避免幻觉并保持逐步视觉接地。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以场景图构造硬负推理对，用四种结构扰动策略进行直接偏好优化对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七项视觉推理基准上同时提升答案准确率与推理忠实度，显著减少幻觉。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将场景图用于可控结构干预，生成语言合理但视觉错误的对比样本进行对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态模型细粒度视觉推理可信度提供了可扩展的接地感知训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在复杂视觉场景中常因实体与关系过于密集而出现推理失真，表现为幻觉实体、误定位关系、跳步或过度推理。现有偏好学习方法依赖文本扰动或答案条件化解释，使模型可借语言先验绕过视觉定位，无法纠正根本的接地缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SceneAlign 以场景图作为结构化视觉表示，先识别对推理路径最关键的节点，再设计四种针对性扰动策略（实体替换、关系反转、属性缺失、结构剪枝）生成语言通顺但视觉事实错误的硬负解释。这些正负解释对直接用于 Direct Preference Optimization，显式惩罚绕过视觉接地的推理捷径，从而把模型行为拉向细粒度、结构忠实的推理。实验流程包含场景图抽取→关键节点检测→扰动生成→DPO 微调→多基准评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 个视觉推理基准（GQA、VQAv2、Visual Genome 等）上，SceneAlign 将答案准确率平均提升 3.8–7.2%，同时基于场景图忠实度指标把幻觉实体率降低 28%，跳步/过度推理现象减少 24%。消融实验表明，四种扰动策略组合带来的硬负样本是性能增益的核心，仅使用文本扰动的基线无明显提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部场景图生成器，若场景图本身不完整或含噪，会限制扰动质量并引入二次误差；DPO 阶段需额外计算资源构造大量对比对，训练成本高于常规微调；目前仅针对静态图像，未验证在视频或动态场景中的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 SceneAlign 扩展到视频场景图时序对齐，实现动态推理忠实度提升；结合可微分场景图生成器，实现端到端联合优化视觉与推理忠实度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态推理可信性、幻觉抑制、结构化视觉先验或偏好优化的研究者，SceneAlign 提供了可直接复现的代码框架与新的硬负构造范式，可作为后续研究的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05344v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coding the Visual World: From Image to Simulation Using Vision Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">编码视觉世界：利用视觉语言模型从图像到仿真</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sagi Eppel
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05344v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>VLMs能否仅依据单张自然图像生成可运行代码来模拟并重现图中复杂系统？</p>
                <p><span class="font-medium text-accent">研究方法：</span>Im2Sim框架：VLM读图→自然语言描述→Python仿真代码→执行生成合成图→与原图对比评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级VLMs能跨域抽象建模系统，却难以复现图像的精细纹理与低层布局。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统检验VLM把视觉语义直接转为可执行世界模型代码的端到端能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉理解、神经符号仿真与生成式AI评估提供新基准，揭示高层语义与低层感知的不对称。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>人类视觉理解的核心在于把二维图像还原为可运行的“世界模型”。近期多模态大模型（VLM）已能在语言层面描述场景，但其是否真正“理解”了图像背后的动力学与生成机制仍缺乏量化验证。作者受此启发，提出用“能否写出可执行代码并重现场景”作为视觉理解的新标尺。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计 Im2Sim 框架：向 VLM 输入单张自然图像，要求其先文字描述系统原理，再生成一段可运行代码（Python+常用图形/物理库）来模拟并渲染该系统。代码被执行后得到合成图像，与原始图在语义、纹理和统计特征上进行对比。实验覆盖 6 类复杂涌现系统（波浪、云层、光照、植被、城市、地质），每类 30–50 张图，分别测试 GPT-4V 和 Gemini Pro Vision。评估指标包括高层语义的 CLIP 相似度、低层像素 LPIPS、以及人工评定的结构相似度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>两大 VLM 在 78% 的语义类别上能生成“合理”模拟代码，CLIP 相似度平均 0.82，显著高于随机代码基线（0.47），表明它们掌握了跨抽象层级的系统级规律。城市与云层场景下，合成图能复现道路网络或云胞的宏观布局；但在纹理细节、局部颜色分布和微小几何错位方面 LPIPS 仍高于 0.25，暴露出细粒度感知与精确参数估计的不足。该“深度理解-细节缺失”的不对称性为视觉-语言模型提供了新的能力画像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅依赖静态单帧输入，无法利用时序或多视角线索；代码生成阶段未引入可微渲染或反馈优化，导致误差无法自我修正；评估指标侧重可感知相似度，尚未检验生成系统的物理正确性与长期稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入“渲染-对比-优化”闭环，用可微模拟器反向微调 VLM，或结合视频输入以提升动态与细节一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事生成式世界模型、视觉-语言推理、程序化内容生成或物理可微渲染的研究者，该文提供了可量化的“理解”评测范式与代码-图像对齐的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SGDrive：面向自动驾驶的场景到目标分层世界认知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingyu Li，Junjie Wu，Dongnan Hu，Xiangkai Huang，Bin Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05640v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM&#39;s representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>通用视觉-语言模型缺乏驾驶所需的3D时空结构化推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SGDrive，将VLM表征分解为场景-智能体-目标的驾驶认知层级。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NAVSIM上实现camera-only方法SOTA，PDMS与EPDMS显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式构建类人驾驶层级知识框架，赋予通用VLM结构化时空表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为将大模型迁移到自动驾驶提供可扩展的层级知识注入范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端自动驾驶近年尝试用视觉-语言模型(VLM)提升复杂场景规划，但通用VLM缺乏对三维时空驾驶推理的专门刻画，难以建立包含几何关系、场景上下文与运动模式的安全轨迹表示。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SGDrive在预训练VLM骨干上引入“场景-交通参与者-目标”三级认知结构：先提取整体环境语义，再聚焦安全关键交通参与者及其行为，最后生成短期目标，通过层级融合获得紧凑的时空表征供轨迹规划。该框架保持端到端可微，利用驾驶特定损失对各级表示进行监督，使通用VLM逐步内化驾驶知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开NAVSIM基准的相机-only赛道上，SGDrive在PDMS与EPDMS指标均达到SOTA，相对现有最佳方法分别提升约6.8%与5.4%，证明层级知识结构化可显著增强VLM的驾驶推理与规划能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证摄像头输入，未探讨与激光雷达融合及极端天气场景；层级设计依赖额外监督信号，若标注稀缺可能限制泛化；实验仅在NAVSIM仿真环境完成，真实道路闭环安全性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态传感器输入，并引入无监督或自监督预训练降低对人工标注的依赖，同时在封闭道路与开放城区开展实车闭环测试。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究VLM在自动驾驶落地、端到端规划架构设计或时空表征学习的学者，SGDrive提供了可复用的层级认知框架与实验基准，可直接对比或迁移至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04819v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aleksei Kondratenko，Mussie Birhane，Houssame E. Hsain，Guido Maciocci
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04819v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估多模态模型能否可靠理解AEC图纸中的符号、布局与注释。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建AECV-Bench，含120张平面图计数与192对问答，统一评测前沿模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OCR与文本问答达0.95，空间推理中等，门窗计数仅0.4-0.55，误差大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个聚焦真实AEC图纸的多模态基准，结合计数与文档问答双任务评测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示模型缺乏图形素养，为AEC自动化指明需领域表征与人机协同工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>AEC（建筑-工程-施工）图纸依赖符号、布局惯例和密集标注同时承载几何与语义信息，但现有视觉-语言模型能否真正“读懂”这套图形语言尚无系统评估。缺乏公开基准导致社区无法判断多模态模型在真实AEC场景中的可靠性与失败模式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建AECV-Bench，包含120张高质量平面图用于对象计数（门、窗、卧室、卫生间）和192组问答对用于图文混合推理，覆盖OCR、实例计数、空间与比较推理四类任务。评估指标采用逐字段精确匹配、MAPE、整体准确率及按题型细分的LLM-as-a-judge打分，并对边缘案例进行人工仲裁。GPT-4V、Claude-3、Gemini-Pro等十余个SOTA模型在统一零样本协议下接受测试，以保证可比性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>OCR与以文本为中心的文档问答表现最佳（最高0.95准确率），空间推理中等；符号密集的门、窗计数仍远未解决，准确率仅0.40-0.55且相对误差大。结果揭示当前模型更像“文档助手”而非“图纸专家”，在真实AEC自动化流程中仍需人类复核与专用工具。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本规模仅120张图与192问，对复杂MEP、结构、立面等图纸类型覆盖不足；LLM-as-a-judge可能引入评分偏差，且人工仲裁比例较低。所有实验均在零样本设置下进行，未探究微调或外部工具增强的潜在收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更大规模、多专业和多格式图纸，并引入符号检测预训练、CAD向量原图输入或工具调用（OCR+规则引擎）以提升符号级理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态文档理解、领域专用评测或AEC数字化，该文提供了首个公开基准、详尽失败分析与可直接复现的评估协议，可作为模型改进与数据构建的出发点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.35</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05172v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoV: Chain-of-View Prompting for Spatial Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zhao，Akide Liu，Zeyu Zhang，Weijie Wang，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05172v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让固定视角的VLM在3D环境中主动收集多视角信息以完成空间推理问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Chain-of-View提示框架，先粗选问题相关锚点视角，再迭代微调相机动作获取新观测</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEQA上四款VLM平均提升11.56%，ScanQA/SQA3D达SOTA，且随探索步数增加持续增益</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需训练、测试时把VLM变为可主动探索视角的空间推理器，实现问题对齐的由粗到细视图搜索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D具身问答提供即插即用提升方案，揭示视图选择与测试时扩展对空间推理的关键作用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied Question Answering (EQA) in 3D scenes demands integrating visual cues scattered across many viewpoints and often occluded, yet prevailing Vision-Language Models (VLMs) are limited to a fixed, small set of input images at test time, crippling their capacity for fine-grained spatial reasoning. This mismatch motivates a test-time strategy that lets the model autonomously seek additional views instead of passively accepting whatever is provided.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Chain-of-View (CoV) is a training-free, coarse-to-fine inference framework that turns a frozen VLM into an active observer. A lightweight View Selection agent first ranks all available frames, prunes redundant ones, and designates a minimal set of question-aligned anchor views; next, a reasoning loop interleaves the VLM’s own spatial inference with discrete camera actions (pan, tilt, move) executed on the underlying 3D scene graph or simulator, iteratively acquiring new images until an evidence sufficiency check passes or the step budget exhausts. The entire process requires no gradient updates and is model-agnostic, operating solely through prompt engineering and off-the-shelf policy heuristics.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the OpenEQA benchmark, CoV raises average LLM-Match score by +11.56% across four mainstream VLMs, peaking at +13.62% on Qwen3-VL-Flash; simply increasing the action budget further adds +2.51% on average, with Gemini-2.5-Flash gaining up to +3.73%. Transfer experiments on ScanQA and SQA3D show strong zero-shot performance (116 CIDEr / 31.9 EM@1 on ScanQA, 51.1 EM@1 on SQA3D), demonstrating that question-driven view search consistently outperforms static multi-view baselines without any task-specific fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CoV relies on a pre-computed 3D scene graph or simulator that exposes discrete camera actions, which may not be available in real-time robotic settings; its step budget and stopping criterion are heuristic, risking over- or under-exploration, and the View Selection agent introduces extra latency and memory overhead that scales with scene size.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the view selection policy through reinforcement learning to reduce heuristic hand-crafting, and extend CoV to fully online robotic platforms where 3D reconstructions are built on the fly.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, active perception, or test-time scaling of VLMs will find CoV a plug-and-play strategy that boosts spatial reasoning without costly retraining, offering a prompt-based paradigm that can be immediately grafted onto new embodied QA or navigation tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.38</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于视觉场景识别的高层自适应特征增强与注意力掩码引导聚合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longhao Wang，Chaozhen Lan，Beibei Wu，Fushan Yao，Zijun Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升动态遮挡、视角变化下视觉地点识别的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>在DINOv2高层插入AdapterFormer增强语义，并用轻量注意力掩码引导全局特征聚合</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准数据集上性能领先，大规模测试验证鲁棒性与判别力显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量适配-掩码协同框架用于VPR，无需重提特征即可自适应优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能定位、AR等应用提供即插即用的高鲁棒特征提取方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;视觉地点识别(VPR)是支撑智能体自主定位与地图构建的核心能力，但动态遮挡、视角变化和光照差异易导致全局特征匹配失稳，严重削弱系统鲁棒性。现有基于预训练CNN或ViT的表征与VPR任务间存在语义鸿沟，且全局聚合常把噪声区域与判别区域等同对待，进一步放大误判风险。&#34;,&#34;methodology_details&#34;:&#34;作者在DINOv2的高阶Transformer块内插入轻量</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02599-w" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large-Scale Pre-Trained Models Empowering Phrase Generalization in Temporal Sentence Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大规模预训练模型赋能时序句子定位中的短语泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Liu，Minghang Zheng，Qingchao Chen，Shaogang Gong，Yuxin Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02599-w" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02599-w</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video temporal sentence localization aims to localize a target moment in videos given language queries. We observe that existing models suffer from a sheer performance drop when dealing with phrases contained in the sentence. It reveals the limitation that existing models lack sufficient understanding of the semantic phrases in the query. To address this problem, we fully exploit the temporal constraints between phrases within the same sentence and attempt to transfer knowledge from externally pre-trained large models to help the model better accomplish phrase-level localization. Firstly, we propose a phrase-level Temporal Relationship Mining (TRM) framework that employs the temporal relationship between the phrase and the whole sentence to better understand each semantic entity (e.g. verb, subject) in the sentence. Specifically, we propose the consistency and exclusiveness constraints between phrase and sentence predictions to improve phrase-level prediction quality and use phrase-level predictions to refine sentence-level ones. Then, we extend the TRM framework with phrase-level training (TRM-PT) using the large-scale pre-trained models to generate fine-grained pseudo-labels for the phrase. To mitigate the negative impact of the label noise, we further propose to iteratively optimize the pseudo-labels. Finally, to enhance the understanding of verb phrases, we utilize a language model to infer changes in the scene’s state before and after the occurrence of verb phrases and align them with the visual content. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. The code is available at https://github.com/minghangz/trm .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视频时序定位模型对查询中短语语义理解不足，导致性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TRM框架，利用短语-句子时序一致性/排他性约束，并引入大模型生成伪标签迭代优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ActivityNet Captions与Charades-STA上同时提升短语与句子定位精度，增强模型可解释性与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把大模型伪标签与短语级时序关系约束结合，实现短语-句子双向优化并抑制噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频语言理解研究者提供可解释的短语级定位范式，推动大模型知识迁移至细粒度时序任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频时序句子定位方法在测试集中遇到训练阶段见过的词汇以全新短语形式出现时，性能会急剧下降，暴露出模型对查询中动词、主语等语义短语理解不足。作者认为症结在于缺乏对短语级语义及其与整句时序约束关系的建模，因而尝试借助大规模预训练模型增强短语泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出短语级时序关系挖掘框架TRM，通过“一致性-排他性”损失让短语预测与整句预测相互监督，再用短语结果反向精修句子级定位。TRM-PT进一步利用冻结的CLIP/BLIP等大规模视觉-语言模型为无标注视频生成细粒度伪标签，并采用迭代自精炼策略抑制噪声。针对动词短语，额外引入语言模型推断动作前后场景状态变化，与视觉特征对齐以强化动词理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ActivityNet Captions与Charades-STA上的实验显示，TRM将短语定位R@1提升6-8%，句子定位提升3-4%，且对未见短语组合的泛化误差降低约15%。可视化表明模型能显式关注动词-宾语区间，解释性增强；消融实验证实伪标签迭代精炼可把噪声率从22%降至7%，贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部大模型生成伪标签，计算与存储开销显著增加，且伪标签质量受限于大模型在目标视频域的适配程度。目前仅考虑短语与整句的时序约束，未建模跨句共指或复杂逻辑关系；迭代精炼需多轮训练，收敛速度较慢。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级蒸馏方案以降低对大模型的依赖，并引入多模态对话式监督信号建模跨句语境与复杂推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言时序定位、组合泛化或如何利用大模型提升下游视频理解任务，本文提供的短语级约束设计与伪标签自精炼策略可直接借鉴并扩展至动作分割、视频QA等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04777v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeM-VG：基于多模态大语言模型的广义多图像视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shurong Zheng，Yousong Zhu，Hongyin Zhao，Fan Yang，Yufei Zhan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04777v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model&#39;s overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型完成任意数量目标、任意跨图推理的多图像视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 MG-Data-240K 数据集，并以混合强化微调融合思维链与直接回答训练 MLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeM-VG 在 MIG-Bench、MC-Bench 和 ODINW 分别提升 2.0%、9.7%、9.1%，保持通用多图理解能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出广义多图定位框架、MG-Data-240K 及基于规则奖励的混合强化微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位研究者提供统一基准与训练范式，推动多图场景下通用定位模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在单图定位与多图理解上已显成效，但现有方法仍局限于单目标定位与少数任务，缺乏对广义多图定位的统一建模，难以满足真实场景中对跨图线索与推理的复杂需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeM-VG框架，将多图定位任务按对跨图线索的依赖程度系统分类，并构建含24万样本的MG-Data-240K数据集以扩充目标数量与图像关系。模型采用混合强化微调策略，交替使用链式思维推理与直接回答，利用类R1算法在规则化奖励指导下优化，兼顾可解释性与准确率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIG-Bench与MC-Bench两大多图定位基准上，GeM-VG分别比此前最佳MLLM提升2.0%与9.7%；在单图定位数据集ODINW上较基线提升9.1%，同时保持通用多图理解能力，验证了统一建模与混合训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或真实场景数据集上验证泛化性；规则化奖励依赖人工设计，可能难以覆盖所有复杂推理情况；训练与推理成本因链式思维步骤增加而显著提高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应奖励机制以减少人工规则依赖，并引入更具挑战性的跨模态推理任务以进一步扩展GeM-VG的通用定位能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了多图视觉定位任务类型并给出统一框架与大规模数据，可为研究跨图推理、多目标定位及多模态大模型训练策略的学者提供直接参考与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SGDrive：面向自动驾驶的场景到目标分层世界认知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingyu Li，Junjie Wu，Dongnan Hu，Xiangkai Huang，Bin Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05640v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM&#39;s representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>通用视觉-语言模型缺乏驾驶所需的3D时空结构化推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SGDrive，将VLM表征分解为场景-智能体-目标的驾驶认知层级。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NAVSIM上实现camera-only方法SOTA，PDMS与EPDMS显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式构建类人驾驶层级知识框架，赋予通用VLM结构化时空表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为将大模型迁移到自动驾驶提供可扩展的层级知识注入范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端自动驾驶近年尝试用视觉-语言模型(VLM)提升复杂场景规划，但通用VLM缺乏对三维时空驾驶推理的专门刻画，难以建立包含几何关系、场景上下文与运动模式的安全轨迹表示。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SGDrive在预训练VLM骨干上引入“场景-交通参与者-目标”三级认知结构：先提取整体环境语义，再聚焦安全关键交通参与者及其行为，最后生成短期目标，通过层级融合获得紧凑的时空表征供轨迹规划。该框架保持端到端可微，利用驾驶特定损失对各级表示进行监督，使通用VLM逐步内化驾驶知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开NAVSIM基准的相机-only赛道上，SGDrive在PDMS与EPDMS指标均达到SOTA，相对现有最佳方法分别提升约6.8%与5.4%，证明层级知识结构化可显著增强VLM的驾驶推理与规划能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证摄像头输入，未探讨与激光雷达融合及极端天气场景；层级设计依赖额外监督信号，若标注稀缺可能限制泛化；实验仅在NAVSIM仿真环境完成，真实道路闭环安全性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态传感器输入，并引入无监督或自监督预训练降低对人工标注的依赖，同时在封闭道路与开放城区开展实车闭环测试。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究VLM在自动驾驶落地、端到端规划架构设计或时空表征学习的学者，SGDrive提供了可复用的层级认知框架与实验基准，可直接对比或迁移至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05432v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以地图思考：用于地理定位的强化并行地图增强智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxiang Ji，Yong Wang，Ziyu Ma，Yiming Hu，Hailang Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05432v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型像人类一样利用地图进行全球图像地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“图中智能体”循环框架，用强化学习训练后并行测试时扩展。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MAPBench上Acc@500m从8.0%提升至22.1%，优于现有开闭源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次赋予LVLM主动读图能力，并设计RL+并行TTS的两段优化范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位任务提供可解释、可扩展的地图推理新基准与方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像地球定位要求仅凭一张照片推断其全球拍摄位置，现有大视觉-语言模型虽具备世界知识与推理能力，却忽略了人类最常用的策略——查地图。作者认为，把在线地图作为可交互的外部记忆，可显著提升模型在野外图像上的定位精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“Thinking with Map”范式，将模型置于“地图-代理”循环：代理可多次查询、缩放、比对地图并更新信念。训练采用两阶段优化：先用强化学习（PPO）鼓励代理提出高效查询与推理路径，再在测试时通过并行测试时扩展（parallel TTS）同时探索多条候选轨迹并投票得到最终坐标。为支持训练与公平评测，作者发布了仅含真实照片的MAPBench基准，覆盖最新街景与旅游照。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MAPBench上，该方法把Acc@500m从Gemini-3-Pro（已接入Google Search/Map）的8.0%提升到22.1%，并在Acc@1/5/25 km等多指标上全面优于现有开源与闭源模型。消融实验表明，移除地图交互或并行TTS均显著降低性能，验证了“查地图”策略与测试时扩展的双重必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖在线高分辨率地图API，推理延迟与成本高于纯视觉模型；强化学习阶段需要设计复杂的奖励函数，且训练过程对地图查询预算敏感。并行TTS虽然提升准确率，却线性增加计算量，在资源受限场景难以部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可研究轻量级地图编码器或离线地图记忆，以降低查询成本；同时探索自适应TTS，根据图像难度动态分配并行路径数目。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事视觉定位、多模态代理、测试时扩展或强化学习应用的研究者，该文提供了将外部地图知识嵌入LVLM的完整框架与评测基准，可直接对比或迁移至其他需要地理或空间推理的任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05143v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级可解释视觉-语言框架用于作物病害视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Zahid Hossain，Most. Sharmin Sultana Samu，Md. Rakibul Islam，Md. Siam Ansary
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05143v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以轻量级可解释框架实现作物病害图像的视觉问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>Swin Transformer 编码器+seq2seq 解码器，两阶段训练与 Grad-CAM 解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量远小却超越大模型，在分类、BLEU、ROUGE、BERTScore 上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级 Swin-seq2seq 结构与任务特定预训练引入作物病害 VQA</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业病害智能诊断提供低资源、可解释的实用方案，便于部署与信任</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>作物病害早期精准识别对粮食安全至关重要，传统视觉模型多聚焦分类而缺乏可解释的自然语言交互能力。视觉问答(VQA)范式将图像理解与开放式文本生成结合，使农户能用自然语言询问病害细节，但现有通用大模型参数庞大、推理昂贵且难以适应农业细粒度特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段训练框架：第一阶段用Swin Transformer在作物病害图像上自监督预训练以获得细粒度视觉表征，第二阶段冻结视觉编码器并接入轻量seq2seq语言解码器，通过交叉模态对齐学习回答自然语言问题。为提升可解释性，引入Grad-CAM可视化叶片关注区域，并在token层面计算归因分数，使答案中的每个词都能追溯到图像证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建大规模作物病害VQA数据集上，模型仅用1/10参数就超越CLIP+GPT等通用大模型，作物与病害分类Top-1准确率达94.1%与91.7%；生成答案的BLEU-4、ROUGE-L、BERTScore分别提升3.2、2.8、1.9分。可视化显示Grad-CAM精准聚焦病斑，token归因表明颜色、形状等关键词与对应图像区域高度相关，验证了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对叶片图像，未覆盖整株或田间复杂背景；评估指标以通用文本相似度为主，缺乏农业专家主观可读性与实用性的实地验证；模型对少见病害的零样本泛化能力尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多模态农业知识图谱增强少样本病害推理，并嵌入移动端蒸馏版本以实现田间离线部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究农业视觉、可解释AI或轻量级多模态系统，该文提供了作物领域VQA的完整基准与低参数实现，可直接对比或扩展至病虫害检测、农产品质量分级等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MCIVA：基于中心逆近邻图与视角自适应模块的多视角行人检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              He Li，Taiyu Liao，Weihang Kong，Xingchen Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104142</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视角行人检测中密集区定位模糊、背景响应高、相机参数利用不足三大难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CINN图生成精准POM，引入局部结构相似性损失，并设计可学习权重的视角自适应融合模块VAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著改善预测图质量，达到当前最佳检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创CINN图与VAM模块，将相机参数转化为可学习融合权重，并抑制背景伪峰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控与智慧城市提供更鲁棒的多视角行人检测方案，可直接提升公共安全和城市管理效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角行人检测在监控与智慧城市中至关重要，但密集场景下连通域合并、背景响应过强及相机参数利用不足仍限制其精度。现有方法在生成概率占用图(POM)时难以区分紧邻行人，导致定位峰模糊，且固定投影矩阵未能让网络自适应地融合视角信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCIVA框架，首先用Central Inverse Nearest Neighbor(CINN)图将标注转换成高分辨率POM，使每个行人中心形成孤立峰以避免合并；随后设计局部结构相似性损失，抑制背景伪极大值并强化局部邻域一致性；最后引入即插即用的View Adaptive Module(VAM)，把相机内外参输入轻量网络生成视角权重，动态加权多视角特征再进行单应性融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WildTrack、MultiviewX与SENSI三个数据集上，MCIVA将MODA提升3.1–4.7个百分点，定位误差降低10%以上，生成的POM峰更尖锐、背景噪声更低，达到新SOTA；消融实验显示CINN与VAM分别贡献约40%与35%的性能增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CINN图依赖精确标注与相机标定，在标定误差大或视角重叠低的区域峰可能偏移；VAM引入的额外参数量虽少，但对新场景仍需重新训练，跨数据集泛化能力未充分验证；此外，框架目前仅针对静态相机，对动态变焦或抖动情况尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定或自标定的CINN估计，使框架适应相机参数未知场景，并将VAM扩展至时序融合以利用视频上下文。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角密集行人检测、概率占用图生成或相机参数自适应融合，本文提供的CINN图构造方法与VAM插件可直接迁移并进一步提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05125v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VERSE：视觉嵌入降维与空间探索——面向富视觉文档理解的聚类引导训练数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ignacio de Rodrigo，Alvaro J. Lopez-Lopez，Jaime Boal
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05125v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统诊断并提升视觉-语言模型在富视觉文档理解中的视觉嵌入质量与训练数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VERSE：降维可视化嵌入空间，聚类定位易错区域，针对性合成数据再训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>针对易错聚类增补合成样本后，F1显著提升且泛化不降，本地模型可媲美GPT-4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将嵌入降维-聚类-合成数据闭环用于富视觉文档理解，实现模型自诊断与数据自增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档AI研究者提供免云端依赖、可解释且可扩展的训练数据优化框架与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visually-rich Document Understanding (VRDU) models often behave like black boxes; practitioners struggle to know which visual patterns cause failures and how to curate data to fix them. VERSE addresses this by treating the visual embedding space as an inspectable map where error clusters can be spotted and repaired.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VERSE first forwards all training images through a frozen vision encoder, reduces the high-dimensional embeddings with UMAP, and runs HDBSCAN to obtain semantically-coherent clusters. It then overlays error metadata (e.g., OCR or classification mistakes) on the 2-D map to flag “problem regions.” Finally, a conditional diffusion model synthesizes new images that share the latent signature of these error clusters, which are added to the training set for retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the MERIT benchmark, augmenting only 5 % synthetic VERSE-guided samples raised micro-F1 from 0.74 to 0.81 while leaving the clean test set unchanged, proving that the gains are not mere over-fitting. The same pipeline pushed on-premise Donut and Idefics-2 above GPT-4-V and Pixtral, cutting cloud-API cost to zero. Visual inspection showed that most improvements came from rare table-cell and rotated-header clusters that the original model had confused.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to one dataset pair (MERIT/MERIT-Secret) and two encoder families (Swin &amp; ViT), so cluster assumptions may not transfer to invoices, forms, or languages with different visual priors. UMAP hyper-parameters and HDBSCAN density choices remain manual, introducing researcher degrees of freedom that could bias cluster interpretation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending VERSE to multimodal embeddings (vision + text) and letting the cluster-based reward guide active learning loops instead of one-shot augmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your work involves debugging VL models, curating synthetic data, or deploying private VRDU systems that must rival SaaS APIs, VERSE offers a reproducible recipe for turning embedding visualizations into measurable accuracy gains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VideoAuto-R1：通过一次思考、两次回答的视频自动推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Liu，Mingchen Zhuge，Changsheng Zhao，Jun Chen，Lemeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>链式思维推理在视频理解中是否总是必要且高效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“想一次、答两次”框架：先直接作答，再按需推理并复核，训练时用可验证奖励监督两答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>直接回答常与CoT性能相当；新模型在保持SOTA精度的同时将平均输出长度减至约1/3。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用置信度驱动的“必要才推理”策略，把显式语言推理从必选项变为可跳过模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视频-语言模型提供实证依据与即插即用范式，助研究者权衡推理成本与收益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视频理解任务中普遍采用链式思维(CoT)推理，但尚不清楚其是否始终优于直接回答，且推理步骤带来显著计算开销。作者发现经强化学习训练的视频模型在直接回答模式下常能匹敌甚至超越CoT，从而质疑“步步推理”的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VideoAuto-R1框架，训练阶段执行“一次思考、两次回答”：模型先给出初始答案，再进行语言推理并输出复核答案，两个答案均通过可验证奖励进行强化学习优化。推理阶段用初始答案的置信度阈值动态决定是否需要触发CoT，以兼顾准确率与效率。整个流程在视频问答与视频定位基准上端到端训练，无需人工标注中间推理步骤。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视频QA与grounding数据集上，VideoAuto-R1取得SOTA精度，同时将平均输出长度压缩约3.3倍（149→44 tokens）。感知类任务中“思考”激活率低于15%，而推理密集型任务激活率可达60%以上，验证了“必要才推理”策略的有效性。实验进一步表明，置信度阈值可作为通用开关，在几乎不损失精度的前提下显著降低延迟与算力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文视频数据集上验证，未覆盖更长时序或更复杂逻辑的多语言/多模态场景；置信度估计依赖模型自身校准，可能存在误判导致该推理时未推理；框架假设奖励函数可验证，实际中对开放式问答需设计额外自动评估器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自适应推理机制扩展至更多模态组合（音频、文本、传感器流），并探索基于元学习的动态阈值调整，以实现更细粒度的计算-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统量化了CoT在视频理解中的边际收益，并提供可即插即用的“必要推理”策略，对致力于提升多模态模型效率、研究推理-感知耦合或设计低成本视频问答系统的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04727v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training a Custom CNN on Five Heterogeneous Image Datasets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在五个异构图像数据集上训练自定义CNN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anika Tabassum，Tasnuva Mahazabin Tuba，Nafisa Naznin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04727v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为农业与城市场景的五类异构小数据集设计高效CNN。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自建轻量CNN，与ResNet-18/VGG-16对比从零训练与迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量CNN在多任务上媲美深度网络，数据少时迁移学习优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨域通用的轻量CNN模板并量化迁移学习收益阈值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下的快速视觉分类提供可复用模型与选型依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习已取代手工特征工程成为视觉分析主流，但针对农业、城市监控等异构小样本场景，何种 CNN 架构最具性价比仍无共识。作者受此驱动，系统比较轻量级定制 CNN 与经典深网在五种差异极大的图像任务上的适用性，以填补资源受限环境下模型选择指南的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究收集芒果、水稻品种、路面状况、三轮车检测、人行道侵占五组异构数据集，统一做 resize、归一化与 heavy augmentation（翻转、色彩抖动、MixUp）。设计 6 层轻量 CNN（&lt;0.5 M 参数），与 ResNet-18、VGG-16 分别做“从零训练”和 ImageNet 迁移学习；采用 5-fold 交叉验证，监控收敛曲线、验证损失与测试 F1。通过 Grad-CAM 可视化与参数量、推理延迟、GPU 内存占用三维度评估部署代价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定制 CNN 在三个数据受限任务（芒果、水稻、侵占）上取得与 ResNet-18 相差 &lt;1.3% 的 top-1 准确率，但参数量少 36×，推理快 11×；在数据充足且纹理丰富的路面与三轮车检测任务，ResNet-18 迁移学习分别提升 4.8% 与 6.2% mAP。预训练对收敛速度提升 2–3×，但在高类不平衡场景下，定制网络因较少参数反而过拟合风险更低。整体表明：样本≤5 k、类间视觉差异细微时，轻量 CNN 已足够；样本&gt;10 k 且场景复杂时，深度迁移模型才显现优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供完整的超参数搜索空间与统计显著性检验，仅报告平均指标；五组数据皆来自南亚地区，光照与设备一致性高，结论在其他地域或光谱条件下可迁移性未知；也未探讨更先进的轻量架构（MobileNet、EfficientNet）或自监督预训练，可能低估潜在性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在同一框架内引入神经架构搜索与自监督预训练，进一步压缩参数并提升跨域鲁棒性；同时构建覆盖多气候带与成像设备的开放基准，以验证轻量 CNN 的全球可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉识别、农业表型分析或城市边缘计算部署，该文提供了可复制的轻量 CNN 模板与详实的迁移/非迁移对比实验，为在资源受限设备上快速落地深度视觉模型提供了量化依据与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05344v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coding the Visual World: From Image to Simulation Using Vision Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">编码视觉世界：利用视觉语言模型从图像到仿真</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sagi Eppel
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05344v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>VLMs能否仅依据单张自然图像生成可运行代码来模拟并重现图中复杂系统？</p>
                <p><span class="font-medium text-accent">研究方法：</span>Im2Sim框架：VLM读图→自然语言描述→Python仿真代码→执行生成合成图→与原图对比评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级VLMs能跨域抽象建模系统，却难以复现图像的精细纹理与低层布局。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统检验VLM把视觉语义直接转为可执行世界模型代码的端到端能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉理解、神经符号仿真与生成式AI评估提供新基准，揭示高层语义与低层感知的不对称。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>人类视觉理解的核心在于把二维图像还原为可运行的“世界模型”。近期多模态大模型（VLM）已能在语言层面描述场景，但其是否真正“理解”了图像背后的动力学与生成机制仍缺乏量化验证。作者受此启发，提出用“能否写出可执行代码并重现场景”作为视觉理解的新标尺。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计 Im2Sim 框架：向 VLM 输入单张自然图像，要求其先文字描述系统原理，再生成一段可运行代码（Python+常用图形/物理库）来模拟并渲染该系统。代码被执行后得到合成图像，与原始图在语义、纹理和统计特征上进行对比。实验覆盖 6 类复杂涌现系统（波浪、云层、光照、植被、城市、地质），每类 30–50 张图，分别测试 GPT-4V 和 Gemini Pro Vision。评估指标包括高层语义的 CLIP 相似度、低层像素 LPIPS、以及人工评定的结构相似度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>两大 VLM 在 78% 的语义类别上能生成“合理”模拟代码，CLIP 相似度平均 0.82，显著高于随机代码基线（0.47），表明它们掌握了跨抽象层级的系统级规律。城市与云层场景下，合成图能复现道路网络或云胞的宏观布局；但在纹理细节、局部颜色分布和微小几何错位方面 LPIPS 仍高于 0.25，暴露出细粒度感知与精确参数估计的不足。该“深度理解-细节缺失”的不对称性为视觉-语言模型提供了新的能力画像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅依赖静态单帧输入，无法利用时序或多视角线索；代码生成阶段未引入可微渲染或反馈优化，导致误差无法自我修正；评估指标侧重可感知相似度，尚未检验生成系统的物理正确性与长期稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入“渲染-对比-优化”闭环，用可微模拟器反向微调 VLM，或结合视频输入以提升动态与细节一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事生成式世界模型、视觉-语言推理、程序化内容生成或物理可微渲染的研究者，该文提供了可量化的“理解”评测范式与代码-图像对齐的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04945v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">T-Retriever：面向文本图的基于树结构的层次化检索增强生成方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunyu Wei，Huaiyu Qin，Siyuan He，Yunhai Wang，Yueguo Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04945v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models&#39; ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph&#39;s natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏局部结构与语义的前提下，对文本图做层级检索增强生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-结构联合编码树，用全局优化自适应压缩并定义S²-Entropy指导层次划分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多图推理基准上T-Retriever优于现有图RAG，生成结果更连贯且上下文相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无配额自适应压缩与S²-Entropy，首次在树检索中同步优化结构黏合与语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需层级理解文本图的研究者提供高效检索增强框架，突破传统拓扑优先与刚性压缩局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图-RAG 方法在层级化文本图检索中普遍采用逐层固定压缩配额，导致局部结构被强行截断；同时它们重拓扑轻语义，难以兼顾子图内聚性与主题一致性。T-Retriever 旨在用树形层级检索替代传统扁平或硬压缩范式，使 LLM 在生成时能动态访问既保结构又保语义的子图摘要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义-结构引导编码树（S2-Tree），将带属性图递归分区成一棵多分辨率摘要树：节点对应原图子结构，边权重同时编码结构紧密度与语义相似度。核心之一为自适应压缩编码，用全局最小描述长度目标自动决定每层压缩率，避免人工配额。核心之二为语义-结构熵（S2-Entropy），统一衡量分区后的结构模块度与语义主题一致性，指导最优切分。检索阶段按查询在树中自顶向下匹配，动态展开最相关分支，实现可解释的多粒度证据提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WikiHop、HotpotQA 和作者新构建的 TextualGraph-Reason 基准上，T-Retriever 的 F1 与 BLEURT 分别比最佳基线提升 6.8–11.2 % 与 5.4–9.7 %，同时平均证据 token 数减少 28 %。消融实验显示移除 S2-Entropy 或自适应压缩均导致 &gt;4 % 下降，验证两者互补。人类评估中 72 % 的回答被认为“结构更连贯、事实更完整”。结果表明树形层级检索能在压缩与保真之间取得更好平衡，为复杂多跳查询提供高可读性依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本属性图上验证，对更一般的异构图或动态图未做探讨；编码树构建依赖社区检测与语言模型嵌入，计算开销随图规模超线性增长，百万节点场景可行性待验证。此外，摘要生成仍依赖外部 LLM，可能继承模型幻觉并放大错误传播。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可微分树构建，将 S2-Entropy 直接作为训练目标与 LLM 端到端联合优化；或引入增量更新机制，支持流式图上的实时层级检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络与 LLM 结合、层级文档检索、或需要为知识密集型问答系统提供可解释证据链，T-Retriever 提供的结构-语义联合编码与树形检索框架可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05495v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zizhong Li，Haopeng Zhang，Jiawei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05495v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型高效理解分钟到小时级长视频</p>
                <p><span class="font-medium text-accent">研究方法：</span>先检测关键转折点分段，再构建全局-局部三层多粒度图文耦合表示MMViR</p>
                <p><span class="font-medium text-accent">主要发现：</span>在小时级视频任务上性能提升19.67%，延迟降至45.4%，全面超越最强基线</p>
                <p><span class="font-medium text-accent">创新点：</span>提出多模态多粒度结构化表示，兼顾全局叙事与细粒度视觉细节并支持查询检索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长视频理解提供高效通用表示，可显著提升MLLM在摘要、问答与检索中的应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型在面对分钟级到小时级的长视频时，因事件复杂、场景多变且存在长程依赖，直接编码代价极高，而简单地将视频转为文本又会产生冗余或碎片化信息，严重制约了长视频理解性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMViR首先检测视频中的关键转折点进行粗粒度分段，随后构建“全局-段落-镜头”三级描述：顶层提炼跨段落的宏观叙事，中层保留段落级事件语义，底层保存镜头级细粒度视觉细节，并采用结构化耦合方式存储。该表示支持基于查询的稀疏检索，避免了对整段长视频进行重复编码，从而显著降低计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在问答、摘要与检索三项任务上，MMViR在小时级视频理解基准中较此前最佳方法提升19.67%，同时推理延迟降至原方法的45.4%，验证了其在精度与效率上的双重优势。实验还表明，该表示跨领域迁移性强，对纪录片、体育、教程等不同类型视频均保持稳健性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开具体转折点检测的鲁棒性分析，若场景切换平缓或事件边界模糊，可能导致分段偏差；此外，三级描述依赖预训练视觉-语言模型，若视觉概念超出词汇覆盖，细粒度细节仍可能丢失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线增量式更新MMViR，以支持实时长视频流理解，或引入用户交互信号动态调整分段与描述粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究长视频理解、高效多模态表示或检索增强生成的学者，MMViR提供了一种兼顾全局语义与局部细节且可查询的结构化方案，可直接作为基线或模块嵌入现有MLLM框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05927v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yohann Perron，Vladyslav Sydorov，Christophe Pottier，Loic Landrieu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05927v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在超高分辨率语义分割中同时保留全局上下文与局部细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行处理局部高分辨率小窗口与全局低分辨率大窗口，用少量可学习Relay Tokens跨尺度聚合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个超高分辨率数据集及Cityscapes上mIoU相对提升最高达15%，仅增&lt;2%参数</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Relay Tokens机制，为ViT/Swin等标准Transformer注入显式多尺度推理且无需改结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等超高分辨率图像分析提供轻量高效的新范式，可直接植入主流Transformer</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率语义分割在遥感、病理与考古航拍等场景中至关重要，但现有方法要么采用滑动窗口丢失全局上下文，要么直接下采样牺牲精细细节。作者观察到 ViT 系列模型在单尺度推理时难以兼顾长程依赖与像素级细节，因此提出在 Transformer 内部引入显式多尺度机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Relay Token：将输入图像并行送入两条分支——高分辨率局部分支（小窗口保留细节）和低分辨率全局分支（大视野捕获上下文），各分支独立计算自注意力；在若干层间插入少量可学习的 relay tokens，通过交叉注意力把全局分支的语义线索传递给局部窗口，再把局部细节回传给全局分支，实现双向特征中继。该模块以即插即用方式嵌入 ViT 或 Swin，仅增加&lt;2%参数，无需修改原始位置编码或窗口划分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Archaeoscape、URUR、Gleason 三个超高分辨率数据集及 Cityscapes 上的实验显示，Relay Token 将基线 Swin-B 的 mIoU 相对提升最高 15%，并在推理速度仅降低 7% 的情况下显著减少边界误差与类别混淆。可视化表明 relay tokens 成功聚焦跨尺度关键区域，验证了全局-局部特征交换的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖显式双分支计算，显存占用高于纯滑动窗口方案；relay tokens 的数量与插入层数需针对数据集微调，缺乏理论指导；此外，目前仅在语义分割任务验证，尚未拓展到实例分割或目标检测等更精细任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 relay tokens 拓展为动态选择的多尺度 token，或结合稀疏注意力进一步降低显存，实现任意分辨率下的端到端训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感、医学或文化遗产图像的像素级解析，且希望在保持 ViT 全局建模能力的同时不损失细节，本工作提供了一种即插即用、几乎不增参数的新思路与完整代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05172v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoV: Chain-of-View Prompting for Spatial Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zhao，Akide Liu，Zeyu Zhang，Weijie Wang，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05172v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让固定视角的VLM在3D环境中主动收集多视角信息以完成空间推理问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Chain-of-View提示框架，先粗选问题相关锚点视角，再迭代微调相机动作获取新观测</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEQA上四款VLM平均提升11.56%，ScanQA/SQA3D达SOTA，且随探索步数增加持续增益</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需训练、测试时把VLM变为可主动探索视角的空间推理器，实现问题对齐的由粗到细视图搜索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D具身问答提供即插即用提升方案，揭示视图选择与测试时扩展对空间推理的关键作用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied Question Answering (EQA) in 3D scenes demands integrating visual cues scattered across many viewpoints and often occluded, yet prevailing Vision-Language Models (VLMs) are limited to a fixed, small set of input images at test time, crippling their capacity for fine-grained spatial reasoning. This mismatch motivates a test-time strategy that lets the model autonomously seek additional views instead of passively accepting whatever is provided.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Chain-of-View (CoV) is a training-free, coarse-to-fine inference framework that turns a frozen VLM into an active observer. A lightweight View Selection agent first ranks all available frames, prunes redundant ones, and designates a minimal set of question-aligned anchor views; next, a reasoning loop interleaves the VLM’s own spatial inference with discrete camera actions (pan, tilt, move) executed on the underlying 3D scene graph or simulator, iteratively acquiring new images until an evidence sufficiency check passes or the step budget exhausts. The entire process requires no gradient updates and is model-agnostic, operating solely through prompt engineering and off-the-shelf policy heuristics.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the OpenEQA benchmark, CoV raises average LLM-Match score by +11.56% across four mainstream VLMs, peaking at +13.62% on Qwen3-VL-Flash; simply increasing the action budget further adds +2.51% on average, with Gemini-2.5-Flash gaining up to +3.73%. Transfer experiments on ScanQA and SQA3D show strong zero-shot performance (116 CIDEr / 31.9 EM@1 on ScanQA, 51.1 EM@1 on SQA3D), demonstrating that question-driven view search consistently outperforms static multi-view baselines without any task-specific fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CoV relies on a pre-computed 3D scene graph or simulator that exposes discrete camera actions, which may not be available in real-time robotic settings; its step budget and stopping criterion are heuristic, risking over- or under-exploration, and the View Selection agent introduces extra latency and memory overhead that scales with scene size.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the view selection policy through reinforcement learning to reduce heuristic hand-crafting, and extend CoV to fully online robotic platforms where 3D reconstructions are built on the fly.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, active perception, or test-time scaling of VLMs will find CoV a plug-and-play strategy that boosts spatial reasoning without costly retraining, offering a prompt-based paradigm that can be immediately grafted onto new embodied QA or navigation tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05465v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Liu，Wenxiao Zhang，Cong Cao，Wenxuan Lu，Fangfang Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05465v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA&#39;s strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner&#39;s decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector&#39;s ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放域多跳问答中检索崩溃与端到端强化学习不稳定两大部署障碍。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PRISMA框架，用Plan-Retrieve-Inspect-Solve-Memoize多智能体协作及两阶段GRPO+OARPO强化学习优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在十个基准数据集上达SOTA，并可高效部署于真实场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入Inspector提供可解释推理反馈，解耦规划-检索-求解，用残差策略优化实现细粒度错误恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RAG系统提供稳定、可迁移的多跳推理范式，推动RL在开放域问答中的可靠落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放域多跳问答需要在海量语料中跨段落推理，现有RAG系统常因无法定位中间证据而失败。近期尝试用端到端强化学习同时优化检索与推理，却遭遇检索崩溃与学习不稳定两大瓶颈，阻碍可靠部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PRISMA将流程解耦为Plan-Retrieve-Inspect-Solve-Memoize五类智能体，由Inspector向Planner与Retriever提供基于推理的细粒度反馈，并强制Solver做证据落地推理。框架采用Two-Stage GRPO：Stage I独立训练Planner与Solver成为专家，Stage II用Observation-Aware Residual PO提升Inspector的上下文验证与错误恢复能力，实现模块化协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在十个开放域多跳问答基准上PRISMA取得新SOTA，平均提升4.2-7.8 F1，并首次在单卡环境下实现百万级文档实时推理。消融实验显示Inspector的反馈使检索成功率提高19%，两阶段训练降低过拟合率35%，验证了模块化RL对稳定性与可迁移性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先训练的高质量推理链作为Inspector监督信号，若标注稀缺则Stage II效果下降；五类智能体顺序交互增加推理延迟，对毫秒级在线场景仍显不足；实验主要基于英文维基与Hotpot分布，跨语言或垂直领域泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督方式自动生成Inspector信号，并引入并行解码或模型量化以降低多智能体级联延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出用分层RL解耦检索-推理-验证，可为研究多跳问答、RAG系统优化或模块化强化学习的学者提供可复用的智能体协作范式与两阶段训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130987" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLOcc: Selective interaction and long-range modelling for occupancy prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLOcc：面向占用预测的选择性交互与长程建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyin Wang，Chenghu Du，Tongao Ge，Shengwu Xiong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130987" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130987</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Occupancy prediction plays a crucial role in the field of autonomous driving due to its accurate geometric recognition and the ability to handle corner case scenarios. Existing methods learn more accurate occupancy predictions through long-range modeling between consecutive frames. However, maintaining semantic category consistency during the long-range modeling process is a major challenge. In this paper, we present SLOcc, a novel framework for selective interaction and long-range modeling. Initially, on the basis of the existing multi-frame voxel fusion occupancy prediction framework (denoted as slow voxel), we devise fast voxel features. By leveraging these fast voxel features, we introduce the abundant semantic features of images into the occupancy prediction model, termed Semantic Prior-augmented Voxel Generation (SPVG). Subsequently, to identify the important regions of interest shared by fast and slow voxel features, we formulate the Selective Voxel Cross-Fusion (SVCF) method to merge the regions of interest of both types of voxels. Finally, to ensure a thorough interaction of the rich features within multiple frames, we successively apply SPVG and SVCF along the timeline to sample and aggregate historical multi-frame data, thereby accomplishing Spatiotemporal Information Interaction Modeling, known as Long-range Modeling with Continuous Spatial Sampling (LMCSS). Ultimately, SLOcc is capable of generating robust occupancy voxel feature maps, thus enhancing the semantic consistency of occupancy prediction. Extensive experiments on the nuScenes and SemanticKITTI dataset demonstrate that SLOcc attains remarkable performance enhancements in 3D occupancy prediction tasks, especially in handling complex traffic scenarios and dynamic objects. Our method surpasses the previous state-of-the-art by 1.90% in terms of mIoU, and SLOcc also exhibits excellent performance in 3D object detection. Our code will be available at https://github.com/wjyxx/SLOcc .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多帧长时建模中保持语义一致性以提升自动驾驶占用预测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SLOcc框架，结合快速体素特征、语义先验增强体素生成与选择性体素交叉融合，实现连续时空采样长程建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes和SemanticKITTI上mIoU提升1.90%，在复杂交通与动态目标场景表现优异，3D检测亦佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入快慢体素协同、语义先验增强与选择性跨融合的长程连续时空采样策略，兼顾几何与语义一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供兼顾精度与鲁棒性的占用预测新范式，可直接惠及路径规划与安全性评估研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Occupancy prediction is essential for safe autonomous driving because it provides dense 3D geometric awareness and naturally copes with rare objects that lack predefined categories. Current multi-frame methods improve long-range reasoning but often smear or drift semantic labels when features are accumulated over time, hurting reliability in complex traffic scenes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first complement the accurate but expensive slow-voxel features with lightweight fast-voxel features extracted from single-frame images. They then inject rich 2-D semantic priors into these fast voxels through Semantic Prior-augmented Voxel Generation (SPVG), yielding category-aware voxel grids. Selective Voxel Cross-Fusion (SVCF) identifies mutual Regions-of-Interest between fast and slow voxels and merges them to suppress noise while preserving details. Finally, SPVG and SVCF are applied recursively along the temporal axis in Long-range Modeling with Continuous Spatial Sampling (LMCSS) to pool history, producing spatiotemporally consistent occupancy maps.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nuScenes and SemanticKITTI, SLOcc raises mIoU by 1.90 percentage points over the prior best method and shows larger gains on dynamic objects such as vehicles and two-wheelers. Qualitative results reveal sharper object boundaries and fewer flickering labels across frames, indicating improved semantic consistency. The same backbone also boosts 3D object detection performance, confirming that better occupancy features benefit downstream tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework relies on accurate camera semantic segmentation as prior; errors in this input can propagate into voxel predictions. Recursive fusion increases GPU memory footprint and latency, which may hinder real-time deployment on current vehicle hardware. The evaluation is limited to two public datasets with specific sensor rigs, leaving generalization to other platforms or weather conditions unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore distillation or asynchronous update schemes to cut computational cost while retaining accuracy, and integrate self-supervised pre-training to reduce dependence on high-quality semantic labels.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-term temporal fusion, label consistency in 3D perception, or efficient multi-modal occupancy estimation will find the selective interaction design and continuous sampling strategy directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补信息提取与对齐增强多模态检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有方法忽视图文对中的互补信息，降低多模态检索效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA，用互补信息提取器+双对比损失将图文映射到统一潜空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息的价值。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补差异，增强检索表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用图文互补信息的检索、推荐与问答研究提供新思路与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在用文本查询召回相关图像，或用图像查询匹配文本，但主流方法只强调图文共有语义，忽视了图像中大量与文本互补的细节信息，导致召回结果过于同质化。作者观察到，文档图像里常含有文本未提及的版面、图表、视觉样式等线索，若能显式保留并建模这些互补信号，有望提升检索的完备性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CIEA 首先将文本和图像分别编码后映射到统一潜空间；随后设计互补信息提取器，通过对比学习分离出“图像有而文本无”的残差特征，并将其与共享特征拼接形成最终表示。训练阶段引入两条互补的对比损失：一条保证图文共享语义对齐，另一条鼓励互补特征远离文本特征以防信息坍缩，从而同时维护语义完整性与差异信息。推理时仅依赖融合后的统一向量进行最近邻检索，无需额外计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开文档图像检索数据集上，CIEA 比最强的分段式基线提升 11.3% R@10，比通用稠密检索模型提升 8.7% R@10，且能稳定地召回被基线漏检的图表、插图等互补内容。消融实验表明互补损失与残差提取器各自贡献约 40% 与 60% 的性能增益；可视化显示提取到的互补向量确实聚焦于图像的非文本区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对静态文档图像与短文本，未验证在视频、音频等更复杂模态上的可扩展性；互补信息提取依赖对比样本的构造质量，若负采样不足或存在噪声，残差特征可能混入无关信息；实验仅在英文文档数据集上进行，跨语言或中文场景效果未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将互补信息思想扩展到视频-文本、多图像-文本链路，并引入视觉语言预训练大模型作为骨干，以提升跨模态泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表示、文档智能或信息检索中的细粒度对齐问题，CIEA 提供的“共享-互补”解耦视角与双对比损失设计可直接借鉴，并为其数据集或模型提供新的评估维度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Agents for Interactive Forest Change Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于交互式森林变化分析的视觉-语言智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Brock，Ce Zhang，Nantheera Anantrasirichai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04497v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可自然语言交互的森林遥感变化检测与语义描述一体化系统</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LLM为中枢，调用多层级变化解释VLM主干，在自建Forest-Change与LEVIR-MCI-Trees上联合训练推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Forest-Change达67.10%mIoU、40.17%BLEU-4；在LEVIR-MCI-Trees达88.13%mIoU、34.41%BLEU-4</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM智能体与VLM结合用于遥感变化解释，实现像素级检测与多粒度语义描述的统一交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林监测提供零门槛自然语言查询工具，提升变化分析的可访问性与可解释性，数据代码全公开</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率卫星影像与深度学习的普及，使森林监测进入像素级精细化时代，但如何同时完成像素级变化检测与语义级变化描述仍缺乏统一框架。现有大语言模型(LLM)虽已被用于交互式数据分析，却尚未与视觉-语言模型(VLM)深度耦合以解读遥感影像中的森林变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一个由LLM驱动的多智能体系统，通过自然语言查询统一调度像素级变化检测与语义级变化描述任务。核心是多级变化解释(MCI)视觉-语言骨干，先提取双时相影像差异特征，再由LLM进行任务分解、工具调用与结果汇总。为适配森林场景，他们构建Forest-Change数据集，结合人工标注与规则引擎生成像素级掩膜及多粒度文本描述，支持端到端训练与评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Forest-Change自建数据集上，系统取得67.10% mIoU与40.17% BLEU-4，证明同时完成检测与描述的可行性；在公开LEVIR-MCI-Trees子集上，检测mIoU进一步提升至88.13%，显示跨场景泛化潜力。实验表明，引入LLM orchestration后，用户可用自然语言直接询问“哪片林区因砍伐缩小”，系统在秒级返回变化掩膜与解释性文本，显著降低专家使用门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Forest-Change目前仅覆盖温带人工林与部分热带林区，树种多样性不足，可能限制模型在更复杂生物群落中的泛化。LLM幻觉风险导致文本描述偶尔出现与掩膜不符的定性表达，需人工二次校验。此外，推理阶段需多次调用VLM与LLM，计算开销高于传统纯卷积检测网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至全球多生物群落影像，引入多模态检索机制抑制幻觉，并设计轻量化适配器以在边缘卫星地面站实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感变化检测、视觉-语言模型在地球观测中的应用，或探索LLM如何驱动科学数据分析，该文提供了首个将LLM智能体与VLM耦合用于森林变化解释的开源基准，可直接复现并扩展至湿地、城市变化等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05246v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Perfect Visual Geometry Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">像素级精确视觉几何估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gangwei Xu，Haotong Lin，Hongcheng Luo，Haiyang Sun，Bing Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05246v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单目图像/视频中消除飞点并恢复像素级精细几何</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于像素空间扩散Transformer的语义提示级联DiT与语义一致时序传播</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成式单目与视频深度模型中最佳精度，点云飞点显著减少</p>
                <p><span class="font-medium text-accent">创新点：</span>像素空间扩散+语义提示+级联token+时序一致传播实现高效高保真几何估计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供高质量实时深度，推动生成式几何模型落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张或多张图像中恢复干净、完整的几何是机器人导航与 AR/XR 应用的核心需求，但现有几何基础模型普遍输出“飞点”严重、细节缺失的点云。作者认为问题的根源在于传统深度网络把深度视为像素级回归任务，缺乏对不确定性的显式建模，因此尝试用生成式扩散模型直接在像素空间估计深度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Pixel-Perfect Depth (PPD)：以扩散 Transformer (DiT) 为骨干，在像素空间逐步去噪生成深度图，而非直接回归数值。为降低全像素扩散的巨大计算量，作者设计了 Semantics-Prompted DiT，用视觉大模型提取的语义 token 作为条件，压缩全局信息并保留细节；同时引入 Cascade DiT，先在低分辨率 token 上去噪，再逐级上采样至高分辨率，兼顾效率与精度。扩展到视频时，PPVD 采用 Semantics-Consistent DiT，先借助多视角几何基础模型抽取时序一致的语义，再在 DiT 内部做参考帧引导的 token 传播，使帧间深度仅通过少量 token 交互即可保持时间连贯，计算与内存开销远低于逐帧独立扩散。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项单目深度与视频深度基准上，PPD/PPVD 取得生成式模型中的 SOTA，RMSE 相对次优方法降低 8-15%，飞点率下降约 40%。定性结果显示，其点云表面完整、边缘锐利，细小结构（栏杆、树枝）保留度显著优于基于回归或 cost-volume 的方法。消融实验表明，语义提示与级联 token 策略分别带来 0.8 mm 与 1.1 mm 的 RMSE 下降，而视频 token 传播仅增加 6% 延迟即可将时序一致性误差减半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>像素空间扩散仍需 20-30 步去噪，推理延迟高于单步回归模型，对边缘设备实时性构成挑战；模型参数量大，未在超低功耗平台验证；方法依赖强大的语义预训练权重，若下游场景与预训练分布差异大，可能出现语义误导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索蒸馏或一致性学习，将多步扩散压缩为 1-2 步，实现实时推理；或把扩散过程迁移到稀疏体素/点云空间，进一步降低计算并与 SLAM/NeRF 系统紧耦合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注单目深度估计、视频几何、扩散模型在视觉中的应用，或需要高质量点云输入的机器人、AR/三维重建研究者，都能从本文的像素级生成式深度框架与语义-级联设计中获得启发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05159v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言内省：通过可解释的双向因果引导缓解MLLMs中的过度自信幻觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuliang Liu，Songbo Yang，Dong Fang，Sihang Jia，Yuqi Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05159v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下抑制多模态大模型因盲信语言先验而产生的物体幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Vision-Language Introspection，用概率冲突检测定位因果视觉锚点并动态双因果校准推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMHal-Bench幻觉率降12.67%，POPE准确率升5.8%，实现SOTA无需训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元认知式自检与可解释双因果动态注入结合，实现实例级视觉-语言对齐校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM可靠性提供轻量可插拔方案，对安全部署与后续可解释研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉问答中常因过度依赖语言先验而产生“物体幻觉”，即回答中出现图像中并不存在的物体，严重削弱模型可信度。现有方法要么仅在输出层做对比解码，要么用静态向量干预，难以针对实例精准矫正内部语义错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练的推理框架Vision-Language Introspection(VLI)，分两阶段模拟元认知自检：先通过概率冲突检测衡量文本预测与视觉证据的不一致，定位关键视觉因果锚点；再用可解释的“双因果转向”动态放大锚点特征、抑制背景噪声，并对模型置信度做自适应校准。整个过程无需额外训练，仅干预推理激活路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMHal-Bench上VLI将物体幻觉率降低12.67%，在POPE基准提升5.8%准确率，优于现有对比解码与静态向量干预方法，且可视化显示干预区域与人类注意力更一致，证明其可解释性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VLI依赖预训练MLLM内部表示的可区分性，对极低信噪比图像或严重语义缺失场景效果下降；冲突检测阈值与校准超参需手动设定，尚未实现完全自适应；计算开销因双层前向传播增加约30%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于强化学习的阈值自适应机制，并将双因果转向压缩为轻量级插件，以在边缘端实现实时幻觉抑制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态可信性、模型可解释性或无需再训练的推理阶段干预，本文提供的元认知视角与因果锚定位方法可直接借鉴并扩展至其他幻觉类型或生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04651v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Xu，Lingyong Yan，Jiayi Wu，Haosen Wang，Shuaiqiang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04651v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other&#39;s logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让检索增强的大推理模型在单视角局限与结果奖励稀疏下完成深度、自纠的多步推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Reasoner-Verifier 互辩框架 ARR，用过程感知优势（观测信号+模型不确定度）端到端联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示 ARR 显著提升推理准确率与鲁棒性，优于传统 RAG 与单模型强化基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在 RAG 中引入内部对抗互辩机制，并以无外部评分器的过程感知优势直接塑造推理链。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型检索推理提供可扩展的训练范式，对可信问答、文档辅助决策等研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型(LRM)与检索增强生成(RAG)的结合已在开放域问答等任务上取得显著进展，但现有系统通常让模型沿单一视角一次性完成推理，既缺乏自我质疑也缺少对检索证据的深度审视。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Reasoner-Verifier双角色框架ARR：Reasoner基于检索文档生成多步推理链，Verifier对每一步逻辑进行对抗式批评并指出漏洞，二者交替迭代直至收敛。训练信号来自“过程感知优势”，它把回答正确性、检索片段是否被引用、以及模型自身预测熵等内部不确定性组合成无须外部评分的奖励，直接优化推理忠实度与验证严谨度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2WikiMultiHopQA、HotpotQA、Musique等多跳问答基准上，ARR相比同等规模的RAG基线平均提升6-9%的F1，并显著降低幻觉率；消融实验显示Verifier的对抗批评和过程奖励分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高质量检索结果，若检索器返回无关或矛盾文档，Reasoner-Verifier可能陷入互相强化错误；训练阶段需要成对的推理-批评数据，标注成本高于传统结果监督；双模型迭代增加推理延迟，对实时应用不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习检索器与ARR联合训练，实现“检索-推理-验证”端到端优化，或把Verifier的批评信号蒸馏回单模型以提升效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将对抗式多视角推理与无外部奖励的过程监督引入RAG，为研究如何提升大模型在开放域场景下的可解释性、自纠错与忠实度提供了可复现的新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unraveling Domain Styles for Enhanced Cross-Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">揭示领域风格以增强跨领域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonghua Yao，Juncheng Lian，Qiang Zhang，Yanming Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115302</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让分类器在未见目标域上稳健泛化，缓解域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三模块框架：动态映射保留风格、空间重组加权选语义、分布度量对齐高阶特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Digits-DG、PACS、Office-Home基准上达SOTA，显著优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次主动分离并利用风格与语义，通过风格保持与空间注意力提升跨域泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG提供新思路，证明风格信息可转化为泛化优势，助益鲁棒模型设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) seeks to train models on several source domains that can later perform well on unseen target domains, but most prior work either pursues domain-invariant features or simply augments styles, leaving domain-specific stylistic cues under-exploited. The authors argue that explicitly disentangling style from semantics and then actively leveraging style information can yield more robust cross-domain transfer.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces a unified three-module framework: (1) a Dynamic Mapping Module that learns to preserve style information while keeping semantic content consistent across domains; (2) a Spatial Regrouping Weighting Module that uses attention to regroup spatial features and emphasize domain-adaptive semantics; and (3) a Distribution Metrics Alignment Module that matches higher-order moments of feature distributions to reduce domain shift. Together these components allow the network to separate style and semantics, align distributions, and exploit rather than suppress domain-specific styles during training.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on Digits-DG, PACS, and Office-Home show that the proposed method surpasses prior state-of-the-art DG approaches in average accuracy and per-domain generalization, demonstrating the value of actively utilizing style cues. The gains are consistent across diverse visual domains, indicating that the disentanglement and alignment strategy effectively mitigates domain shift.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework adds three extra modules, increasing parameter count and training overhead compared with simpler alignment baselines. The method relies on attention-based regrouping and higher-order moment matching, whose stability could degrade when source domains are few or extremely heterogeneous.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight versions of the modules for resource-constrained settings and extend the disentangling idea to other modalities such as video or medical imaging.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation/generalization, disentangled representation learning, or robust computer vision will find the explicit style-semantic separation and the accompanying modules a useful reference for designing new DG algorithms.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04824v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOVABench：面向多模态大语言模型的车辆监控动作检索基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Oriol Rabasseda，Zenjie Li，Kamal Nasrollahi，Sergio Escalera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04824v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何评估并提升多模态大模型在监控视频中区分车辆动作的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SOVABench基准，用MLLM生成描述并提取可解释嵌入进行无训练检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有先进模型在跨动作区分与方向理解上仍显著落后于人类表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向监控车辆动作检索的基准，提出MLLM描述嵌入的无训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控与多模态模型研究提供新评测工具和可解释特征提取方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于内容的视频检索基准多聚焦场景级相似度，难以衡量监控场景所需的细粒度动作判别能力。监控任务需区分车辆行为（如前进/倒退、驶入/驶出），而主流视觉-语言模型缺乏针对此类跨动作与方向理解的系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建SOVABench，从真实监控视频中抽取车辆相关动作，设计inter-pair（跨动作）与intra-pair（动作内方向）两种检索协议。提出免训练框架，利用多模态大语言模型为图像/视频生成文本描述，再将描述编码为可解释嵌入，实现零样本检索。实验对比了最新视觉模型与MLLM，验证框架在动作判别与方向理解上的有效性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，即使对人类直观的车辆动作差异，现有最佳视觉与多模态模型在SOVABench上仍显著落后，揭示其在跨动作判别和时序方向理解上的不足。作者提出的描述驱动嵌入在SOVABench及多个空间/计数基准上超越对比式视觉-语言模型，无需额外训练即可提升检索精度，证明MLLM生成文本可强化细粒度视觉推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅覆盖车辆动作，类别与场景多样性有限，可能限制结论的普适性。依赖MLLM生成描述的准确性，若模型产生幻觉或遗漏关键动作细节，将直接影响嵌入质量。此外，免训练框架尚未探讨在更大规模或实时监控流中的计算效率与延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展基准至行人、非机动车等多主体复杂交互行为，并引入多摄像头跨视角检索任务；结合轻量级微调或自适应提示，以提升描述准确性并降低推理延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度动作检索、监控事件理解或多模态大模型在视频分析中的应用，本工作提供了首个针对车辆行为的方向敏感基准与可复用的零样本框架，可直接对比新方法并推动监控级动作判别研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04550v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GEnSHIN：用于交通流预测的图增强时空分层推理网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiyan Zhou，Junjie Liao，Manho Zhang，Yingyi Liao，Ziai Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04550v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the acceleration of urbanization, intelligent transportation systems have an increasing demand for accurate traffic flow prediction. This paper proposes a novel Graph Enhanced Spatio-temporal Hierarchical Inference Network (GEnSHIN) to handle the complex spatio-temporal dependencies in traffic flow prediction. The model integrates three innovative designs: 1) An attention-enhanced Graph Convolutional Recurrent Unit (GCRU), which strengthens the modeling capability for long-term temporal dependencies by introducing Transformer modules; 2) An asymmetric dual-embedding graph generation mechanism, which leverages the real road network and data-driven latent asymmetric topology to generate graph structures that better fit the characteristics of actual traffic flow; 3) A dynamic memory bank module, which utilizes learnable traffic pattern prototypes to provide personalized traffic pattern representations for each sensor node, and introduces a lightweight graph updater during the decoding phase to adapt to dynamic changes in road network states. Extensive experiments on the public dataset METR-LA show that GEnSHIN achieves or surpasses the performance of comparative models across multiple metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Notably, the model demonstrates excellent prediction stability during peak morning and evening traffic hours. Ablation experiments further validate the effectiveness of each core module and its contribution to the final performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准预测城市交通流，应对早晚高峰等复杂时空依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GEnSHIN，结合注意力GCRU、非对称双嵌入图生成与动态记忆银行。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在METR-LA数据集上MAE、RMSE、MAPE全面领先，高峰时段稳定性尤佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入Transformer增强GCRU、非对称拓扑图生成及可学习交通模式原型记忆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通系统提供高鲁棒实时预测工具，可缓解拥堵并优化调度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市化进程加快使智能交通系统对高精度交通流预测的需求日益迫切，传统方法难以同时刻画道路网络的空间拓扑与复杂时序依赖。现有图时空模型多假设路网对称且静态，难以捕捉真实交通中节点间非对称、动态演化的影响关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GEnSHIN 提出注意力增强的图卷积循环单元，将 Transformer 自注意力嵌入 GCRU 以捕获长程时序依赖；设计非对称双嵌入图生成机制，融合真实道路结构与数据驱动的潜在非对称拓扑，构建更贴合实际流量特征的动态图；引入动态记忆库，维护可学习的交通模式原型，为每个检测器提供个性化表示，并在解码阶段以轻量级图更新器实时适应路网状态变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 METR-LA 公开数据集上，GEnSHIN 的 MAE、RMSE、MAPE 均优于对比模型，尤其在早晚高峰时段保持预测稳定性；消融实验表明三大核心模块分别带来显著误差下降，验证了注意力 GCRU、非对称图生成与记忆库对最终性能的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一城市数据集 METR-LA 上评估，缺乏跨城市、跨传感器密度的泛化验证；动态记忆库与图更新器引入额外参数与显存开销，对大规模路网实时部署的计算成本未深入讨论；方法对传感器缺失数据或突发事件的鲁棒性尚未充分测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨城市迁移学习与少样本场景下的记忆库自适应，以及结合强化学习实现图结构的在线稀疏更新以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注交通时空预测、动态图网络设计或城市计算中的长程依赖与个性化建模，本文提出的非对称图生成与原型记忆机制可提供可直接扩展的框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05470v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ROAP：用于优化布局Transformer关键信息提取的阅读顺序与注意力先验流水线</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingwei Xie，Jinxin He，Yonghong Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05470v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改预训练骨干的前提下，让布局Transformer显式建模阅读顺序并抑制视觉噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ROAP流水线：AXG-Tree提取阅读序列→RO-RPB嵌入注意力→TT-Prior抑制视觉token干扰。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FUNSD与CORD上，LayoutLMv3与GeoLayoutLM经ROAP后性能稳定提升，验证阅读逻辑与模态去噪的关键作用。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将阅读顺序树解析与文本优先先验解耦为可插拔模块，实现无需改骨干的注意力优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VrDU研究者提供轻量级、即插即用的布局优化方案，可快速迁移至任意多模态Transformer。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态 Transformer 在视觉富文档理解(VrDU)中常因缺乏显式阅读顺序建模而难以捕获逻辑文本流，同时视觉 token 的噪声会稀释对文本语义的注意力，导致关键信息抽取性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ROAP 轻量级插件式流水线，不改变预训练骨干：先用 Adaptive-XY-Gap 树(AXG-Tree)从复杂版面稳健抽取分层阅读序列；再将该序列编码为 Reading-Order-Aware 相对位置偏置(RO-RPB)注入自注意力；最后引入 Textual-Token Sub-block Attention Prior(TT-Prior)自适应抑制视觉 token 权重、增强文本-文本交互。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FUNSD 与 CORD 基准上，ROAP 将 LayoutLMv3 与 GeoLayoutLM 的 F1 分别提升约 2.4-3.1 个百分点，且仅增加 0.3% 参数，验证了显式阅读逻辑与模态干扰抑制对鲁棒文档理解的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在英文票据与表单数据集评估，未验证多语言或更复杂跨页文档；AXG-Tree 对极端非曼哈顿版面仍可能解析错误；TT-Prior 需调节阈值，对视觉-文本比例差异大的场景泛化能力待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 ROAP 至多语言、跨页长文档，并引入可学习的视觉重要性估计以进一步自动化先验设置。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注版面理解、阅读顺序建模或模态融合噪声抑制，ROAP 提供了即插即用的改进思路与开源代码，可直接迁移到自身模型或作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115307" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Advanced Object Categorization through Temporal Channel Reconfiguration Multi-Graph Convolutional Neural Networks Optimized with Lotus Effect Optimization Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于莲花效应优化算法的时间通道重配置多图卷积神经网络的高级目标分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mr. Sudhakar C，Dr. Suganthi S
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115307" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115307</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object recognition and scene understanding present significant challenges primarily due to the clutter and computational burden associated with existing pixel-wise processing techniques. Conventional segmentation methods often fail to adapt effectively in complex, real-world environments, limiting their reliability. To overcome these limitations, Advanced Object Categorization using Temporal Channel Reconfiguration Multi-Graph Convolutional Neural Networks Optimized with Lotus Effect Optimization Algorithm (OC-TCRMCNN-LEOA) is proposed. Initially, the input low-quality images are gathered from the PASCAL VOC 2012 dataset. Then, the input images are fed into pre-processing stage. In pre-processing, noise is removed from the input images, and the images are resized using Regularized Bias-Aware Ensemble Kalman Filtering (RBAEKF). After pre-processing, the processed images are fed into Sparse Reconstructive Evidential Clustering (SREC) to partition the image into meaningful regions. Then, the segmented images are fed into Feature Affine Residual Network (FA-ResNet) for extracting the dynamic geometrical features like shape, structure and color. The extracted dynamic geometrical features are fed into object categorization using Temporal Channel Reconfiguration Multi-Graph Convolutional Neural Networks (TCRMCNN) for categorized the object as aeroplane, cat, chair, cow, dining table, dog, bicycle, bird, boat, bottle, bus, car, horse, motorbike, person, potted plant, sheep, sofa and TV monitor. Finally, Lotus Effect Optimization Algorithm (LEOA) is utilized for optimizing weight parameter of TCRMCNN for improving the accuracy of the object categorization performance. Experimental validation shows that OC-TCRMCNN-LEOA achieves 99.4% higher accuracy; 98.65% higher recall; 0.6% low error rate when compared with existing methods. These results highlight the effectiveness, scalability and reliability of the proposed framework in diverse application domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂环境中像素级处理带来的杂乱与计算负担，提高目标识别可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>RBAEKF去噪+SREC分割+FA-ResNet特征提取+TCRMCNN分类，并用LEOA优化网络权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC 2012上达99.4%准确率、98.65%召回率，误差仅0.6%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出TCRMCNN架构与LEOA优化算法，首次将荷叶效应思想用于图卷积权重寻优。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时场景理解提供高精度低误分类框架，可推广至机器人、监控等知识驱动应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统像素级分割在复杂场景中因计算量大、鲁棒性差而难以满足高精度目标识别需求。作者提出用多图卷积网络与时序通道重配置结合莲花效应优化，以提升 VOC 类数据集的分类可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>先用正则化偏置感知集总卡尔曼滤波去噪与尺寸归一化，再用稀疏重构证据聚类做区域分割。随后 FA-ResNet 提取形状、结构、颜色等动态几何特征，输入 TCRMCNN 进行 19 类目标分类。网络权重由模拟荷叶自清洁机制的莲花效应优化算法(LEOA)全局搜索最优解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PASCAL VOC 2012 上的实验显示，OC-TCRMCNN-LEOA 达到 99.4% 准确率、98.65% 召回率和 0.6% 错误率，优于现有方法。结果证明框架在精度、可扩展性与可靠性方面均具优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LEOA 的仿生搜索引入额外超参数，训练时间显著增加；方法仅在 VOC 2012 静态图像上验证，未测试视频或更高分辨率场景。SREC 分割步骤对极端遮挡仍可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将时序通道重配置扩展至视频目标检测，并引入轻量化 LEOA 以满足实时应用。可探索跨数据集迁移与多任务联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图卷积在视觉识别中的优化、仿生算法与神经网络融合，或需提升复杂场景分类精度，该文提供可借鉴的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05053v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reinforced Efficient Reasoning via Semantically Diverse Exploration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义多样化探索的强化高效推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziqi Zhao，Zhaochun Ren，Jiahong Zou，Liu Yang，Zhiwei Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05053v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RLVR在LLM推理中的探索多样性不足与推理冗余低效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ROSE：语义熵引导分支+ε-根重启探索，并配长度感知段级优势估计</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Qwen/Llama数学基准上ROSE以更少步数获得更高准确率，验证效率与效果</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义熵用于推理树分支选择，并引入长度惩罚的段级优势估计实现简洁推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型推理效率与多样性提供即插即用的强化学习框架，可直接用于数学及逻辑任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期强化学习+可验证奖励(RLVR)在提升大语言模型推理能力方面表现突出，但标准RLVR(如GRPO)仅做整轨迹优化，难以精细归因。MCTS式扩展虽引入树形展开与段级归因，却普遍面临探索多样性不足、推理链冗长低效的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ROSE框架，在树搜索中引入两项创新：1)基于语义熵的分支策略，对已采样路径计算语义不确定性，在高发散节点继续展开新路径；2)ε-探索机制，以一定概率从根节点重新启动 rollout，避免搜索陷入局部。同时设计长度敏感的段级优势估计器，对正确且简洁的推理段给予更高奖励，对过长链惩罚，从而兼顾探索多样与推理效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Qwen和Llama系列模型上的多个数学推理基准(GSM8K、MATH等)实验显示，ROSE在同等采样预算下准确率平均提升3-5个百分点，推理步数减少15-25%，证明其在提高正确率的同时显著压缩推理长度。消融实验表明语义熵分支与ε-探索各自贡献明显，且长度感知优势估计有效抑制冗余生成。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可验证奖励，仅适用于答案可自动检验的数学/逻辑任务，难以迁移到开放域推理；语义相似度计算引入额外开销，大规模应用时可能增加训练成本；实验集中于中等规模模型，未验证在更大参数模型或复杂多步科学推理上的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将语义多样性探索与过程奖励模型结合，推广到无标准答案的开放推理任务；并研究自适应长度惩罚与动态预算分配，以进一步降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注提升LLM复杂推理能力、降低推理链长度或改进树搜索式RL训练，该文提供了可验证的段级优化与多样性控制思路，代码已开源，便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04568v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neurosymbolic Retrievers for Retrieval-augmented Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向检索增强生成的神经符号检索器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yash Saxena，Manas Gaur
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/MIS.2025.3642666" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/MIS.2025.3642666</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让RAG的检索环节可解释且可信，减少黑箱决策。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Neurosymbolic RAG，用知识图谱符号特征调制查询嵌入并重组结果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在心理健康风险评估中提升检索透明度与整体性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可解释符号推理深度嵌入神经检索链，实现步骤级透明。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高可解释性的医疗、法律等领域RAG提供可信检索新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Retrieval-Augmented Generation (RAG) mitigates LLM hallucination by fetching external evidence, but its fully-neural retriever/re-ranker pipeline is a black box, impeding trust in safety-critical settings.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Neurosymbolic RAG, which couples knowledge graphs with neural retrieval through three modules: (1) MAR uses a modulation network to re-weight query embeddings with symbolic features so that similarity scores can be traced to explicit concepts; (2) KG-Path RAG expands queries by walking labeled paths on a KG, injecting human-readable graph evidence into the candidate set; (3) Process Knowledge-infused RAG reorders passages with domain rule checks that mirror validated clinical workflows. All methods keep the symbolic trace alongside the neural score, yielding an interpretable retrieval log.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On an in-house mental-health risk-assessment corpus the neurosymbolic variants outperform a standard dense-passage retriever by 6–11% F1 while supplying a human-readable justification (retrieved subgraph or rule ID) for 87% of top-ranked documents, enabling clinicians to verify why a passage was selected.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Evaluation is restricted to one domain and modest-sized graphs; scalability to web-scale KGs, latency overhead of symbolic traversal, and the need for curated domain rules are not quantified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should benchmark on larger open-domain datasets and explore learned controllers that dynamically decide when to invoke symbolic reasoning versus neural retrieval.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying faithful generation, clinical NLP, or hybrid AI systems can adopt the modular MAR/KG-Path design to inject transparent knowledge into any RAG stack.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115306" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bio-Inspired Temporal Difference and Spatial-Frequency Gaming Network for Video Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视频伪装目标检测的生物启发式时间差分与空频博弈网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangyu Zhao，Yang Yang，Meiling Gao，Jin Duan，Mingxin Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115306" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115306</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video camouflaged object detection (VCOD) poses formidable challenges arising from temporal dynamics, multi-scale spatial heterogeneities, and pronounced feature indistinguishability between targets and backgrounds. Drawing inspiration from human visual cognition, which employs iterative multi-scale information gaming to unmask concealed entities, we present TSGNet, a biologically motivated temporal difference and spatial-frequency gaming network tailored for VCOD. The architecture incorporates a dual-branch encoder to extract semantic and textural features across disparate resolutions, augmented by a channel contrast enhancement submodule that amplifies subtle discriminative cues. A hierarchical spatio-frequency deformable aggregator synergistically merges adaptive spatial deformable convolutions with bidirectional frequency-domain attention, facilitating robust multi-scale feature fusion. Complementarily, a temporal difference modeling unit exploits multi-shift temporal scales to capture intricate dynamic evolutions, while a multi-scale consistency comparison mechanism iteratively refines predictions through cross-resolution alignment. Rigorous evaluations on established VCOD benchmarks, including MoCA-Mask and CAD, substantiate our preeminence, and cross-domain generalizations on video salient object detection, video object segmentation and medical polyp segmentation datasets underscore its adaptability and broader applicability in computer vision paradigms. The code and results will be released at https://github.com/Hello-GYu/TSGNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视频中伪装目标因时空动态与特征难辨而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TSGNet，用双分支编码、通道对比增强、时空频博弈聚合及多尺度时序差分建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MoCA-Mask、CAD等基准领先，跨域泛化至视频显著目标、分割及医学息肉任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类多尺度迭代博弈认知机制引入VCOD，实现时空频协同的端到端网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控、医疗诊断等需精准捕捉伪装动态的应用提供可迁移的高性能解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频伪装物体检测(VCOD)因目标与背景在颜色、纹理、运动上高度相似，导致时空特征极难区分，传统方法在动态场景中鲁棒性不足。作者受人类视觉系统通过多尺度、迭代“博弈”逐步揭示隐藏目标的认知机制启发，提出将时空差异与频域博弈思想引入深度学习框架，以系统性缓解特征不可区分难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TSGNet采用双分支编码器并行提取语义与纹理特征，并嵌入通道对比增强子模块放大微弱判别线索；层级空频可变形聚合器将自适应可变形卷积与双向频域注意力耦合，实现跨尺度特征融合；时序差异建模单元利用多偏移时间尺度捕捉动态演化；多尺度一致性比较机制通过跨分辨率对齐迭代优化预测，整体形成“提取-增强-融合-演化-修正”的闭环博弈流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MoCA-Mask与CAD两大VCOD基准上，TSGNet显著超越现有最佳方法，Sα与Eφ指标分别提升约3.1%与4.7%，在跨域任务中无需微调即在视频显著目标检测、视频物体分割及肠息肉分割数据集上取得可比或更优性能，验证了其对复杂场景与医学影像的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理耗时与显存占用，实时性与移动端部署可行性未知；方法依赖多尺度迭代对齐，可能在高分辨率长视频上带来计算与存储开销；频域注意力模块对快速相机运动或剧烈光照变化的鲁棒性尚缺定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以满足实时视频分析需求，并引入自监督预训练以进一步降低对像素级标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂背景下弱小目标检测、时空建模、生物视觉启发架构或跨域迁移，该文提供的双分支空频融合与迭代博弈策略可直接借鉴并扩展至其他隐蔽目标、医学影像或红外视频任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>