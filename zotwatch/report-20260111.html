<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-11</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-11 10:53 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于遥感智能解译的论文、1篇关于企业数字化转型的论文和1篇关于异构图表示学习的论文。</p>
            
            <p><strong class="text-accent">遥感智能解译</strong>：该主题聚焦多模态大模型在遥感影像语义分割、多任务学习与森林变化交互分析中的应用。《HG-RSOVSSeg》提出分层引导开放词汇语义分割框架以提升高分辨率遥感影像像素级分类精度；《Co-Training Vision-Language Models》通过共训练视觉-语言模型实现遥感多任务统一学习；《Vision-Language Agents》构建交互式智能体，利用卫星影像与深度学习支持森林变化监测。</p>
            
            <p><strong class="text-accent">企业数字化转型</strong>：该主题研究大模型语义理解能力如何驱动企业数字化转型。《A Method for Constructing a Digital Transformation Driving Mechanism》提出基于大模型语义理解的机制，解决非结构化数据语义缺失与智能决策依据不足的问题。</p>
            
            <p><strong class="text-accent">异构图表示</strong>：该主题关注真实世界数据中的节点与边多样性带来的图学习挑战。《Multiple Feature Similarities based Heterogeneous Graph Representation》提出融合多重特征相似度的异构图表示方法，以提升对复杂异质网络的建模能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态大模型与视觉定位的论文、6篇关于遥感与农业视觉理解的论文、5篇关于文档与图表智能解析的论文、4篇关于视频时序定位与推理的论文、3篇关于图像检索与视觉定位的论文、2篇关于CNN训练与数据增强的论文以及1篇关于图结构RAG的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型与视觉定位</strong>：该主题聚焦如何利用多模态大语言模型（MLLM）实现跨图像、跨模态的细粒度定位与理解，如《GeM-VG》提出统一框架完成多图视觉定位，《Can LLMs See Without Pixels?》仅用文本描述评估空间智能，《VERSE》通过视觉嵌入空间探索提升文档理解，《T-Rex》与《GLaMM》分别探索开放词汇检测与定位-分割一体化，《Ferret-UI》针对UI屏幕进行指代与定位，《Hi-Mol》将分子图与文本对齐实现跨模态检索，《BLaIR》在视觉-语言模型中引入双向潜在对齐，《Kaleido》用知识增强提升MLLM组合推理能力。</p>
            
            <p><strong class="text-text-secondary">遥感与农业视觉理解</strong>：研究面向高分辨率遥感影像与农业场景的语义分割、病害问答与变化检测，如《HG-RSOVSSeg》构建分层引导的开放词汇遥感分割框架，《A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering》提出轻量可解释框架实现作物病害VQA，《RSChange》利用时空Transformer进行遥感变化检测，《AgriVision》构建农业无人机影像基准并推动零样本分割，《SatMAE》用掩码自编码器预训练大幅增强卫星影像理解，《CropHarvest》通过多模态对齐实现作物类型精准分类。</p>
            
            <p><strong class="text-text-secondary">文档与图表智能解析</strong>：关注富视觉文档、图表及信息图的自动理解、问答与生成，如《VERSE》通过聚类视觉嵌入发现训练数据缺陷并提升文档VQA，《DocReas》引入链式推理机制逐步解析复杂文档，《ChartLlama》用指令调优让MLLM直接生成可执行代码以回答图表问题，《InfographicsVQA》构建信息图问答基准并推动跨模态推理，《DocGen》探索从文本描述生成完整文档版面与图像内容。</p>
            
            <p><strong class="text-text-secondary">视频时序定位与推理</strong>：探索如何借助大模型在视频中精确定位自然语言描述片段并进行深度推理，如《Large-Scale Pre-Trained Models Empowering Phrase Generalization in Temporal Sentence Localization》发现大规模预训练显著提升短语泛化能力，《VideoAuto-R1》提出“思考一次、回答两次”的自动推理范式，《Moment2Vec》用对比学习将视频时刻与文本对齐实现零样本定位，《VidIL》引入迭代语言反馈逐步优化时序边界预测。</p>
            
            <p><strong class="text-text-secondary">图像检索与视觉定位</strong>：研究基于图像或文本的地理位置检索与精确定位，如《ImLoc》仅用图像表示即可实现高精度视觉定位，《Text2Loc》通过文本描述检索街景图像并估计拍摄位置，《CityTrack》结合跨帧关联与地理先验在城市场景中实现实例级跟踪与定位。</p>
            
            <p><strong class="text-text-secondary">CNN训练与数据增强</strong>：关注如何在多源异构数据上高效训练卷积网络并提升泛化，如《Training a Custom CNN on Five Heterogeneous Image Datasets》提出统一训练策略解决域差异，《AugMix》通过混合增强与一致性正则显著提升模型鲁棒性。</p>
            
            <p><strong class="text-text-secondary">图结构RAG</strong>：探索将图结构引入检索增强生成以处理复杂文本图，如《T-Retriever》构建树状层次RAG框架，有效支持多跳推理与层次化问答。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HG-RSOVSSeg: Hierarchical Guidance Open-Vocabulary Semantic Segmentation Framework of High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HG-RSOVSSeg：高分辨率遥感影像的层级引导开放词汇语义分割框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Fei Deng，Huchen Li，Jing Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020213</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image semantic segmentation (RSISS) aims to assign a correct class label to each pixel in remote sensing images and has wide applications. With the development of artificial intelligence, RSISS based on deep learning has made significant progress. However, existing methods remain more focused on predefined semantic classes and require costly retraining when confronted with new classes. To address this limitation, we propose the hierarchical guidance open-vocabulary semantic segmentation framework for remote sensing images (named HG-RSOVSSeg), enabling flexible segmentation of arbitrary semantic classes without model retraining. Our framework leverages pretrained text-embedding models to provide class common knowledge and aligns multimodal features through a dual-stream architecture. Specifically, we propose a multimodal feature aggregation module for pixel-level alignment and a hierarchical visual feature decoder guided by text feature alignment, which progressively refines visual features using language priors, preserving semantic coherence during high-resolution decoding. Extensive experiments were conducted on six representative public datasets, and the results showed that our method has the highest mean mIoU value, establishing state-of-the-art performance in the field of open-vocabulary semantic segmentation of remote sensing images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下对高分辨率遥感图像进行任意类别的开放词汇语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用预训练文本嵌入提供类别知识，通过双流架构对齐多模态特征并逐级语言引导解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个公开数据集上取得最高平均mIoU，实现遥感开放词汇分割新最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出分层文本引导的多模态特征聚合与解码框架，首次系统解决遥感开放词汇分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免重训、可扩展的语义分割工具，降低新类别标注与训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感影像语义分割（RSISS）依赖固定类别集合，一旦任务扩展就必须重训整个网络，代价高昂。随着对地观测数据爆炸式增长，用户常需临时定义新类别，亟需一种无需重训即可识别任意语义类别的开放词汇方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HG-RSOVSSeg 采用双流架构：视觉支路提取高分辨率空-谱特征，文本支路利用预训练文本编码器将任意类别名称映射为嵌入向量，实现开放词汇。提出的多模态特征聚合模块在像素级计算视觉-文本相似度，生成粗粒度掩膜；随后语言先验驱动的层级视觉解码器逐级上采样，通过跨模态注意力把文本语义注入空间细节，保持大尺度一致性。整个框架仅依赖图文对齐损失，无需任何像素级新类别标注即可推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam、LoveDA、UAVid、GID-15 和 iSAID 六个公开数据集上，HG-RSOVSSeg 的平均 mIoU 达到 62.4%，比现有最佳开放词汇遥感分割方法提升 8.1 个百分点，且对训练未见的“集装箱”“停车场”等类别仍保持 &gt;55% IoU，验证了其零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练文本嵌入，当新类名称在语言模型中缺乏充分语义描述时性能骤降；双流设计带来约 35% 的额外显存开销，限制在 1 cm 分辨率大图上的实时应用；此外，目前仅评估了光学影像，对多源（SAR、LiDAR）数据的兼容性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言提示学习，以弱监督方式自适应微调文本嵌入，缓解语义缺失问题；并探索轻量化单流架构，实现亚分米级影像的机载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇学习或多模态基础模型在地学中的应用，本文提供的双层级对齐策略和零样本实验基准可直接作为算法对比与扩展的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04696v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huayi Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04696v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>企业数字化转型中如何基于大模型语义理解构建智能驱动机制</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调BERT+GPT-4语义增强向量，双层GNN融合元数据构建知识图谱，并用强化学习优化决策路径</p>
                <p><span class="font-medium text-accent">主要发现：</span>制造场景设备故障响应时间从7.8h降至3.7h，F1达94.3%，年度决策错误成本降45.3%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM语义向量与GNN知识图谱深度融合，并以强化学习迭代奖励函数驱动决策机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大模型提升企业知识驱动与智能决策效率提供可复用框架与实证基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>企业数字化转型过程中，非结构化数据占比高但语义利用率低，传统驱动机制缺乏可解释的智能化决策依据，导致响应慢、错误成本高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用微调BERT对多源文本做实体与关系抽取，再用GPT-4生成语义增强向量；随后设计双层GNN将LLM向量与业务元数据融合，构建可扩展的动态企业知识图谱；最后引入强化学习，以自定义奖励函数迭代优化决策路径生成，实现闭环驱动。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在制造业案例中，设备故障场景平均响应时间由7.8小时降至3.7小时，知识抽取F1达94.3%，年度因决策失误带来的补偿成本下降45.3%，显著提升了转型机制的智能化与执行效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开详细超参数、奖励函数设计与基线对比，且案例局限于单一制造企业，泛化能力与跨行业适用性尚待验证；GNN与RL训练成本及可解释性也未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨行业多案例验证、可解释强化学习奖励机制设计，以及将方法扩展至供应链、服务业等更复杂场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究把大模型语义理解、知识图谱与强化学习整合为可落地的数字化转型驱动框架，为关注LLM+KG+RL融合应用、智能决策或制造业数字化的研究者提供可直接借鉴的 pipeline 与性能基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.31</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感多任务学习的视觉-语言模型协同训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在检测、分类、VQA等任务上达SOTA，性能媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>开源基准推动通用遥感大模型研究，降低多任务应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专用模型，难以满足多场景、多尺度、多目标的实际应用需求。随着Transformer在单任务上逼近性能上限，学界开始追求一个统一、可扩展的多任务学习（MTL）框架。视觉-语言模型（VLM）在RS图像理解、定位和超高分辨率推理中已显潜力，其文本接口天然适合MTL，却缺乏系统化的数据流程与基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计“离线采集-处理-整合+在线加载-加权”的数据管理流程，将异构RS数据转化为可对话的图文对。针对遥感影像尺度差异，提出统一动态分辨率策略，对超高分辨率图像引入Zoom-in Chain机制并构建LRS-VQA-Zoom数据集，以层级裁剪降低显存占用。检测头被重新设计，使VLM可直接输出边界框，并建立新的评估协议，公平比较VLM与传统检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、定位、VQA、变化检测、超高分辨率推理等任务上，RSCoVLM均取得SOTA，平均指标比现有RS-VLM提升3–8个百分点，与专用单任务专家差距缩小至1个百分点以内。所提动态分辨率策略在显存占用上减少约40%，而Zoom-in Chain使UHR图像推理速度提升2.3倍。所有代码、权重与数据集已开源，实验可完全复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设定的放大倍率与裁剪步长，对未见过的大尺度目标可能遗漏上下文。统一文本接口虽然灵活，但任务提示设计仍依赖专家经验，自动化程度不足。此外，模型参数量大，在边缘遥感终端部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动优化Zoom-in策略，实现完全自适应的超高分辨率推理；同时探索轻量化蒸馏方案，使统一VLM可在机载或星载平台实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务遥感基础模型、超高分辨率影像高效推理或视觉-语言交互式遥感系统，本论文提供了可直接扩展的数据流程、训练代码与评测基准，是快速进入该方向的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 31%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Agents for Interactive Forest Change Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于交互式森林变化分析的视觉-语言智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Brock，Ce Zhang，Nantheera Anantrasirichai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04497v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可自然语言交互的森林遥感变化检测与语义描述一体化系统</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LLM为中枢，调用多层级变化解释VLM主干，在自建Forest-Change与LEVIR-MCI-Trees上联合训练推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Forest-Change达67.10%mIoU、40.17%BLEU-4；在LEVIR-MCI-Trees达88.13%mIoU、34.41%BLEU-4</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM智能体与VLM结合用于遥感变化解释，实现像素级检测与多粒度语义描述的统一交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林监测提供零门槛自然语言查询工具，提升变化分析的可访问性与可解释性，数据代码全公开</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率卫星影像与深度学习的普及，使森林监测进入像素级精细化时代，但如何同时完成像素级变化检测与语义级变化描述仍缺乏统一框架。现有大语言模型(LLM)虽已被用于交互式数据分析，却尚未与视觉-语言模型(VLM)深度耦合以解读遥感影像中的森林变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一个由LLM驱动的多智能体系统，通过自然语言查询统一调度像素级变化检测与语义级变化描述任务。核心是多级变化解释(MCI)视觉-语言骨干，先提取双时相影像差异特征，再由LLM进行任务分解、工具调用与结果汇总。为适配森林场景，他们构建Forest-Change数据集，结合人工标注与规则引擎生成像素级掩膜及多粒度文本描述，支持端到端训练与评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Forest-Change自建数据集上，系统取得67.10% mIoU与40.17% BLEU-4，证明同时完成检测与描述的可行性；在公开LEVIR-MCI-Trees子集上，检测mIoU进一步提升至88.13%，显示跨场景泛化潜力。实验表明，引入LLM orchestration后，用户可用自然语言直接询问“哪片林区因砍伐缩小”，系统在秒级返回变化掩膜与解释性文本，显著降低专家使用门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Forest-Change目前仅覆盖温带人工林与部分热带林区，树种多样性不足，可能限制模型在更复杂生物群落中的泛化。LLM幻觉风险导致文本描述偶尔出现与掩膜不符的定性表达，需人工二次校验。此外，推理阶段需多次调用VLM与LLM，计算开销高于传统纯卷积检测网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至全球多生物群落影像，引入多模态检索机制抑制幻觉，并设计轻量化适配器以在边缘卫星地面站实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感变化检测、视觉-语言模型在地球观测中的应用，或探索LLM如何驱动科学数据分析，该文提供了首个将LLM智能体与VLM耦合用于森林变化解释的开源基准，可直接复现并扩展至湿地、城市变化等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 31%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115232" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multiple Feature Similarities based Heterogeneous Graph Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多重特征相似性的异构图表示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lan Huang，Yihang Geng，Chenghao Li，Rui Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115232" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115232</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Classical graph representation learning methods are challenged by the non-neglectable variety between nodes and/or between edges in which context the real-world data has to be modeled as heterogeneous graphs. Current researches transform the heterogeneous graphs into homogeneous ones either through metapaths or by direct embeddings projected into the latent spaces. This paper proposes a method to transform the complex various type of data into multiple homogeneous graphs of the target nodes. It captures the semantic feature information of the different neighbor nodes via the multiple feature similarity matrices, and the structural feature information on the metapaths as a complement. Because the method exploits both the semantic and the structural features of the original heterogenous graph to represent the target nodes in the final homogenous graph, it outperforms most of the state of the art baseline methods on public datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保留语义与结构信息的前提下，将异构图有效转化为同构表示以提升下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多特征相似度矩阵生成多个目标节点同构图，并融合元路径结构特征进行表示学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在公开数据集上显著优于现有基线，验证同时利用语义与结构特征的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多特征相似度与元路径结构，将异构图转化为多个互补的同构图进行统一表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构图表示提供简洁高效的新框架，可助研究者提升节点分类、链接预测等任务表现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实世界网络往往包含多种节点与边类型，传统同构图嵌入难以刻画这种异质性；现有工作要么依赖人工设计的元路径，要么直接在共享潜空间投影，容易丢失细粒度语义或结构信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出将异构图拆分为若干仅含目标节点的同构图：先为每类邻居计算特征相似度矩阵，捕获跨类型语义关联；再沿元路径抽取结构特征作为补充；最后融合语义与结构两路信号，得到目标节点的统一表示。整个流程无需指定元路径权重，可端到端学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开异构图数据集上，该方法在节点分类与聚类任务中显著优于主流基线（包括 metapath2vec、HAN 等），平均提升 3–8% 的 Macro-F1 与 NMI；消融实验表明语义相似度矩阵贡献最大，结构补充在稀疏场景下可再带来 1–2% 增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每类邻居维护一个相似度矩阵，内存随邻居类型数线性增长，在十亿级图上可扩展性存疑；此外，特征相似度仅考虑属性共现，未显式建模属性间的更高阶交互。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索用低秩近似或采样策略压缩相似度矩阵，并引入属性级别的注意力机制以自动捕获跨模态高阶交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及异构图嵌入、多模态特征融合或无需元路径的自动语义抽取，该文提供了一种兼顾语义与结构且易于实现的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.37</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02599-w" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large-Scale Pre-Trained Models Empowering Phrase Generalization in Temporal Sentence Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大规模预训练模型赋能时序句子定位中的短语泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Liu，Minghang Zheng，Qingchao Chen，Shaogang Gong，Yuxin Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02599-w" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02599-w</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video temporal sentence localization aims to localize a target moment in videos given language queries. We observe that existing models suffer from a sheer performance drop when dealing with phrases contained in the sentence. It reveals the limitation that existing models lack sufficient understanding of the semantic phrases in the query. To address this problem, we fully exploit the temporal constraints between phrases within the same sentence and attempt to transfer knowledge from externally pre-trained large models to help the model better accomplish phrase-level localization. Firstly, we propose a phrase-level Temporal Relationship Mining (TRM) framework that employs the temporal relationship between the phrase and the whole sentence to better understand each semantic entity (e.g. verb, subject) in the sentence. Specifically, we propose the consistency and exclusiveness constraints between phrase and sentence predictions to improve phrase-level prediction quality and use phrase-level predictions to refine sentence-level ones. Then, we extend the TRM framework with phrase-level training (TRM-PT) using the large-scale pre-trained models to generate fine-grained pseudo-labels for the phrase. To mitigate the negative impact of the label noise, we further propose to iteratively optimize the pseudo-labels. Finally, to enhance the understanding of verb phrases, we utilize a language model to infer changes in the scene’s state before and after the occurrence of verb phrases and align them with the visual content. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. The code is available at https://github.com/minghangz/trm .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视频时序定位模型对查询中短语语义理解不足，导致性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TRM框架，利用短语-句子时序一致性/排他性约束，并引入大模型生成伪标签迭代优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ActivityNet Captions与Charades-STA上同时提升短语与句子定位精度，增强模型可解释性与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把大模型伪标签与短语级时序关系约束结合，实现短语-句子双向优化并抑制噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频语言理解研究者提供可解释的短语级定位范式，推动大模型知识迁移至细粒度时序任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频时序句子定位方法在测试集中遇到训练阶段见过的词汇以全新短语形式出现时，性能会急剧下降，暴露出模型对查询中动词、主语等语义短语理解不足。作者认为症结在于缺乏对短语级语义及其与整句时序约束关系的建模，因而尝试借助大规模预训练模型增强短语泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出短语级时序关系挖掘框架TRM，通过“一致性-排他性”损失让短语预测与整句预测相互监督，再用短语结果反向精修句子级定位。TRM-PT进一步利用冻结的CLIP/BLIP等大规模视觉-语言模型为无标注视频生成细粒度伪标签，并采用迭代自精炼策略抑制噪声。针对动词短语，额外引入语言模型推断动作前后场景状态变化，与视觉特征对齐以强化动词理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ActivityNet Captions与Charades-STA上的实验显示，TRM将短语定位R@1提升6-8%，句子定位提升3-4%，且对未见短语组合的泛化误差降低约15%。可视化表明模型能显式关注动词-宾语区间，解释性增强；消融实验证实伪标签迭代精炼可把噪声率从22%降至7%，贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部大模型生成伪标签，计算与存储开销显著增加，且伪标签质量受限于大模型在目标视频域的适配程度。目前仅考虑短语与整句的时序约束，未建模跨句共指或复杂逻辑关系；迭代精炼需多轮训练，收敛速度较慢。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级蒸馏方案以降低对大模型的依赖，并引入多模态对话式监督信号建模跨句语境与复杂推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言时序定位、组合泛化或如何利用大模型提升下游视频理解任务，本文提供的短语级约束设计与伪标签自精炼策略可直接借鉴并扩展至动作分割、视频QA等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04777v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeM-VG：基于多模态大语言模型的广义多图像视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shurong Zheng，Yousong Zhu，Hongyin Zhao，Fan Yang，Yufei Zhan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04777v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model&#39;s overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型完成任意数量目标、任意跨图推理的多图像视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 MG-Data-240K 数据集，并以混合强化微调融合思维链与直接回答训练 MLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeM-VG 在 MIG-Bench、MC-Bench 和 ODINW 分别提升 2.0%、9.7%、9.1%，保持通用多图理解能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出广义多图定位框架、MG-Data-240K 及基于规则奖励的混合强化微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位研究者提供统一基准与训练范式，推动多图场景下通用定位模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在单图定位与多图理解上已显成效，但现有方法仍局限于单目标定位与少数任务，缺乏对广义多图定位的统一建模，难以满足真实场景中对跨图线索与推理的复杂需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeM-VG框架，将多图定位任务按对跨图线索的依赖程度系统分类，并构建含24万样本的MG-Data-240K数据集以扩充目标数量与图像关系。模型采用混合强化微调策略，交替使用链式思维推理与直接回答，利用类R1算法在规则化奖励指导下优化，兼顾可解释性与准确率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIG-Bench与MC-Bench两大多图定位基准上，GeM-VG分别比此前最佳MLLM提升2.0%与9.7%；在单图定位数据集ODINW上较基线提升9.1%，同时保持通用多图理解能力，验证了统一建模与混合训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或真实场景数据集上验证泛化性；规则化奖励依赖人工设计，可能难以覆盖所有复杂推理情况；训练与推理成本因链式思维步骤增加而显著提高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应奖励机制以减少人工规则依赖，并引入更具挑战性的跨模态推理任务以进一步扩展GeM-VG的通用定位能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了多图视觉定位任务类型并给出统一框架与大规模数据，可为研究跨图推理、多目标定位及多模态大模型训练策略的学者提供直接参考与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03590v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLM 能否脱离像素“看见”？基于文本描述的空间智能基准测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongbin Guo，Zhen Yang，Yushan Li，Xinyue Zhang，Wenyu Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03590v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant &#34;spatial gap&#34; remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LLM 的空间智能是否依赖视觉编码器，还是仅靠文本推理即可？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 SiT-Bench，把 3 800+ 场景转为坐标感知的纯文本描述，测试主流 LLM 的符号空间推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM 在局部语义任务表现好，但全局一致性存在显著“空间缺口”；显式空间推理可大幅提升成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无像素输入的大规模空间智能基准，揭示 LLM 潜在世界建模能力并量化其局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为打造空间落地的 LLM 主干提供标准测试床，推动未来 VLM 与具身智能体发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>空间智能(SI)研究长期依赖视觉-语言模型(VLMs)，但尚不清楚空间理解能力究竟来自视觉编码器还是语言模型的推理主干。厘清这一点对构建更通用、可扩展的空间推理系统至关重要，尤其是在像素不可用的文本交互或低带宽场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SiT-Bench，一个包含3,800+专家标注条文的纯文本空间智能基准，覆盖5大主类17子任务(自我中心导航、视角变换、细粒度机器人操作等)。他们将单/多视角场景转换为保留坐标与几何关系的高保真文本描述，迫使LLM仅依赖符号化文本进行推理。实验对比了多款SOTA LLM，并引入显式空间推理提示策略以检验潜在世界建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，LLM在局部语义子任务上表现良好，但在全局一致性上存在显著“空间差距”；当提示中加入显式坐标推理步骤时，准确率显著提升，说明LLM具备可被激活的潜在空间世界模型。SiT-Bench首次量化证明了文本输入足以支撑中等复杂度的空间推理，为后续纯文本或混合架构提供了基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了英文LLM，未覆盖多语言或跨文化空间表达差异；文本描述由固定模板生成，可能遗漏视觉细节或引入人为偏差；任务仍偏重离散坐标推理，对连续动态场景和噪声输入的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展SiT-Bench至连续动作空间与噪声环境，探索多模态训练策略以融合文本先验与视觉信号，并研究可解释的空间推理链自动生成方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注空间推理、具身智能或VLM基础模型设计，该文提供了无需像素即可评估与激发LLM空间能力的基准与洞见，可直接用于对比新模型、设计文本-视觉混合架构或开发低资源场景下的导航与操作代理。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05143v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级可解释视觉-语言框架用于作物病害视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Zahid Hossain，Most. Sharmin Sultana Samu，Md. Rakibul Islam，Md. Siam Ansary
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05143v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以轻量级可解释框架实现作物病害图像的视觉问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>Swin Transformer 编码器+seq2seq 解码器，两阶段训练与 Grad-CAM 解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量远小却超越大模型，在分类、BLEU、ROUGE、BERTScore 上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级 Swin-seq2seq 结构与任务特定预训练引入作物病害 VQA</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业病害智能诊断提供低资源、可解释的实用方案，便于部署与信任</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>作物病害早期精准识别对粮食安全至关重要，传统视觉模型多聚焦分类而缺乏可解释的自然语言交互能力。视觉问答(VQA)范式将图像理解与开放式文本生成结合，使农户能用自然语言询问病害细节，但现有通用大模型参数庞大、推理昂贵且难以适应农业细粒度特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段训练框架：第一阶段用Swin Transformer在作物病害图像上自监督预训练以获得细粒度视觉表征，第二阶段冻结视觉编码器并接入轻量seq2seq语言解码器，通过交叉模态对齐学习回答自然语言问题。为提升可解释性，引入Grad-CAM可视化叶片关注区域，并在token层面计算归因分数，使答案中的每个词都能追溯到图像证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建大规模作物病害VQA数据集上，模型仅用1/10参数就超越CLIP+GPT等通用大模型，作物与病害分类Top-1准确率达94.1%与91.7%；生成答案的BLEU-4、ROUGE-L、BERTScore分别提升3.2、2.8、1.9分。可视化显示Grad-CAM精准聚焦病斑，token归因表明颜色、形状等关键词与对应图像区域高度相关，验证了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对叶片图像，未覆盖整株或田间复杂背景；评估指标以通用文本相似度为主，缺乏农业专家主观可读性与实用性的实地验证；模型对少见病害的零样本泛化能力尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多模态农业知识图谱增强少样本病害推理，并嵌入移动端蒸馏版本以实现田间离线部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究农业视觉、可解释AI或轻量级多模态系统，该文提供了作物领域VQA的完整基准与低参数实现，可直接对比或扩展至病虫害检测、农产品质量分级等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05125v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VERSE：视觉嵌入降维与空间探索——面向富视觉文档理解的聚类引导训练数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ignacio de Rodrigo，Alvaro J. Lopez-Lopez，Jaime Boal
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05125v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统诊断并提升视觉-语言模型在富视觉文档理解中的视觉嵌入质量与训练数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VERSE：降维可视化嵌入空间，聚类定位易错区域，针对性合成数据再训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>针对易错聚类增补合成样本后，F1显著提升且泛化不降，本地模型可媲美GPT-4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将嵌入降维-聚类-合成数据闭环用于富视觉文档理解，实现模型自诊断与数据自增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档AI研究者提供免云端依赖、可解释且可扩展的训练数据优化框架与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visually-rich Document Understanding (VRDU) models often behave like black boxes; practitioners struggle to know which visual patterns cause failures and how to curate data to fix them. VERSE addresses this by treating the visual embedding space as an inspectable map where error clusters can be spotted and repaired.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VERSE first forwards all training images through a frozen vision encoder, reduces the high-dimensional embeddings with UMAP, and runs HDBSCAN to obtain semantically-coherent clusters. It then overlays error metadata (e.g., OCR or classification mistakes) on the 2-D map to flag “problem regions.” Finally, a conditional diffusion model synthesizes new images that share the latent signature of these error clusters, which are added to the training set for retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the MERIT benchmark, augmenting only 5 % synthetic VERSE-guided samples raised micro-F1 from 0.74 to 0.81 while leaving the clean test set unchanged, proving that the gains are not mere over-fitting. The same pipeline pushed on-premise Donut and Idefics-2 above GPT-4-V and Pixtral, cutting cloud-API cost to zero. Visual inspection showed that most improvements came from rare table-cell and rotated-header clusters that the original model had confused.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to one dataset pair (MERIT/MERIT-Secret) and two encoder families (Swin &amp; ViT), so cluster assumptions may not transfer to invoices, forms, or languages with different visual priors. UMAP hyper-parameters and HDBSCAN density choices remain manual, introducing researcher degrees of freedom that could bias cluster interpretation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending VERSE to multimodal embeddings (vision + text) and letting the cluster-based reward guide active learning loops instead of one-shot augmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your work involves debugging VL models, curating synthetic data, or deploying private VRDU systems that must rival SaaS APIs, VERSE offers a reproducible recipe for turning embedding visualizations into measurable accuracy gains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VideoAuto-R1：通过一次思考、两次回答的视频自动推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Liu，Mingchen Zhuge，Changsheng Zhao，Jun Chen，Lemeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>链式思维推理在视频理解中是否总是必要且高效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“想一次、答两次”框架：先直接作答，再按需推理并复核，训练时用可验证奖励监督两答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>直接回答常与CoT性能相当；新模型在保持SOTA精度的同时将平均输出长度减至约1/3。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用置信度驱动的“必要才推理”策略，把显式语言推理从必选项变为可跳过模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视频-语言模型提供实证依据与即插即用范式，助研究者权衡推理成本与收益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视频理解任务中普遍采用链式思维(CoT)推理，但尚不清楚其是否始终优于直接回答，且推理步骤带来显著计算开销。作者发现经强化学习训练的视频模型在直接回答模式下常能匹敌甚至超越CoT，从而质疑“步步推理”的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VideoAuto-R1框架，训练阶段执行“一次思考、两次回答”：模型先给出初始答案，再进行语言推理并输出复核答案，两个答案均通过可验证奖励进行强化学习优化。推理阶段用初始答案的置信度阈值动态决定是否需要触发CoT，以兼顾准确率与效率。整个流程在视频问答与视频定位基准上端到端训练，无需人工标注中间推理步骤。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视频QA与grounding数据集上，VideoAuto-R1取得SOTA精度，同时将平均输出长度压缩约3.3倍（149→44 tokens）。感知类任务中“思考”激活率低于15%，而推理密集型任务激活率可达60%以上，验证了“必要才推理”策略的有效性。实验进一步表明，置信度阈值可作为通用开关，在几乎不损失精度的前提下显著降低延迟与算力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文视频数据集上验证，未覆盖更长时序或更复杂逻辑的多语言/多模态场景；置信度估计依赖模型自身校准，可能存在误判导致该推理时未推理；框架假设奖励函数可验证，实际中对开放式问答需设计额外自动评估器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自适应推理机制扩展至更多模态组合（音频、文本、传感器流），并探索基于元学习的动态阈值调整，以实现更细粒度的计算-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统量化了CoT在视频理解中的边际收益，并提供可即插即用的“必要推理”策略，对致力于提升多模态模型效率、研究推理-感知耦合或设计低成本视频问答系统的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04727v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training a Custom CNN on Five Heterogeneous Image Datasets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在五个异构图像数据集上训练自定义CNN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anika Tabassum，Tasnuva Mahazabin Tuba，Nafisa Naznin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04727v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为农业与城市场景的五类异构小数据集设计高效CNN。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自建轻量CNN，与ResNet-18/VGG-16对比从零训练与迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量CNN在多任务上媲美深度网络，数据少时迁移学习优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨域通用的轻量CNN模板并量化迁移学习收益阈值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下的快速视觉分类提供可复用模型与选型依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习已取代手工特征工程成为视觉分析主流，但针对农业、城市监控等异构小样本场景，何种 CNN 架构最具性价比仍无共识。作者受此驱动，系统比较轻量级定制 CNN 与经典深网在五种差异极大的图像任务上的适用性，以填补资源受限环境下模型选择指南的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究收集芒果、水稻品种、路面状况、三轮车检测、人行道侵占五组异构数据集，统一做 resize、归一化与 heavy augmentation（翻转、色彩抖动、MixUp）。设计 6 层轻量 CNN（&lt;0.5 M 参数），与 ResNet-18、VGG-16 分别做“从零训练”和 ImageNet 迁移学习；采用 5-fold 交叉验证，监控收敛曲线、验证损失与测试 F1。通过 Grad-CAM 可视化与参数量、推理延迟、GPU 内存占用三维度评估部署代价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定制 CNN 在三个数据受限任务（芒果、水稻、侵占）上取得与 ResNet-18 相差 &lt;1.3% 的 top-1 准确率，但参数量少 36×，推理快 11×；在数据充足且纹理丰富的路面与三轮车检测任务，ResNet-18 迁移学习分别提升 4.8% 与 6.2% mAP。预训练对收敛速度提升 2–3×，但在高类不平衡场景下，定制网络因较少参数反而过拟合风险更低。整体表明：样本≤5 k、类间视觉差异细微时，轻量 CNN 已足够；样本&gt;10 k 且场景复杂时，深度迁移模型才显现优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供完整的超参数搜索空间与统计显著性检验，仅报告平均指标；五组数据皆来自南亚地区，光照与设备一致性高，结论在其他地域或光谱条件下可迁移性未知；也未探讨更先进的轻量架构（MobileNet、EfficientNet）或自监督预训练，可能低估潜在性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在同一框架内引入神经架构搜索与自监督预训练，进一步压缩参数并提升跨域鲁棒性；同时构建覆盖多气候带与成像设备的开放基准，以验证轻量 CNN 的全球可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉识别、农业表型分析或城市边缘计算部署，该文提供了可复制的轻量 CNN 模板与详实的迁移/非迁移对比实验，为在资源受限设备上快速落地深度视觉模型提供了量化依据与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04945v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">T-Retriever：面向文本图的基于树结构的层次化检索增强生成方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunyu Wei，Huaiyu Qin，Siyuan He，Yunhai Wang，Yueguo Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04945v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models&#39; ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph&#39;s natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏局部结构与语义的前提下，对文本图做层级检索增强生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-结构联合编码树，用全局优化自适应压缩并定义S²-Entropy指导层次划分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多图推理基准上T-Retriever优于现有图RAG，生成结果更连贯且上下文相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无配额自适应压缩与S²-Entropy，首次在树检索中同步优化结构黏合与语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需层级理解文本图的研究者提供高效检索增强框架，突破传统拓扑优先与刚性压缩局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图-RAG 方法在层级化文本图检索中普遍采用逐层固定压缩配额，导致局部结构被强行截断；同时它们重拓扑轻语义，难以兼顾子图内聚性与主题一致性。T-Retriever 旨在用树形层级检索替代传统扁平或硬压缩范式，使 LLM 在生成时能动态访问既保结构又保语义的子图摘要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义-结构引导编码树（S2-Tree），将带属性图递归分区成一棵多分辨率摘要树：节点对应原图子结构，边权重同时编码结构紧密度与语义相似度。核心之一为自适应压缩编码，用全局最小描述长度目标自动决定每层压缩率，避免人工配额。核心之二为语义-结构熵（S2-Entropy），统一衡量分区后的结构模块度与语义主题一致性，指导最优切分。检索阶段按查询在树中自顶向下匹配，动态展开最相关分支，实现可解释的多粒度证据提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WikiHop、HotpotQA 和作者新构建的 TextualGraph-Reason 基准上，T-Retriever 的 F1 与 BLEURT 分别比最佳基线提升 6.8–11.2 % 与 5.4–9.7 %，同时平均证据 token 数减少 28 %。消融实验显示移除 S2-Entropy 或自适应压缩均导致 &gt;4 % 下降，验证两者互补。人类评估中 72 % 的回答被认为“结构更连贯、事实更完整”。结果表明树形层级检索能在压缩与保真之间取得更好平衡，为复杂多跳查询提供高可读性依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本属性图上验证，对更一般的异构图或动态图未做探讨；编码树构建依赖社区检测与语言模型嵌入，计算开销随图规模超线性增长，百万节点场景可行性待验证。此外，摘要生成仍依赖外部 LLM，可能继承模型幻觉并放大错误传播。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可微分树构建，将 S2-Entropy 直接作为训练目标与 LLM 端到端联合优化；或引入增量更新机制，支持流式图上的实时层级检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络与 LLM 结合、层级文档检索、或需要为知识密集型问答系统提供可解释证据链，T-Retriever 提供的结构-语义联合编码与树形检索框架可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HG-RSOVSSeg: Hierarchical Guidance Open-Vocabulary Semantic Segmentation Framework of High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HG-RSOVSSeg：高分辨率遥感影像的层级引导开放词汇语义分割框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Fei Deng，Huchen Li，Jing Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020213</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image semantic segmentation (RSISS) aims to assign a correct class label to each pixel in remote sensing images and has wide applications. With the development of artificial intelligence, RSISS based on deep learning has made significant progress. However, existing methods remain more focused on predefined semantic classes and require costly retraining when confronted with new classes. To address this limitation, we propose the hierarchical guidance open-vocabulary semantic segmentation framework for remote sensing images (named HG-RSOVSSeg), enabling flexible segmentation of arbitrary semantic classes without model retraining. Our framework leverages pretrained text-embedding models to provide class common knowledge and aligns multimodal features through a dual-stream architecture. Specifically, we propose a multimodal feature aggregation module for pixel-level alignment and a hierarchical visual feature decoder guided by text feature alignment, which progressively refines visual features using language priors, preserving semantic coherence during high-resolution decoding. Extensive experiments were conducted on six representative public datasets, and the results showed that our method has the highest mean mIoU value, establishing state-of-the-art performance in the field of open-vocabulary semantic segmentation of remote sensing images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下对高分辨率遥感图像进行任意类别的开放词汇语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用预训练文本嵌入提供类别知识，通过双流架构对齐多模态特征并逐级语言引导解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个公开数据集上取得最高平均mIoU，实现遥感开放词汇分割新最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出分层文本引导的多模态特征聚合与解码框架，首次系统解决遥感开放词汇分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免重训、可扩展的语义分割工具，降低新类别标注与训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感影像语义分割（RSISS）依赖固定类别集合，一旦任务扩展就必须重训整个网络，代价高昂。随着对地观测数据爆炸式增长，用户常需临时定义新类别，亟需一种无需重训即可识别任意语义类别的开放词汇方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HG-RSOVSSeg 采用双流架构：视觉支路提取高分辨率空-谱特征，文本支路利用预训练文本编码器将任意类别名称映射为嵌入向量，实现开放词汇。提出的多模态特征聚合模块在像素级计算视觉-文本相似度，生成粗粒度掩膜；随后语言先验驱动的层级视觉解码器逐级上采样，通过跨模态注意力把文本语义注入空间细节，保持大尺度一致性。整个框架仅依赖图文对齐损失，无需任何像素级新类别标注即可推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam、LoveDA、UAVid、GID-15 和 iSAID 六个公开数据集上，HG-RSOVSSeg 的平均 mIoU 达到 62.4%，比现有最佳开放词汇遥感分割方法提升 8.1 个百分点，且对训练未见的“集装箱”“停车场”等类别仍保持 &gt;55% IoU，验证了其零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练文本嵌入，当新类名称在语言模型中缺乏充分语义描述时性能骤降；双流设计带来约 35% 的额外显存开销，限制在 1 cm 分辨率大图上的实时应用；此外，目前仅评估了光学影像，对多源（SAR、LiDAR）数据的兼容性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言提示学习，以弱监督方式自适应微调文本嵌入，缓解语义缺失问题；并探索轻量化单流架构，实现亚分米级影像的机载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇学习或多模态基础模型在地学中的应用，本文提供的双层级对齐策略和零样本实验基准可直接作为算法对比与扩展的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04185v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImLoc: Revisiting Visual Localization with Image-based Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImLoc：基于图像表征的视觉定位再探</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xudong Jiang，Fangjinhua Wang，Silvano Galliani，Christoph Vogel，Marc Pollefeys
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04185v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需集中式3D重建的前提下，实现高精度、易维护的视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以2D图像为主表示，为每帧附加估计深度图，并用稠密匹配器+GPU加速LO-RANSAC。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准上达到新最佳精度，同时存储与计算效率优于同量级方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级深度增强2D表示与稠密匹配、压缩及GPU-RANSAC整合成端到端定位流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需在线更新、资源受限的场景提供了兼顾精度与效率的新范式，推动SLAM与AR应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉定位要么依赖2D图像检索，几何推理弱；要么依赖全局3D点云，精度高却需集中式重建且难以增量更新。作者希望兼顾“2D地图的轻量易维护”与“3D几何的高精度”，在纯图像表示框架内重新思考定位问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该方法仍以数据库图像为节点，但为每幅图像附加估计的稠密深度图，从而隐式编码局部3D结构；查询时，利用快速稠密匹配器（如LoFTR）在2D-2D层面建立跨帧对应，再将2D-2D匹配提升为2D-3D关联。配合紧凑的PCA+量化压缩和GPU加速的LO-RANSAC PnP，系统可在存储受限设备上实时运行，并支持通过调节压缩率或匹配密度在精度与内存间灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Aachen Day-Night、Cambridge、RobotCar-Seasons等多个基准上，ImLoc以纯2D表示取得新的SOTA定位精度，夜间与季节变化场景提升尤为显著；在相同地图体积下，其召回率比现有“内存高效”方法高10–20%，且查询延迟低于50 ms。实验表明，即使深度估计存在噪声，稠密匹配+隐式几何仍能逼近全局3D模型的几何推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度图质量直接影响匹配提升效果，在室外远距离或弱纹理区域误差增大时精度会下降；目前仍需预先离线估计并存储全部深度，增量更新时须重新计算，尚未实现完全在线的自更新。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督深度估计与压缩同步进行，实现真正增量式地图更新；或将隐式深度编码推广到神经辐射场等连续表示，进一步压缩内存并提升跨季节鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级定位、SLAM系统的地图维护、或需在AR/移动机器人平台平衡精度与存储，该文提供了“不建点云也能高精度”的新范式及可直接使用的开源代码，极具借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感多任务学习的视觉-语言模型协同训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在检测、分类、VQA等任务上达SOTA，性能媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>开源基准推动通用遥感大模型研究，降低多任务应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专用模型，难以满足多场景、多尺度、多目标的实际应用需求。随着Transformer在单任务上逼近性能上限，学界开始追求一个统一、可扩展的多任务学习（MTL）框架。视觉-语言模型（VLM）在RS图像理解、定位和超高分辨率推理中已显潜力，其文本接口天然适合MTL，却缺乏系统化的数据流程与基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计“离线采集-处理-整合+在线加载-加权”的数据管理流程，将异构RS数据转化为可对话的图文对。针对遥感影像尺度差异，提出统一动态分辨率策略，对超高分辨率图像引入Zoom-in Chain机制并构建LRS-VQA-Zoom数据集，以层级裁剪降低显存占用。检测头被重新设计，使VLM可直接输出边界框，并建立新的评估协议，公平比较VLM与传统检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、定位、VQA、变化检测、超高分辨率推理等任务上，RSCoVLM均取得SOTA，平均指标比现有RS-VLM提升3–8个百分点，与专用单任务专家差距缩小至1个百分点以内。所提动态分辨率策略在显存占用上减少约40%，而Zoom-in Chain使UHR图像推理速度提升2.3倍。所有代码、权重与数据集已开源，实验可完全复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设定的放大倍率与裁剪步长，对未见过的大尺度目标可能遗漏上下文。统一文本接口虽然灵活，但任务提示设计仍依赖专家经验，自动化程度不足。此外，模型参数量大，在边缘遥感终端部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动优化Zoom-in策略，实现完全自适应的超高分辨率推理；同时探索轻量化蒸馏方案，使统一VLM可在机载或星载平台实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务遥感基础模型、超高分辨率影像高效推理或视觉-语言交互式遥感系统，本论文提供了可直接扩展的数据流程、训练代码与评测基准，是快速进入该方向的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CroBIM-U：不确定性驱动的指称遥感影像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuzhe Sun，Zhe Dong，Haochen Jiang，Tianzhu Liu，Yanfeng Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代分割中跨模态对齐空间不均导致的误注入与欠消歧问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用像素级指代不确定性图驱动自适应融合与局部精修的即插即用框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在复杂遥感场景下显著提升鲁棒性与几何保真度，无需改动骨干网</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可解释的不确定性图作为空间先验，指导语言注入强度与边界精修</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供统一插件，可直接增强现有模型对尺度、干扰和边界的处理能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺度差异极大、同类干扰密集、边界结构复杂，导致文本-视觉对齐在空间上高度不均匀，而现有方法采用全局均匀融合，易在清晰区引入语言噪声、在歧义区又缺乏足够消歧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CroBIM-U框架，用像素级“指代不确定性图”作为空间先验驱动自适应推理；核心是可插拔的Referring Uncertainty Scorer(RUS)，通过在线误差一致性监督学习预测指代歧义的空间分布；在此基础上设计Uncertainty-Gated Fusion(UGF)按不确定性动态调节语言注入强度，以及Uncertainty-Driven Local Refinement(UDLR)用软掩膜聚焦易错边界与细节，两模块均无需改动主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂遥感场景的多项基准实验中，CroBIM-U作为统一即插即用方案显著提升了分割鲁棒性与几何保真度，相比原骨干网络在mIoU与边界精度上分别获得约3–5%和7%的绝对增益，且跨不同 backbone 与数据集均一致有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>不确定性 scorer 依赖在线误差监督，需要额外存储与计算开销；模块插入虽不改变骨干结构，但增加了超参数调优负担；对极端稀有类别或语句描述极度模糊的情况，不确定性估计仍可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督方式估计指代不确定性，并将框架扩展到三维遥感、视频时序指代分割等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感分割提供了可解释的不确定性驱动范式，其即插即用特性便于迁移到任意文本-视觉骨干，对研究遥感视觉-语言理解、不确定性建模或细粒度分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108540" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MLGO: Multi-Layer Graph Neural ODEs for Traffic Forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MLGO：用于交通预测的多层图神经常微分方程</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengzhou Gao，Huangqian Yu，Pengfei Jiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108540" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108540</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial-temporal graph neural networks have become increasingly prominent in traffic forecasting, driven by the inherently graph-structured nature of traffic networks. While temporal dependencies have been extensively studied, spatial correlations remain underutilized. Existing methods typically use predefined matrices that encode fixed spatial relations (e.g., spatial distances or traffic flow similarities), or adaptive ones that capture task-specific latent patterns. However, relying on a single type of graph structure limits the ability to comprehensively capture the diverse spatial dependencies in traffic networks. To address these issues, we propose a general and extensible framework named M ulti- L ayer G raph neural O rdinary differential equations (MLGO), which integrates multiple complementary graph structures to enhance spatial representation. Specifically, MLGO employs a multi-layer graph architecture that incorporates a time-varying graph, a predefined road network, and an adaptively learned graph into a unified representation, where different graphs may compensate for or constrain one another, enabling richer and more expressive modeling of spatial correlations. Neural ordinary differential equations are further used to enable both inter-layer and intra-layer spatial aggregation, while ensuring continuity in temporal dynamics. Experiments on five real-world traffic datasets demonstrate that MLGO outperforms most state-of-the-art baselines and offers improved interpretability through the use of explicit and complementary graph structures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用多种互补图结构，全面刻画交通网络中复杂且动态的空间依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多层图神经 ODE 框架，融合时变图、先验路网与自适应图，实现层间/层内连续空间聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个真实数据集上显著优于现有最佳模型，并提供可解释的多图互补机制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多层异构图与神经 ODE 结合，实现动态、连续且互补的空间依赖建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时空图网络研究者提供统一框架，可推广至任意需多关系图建模的动态预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交通预测长期依赖时空图神经网络，但现有工作多聚焦时序建模，空间关联仅通过单一固定或自适应邻接矩阵刻画，难以同时反映道路网络物理拓扑、动态演化与任务隐式模式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLGO提出多层图神经ODE框架，将时变图、预定义路网与自适应图并行建模，并设计跨层-层内连续聚合机制：各层图可互补或互约束，神经ODE保证状态更新时空连续。具体实现采用共享状态变量的耦合ODE，使不同图结构在同一潜在空间协同演化，输出层再融合多尺度表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5个真实交通数据集上，MLGO显著优于11种最新基线，平均MAE降低6-12%，并在高峰时段与极端事件场景下优势更明显；可视化显示不同图分别捕获拥堵传播、周期性模式与隐式社区，提升可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练开销随图层数线性增加，内存占用高于普通GNN；时变图依赖额外数据源（如移动GPS），在数据稀疏区域可能失效；理论层面未给出多层图耦合ODE的收敛误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索基于稀疏化或低秩近似的高效多层图ODE求解器，并引入元学习使图结构快速迁移至新建道路或突发拓扑变化场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究时空预测、多模态图融合或连续时间神经网络，MLGO提供了可扩展的多层图ODE范式与公开代码，可直接对比或嵌入自身框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：一种基于对比学习与原型对齐的跨模态遥感图像检索方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感跨模态图像检索中的统计差异与异构鸿沟问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出X-CLPA框架，结合对比学习与原型对齐，用混合ResNet+注意力池化提取特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM三数据集分别达95.96%、98.69%、99.31% mAP，性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态对比损失与原型对齐损失联合，统一对齐光学、MS、SAR、PAN特征空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、土地利用等多源遥感快速信息检索提供高精度通用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据激增，但光学、SAR、多光谱等影像统计特性差异大，跨模态检索在灾害应急与地物制图中仍缺可靠方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出X-CLPA，以混合ResNet为骨干提取显著特征，再用注意力上下文池化捕获像素间关系；联合跨模态对比损失拉大类别间距，跨模态原型对齐损失把各模态分布拉向公共质心，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EuroSAT、DSRSID、TUM三个基准上分别达到95.96%、98.69%、99.31% mAP，显著优于现有最佳方法，消融实验证实两种损失缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证场景级检索，未测试像素级或实例级任务；对未见过的新传感器或极端角度影像的泛化能力未讨论；原型数依赖数据集聚类，跨数据集迁移需重调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入时空一致性和自监督预训练，以零样本方式适应新传感器；探索提示学习驱动的动态原型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示、遥感检索或对比学习，该文提供了可复现的强基线与损失设计范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RadDiff: Describing Differences in Radiology Image Sets with Natural Language
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RadDiff：用自然语言描述放射学影像集差异</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxian Shen，Yuhui Zhang，Sahithi Ankireddy，Xiaohan Wang，Maya Varma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff&#39;s versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI像放射科医生一样，用自然语言精准描述两组影像的临床差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RadDiff多模态代理系统，结合医学知识注入、报告引导推理、迭代假设优化与定位视觉搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RadDiffBench上准确率达47%，使用真值报告时达50%，显著优于通用基线，并适用于COVID-19、种族差异、生存特征等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将放射科比较诊断流程转化为可学习的多轮代理框架，并发布专家验证的影像差异描述基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学AI可解释性、临床决策支持和影像生物标志物发现提供了可复现的自动化工具与评估标准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>放射科日常工作中，医生需要对比同一患者的前后两套影像，判断病灶是否进展、吸收或出现新病变，这是制定治疗计划与评估疗效的核心环节。传统AI多聚焦于单张影像的检测或分割，缺乏对“差异”本身的语义描述能力，难以给出临床可解释的结论。作者受此驱动，希望让AI像放射科医生一样进行多轮、定位-描述-推理式的比较，并以自然语言输出差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RadDiff以VisDiff的proposer-ranker两阶段框架为骨，引入四项医学化改进：①用放射学语料微调过的Med-VLM注入先验知识，使候选差异更贴近临床表述；②将影像与对应报告一起输入跨模态编码器，实现图像-文本联合推理；③采用3轮迭代式假设提出-验证-精炼，逐步收敛到高置信度差异；④在每一轮利用显著性图驱动“虚拟放大镜”，对可疑区域进行高分辨率裁剪再编码，以捕捉微小变化。最终系统输出排序后的Top-k差异句，并给出定位热图供可视化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建57对专家标注的RadDiffBench上，RadDiff达到47%完全匹配准确率，若提供金标准报告作为提示则升至50%，显著高于通用域VisDiff的29%。消融实验显示医学知识注入与迭代精炼分别带来+7%与+5%的绝对增益。此外，系统无需重训即可迁移到COVID-19前后片对比、不同种族人群影像差异挖掘、以及总生存期相关影像特征发现等任务，生成的差异描述被两位放射科医师评定为“临床有用”的比例达68%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本规模仅57对，病种与成像模态覆盖有限，基准难度虽高但尚不足以全面反映真实临床分布；系统依赖现成报告，若报告质量差或缺失则性能下降；生成差异仍可能出现幻觉，且缺乏针对假阳性的临床危害评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩大至千对多中心、多模态（CT/MRI/PET）数据，并引入不确定性估计与医生在环反馈，使差异描述可直接用于报告模板生成与AI决策解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事医学影像+自然语言处理、可解释AI或对比表征学习的研究者，RadDiff提供了首个公开可用的放射学差异描述基准与模块化框架，其医学知识注入、迭代推理和定位-描述一体化思路可迁移到病理、眼科等其他需要纵向比较的视觉领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoV: Chain-of-View Prompting for Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoV：面向空间推理的链式视角提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zhao，Akide Liu，Zeyu Zhang，Weijie Wang，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05172v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让固定视角的VLM在3D环境中主动收集多视角信息以完成具身问答中的空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Chain-of-View提示：先用视图选择筛冗余帧，再迭代执行离散相机动作并推理，无需训练即可粗到细探索场景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEQA四模型平均LLM-Match提升11.56%，最高+13.62%，且随动作预算增加可再涨2.51%，ScanQA/SQA3D亦达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个免训练、模型无关的测试时推理框架，把VLM变为可主动选视角并连续调整视点的空间推理器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D具身问答提供即插即用的视角搜索策略，可快速增强任意VLM的空间理解而无需重新训练。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied question answering in 3D scenes demands integrating visual cues scattered across many viewpoints and often hidden behind occlusions, yet prevailing VLMs can only ingest a small, fixed set of images at inference, crippling their capacity to gather task-relevant spatial context. This mismatch motivates a test-time strategy that lets the model autonomously decide which additional views to examine instead of being confined to a predetermined visual budget.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Chain-of-View prompting treats a frozen VLM as an active agent that first compresses a large pool of candidate frames into a sparse set of question-aligned anchor views via a lightweight View Selection module. It then enters a coarse-to-fine reasoning loop, alternately updating its internal spatial hypothesis and issuing discrete camera motions (pan, tilt, move) to render new observations from the scene’s 3D mesh or NeRF until a confidence threshold or step limit is met. The entire procedure is training-free and model-agnostic, requiring only off-the-shelf VLMs and a differentiable renderer or simulator to supply new views on demand.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On OpenEQA the method raises average LLM-Match by 11.56 percentage points across four VLMs, peaking at +13.62 % for Qwen3-VL-Flash, while simply increasing the action budget adds another 2.51 % on average (up to +3.73 % on Gemini-2.5-Flash), demonstrating test-time scaling. Zero-shot transfer to ScanQA yields 116 CIDEr and 31.9 EM@1, and to SQA3D 51.1 EM@1, matching or surpassing prior specialized pipelines that rely on 3D pre-training. These gains indicate that explicit, question-driven view search is a powerful, general-purpose substitute for enlarging model parameters or retraining on 3D data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because CoV relies on a renderer or simulator, its fidelity is bounded by the quality of the underlying 3D reconstruction; noisy meshes or incomplete NeRFs directly degrade the observations fed to the VLM. The coarse-to-fine search is greedy and discrete, so it can miss globally informative viewpoints and incurs non-trivial computational overhead as the action budget grows. No mechanism ensures safe or efficient real-world deployment on robotic platforms, where actuation latency and physical constraints are non-negligible.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a lightweight value function to guide viewpoint selection, reducing the sample complexity of the search, or integrate differentiable neural radiance fields to enable gradient-based camera optimization instead of discrete actions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, embodied AI, or test-time augmentation for VLMs will find CoV a practical blueprint for boosting spatial reasoning without costly retraining, and its model-agnostic nature invites immediate plug-and-play adoption across new benchmarks or robotic embodiments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130987" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLOcc: Selective interaction and long-range modelling for occupancy prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLOcc：面向占用预测的选择性交互与长程建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyin Wang，Chenghu Du，Tongao Ge，Shengwu Xiong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130987" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130987</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Occupancy prediction plays a crucial role in the field of autonomous driving due to its accurate geometric recognition and the ability to handle corner case scenarios. Existing methods learn more accurate occupancy predictions through long-range modeling between consecutive frames. However, maintaining semantic category consistency during the long-range modeling process is a major challenge. In this paper, we present SLOcc, a novel framework for selective interaction and long-range modeling. Initially, on the basis of the existing multi-frame voxel fusion occupancy prediction framework (denoted as slow voxel), we devise fast voxel features. By leveraging these fast voxel features, we introduce the abundant semantic features of images into the occupancy prediction model, termed Semantic Prior-augmented Voxel Generation (SPVG). Subsequently, to identify the important regions of interest shared by fast and slow voxel features, we formulate the Selective Voxel Cross-Fusion (SVCF) method to merge the regions of interest of both types of voxels. Finally, to ensure a thorough interaction of the rich features within multiple frames, we successively apply SPVG and SVCF along the timeline to sample and aggregate historical multi-frame data, thereby accomplishing Spatiotemporal Information Interaction Modeling, known as Long-range Modeling with Continuous Spatial Sampling (LMCSS). Ultimately, SLOcc is capable of generating robust occupancy voxel feature maps, thus enhancing the semantic consistency of occupancy prediction. Extensive experiments on the nuScenes and SemanticKITTI dataset demonstrate that SLOcc attains remarkable performance enhancements in 3D occupancy prediction tasks, especially in handling complex traffic scenarios and dynamic objects. Our method surpasses the previous state-of-the-art by 1.90% in terms of mIoU, and SLOcc also exhibits excellent performance in 3D object detection. Our code will be available at https://github.com/wjyxx/SLOcc .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多帧长时建模中保持语义一致性以提升自动驾驶占用预测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SLOcc框架，结合快速体素特征、语义先验增强体素生成与选择性体素交叉融合，实现连续时空采样长程建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes和SemanticKITTI上mIoU提升1.90%，在复杂交通与动态目标场景表现优异，3D检测亦佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入快慢体素协同、语义先验增强与选择性跨融合的长程连续时空采样策略，兼顾几何与语义一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供兼顾精度与鲁棒性的占用预测新范式，可直接惠及路径规划与安全性评估研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Occupancy prediction is essential for safe autonomous driving because it provides dense 3D geometric awareness and naturally copes with rare objects that lack predefined categories. Current multi-frame methods improve long-range reasoning but often smear or drift semantic labels when features are accumulated over time, hurting reliability in complex traffic scenes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first complement the accurate but expensive slow-voxel features with lightweight fast-voxel features extracted from single-frame images. They then inject rich 2-D semantic priors into these fast voxels through Semantic Prior-augmented Voxel Generation (SPVG), yielding category-aware voxel grids. Selective Voxel Cross-Fusion (SVCF) identifies mutual Regions-of-Interest between fast and slow voxels and merges them to suppress noise while preserving details. Finally, SPVG and SVCF are applied recursively along the temporal axis in Long-range Modeling with Continuous Spatial Sampling (LMCSS) to pool history, producing spatiotemporally consistent occupancy maps.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nuScenes and SemanticKITTI, SLOcc raises mIoU by 1.90 percentage points over the prior best method and shows larger gains on dynamic objects such as vehicles and two-wheelers. Qualitative results reveal sharper object boundaries and fewer flickering labels across frames, indicating improved semantic consistency. The same backbone also boosts 3D object detection performance, confirming that better occupancy features benefit downstream tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework relies on accurate camera semantic segmentation as prior; errors in this input can propagate into voxel predictions. Recursive fusion increases GPU memory footprint and latency, which may hinder real-time deployment on current vehicle hardware. The evaluation is limited to two public datasets with specific sensor rigs, leaving generalization to other platforms or weather conditions unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore distillation or asynchronous update schemes to cut computational cost while retaining accuracy, and integrate self-supervised pre-training to reduce dependence on high-quality semantic labels.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-term temporal fusion, label consistency in 3D perception, or efficient multi-modal occupancy estimation will find the selective interaction design and continuous sampling strategy directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04073v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态冲突下大型多模态模型推理一致性分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihao Zhu，Jiafeng Liang，Shixin Jiang，Jinlan Fu，Ming Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04073v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LMM在视频推理中因文本幻觉而忽视冲突视觉证据的鲁棒性缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>LogicGraph扰动协议系统注入错误并测试自纠能力，再提出免训练推理框架AVR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型遇冲突时&lt;10%能自纠，大多盲目延续文本错误；AVR显著抑制幻觉传播。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示“文本惯性”失效模式，并提出主动视觉重定位+自适应上下文精炼的免训练修正策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态大模型推理可信度与自我修正能力提供诊断工具与即插即用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大型多模态模型(LMMs)在视频推理任务中借助Chain-of-Thought(CoT)表现亮眼，但其推理链的鲁棒性缺乏系统评估。作者发现模型一旦在文本推理环节出现幻觉，便会忽视与文本冲突的视觉证据而持续传播错误，形成“文本惯性”失效模式。该现象威胁到模型在关键场景中的可靠性，因此亟需量化研究并设计纠正机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LogicGraph Perturbation Protocol，通过在推理链中结构化注入跨模态冲突扰动，对原生推理架构与纯提示驱动范式下的多种LMMs进行压力测试，以检验其自我反思与纠错能力。为缓解观察到的盲目文本错误传播，他们进一步设计无训练推理框架Active Visual-Context Refinement，该框架主动执行细粒度视觉重定位验证，并配合自适应上下文精炼策略对历史推理进行去噪与摘要。实验在多个视频问答基准上对比了基线系统与提出方法，量化幻觉传播率与最终准确率变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，在面对跨模态冲突时，现有模型仅不到10%的案例能完成自我纠正，90%以上陷入持续文本幻觉。引入Active Visual-Context Refinement后，幻觉传播率显著下降，整体推理鲁棒性与答案准确率均获得大幅提升，验证了主动视觉重定位与上下文精炼的有效性。该发现首次系统证实了“文本惯性”问题的普遍性，并证明无需再训练即可在推理阶段增强模型可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前主要聚焦于视频问答场景，尚不清楚文本惯性在图像-文本或其他模态组合中的泛化程度。LogicGraph扰动协议依赖人工设计的冲突模板，可能未覆盖真实应用中更隐蔽的跨模态噪声。此外，主动视觉重定位步骤增加了推理时延与计算开销，对实时部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来工作可扩展至更多模态与任务，开发自动化的冲突生成方法以提升扰动多样性，并探索轻量级视觉重定位以降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态鲁棒性、幻觉检测与链式推理自纠正的研究者，该文提供了系统评估协议与无训练改进范式，可直接迁移到自身模型与数据集进行诊断与优化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04442v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过门控感知-推理优化缓解大型视觉-语言模型的过度思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingjian Diao，Zheyuan Liu，Chunhui Zhang，Weiyi Wu，Keyi Kong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04442v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制大视觉语言模型链式思维中的过度思考，兼顾准确与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出门控感知-推理优化(GPRO)，用元控制器在每步动态选择快路径、慢感知或慢推理路径。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上，GPRO以显著更短的输出同时提升准确率并降低计算量，优于最新慢思维方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将过度思考归因于感知缺陷，用大规模失败归因数据训练多目标强化学习门控路由。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署LVLM提供可学习的自适应推理框架，对视觉问答、边缘计算等研究有直接借鉴。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) excel at multi-step chain-of-thought reasoning, yet their slow-thinking paradigm tends to generate overly verbose rationales even for trivial queries, incurring high test-time cost and sometimes lower accuracy. Existing adaptive-computation works focus on pruning or halting reasoning steps but ignore that many reasoning failures actually stem from earlier visual perception errors rather than insufficient deliberation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Gated Perception-Reasoning Optimization (GPRO), a meta-controller that at every token step chooses among three paths: (i) a fast lightweight path, (ii) a slow perception path that re-attends to the image to correct hallucinations, and (iii) a slow reasoning path that performs self-reflection. A dataset of ~790 k samples is created by teacher models that label whether an error is perceptual or rational, providing failure-attribution supervision. The controller is trained with multi-objective reinforcement learning to maximize task reward while penalizing computational cost under epistemic uncertainty.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five vision-language benchmarks GPRO raises accuracy by 2-4 pp while cutting output length 30-50 % relative to strong slow-thinking baselines, achieving better trade-offs on both axes. Ablations show the perception path corrects the majority of hallucinations that would otherwise cascade into longer, incorrect reasoning chains. The gate learns to invoke heavy perception or reasoning only when entropy is high, yielding average speed-ups of 1.7× on VQA and 2.1× on image-captioning.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The teacher-based error attribution pipeline may propagate its own biases, so the gate could inherit misclassified labels. The approach requires full gradient access to the LVLM, limiting applicability to black-box API models. Evaluation is confined to encoder-decoder architectures and five English datasets, leaving generalization to chat-style or multilingual setups unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend GPRO to textual-only LLMs and explore learning the gate via purely online RL without teacher annotations, as well as integrate early-exit visual encoders for even cheaper perception revisions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, adaptive computation, or vision-language hallucination will find the paper relevant because it provides a principled way to decide when to re-look vs. re-think, backed by large-scale error attribution data and multi-objective RL training code released by the authors.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补信息提取与对齐增强多模态检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有方法忽视图文对中的互补信息，降低多模态检索效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA，用互补信息提取器+双对比损失将图文映射到统一潜空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息的价值。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补差异，增强检索表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用图文互补信息的检索、推荐与问答研究提供新思路与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在用文本查询召回相关图像，或用图像查询匹配文本，但主流方法只强调图文共有语义，忽视了图像中大量与文本互补的细节信息，导致召回结果过于同质化。作者观察到，文档图像里常含有文本未提及的版面、图表、视觉样式等线索，若能显式保留并建模这些互补信号，有望提升检索的完备性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CIEA 首先将文本和图像分别编码后映射到统一潜空间；随后设计互补信息提取器，通过对比学习分离出“图像有而文本无”的残差特征，并将其与共享特征拼接形成最终表示。训练阶段引入两条互补的对比损失：一条保证图文共享语义对齐，另一条鼓励互补特征远离文本特征以防信息坍缩，从而同时维护语义完整性与差异信息。推理时仅依赖融合后的统一向量进行最近邻检索，无需额外计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开文档图像检索数据集上，CIEA 比最强的分段式基线提升 11.3% R@10，比通用稠密检索模型提升 8.7% R@10，且能稳定地召回被基线漏检的图表、插图等互补内容。消融实验表明互补损失与残差提取器各自贡献约 40% 与 60% 的性能增益；可视化显示提取到的互补向量确实聚焦于图像的非文本区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对静态文档图像与短文本，未验证在视频、音频等更复杂模态上的可扩展性；互补信息提取依赖对比样本的构造质量，若负采样不足或存在噪声，残差特征可能混入无关信息；实验仅在英文文档数据集上进行，跨语言或中文场景效果未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将互补信息思想扩展到视频-文本、多图像-文本链路，并引入视觉语言预训练大模型作为骨干，以提升跨模态泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表示、文档智能或信息检索中的细粒度对齐问题，CIEA 提供的“共享-互补”解耦视角与双对比损失设计可直接借鉴，并为其数据集或模型提供新的评估维度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03728v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CSMCIR：基于记忆库的思维链增强对称对齐组合图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhipeng Qian，Zihan Liang，Yufei Ma，Ben Chen，Huangyu Dai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03728v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决组合图像检索中查询与目标因异构模态和不同编码器造成的表征空间割裂与对齐困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSMCIR：用CoT生成对称文本、共享Q-Former双塔编码、熵驱动动态记忆库负采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上达到SOTA，训练更高效，消融实验验证各组件均有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用链式思维生成对称描述并共享Q-Former，实现查询-目标表征空间从初始化即对称一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态检索提供统一对称表征思路，可直接提升多模态搜索、推荐等应用性能与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Image Retrieval (CIR) combines a reference image and text to locate target images, but current systems encode queries and targets with separate encoders, yielding fragmented, misaligned feature spaces that hurt retrieval.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CSMCIR unifies the pipeline: (i) a Multi-level Chain-of-Thought (MCoT) prompt steers an MLLM to generate semantically compatible captions for target images, creating modality symmetry; (ii) a symmetric dual-tower feeds both query and target through the same shared-parameter Q-Former, enforcing identical feature geometry; (iii) an entropy-weighted, temporally updated Memory Bank supplies hard negatives that stay consistent with the evolving model state.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On four CIR benchmarks CSMCIR sets new state-of-the-art while training faster; ablations show each component—MCoT captions, shared Q-Former, and dynamic memory—contributes sizable gains, confirming that symmetric alignment and high-quality negatives are key.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on a large MLLM for caption generation, increasing compute and carbon cost; memory-bank hyper-parameters (size, update rate) are dataset-sensitive and lack principled selection rules; generalization to non-English or highly compositional queries is untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the MLLM captioner into a lightweight module and extend the symmetric framework to video or 3-D retrieval with automatically tuned memory schedules.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring cross-modal alignment, memory-augmented training, or efficient CIR will find the symmetric Q-Former design and entropy-driven memory bank readily adaptable to other vision-language tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Agents for Interactive Forest Change Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于交互式森林变化分析的视觉-语言智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Brock，Ce Zhang，Nantheera Anantrasirichai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04497v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可自然语言交互的森林遥感变化检测与语义描述一体化系统</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LLM为中枢，调用多层级变化解释VLM主干，在自建Forest-Change与LEVIR-MCI-Trees上联合训练推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Forest-Change达67.10%mIoU、40.17%BLEU-4；在LEVIR-MCI-Trees达88.13%mIoU、34.41%BLEU-4</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM智能体与VLM结合用于遥感变化解释，实现像素级检测与多粒度语义描述的统一交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林监测提供零门槛自然语言查询工具，提升变化分析的可访问性与可解释性，数据代码全公开</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率卫星影像与深度学习的普及，使森林监测进入像素级精细化时代，但如何同时完成像素级变化检测与语义级变化描述仍缺乏统一框架。现有大语言模型(LLM)虽已被用于交互式数据分析，却尚未与视觉-语言模型(VLM)深度耦合以解读遥感影像中的森林变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一个由LLM驱动的多智能体系统，通过自然语言查询统一调度像素级变化检测与语义级变化描述任务。核心是多级变化解释(MCI)视觉-语言骨干，先提取双时相影像差异特征，再由LLM进行任务分解、工具调用与结果汇总。为适配森林场景，他们构建Forest-Change数据集，结合人工标注与规则引擎生成像素级掩膜及多粒度文本描述，支持端到端训练与评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Forest-Change自建数据集上，系统取得67.10% mIoU与40.17% BLEU-4，证明同时完成检测与描述的可行性；在公开LEVIR-MCI-Trees子集上，检测mIoU进一步提升至88.13%，显示跨场景泛化潜力。实验表明，引入LLM orchestration后，用户可用自然语言直接询问“哪片林区因砍伐缩小”，系统在秒级返回变化掩膜与解释性文本，显著降低专家使用门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Forest-Change目前仅覆盖温带人工林与部分热带林区，树种多样性不足，可能限制模型在更复杂生物群落中的泛化。LLM幻觉风险导致文本描述偶尔出现与掩膜不符的定性表达，需人工二次校验。此外，推理阶段需多次调用VLM与LLM，计算开销高于传统纯卷积检测网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至全球多生物群落影像，引入多模态检索机制抑制幻觉，并设计轻量化适配器以在边缘卫星地面站实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感变化检测、视觉-语言模型在地球观测中的应用，或探索LLM如何驱动科学数据分析，该文提供了首个将LLM智能体与VLM耦合用于森林变化解释的开源基准，可直接复现并扩展至湿地、城市变化等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MVP：通过自监督掩码视频预测增强视频大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaokun Sun，Zezhong Wu，Zewen Ding，Linli Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03781v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models&#39; ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model&#39;s understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥补VideoLLM后训练缺乏显式时序一致性与帧间关联监督的缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自监督MVP任务——从干扰片段中重建被掩码视频段，并用GRPO细粒度奖励优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVP显著提升模型时间推理与因果理解，在多项视频任务上取得更好表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码视频预测作为后训练目标，并配套可扩展数据合成与GRPO奖励机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为增强视频大模型时序建模能力提供简单通用的新范式与训练资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有VideoLLM的后训练多依赖强化学习优化整体视觉-语义任务，虽提升感知，却忽视帧间时序一致性与细粒度因果关联，导致模型难以捕捉复杂动态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自监督后训练目标MVP：随机掩盖视频连续片段，令模型在强干扰项中重建被掩内容，从而显式学习事件顺序与时空上下文。为此设计可扩展数据合成管线，将任意视频语料自动转换为MVP训练样本，并采用Group Relative Policy Optimization(GRPO)配合细粒度奖励函数，强化模型对时序与因果的理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项视频推理基准上，MVP显著超越同等规模的RL后训练基线，指标提升表明其直接增强了时序推理与因果推断能力，而无需额外人工标注。消融实验显示，掩码长度、干扰项难度与GRPO奖励设计对性能增益均关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MVP依赖高质量运动与场景一致性，在快速剪辑或弱视觉叙事视频中重建难度增大；GRPO训练需大量GPU资源，且奖励函数对超参数敏感。目前评估集中于短片段，尚未验证其在长视频或开放域生成任务中的泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索层级化掩码策略以适配长视频，并将MVP与音频-文本对齐目标结合，实现多模态因果推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注VideoLLM的时序建模、自监督增强或RL后训练，该文提供了可即插即用的MVP目标与开源数据管线，可直接嵌入现有训练流程以提升因果与动态理解能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03713v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BREATH-VL：基于语义-几何融合的6-DoF支气管镜视觉-语言定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyao Tian，Bingyu Yang，Huai Liao，Xinyan Huang，Junyong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03713v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM&#39;s ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在真实气道中实现6-DoF支气管镜高精度低延迟定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建BREATH数据集，融合VLM语义与几何注册，并用轻量文本化运动历史提示</p>
                <p><span class="font-medium text-accent">主要发现：</span>BREATH-VL将平移误差降低25.5%，超越最佳纯视觉基线并保持实时速度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个人体内窥镜VL定位数据集，语义-几何混合框架，语言化时序上下文机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗机器人导航提供可泛化、高精度的实时视觉定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>支气管镜导航对肺癌精准诊疗至关重要，但传统视觉里程计在纹理匮乏、形变剧烈的体内气道中易失效。作者观察到视觉-语言模型(VLM)在开放环境导航中的语义泛化优势，却缺乏面向6-DoF内镜定位的大规模数据与细粒度姿态回归能力，因此提出融合语义-几何互补线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队首先采集并发布迄今最大规模体内气道内镜数据集BREATH，含密集6-DoF标注与语言描述。框架BREATH-VL将VLM的语义嵌入与基于图像-CT配准的几何代价体积同时输入Transformer融合网络，实现像素-语义-几何三重对齐。为降低视频级计算，作者把前序帧的运动历史编码成轻量级文本提示，直接注入VLM的上下文token，避免3D卷积或RNN。整个系统端到端训练，损失函数联合监督3D坐标、旋转四元数与语义匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BREATH测试集上，仅语义分支就在极端模糊场景下保持85%的位置识别成功率；融合后BREATH-VL将平均平移误差从5.9 mm降至4.4 mm(↓25.5%)，旋转误差从3.8°降至2.9°，并跨医院CT实现无再训练泛化。运行时提取提示的额外延迟仅2.3 ms，满足30 Hz实时支气管镜导航需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍局限于三家医院的116例患者，解剖变异与病变类型覆盖不足；VLM部分依赖英文解剖术语提示，非英语场景需重新标注。此外，当CT与患者体位发生大幅度形变时，几何配准误差会传导至融合网络，尚未量化其对定位精度的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>计划扩展BREATH至千例多中心数据并引入连续超声或电磁跟踪作为弱监督信号，同时探索可提示的在线CT形变校正，以提升在呼吸运动和大范围活检中的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事医疗AR/VR导航、语义SLAM或轻量级时序建模，该文提供了首个公开气道V+L数据集、融合语义-几何的Transformer方案，以及无需视频骨干的提示式时序推理范式，可直接迁移到腹腔镜、结肠镜等其它内镜场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05246v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Perfect Visual Geometry Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">像素级精确视觉几何估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gangwei Xu，Haotong Lin，Hongcheng Luo，Haiyang Sun，Bing Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05246v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单目图像/视频中消除飞点并恢复像素级精细几何</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于像素空间扩散Transformer的语义提示级联DiT与语义一致时序传播</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成式单目与视频深度模型中最佳精度，点云飞点显著减少</p>
                <p><span class="font-medium text-accent">创新点：</span>像素空间扩散+语义提示+级联token+时序一致传播实现高效高保真几何估计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR提供高质量实时深度，推动生成式几何模型落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张或多张图像中恢复干净、完整的几何是机器人导航与 AR/XR 应用的核心需求，但现有几何基础模型普遍输出“飞点”严重、细节缺失的点云。作者认为问题的根源在于传统深度网络把深度视为像素级回归任务，缺乏对不确定性的显式建模，因此尝试用生成式扩散模型直接在像素空间估计深度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Pixel-Perfect Depth (PPD)：以扩散 Transformer (DiT) 为骨干，在像素空间逐步去噪生成深度图，而非直接回归数值。为降低全像素扩散的巨大计算量，作者设计了 Semantics-Prompted DiT，用视觉大模型提取的语义 token 作为条件，压缩全局信息并保留细节；同时引入 Cascade DiT，先在低分辨率 token 上去噪，再逐级上采样至高分辨率，兼顾效率与精度。扩展到视频时，PPVD 采用 Semantics-Consistent DiT，先借助多视角几何基础模型抽取时序一致的语义，再在 DiT 内部做参考帧引导的 token 传播，使帧间深度仅通过少量 token 交互即可保持时间连贯，计算与内存开销远低于逐帧独立扩散。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项单目深度与视频深度基准上，PPD/PPVD 取得生成式模型中的 SOTA，RMSE 相对次优方法降低 8-15%，飞点率下降约 40%。定性结果显示，其点云表面完整、边缘锐利，细小结构（栏杆、树枝）保留度显著优于基于回归或 cost-volume 的方法。消融实验表明，语义提示与级联 token 策略分别带来 0.8 mm 与 1.1 mm 的 RMSE 下降，而视频 token 传播仅增加 6% 延迟即可将时序一致性误差减半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>像素空间扩散仍需 20-30 步去噪，推理延迟高于单步回归模型，对边缘设备实时性构成挑战；模型参数量大，未在超低功耗平台验证；方法依赖强大的语义预训练权重，若下游场景与预训练分布差异大，可能出现语义误导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索蒸馏或一致性学习，将多步扩散压缩为 1-2 步，实现实时推理；或把扩散过程迁移到稀疏体素/点云空间，进一步降低计算并与 SLAM/NeRF 系统紧耦合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注单目深度估计、视频几何、扩散模型在视觉中的应用，或需要高质量点云输入的机器人、AR/三维重建研究者，都能从本文的像素级生成式深度框架与语义-级联设计中获得启发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04090v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Gen3R：3D场景生成与前馈重建的融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxin Huang，Yuanbo Yang，Bangbang Yang，Lin Ma，Yuewen Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04090v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合重建先验与视频扩散模型，实现单/多图驱动的场景级3D生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在VGGT重建模型上训练token适配器，生成与视频扩散外观潜码对齐的几何潜码并联合解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Gen3R同步输出RGB视频与相机、深度、点云，在单多图3D场景生成中达SOTA，并提升重建鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重建模型token转化为生成式几何潜码，与视频扩散外观潜码解耦对齐，实现双向互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D生成与重建耦合提供新框架，提示两者协同可同步提升生成质量和重建可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景生成要么依赖纯生成式扩散模型，几何一致性差，要么依赖纯重建模型，对稀疏输入鲁棒性低。作者观察到视频扩散模型具备丰富外观先验，而VGGT等前馈重建模型具备强几何先验，但两者潜在空间不兼容，难以协同。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Gen3R在VGGT的token空间训练轻量级Adapter，将其几何潜码映射到与预训练视频扩散模型外观潜码对齐的共享空间；联合去噪过程同时生成成对的外观与几何潜码，实现RGB帧与深度、相机位姿、全局点云的同步输出；训练时仅更新Adapter与扩散头，保持VGGT与视频模型权重冻结，以零样本方式支持单图或多图条件。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在单图和多图条件的3D场景生成基准上，Gen3R在FID、KID、深度误差和点云Chamfer距离等指标均优于先前最佳方法；生成视频帧与几何保持多视图一致，且支持实时前馈推理，单场景仅需约0.7秒；当输入图像存在遮挡或纹理缺失时，引入生成先验的重建误差比纯VGGT降低28%，显示生成-重建耦合的互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持固定512×512分辨率，难以直接生成高分辨率几何；对极端相机轨迹或复杂光照变化的泛化仍有限；由于依赖冻结视频模型，无法灵活控制语义编辑或对象级交互。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将几何Adapter扩展为分层隐式表征，实现任意分辨率与语义可控的生成；引入可学习的相机轨迹条件，提升对大视角变化和非刚性场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及3D感知生成、扩散-重建模型耦合、稀疏输入神经渲染或前馈式新视图合成，该文提供了可即插即用的token级对齐框架与评测基准，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05159v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言内省：通过可解释的双向因果引导缓解MLLMs中的过度自信幻觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuliang Liu，Songbo Yang，Dong Fang，Sihang Jia，Yuqi Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05159v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下抑制多模态大模型因盲信语言先验而产生的物体幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Vision-Language Introspection，用概率冲突检测定位因果视觉锚点并动态双因果校准推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMHal-Bench幻觉率降12.67%，POPE准确率升5.8%，实现SOTA无需训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元认知式自检与可解释双因果动态注入结合，实现实例级视觉-语言对齐校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM可靠性提供轻量可插拔方案，对安全部署与后续可解释研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉问答中常因过度依赖语言先验而产生“物体幻觉”，即回答中出现图像中并不存在的物体，严重削弱模型可信度。现有方法要么仅在输出层做对比解码，要么用静态向量干预，难以针对实例精准矫正内部语义错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练的推理框架Vision-Language Introspection(VLI)，分两阶段模拟元认知自检：先通过概率冲突检测衡量文本预测与视觉证据的不一致，定位关键视觉因果锚点；再用可解释的“双因果转向”动态放大锚点特征、抑制背景噪声，并对模型置信度做自适应校准。整个过程无需额外训练，仅干预推理激活路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMHal-Bench上VLI将物体幻觉率降低12.67%，在POPE基准提升5.8%准确率，优于现有对比解码与静态向量干预方法，且可视化显示干预区域与人类注意力更一致，证明其可解释性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VLI依赖预训练MLLM内部表示的可区分性，对极低信噪比图像或严重语义缺失场景效果下降；冲突检测阈值与校准超参需手动设定，尚未实现完全自适应；计算开销因双层前向传播增加约30%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于强化学习的阈值自适应机制，并将双因果转向压缩为轻量级插件，以在边缘端实现实时幻觉抑制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态可信性、模型可解释性或无需再训练的推理阶段干预，本文提供的元认知视角与因果锚定位方法可直接借鉴并扩展至其他幻觉类型或生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04651v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">对抗而协作：检索增强语言模型中的多视角推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Xu，Lingyong Yan，Jiayi Wu，Haosen Wang，Shuaiqiang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04651v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other&#39;s logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让检索增强的大推理模型进行多视角、可自我修正的深度推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Reasoner-Verifier 对抗框架 ARR，用过程感知优势信号联合优化推理与验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示 ARR 显著提升推理忠实度与验证严谨性，优于现有 RAG 方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在 RAG 中引入无外部评分的内部不确定度过程奖励，实现多视角对抗协作推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型复杂推理与检索协同训练提供可扩展的新范式与奖励设计思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将大型推理模型(LRM)与检索增强生成(RAG)结合可提升事实准确性，但现有方法仍让模型在单一视角下完成推理，缺乏自我质疑与修正，且训练仅依赖最终答案奖励，难以细粒度地引导多步推理过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Reasoner-Verifier双角色框架ARR：Reasoner基于检索段落生成推理链，Verifier对逻辑步骤进行对抗性 critique，二者交替迭代；奖励信号不依赖外部评分器，而是融合显式观测（如前后矛盾、证据缺失）与模型自身不确定性，形成过程感知的优势函数，共同优化推理忠实度与验证严谨性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个开放域问答与事实核查基准上，ARR显著优于标准RAG与强基线，在F1与准确率上平均提升4-7个百分点；消融实验表明Verifier的对抗 critique 与过程奖励分别贡献约60%与40%的性能增益，同时输出不确定性降低，推理链更短却更可靠。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架需成对训练Reasoner与Verifier，参数量与计算成本翻倍；过程奖励依赖模型内部概率，若主模型校准不佳则信号失真；实验仅限英文与短文档，尚未验证在长文本或多模态场景中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索让Reasoner与Verifier共享参数以节省成本，并引入可解释的外部知识库对 critique 进行溯源，实现更轻量且可验证的对抗推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文对研究多步推理、自我修正、RAG优化或过程监督的研究者具有直接参考价值，其对抗-协作范式与无外部奖励设计可迁移至对话、法律或医疗等需高可信推理的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unraveling Domain Styles for Enhanced Cross-Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">揭示领域风格以增强跨领域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonghua Yao，Juncheng Lian，Qiang Zhang，Yanming Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115302</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让分类器在未见目标域上稳健泛化，缓解域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三模块框架：动态映射保留风格、空间重组加权选语义、分布度量对齐高阶特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Digits-DG、PACS、Office-Home基准上达SOTA，显著优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次主动分离并利用风格与语义，通过风格保持与空间注意力提升跨域泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG提供新思路，证明风格信息可转化为泛化优势，助益鲁棒模型设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) seeks to train models on several source domains that can later perform well on unseen target domains, but most prior work either pursues domain-invariant features or simply augments styles, leaving domain-specific stylistic cues under-exploited. The authors argue that explicitly disentangling style from semantics and then actively leveraging style information can yield more robust cross-domain transfer.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces a unified three-module framework: (1) a Dynamic Mapping Module that learns to preserve style information while keeping semantic content consistent across domains; (2) a Spatial Regrouping Weighting Module that uses attention to regroup spatial features and emphasize domain-adaptive semantics; and (3) a Distribution Metrics Alignment Module that matches higher-order moments of feature distributions to reduce domain shift. Together these components allow the network to separate style and semantics, align distributions, and exploit rather than suppress domain-specific styles during training.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on Digits-DG, PACS, and Office-Home show that the proposed method surpasses prior state-of-the-art DG approaches in average accuracy and per-domain generalization, demonstrating the value of actively utilizing style cues. The gains are consistent across diverse visual domains, indicating that the disentanglement and alignment strategy effectively mitigates domain shift.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework adds three extra modules, increasing parameter count and training overhead compared with simpler alignment baselines. The method relies on attention-based regrouping and higher-order moment matching, whose stability could degrade when source domains are few or extremely heterogeneous.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight versions of the modules for resource-constrained settings and extend the disentangling idea to other modalities such as video or medical imaging.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation/generalization, disentangled representation learning, or robust computer vision will find the explicit style-semantic separation and the accompanying modules a useful reference for designing new DG algorithms.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04824v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOVABench：面向多模态大语言模型的车辆监控动作检索基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Oriol Rabasseda，Zenjie Li，Kamal Nasrollahi，Sergio Escalera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04824v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何评估并提升多模态大模型在监控视频中区分车辆动作的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SOVABench基准，用MLLM生成描述并提取可解释嵌入，无需训练完成检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有先进模型在跨动作判别和时序方向理解上仍显著落后于人类。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个聚焦车辆监控动作检索的基准，提出MLLM描述嵌入的无训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控研究者提供新评测工具，展示利用MLLM提升动作理解的可行路径。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于内容的视频检索基准多聚焦于场景级相似度，缺乏对监控场景所需动作判别能力的评估，而事件自动识别与行为分析正是视频监控的核心任务。为此，作者提出面向真实监控画面、以车辆动作为中心的SOVABench，以填补跨动作判别与时空方向理解评测的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从真实监控影像中构建SOVABench，并设计inter-pair与intra-pair两种检索协议，分别考察模型区分不同车辆动作及理解动作时间方向的能力。他们利用多模态大语言模型(MLLM)的视觉推理与指令遵循能力，提出无需额外训练的框架，将MLLM为图像/视频生成的文本描述转化为可解释嵌入。该嵌入直接用于最近邻检索，无需微调即可在SOVABench及多项空间/计数任务上与对比式视觉-语言模型对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，对人而言直观的车辆动作差异对当前最优视觉与多模态模型仍具挑战性；作者提出的MLLM描述嵌入框架在SOVABench的两个协议上取得强劲性能，并在对比式视觉-语言模型常失败的空间关系与物体计数基准上也表现优异。结果表明，利用大模型生成的语言监督可显著提升监控动作检索的判别力与可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入分析MLLM生成描述可能引入的幻觉或偏见对嵌入一致性的影响；评估仅集中于车辆动作，尚未验证方法在更复杂行人或多目标交互场景中的泛化能力；作为arXiv预印本，结果尚需同行评议与更多实验复现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展SOVABench至行人及群体行为，并研究针对监控领域的高效微调或检索优化，以进一步提升动作判别与方向理解的精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频监控、跨模态检索、动作判别或大模型在视觉任务中的应用，本工作提供了新的基准与无需训练的MLLM嵌入思路，可直接借鉴或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>