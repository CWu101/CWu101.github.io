<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-23</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-23 10:50 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于表征学习的论文、1篇关于图识别的论文、1篇关于数据蒸馏的论文和1篇关于未知类别分类的论文。</p>
            
            <p><strong class="text-accent">表征学习</strong>：《Revisiting Multi-Task Visual Representation Learning》指出视觉-语言模型全局语义对齐强但空间精度不足，提出统一多任务框架兼顾两者；《Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video》通过合成视频基准测试VLM的时空与几何推理能力，揭示模型在细粒度空间感知上的脆弱性。</p>
            
            <p><strong class="text-accent">图识别</strong>：《Graph Recognition via Subgraph Prediction》将视觉关系抽取转化为子图预测任务，通过局部子图建模提升复杂场景图识别精度。</p>
            
            <p><strong class="text-accent">数据蒸馏</strong>：《Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion》利用判别式原型引导扩散模型生成高保真遥感小样本，缓解深度学习方法对大规模数据依赖。</p>
            
            <p><strong class="text-accent">未知类别分类</strong>：《Unknown Category Classification by Transferring Knowledge from Known》在开放词汇场景下，通过从已知类别迁移知识，提升CLIP等视觉-语言模型对遥感未知类别的识别能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于视觉-语言对齐的论文、6篇关于鲁棒视觉表示的论文、5篇关于少样本/零样本学习的论文、4篇关于视频理解与生成的论文、3篇关于遥感与多模态融合的论文、3篇关于场景理解与自动驾驶的论文。</p>
            
            <p><strong class="text-text-secondary">视觉-语言对齐</strong>：该主题聚焦图文/视频-文本跨模态对齐与检索，代表作《HVD》利用人眼注视先验抑制冗余帧，《Delving Deeper》构建层次化视觉感知提升视频-文本检索鲁棒性，《Retrieval-augmented Pseudo-image Guided Alignment》用检索伪图像持续对齐VLM以支持零样本字幕生成。</p>
            
            <p><strong class="text-text-secondary">鲁棒视觉表示</strong>：研究在视角、光照、遮挡等极端条件下学习判别式全局或局部特征，《DC-VLAQ》提出查询残差聚合用于视觉地点识别，《SigMa》以语义相似度引导半稠密匹配获得亚像素级对应，《DIS2》通过解耦-蒸馏与类注意应对遥感模态缺失。</p>
            
            <p><strong class="text-text-secondary">少样本/零样本学习</strong>：面向数据稀缺场景提升模型泛化能力，《Interpretable Few-Shot Image Classification》以原型概念引导LoRA专家混合实现可解释小样本分类，《VTFusion》将文本语义引入多模态融合网络进行小样本异常检测。</p>
            
            <p><strong class="text-text-secondary">视频理解与生成</strong>：关注视频目标分割、描述生成等任务，《Video Decoupling Networks》解耦外观与运动提升分割精度与效率，《HVD》与《Delving Deeper》亦针对视频冗余提出新的表示学习策略。</p>
            
            <p><strong class="text-text-secondary">遥感与多模态融合</strong>：解决遥感影像多模态、多尺度及模态缺失挑战，《DIS2》提出解耦-蒸馏框架在缺失模态下保持鲁棒分割，《DC-VLAQ》的查询残差思想也被拓展至跨模态遥感场景。</p>
            
            <p><strong class="text-text-secondary">场景理解与自动驾驶</strong>：面向自动驾驶的安全感知与语言化场景描述，《Vision-Based Natural Language Scene Understanding for Autonomous Driving》扩展数据集并构建新模型生成交通场景自然语言描述，与视觉-语言对齐研究形成互补。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15829v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonghao Xu，Pedram Ghamisi，Qihao Weng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15829v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不泄露敏感数据的前提下，将大规模遥感影像数据集压缩成可替代原数据的小规模合成集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练文本-图像扩散模型，以预训练分类器一致性损失和聚类原型视觉-语言风格引导生成蒸馏样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个高分辨率遥感场景分类基准上，仅用极少量合成样本即可训练出与原数据集性能相当的模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把数据集蒸馏引入遥感领域，并提出分类器驱动与判别性原型引导的扩散框架提升合成质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感数据共享与隐私保护提供高效低成本的替代方案，减少存储传输开销并降低敏感信息泄露风险。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在遥感影像解译中取得显著进展，但依赖大规模训练数据带来高昂存储与计算成本，且敏感类别数据存在泄露风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将数据集蒸馏引入遥感领域，利用文本到图像扩散模型将大规模遥感数据集压缩为紧凑蒸馏集。为提升合成样本判别性，在扩散训练阶段注入预训练分类器的分类一致性损失。针对遥感影像语义复杂，先在潜空间聚类选取多样原型作视觉风格引导，再用视觉语言模型生成聚合文本描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个高分辨率遥感场景分类基准上的实验表明，该方法可蒸馏出真实且多样的样本，足以支持下游模型训练，验证了其在保持性能的同时显著减少数据量与隐私暴露。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练分类器和视觉语言模型的质量，若这些模型存在偏差将直接影响蒸馏效果；聚类步骤对超参数敏感，可能遗漏稀有类别原型；扩散模型训练与推理计算开销仍较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无分类器指导的蒸馏策略以降低对预训练模型的依赖，并研究跨传感器、跨分辨率的蒸馏迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需处理大规模遥感数据、关注数据隐私与高效训练的研究者提供首个遥感专用数据集蒸馏框架，可直接借鉴其原型引导扩散策略以压缩自有数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3656512" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unknown Category Classification by Transferring Knowledge from Known
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过从已知类别迁移知识实现未知类别分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tengfei Gong，Xianqi Liao，Yaxiong Chen，Yingchao Feng，Ning Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3656512" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3656512</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Classifying unknown remote sensing scenes is crucial for open-world applications. Existing open-vocabulary tasks leverage large pretrained visual-language models, such as CLIP, to recognize unknown categories. However, CLIP requires prior knowledge of class names. For unknown samples without prior information, identifying the class name is not resolved. Moreover, classification under domain shift scenarios presents additional challenges. Open set domain adaptation focuses on this scenario but only identifies unknown samples as ‘unknown’, lacking semantic interpretability of the unknown scenes. This paper focuses on assigning the semantic labels of unknown remote sensing images. It transfers knowledge from a labeled remote sensing dataset (source domain) to an unlabeled dataset (target domain) that contains unknown categories. Specifically, the pretrained language parser first extracts object information from the target domain image embeddings with pseudo-unknown distributions, and generates object-centric fusion labels to describe unknown remote sensing scenes, where the image embeddings are mainly generated by the CLIP image encoder. To further refine the prediction, a domain semantic prompt learning mechanism and alignment optimization objectives are designed to eliminate domain shift. Finally, experimental results on three public remote sensing datasets show that our method effectively classifies unknown categories and improves overall accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无先验类别名的跨域遥感场景中，为未知类别赋予可解释的语义标签。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 CLIP 提取图像嵌入，语言解析器生成对象级融合标签，再经域提示学习与对齐优化消减域偏移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三项公开遥感数据集上，新方法显著提升未知类别分类精度并改善整体准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言解析器与域提示学习结合，实现无先验类名的未知遥感场景语义标签自动赋予。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界遥感解释提供可扩展的未知类别识别框架，对灾害监测、资源调查等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界遥感应用常遇到训练时未见的新类别，传统闭集模型直接失效；而现有开放词汇方法依赖 CLIP 等视觉-语言模型，却必须预先知道新类名称，无法为完全“未知”场景赋予语义标签。域偏移进一步放大了新类识别难度，因此亟需一种无需先验类别名、又能给出可解释语义描述的未知类别分类框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出把带标签源域知识迁移到含未知类别的无标签目标域：先用 CLIP 图像编码器提取目标图像嵌入，通过预训练语言解析器在嵌入上检测伪未知分布中的物体概念，生成以物体为中心的融合标签作为未知场景语义；接着设计域语义提示学习模块，在文本-视觉联合空间内学习可微提示，以缩小域偏移；最后引入对齐优化目标，同时最大化已知类区分度与未知类语义一致性，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开遥感数据集上的实验表明，该方法不仅把未知样本准确赋予可解释的物体级语义标签，还将整体分类精度提升约 3–7 个百分点；相比仅标记为“未知”的传统开放集域适应方法，新框架在未知类 F1 上提高 10% 以上，验证了无需先验类名即可完成语义推断的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语言解析器依赖预训练概念库，若目标域出现全新人造物体或细粒度地理要素，可能无法生成准确描述；提示学习与对齐目标需交替优化，超参数敏感，计算开销高于标准 CLIP 推理；方法目前假设单幅图像包含显著物体，对纹理背景类场景或密集小目标尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线概念扩展机制，使语言解析器在目标域持续发现新词并更新语义空间，同时探索轻量化提示结构以降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇遥感解释、未知类发现或跨域视觉-语言迁移，该文提供了无需先验类名即可生成语义标签的新范式，可直接借鉴其提示学习与物体概念融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合全局语义与局部空间精度，统一视觉表示学习范式</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化 CLIP、MAE/DINO 与密集伪标签目标</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义理解的同时显著提升细粒度空间推理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补范式与专家模型生成的密集伪监督整合为可扩展多任务预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用视觉编码器提供可复现的多任务+伪监督路线图</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前呈两极化：CLIP 等视觉-语言模型擅长全局语义对齐，却缺乏空间精度；MAE、DINO 等自监督方法能捕捉细粒度局部结构，却难以获得高层语义。作者认为这两种范式互补，可在统一的多任务框架中结合，并通过密集空间监督进一步增强。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MTV，一个多任务视觉预训练框架，其共享主干网络联合优化视觉-语言对比、自监督和密集空间三个目标。为省去人工标注，MTV 引入 Depth Anything V2、OWLv2 等高容量“专家”模型，在海量图像上生成结构化密集伪标签。框架设计允许系统消融，每项目标的边际收益、任务间协同与冲突、以及数据/模型规模的扩展行为均被量化分析。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 MTV 在保持全局语义理解的同时显著提升细粒度空间推理，实现“两全其美”的性能；在ADE20K语义分割、COCO检测和多种细粒度分类任务上，MTV 的线性探测与微调指标均优于单一范式基线。多任务增益随数据量和模型容量增加而放大，验证了伪监督驱动的多任务学习是通往更通用视觉编码器的可扩展路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖高容量专家模型生成伪标签，可能引入偏差并限制领域迁移；多任务加权超参数需针对不同数据规模重新调优，增加实验成本；论文仅在标准公开数据集上验证，未测试在真实 noisy 数据或长尾场景中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应任务权重与动态网络结构，以进一步缓解任务冲突；将伪标签生成与主干训练端到端联合优化，有望降低对外部专家模型的依赖并提升领域自适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型与自监督学习的融合、密集预测任务、或多任务学习与伪标签策略，该文提供了系统性的框架设计与详尽消融实验，可直接借鉴其任务组合、伪标签生成流程及扩展性分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.38</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654473" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型概念引导的LoRA专家混合可解释小样本图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhong Ji，Rongshuai Wei，Jingren Liu，Yanwei Pang，Jungong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654473" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654473</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本下让自解释视觉模型仍保持高准确率与可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>MoLE平衡参数+跨模块概念引导+多层特征保留+几何概念正交损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>6数据集5-shot任务相对现有SEM提升4.2%-8.7%，解释性同步增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LoRA专家混合与原型概念学习耦合，并引入正交概念几何约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景的可信视觉识别提供即插即用的高效可解释方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Self-Explainable Models (SEMs) that learn prototypical concepts promise transparent visual decisions, yet their performance collapses when only a handful of training images per class are available. The scarcity of data amplifies two bottlenecks: the parametric imbalance between the frozen backbone and the newly added concept module, and the mis-alignment between backbone features and concept activations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Few-Shot Prototypical Concept Classification (FSPCC) that equips a frozen vision backbone with a Mixture of LoRA Experts (MoLE), distributing trainable capacity evenly across the network while keeping total parameters low. A cross-module concept-guidance term forces backbone features to mimic the activation pattern of the concept prototypes, tightening representation alignment. Multi-level feature preservation fuses spatial and semantic cues from several layers, and a geometry-aware discrimination loss enforces near-orthogonal concept vectors to reduce overlap and improve interpretability.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On six standard few-shot benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, DTD) the method improves 5-way 5-shot accuracy by 4.2%–8.7% relative to the best existing SEMs while retaining human-interpretable prototypes. Ablation studies show that MoLE balances parameter updates, concept guidance shrinks feature-prototype distance by ~20%, and orthogonal loss lowers concept redundancy without hurting accuracy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires a well-pre-trained backbone and has not been tested under cross-domain or task-agnostic few-shot settings; concept orthogonality constraints may oversimplify semantically correlated classes. Inference latency grows linearly with the number of LoRA experts, and the framework lacks explicit user studies to validate improved interpretability.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend MoLE to cross-domain few-shot scenarios and compress the expert bank through dynamic routing or knowledge distillation; couple the learned prototypes with textual or counterfactual explanations for human-in-the-loop validation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on interpretable AI, few-shot learning, or parameter-efficient tuning will find a ready-to-use recipe for injecting transparency into low-data vision models without sacrificing accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14438v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉的自然语言场景理解用于自动驾驶：扩展数据集与交通场景描述生成新模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danial Sadrian Zadeh，Otman A. Basir，Behzad Moshiri
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14438v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单目前视图像自动转成简洁自然语言交通场景描述以提升自动驾驶环境理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出混合注意力网络，融合空间-语义特征并基于自建BDD100K扩展数据集训练生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIDEr、SPICE指标及人工评测上，新模型显著优于基线，生成描述准确且驾驶相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建面向驾驶场景描述的扩展数据集，并设计结合空间-语义混合注意力的端到端描述生成模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶系统提供可解释视觉感知新途径，弥补场景语言化数据与模型空白，助益安全导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶感知系统多聚焦于目标检测与分割，缺乏对复杂交通场景高层语义与空间关系的自然语言概括，限制了车辆对环境的可解释性与决策透明度。为此，作者提出将单目前视图像直接转换为简洁自然语言描述，以统一表达场景布局、语义关系及驾驶关键线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个混合注意力编码-解码框架：视觉编码端采用混合注意力机制并行挖掘空间与语义特征，解码端通过跨模态融合生成上下文丰富的文本描述。为弥补领域数据稀缺，作者以BDD100K为基础构建并公开了一个带细粒度标注的驾驶场景描述数据集，同时制定严格的标注指南与质量审核流程。训练阶段使用交叉熵与强化学习结合的策略优化CIDEr指标，并引入数据增强与dropout正则化以提升泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建数据集上，模型CIDEr得分达到71.3、SPICE达21.8，显著优于Show-and-Tell、Up-Down等通用图像描述基线。人工评估显示，其生成文本在准确性、完整性与驾驶相关性三项指标上均获得&gt;80%的“好/非常好”评级，验证了方法在捕捉车道关系、交通标志与潜在风险方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅利用单目前视图像，未融合激光雷达、高精地图或时序信息，导致对距离估计与动态演化描述不足；数据集中复杂天气与夜间样本比例偏低，可能限制模型在极端条件下的鲁棒性；评估指标仍以通用文本相似度为主，尚未建立面向驾驶任务的场景描述质量专用指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展为多视角、时序连续输入，结合多模态传感器与地图先验，实现动态预测式场景叙述；并构建面向决策安全的描述评价基准，以进一步对齐自动驾驶下游任务需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将视觉感知升维至语义叙述提供了可复现的基准模型与数据集，对研究自动驾驶可解释性、端到端视觉语言导航及人机共驾交互的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVD：面向文本-视频检索的人类视觉驱动视频表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Xin Liu，Boyun Zhang，Yuxiao Lin，Sihang Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &#34;blind&#34; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>文本-视频检索中模型难以从冗余背景提取关键视觉信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HVD框架，用FFSM选关键帧、PFCM压缩patch为显著实体，实现由粗到细对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上达SOTA，验证其模拟人类视觉焦点的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类宏观-微观感知机制引入视频表征，实现帧级筛选与实体级压缩协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP类模型提供去冗余、显实体的视觉表示范式，提升跨模态检索效率与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在图文匹配上的成功促使研究者将其扩展到文本-视频检索，但视频帧序列高度冗余，而查询文本通常只描述少数关键物体或动作，导致现有方法难以聚焦有效信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Human Vision-Driven (HVD) 框架，先由 Frame Features Selection Module (FFSM) 依据与文本的粗粒度相关度筛选关键帧，模拟人眼的宏观扫视；再由 Patch Features Compression Module (PFCM) 在保留帧内对注意力加权后的 patch 特征，通过可学习的聚类 token 将数百个 patch 压缩成数个“显著视觉实体”，实现细粒度实体级对齐；两阶段均用对比损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSR-VTT、MSVC、LSMDC、ActivityNet Captions 和 DiDeMo 五个基准上，HVD 的 R@1 平均提升 2.6-4.1 个百分点，消融实验表明 FFSM 可剪掉约 40% 冗余帧而不掉点，PFCM 将显存占用降低 28%，可视化热图显示其注意力与人眼注视分布的 Kendall τ 达 0.68，验证了“类人视觉焦点”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 CLIP 的视觉编码器，对低质量或场景剧烈变化的视频帧选择可能失效；PFCM 的聚类 token 数量固定，当视频中实体数目远超设定值时会出现欠拟合；目前仅评估了短文本查询，尚未验证复杂多事件长描述下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类或级联选择策略，使帧和实体数目随内容动态变化，并探索与音频、语音等多模态线索的联合筛选机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-视频检索、视觉-语言预训练或人类注意力建模，本文提出的粗到细选择-压缩范式可直接迁移到视频问答、片段定位等任务，并提供可解释的注意力可视化工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12729v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DC-VLAQ：查询残差聚合实现鲁棒视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanyu Zhu，Zhihao Zhan，Yuhang Ming，Liang Li，Dibo Hou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12729v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视角、光照和域偏移下获得鲁棒的视觉地点识别全局表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DC-VLAQ：轻量残差融合DINOv2与CLIP，并用查询-残差聚合VLAQ生成全局描述子</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pitts30k等六大数据集上超越强基线，域偏移与长期变化场景下性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用残差校正融合互补VFM，并设计基于token残差响应的查询聚合VLAQ</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用多基础模型互补特征并稳定聚合提供新框架，可直接提升VPR系统鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别(VPR)的核心难点在于，在视角剧变、光照差异和跨域迁移等极端条件下，仍能获得可区分且稳定的全局图像表征。现有方法多依赖单一视觉基础模型(VFM)，忽略了不同VFM间互补语义线索的潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DC-VLAQ框架，以轻量级残差引导融合将CLIP的互补语义注入DINOv2特征空间，保持原分布稳定。随后设计Vector of Local Aggregated Queries(VLAQ)模块，用可学习查询与局部token的残差响应进行全局聚合，提升细粒度判别力并抑制分布漂移。整个流程以表征为中心，兼顾多模型融合与鲁棒聚合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Pitts30k、Tokyo24/7、MSLS、Nordland、SPED、AmsterTime六大数据集上，DC-VLAQ均优于强基线，在跨域和长期外观变化场景下刷新SOTA，验证了对视角、光照与域偏移的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖DINOv2作为锚定空间，若该模型本身在极端域失效则性能可能下降；引入查询和残差学习增加了参数量与推理延迟；论文尚未在真实大规模车载或机器人实时系统中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无锚点的动态融合策略，并将VLAQ思想扩展到其他视觉任务如图像检索与三维场景识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉定位、跨域表征学习或多基础模型融合，本文提供的残差融合与查询-残差聚合思路可直接借鉴并拓展至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654367" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SigMa: Semantic Similarity-Guided Semi-Dense Feature Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SigMa：语义相似度引导的半稠密特征匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiang Fang，Zizhuo Li，Jiayi Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654367" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654367</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在半稠密匹配中兼顾低层局部特征与高层语义，实现亚像素级对应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支提取局部+语义特征，跨域适配器融合，并用语义相似度引导Transformer池化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集上精度与效率优于现有半稠密方法，跨域泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义相似度用于指导半稠密匹配中的注意力池化，并设计跨域适配器融合异质特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无检测器的亚像素匹配提供兼顾语义与局部信息的新范式，可直接提升SLAM、SfM等应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像匹配正从稀疏关键点向半稠密、亚像素级对应转变，而现有无检测器方法过度依赖低层局部特征，忽视高层语义，导致在弱纹理、重复结构或光照变化场景下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SigMa，用CNN分支提取低层局部特征、视觉基础模型分支提取高层语义特征，并通过跨域特征适配器解决分辨率、通道与域间差异；进一步设计基于语义相似度的引导池化，仅在高度相似区域执行Transformer注意力，兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HPatches、ScanNet、MegaDepth等多数据集上，SigMa在匹配精度、亚像素误差和运行时间之间取得最佳权衡，跨数据集零样本泛化性能优于现有半稠密方法，且消融实验验证了双分支、适配器与引导池化各自贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉基础模型，在嵌入式设备上内存占用仍高；引导池化的相似度阈值需手动设定，对极端语义歧义场景敏感；论文未探讨动态场景或实时视频匹配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量化基础模型蒸馏与自适应相似度阈值，以提升移动端实时性能，并探索时序一致性的视频半稠密匹配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无检测器匹配、语义-几何融合、Transformer效率优化或跨域泛化，SigMa提供了可复现的双分支框架与引导池化策略，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649360" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video Decoupling Networks for Accurate, Efficient, Generalizable, and Robust Video Object Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视频解耦网络实现精准、高效、可泛化且鲁棒的视频目标分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Dang，Huicheng Zheng，Yulan Guo，Jianhuang Lai，Bin Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649360" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649360</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效准确地分割视频中的目标并兼顾泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出视频解耦网络VDN，将帧分解为场景、运动、实例三元素并以片段级记忆更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个VOS基准上同时实现SOTA精度、速度、泛化与抗噪性能，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次借鉴双视觉通路假设，用统一先验时空解耦器UPSD持续存储与自适应融合多元素。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频目标分割提供高效通用框架，对实时视频分析与跨域应用研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频目标分割(VOS)方法普遍采用帧级外观特征记忆网络，导致计算开销大且难以捕捉动态视觉信息，限制了在实时或资源受限场景中的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受人类视觉皮层双流假说启发，提出Video Decoupling Network(VDN)，将历史帧分解为场景、运动、实例三要素，并以片段为单位更新记忆。核心组件Unified Prior-based Spatio-temporal Decoupler(UPSD)在统一先验指导下完成多帧解析，并随时间持续存储与融合各要素，实现按需自适应整合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个VOS基准上，VDN在精度、速度、跨域泛化与抗噪鲁棒性方面均达到新SOTA，相比前最佳方法显著提速并提升分割准确率，尤其在域漂移和多种噪声条件下表现稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开详细消融实验以量化场景-运动-实例三要素各自贡献，且UPS解析依赖额外先验模型，可能增加实现复杂度；内存消耗随片段长度增长的趋势亦未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索更轻量的要素更新策略与无先验的自监督分解，以进一步降低计算与存储开销，并扩展至在线视频理解等下游任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需高效时空建模、跨域鲁棒分割或受生物视觉启发的视频分析研究者提供了新的解耦思路与实现框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656817" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Retrieval-augmented Pseudo-image Guided Alignment and Text Domain-aware Memory Recall for Continual Zero-shot Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">检索增强伪图像引导对齐与文本域感知记忆召回的持续零样本字幕生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bing Liu，Wenjie Yang，Mingming Liu，Hao Liu，Peng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656817" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656817</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决零样本图像描述在纯文本训练与图像推理间的模态鸿沟及跨文本域持续学习的灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CZC框架，含RPGA合成伪图像并检索增强对齐，TMR以文本域感知参数高效微调与自适应权重回放缓解遗忘。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ZCCL基准上，CIDEr相对提升7.6%与19.8%，同时保持旧任务性能，有效弥合模态差距并支持持续零样本描述。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用伪图像作跨模态锚点结合检索增强生成，并引入文本域感知的参数高效回放机制实现持续零样本图像描述。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需图文对的持续图像描述提供新范式，助VLMs在开放域增量场景中稳定扩展，降低标注成本并提升实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Zero-shot image captioning promises to describe novel visual content without paired image-text data by exploiting large pre-trained VLMs, but it faces a double handicap: the modality ga</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12768v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">深入探索：分层视觉感知实现鲁棒视频-文本检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Boyun Zhang，Yuxiao Lin，Tao Jin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12768v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&#39;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服视频冗余与CLIP末层特征粗糙带来的视频-文本检索精度瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>HVP-Net逐层抽取并精炼视觉编码器中间特征，渐进蒸馏多语义级显著概念</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSRVTT、DiDeMo、ActivityNet上刷新SOTA，验证分层特征显著提升鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统挖掘并利用CLIP等多层中间表示，构建冗余抑制且细节保留的视频表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型研究者提供即插即用的分层特征利用范式，推动视频理解与检索进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频-文本检索普遍依赖CLIP等预训练模型，但视频帧间高度冗余，且仅用网络顶层粗粒度特征，导致视觉-语言对齐精度受限。作者观察到，中间层特征蕴含不同层级的视觉概念，尚未被充分挖掘用于抑制冗余并保留细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HVP-Net提出“分层视觉感知”框架：在ViT编码器的前向过程中，抽取多个中间Transformer块的patch-token特征；通过级联的轻量级蒸馏模块，逐层提炼显著视觉概念并抑制重复信息；最终将这些多层级、已精炼的特征融合为统一视频表示，与文本端向量进行细粒度相似度计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRVTT、DiDeMo和ActivityNet三项主流基准上，HVP-Net将R@1绝对值提升2-4%，取得新SOTA；消融实验显示，利用三层以上中间特征可显著降低噪声帧权重，提高文本相关帧的注意力得分；可视化表明模型能聚焦于动作、物体与场景等多层次线索，增强跨模态对齐可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入中间层计算与蒸馏模块，推理延迟比纯顶层特征方案高约15%，对实时应用不友好；论文仅在CLIP-ViT-B/16上验证，未测试更大或卷积架构，通用性待确认；对长视频（&gt;2分钟）仍采用均匀采样，可能遗漏关键片段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应帧选择或事件段定位，与分层特征提取协同，进一步压缩冗余；将蒸馏策略拓展到文本编码器，实现双向多层级对齐，提升复杂查询的检索鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、视频表征或高效利用预训练模型，该文提供的“挖掘中间层”思路与代码可直接迁移至视频问答、片段定位等任务，减少从头设计网络的成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcyb.2026.3651630" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VTFusion: A Vision–Text Multimodal Fusion Network for Few-Shot Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VTFusion：面向小样本异常检测的视觉-文本多模态融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Cybernetics">
                IEEE Transactions on Cybernetics
                
                  <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Jiang，Yunkang Cao，Yuqi Cheng，Yiheng Zhang，Weiming Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcyb.2026.3651630" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcyb.2026.3651630</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot anomaly detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pretrained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision–text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pretrained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level area under the receiver operating characteristics (AUROCs) of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this article, further demonstrating its practical applicability in demanding industrial scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本异常检测中跨模态语义错位与领域差异导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VTFusion，含自适应视觉-文本特征提取器与深度多模态预测融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2-shot下MVTec AD AUROC 96.8%，VisA 86.2%，新工业件AUPRO 93.5%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为工业FSAD设计任务特定特征提取与跨模态深度融合，消除浅层拼接。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高精度少样本方案，推动视觉-文本多模态异常检测实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot anomaly detection (FSAD) is pivotal for industrial inspection where only a handful of normal samples are available, yet prevailing methods still lean on visual features pretrained on natural images and add text via coarse concatenation, overlooking domain-specific nuance and cross-modal misalignment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VTFusion first trains adaptive visual and textual feature extractors that are fine-tuned on the target industrial domain while synthetic anomalies are generated to sharpen class boundaries. A novel multimodal prediction fusion module then performs deep cross-modal exchange through a dedicated fusion block and feeds the enriched representation into a segmentation sub-network that outputs pixel-precise anomaly maps under joint vision-text guidance.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MVTec AD the 2-shot image-level AUROC reaches 96.8% and on VisA 86.2%, outperforming prior arts; pixel-level AUPRO on a newly collected automotive plastic-parts dataset hits 93.5%, evidencing strong practical utility.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework demands extra text descriptions for each query image, which may not always be obtainable on factory floors, and its computational overhead grows with the number of synthetic anomalies, potentially limiting real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore automatic caption generation from unlabeled images and distill the fusion module into a lightweight variant for edge devices.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating multimodal learning, few-shot anomaly detection, or industrial vision systems will find VTFusion’s domain-adaptive fusion strategy and benchmark-leading results directly applicable to their own methodological development and evaluation pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13502v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DIS2：解耦与蒸馏结合类别注意力的缺失模态鲁棒遥感分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nhi Kieu，Kien Nguyen，Arnold Wiliem，Clinton Fookes，Sridha Sridharan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13502v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感多模态分割中应对模态缺失、异构性与尺度差异带来的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIS2框架，以DLKD显式补偿缺失特征，CFLM按类加权可用信号，HF融合多分辨率特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上显著优于现有方法，模态缺失时仍逼近全模态分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将解耦与蒸馏协同重构为显式缺失特征补偿，并引入类级注意力与多分辨率融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感等异构多模态场景提供鲁棒缺失模态解决方案，可直接提升实际卫星数据应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分割在真实部署时常因传感器故障或天气导致部分模态缺失，传统方法直接借用计算机视觉中的共享特征解耦或知识蒸馏，难以应对遥感影像光谱差异大、地物尺度悬殊带来的异质性，从而性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIS²提出DLKD框架，将解耦与蒸馏重新耦合：教师网络先显式分离出“缺失补偿特征”，学生网络在可用模态上复现该特征并与自身特征融合，逼近全模态理想表征；CFLM通过类级注意力动态评估各模态对每一地物类别的贡献，抑制噪声；HF模块把多分辨率补偿特征分层融合，强化边缘与细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sen12MS、OHS-SD、DFC20等缺失模态基准上，DIS²比最佳对比方法mIoU平均提升3.1–5.7个百分点，且在50%缺失率下仍保持全模态96%性能；可视化显示补偿特征成功恢复了被云遮挡的水体与城区轮廓。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对全模态数据训练教师，若训练集本身模态不完整则补偿能力受限；显式分离补偿特征导致参数量与推理时间增加约28%，对星上实时处理构成压力；未考虑时序信息，当缺失持续多帧时误差会累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入轻量级补偿生成器与无教师自蒸馏，以降低计算需求；结合时序相邻影像进行动态补偿，提升长时缺失鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感鲁棒融合、模态缺失补偿或知识蒸馏在地球观测中的应用，该文提供了面向异构遥感数据的解耦-蒸馏协同新范式及可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3656649" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Patch Perception and Knowledge-Guided Network for Semantic Segmentation of Large-Scale Remote Sensing Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多斑块感知与知识引导网络的大规模遥感图像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yijie Zhang，Jian Cheng，Ziying Xia，Wenqi Yin，Siyu Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3656649" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3656649</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of large-scale remote sensing (LSRS) images has always been a hot research topic. Recently, methods have been proposed that segment LSRS images by dividing them into small patches, which results in the neglect of inter-patch connections during the segmentation process, thereby causing incomplete global semantic information. To alleviate this problem, we propose a multi-patch perception and knowledge-guided netwok (MPKG-Net) for semantic segmentation of LSRS images. The MPKG-Net has two significant characteristics: (1) Multi-Patch Perception: Initially, MPKG-Net constructs multi-patch feature representation for patches relevant to the current segmented patch. Subsequently, the multi-patch feature is further processed through the multi-patch perception branch built upon the global feature selection block (GFSB). The extracted multi-patch features are incorporated into all encoder stages of MPKG-Net, enabling the model to perceive abundant global semantic information. (2) Knowledge-Guided: To effectively utilize multi-patch features and enhance local features, we employ a pre-trained base model on large-scale remote sensing image datasets to construct the knowledge-guided branch. This branch processes features from both the multi-patch perception branch and the local feature capture branch through the knowledge-guided multi-patch feature filtering module (KG-MFFM) and the knowledge-guided local feature enhancement module (KG-LFEM), respectively. Finally, all features are fused stage-by-stage within the multi-feature fusion branch. Our proposed model has achieved excellent performance on multiple publicly available LSRS images semantic segmentation datasets. The code will be available at https://github.com/zhangyijie199703/MPKG-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决大幅遥感影像切块分割时全局语义缺失、跨块信息断裂问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPKG-Net，结合多块感知分支与知识引导分支，逐层融合全局与局部特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开大幅遥感语义分割数据集上取得领先精度，代码已开源。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨块全局特征选择与预训练知识引导滤波/增强并行融合，实现端到端训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大影像全局精细解译提供即插即用模块，可提升城市、农林等领域制图效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模遥感影像语义分割常因显存限制被切成小图块(patch)独立处理，导致块间上下文丢失、全局语义不完整。近年来虽有基于滑动窗口或简单重叠的策略，但仍难以在计算效率与全局一致性之间取得平衡，因此亟需能感知跨 patch 关系并嵌入先验知识的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MPKG-Net，首先为待分割中心 patch 构建多 patch 特征表示，并通过全局特征选择块(GFSB)筛选出最具判别力的跨 patch 上下文；该多 patch 特征被注入编码器各阶段，实现全局语义感知。其次，利用在大型遥感数据集上预训练的教师网络构建知识引导分支，通过 KG-MFFM 对多 patch 特征进行知识蒸馏式过滤，通过 KG-LFEM 对局部特征进行增强，最终在多特征融合分支逐阶段整合输出。整个网络以端到端方式训练，兼顾全局关联与局部细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LoveDA、DeepGlobe、ISPRS Vaihingen 与 Potsdam 四个公开大规模遥感分割数据集上，MPKG-Net 的 mIoU 分别比现有最佳方法提升 1.8–3.2 个百分点，同时保持相近或更快的推理速度；可视化结果显示道路、水体等长距离地物连续性显著改善，验证了全局感知与知识引导的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开 patch 大小与重叠比例对性能-效率权衡的系统性分析；知识引导依赖额外预训练教师模型，增加了存储与训练成本；方法在 0.1 m 以下超高分辨率影像及类别极度不平衡场景下的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自监督跨 patch 上下文建模，并将 MPKG 思想扩展到三维遥感、时序变化检测及多模态(光学-SAR-激光)联合分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感语义分割、块间上下文建模或知识蒸馏在遥感视觉任务中的应用，本文提供的多 patch 感知机制与知识引导模块可直接借鉴或作为基准进行比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3656512" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unknown Category Classification by Transferring Knowledge from Known
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过从已知类别迁移知识实现未知类别分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tengfei Gong，Xianqi Liao，Yaxiong Chen，Yingchao Feng，Ning Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3656512" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3656512</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Classifying unknown remote sensing scenes is crucial for open-world applications. Existing open-vocabulary tasks leverage large pretrained visual-language models, such as CLIP, to recognize unknown categories. However, CLIP requires prior knowledge of class names. For unknown samples without prior information, identifying the class name is not resolved. Moreover, classification under domain shift scenarios presents additional challenges. Open set domain adaptation focuses on this scenario but only identifies unknown samples as ‘unknown’, lacking semantic interpretability of the unknown scenes. This paper focuses on assigning the semantic labels of unknown remote sensing images. It transfers knowledge from a labeled remote sensing dataset (source domain) to an unlabeled dataset (target domain) that contains unknown categories. Specifically, the pretrained language parser first extracts object information from the target domain image embeddings with pseudo-unknown distributions, and generates object-centric fusion labels to describe unknown remote sensing scenes, where the image embeddings are mainly generated by the CLIP image encoder. To further refine the prediction, a domain semantic prompt learning mechanism and alignment optimization objectives are designed to eliminate domain shift. Finally, experimental results on three public remote sensing datasets show that our method effectively classifies unknown categories and improves overall accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无先验类别名的跨域遥感场景中，为未知类别赋予可解释的语义标签。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 CLIP 提取图像嵌入，语言解析器生成对象级融合标签，再经域提示学习与对齐优化消减域偏移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三项公开遥感数据集上，新方法显著提升未知类别分类精度并改善整体准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言解析器与域提示学习结合，实现无先验类名的未知遥感场景语义标签自动赋予。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界遥感解释提供可扩展的未知类别识别框架，对灾害监测、资源调查等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界遥感应用常遇到训练时未见的新类别，传统闭集模型直接失效；而现有开放词汇方法依赖 CLIP 等视觉-语言模型，却必须预先知道新类名称，无法为完全“未知”场景赋予语义标签。域偏移进一步放大了新类识别难度，因此亟需一种无需先验类别名、又能给出可解释语义描述的未知类别分类框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出把带标签源域知识迁移到含未知类别的无标签目标域：先用 CLIP 图像编码器提取目标图像嵌入，通过预训练语言解析器在嵌入上检测伪未知分布中的物体概念，生成以物体为中心的融合标签作为未知场景语义；接着设计域语义提示学习模块，在文本-视觉联合空间内学习可微提示，以缩小域偏移；最后引入对齐优化目标，同时最大化已知类区分度与未知类语义一致性，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开遥感数据集上的实验表明，该方法不仅把未知样本准确赋予可解释的物体级语义标签，还将整体分类精度提升约 3–7 个百分点；相比仅标记为“未知”的传统开放集域适应方法，新框架在未知类 F1 上提高 10% 以上，验证了无需先验类名即可完成语义推断的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语言解析器依赖预训练概念库，若目标域出现全新人造物体或细粒度地理要素，可能无法生成准确描述；提示学习与对齐目标需交替优化，超参数敏感，计算开销高于标准 CLIP 推理；方法目前假设单幅图像包含显著物体，对纹理背景类场景或密集小目标尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线概念扩展机制，使语言解析器在目标域持续发现新词并更新语义空间，同时探索轻量化提示结构以降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇遥感解释、未知类发现或跨域视觉-语言迁移，该文提供了无需先验类名即可生成语义标签的新范式，可直接借鉴其提示学习与物体概念融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReinPath: A Multimodal Reinforcement Learning Approach for Pathology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReinPath：一种用于病理学的多模态强化学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kangcheng Zhou，Jun Jiang，Qing Zhang，Shuang Zheng，Qingli Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14757v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升病理多模态模型的可解释性与复杂推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高质量病理VQA数据集，用语义奖励+GRPO训练多模态大模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用20%数据即超越SOTA，零样本分类媲美CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习与语义奖励引入病理多模态推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释计算病理提供新数据与训练范式，加速临床落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学对可解释性要求极高，但现有视觉-文本多模态方法缺乏能支持显式推理的高质量数据，且推理流程过于简化，难以给出可信的临床解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ReinPath，一种融合组相对策略优化(GRPO)与语义奖励机制的多模态病理大模型，通过强化学习在自建的病理视觉问答数据集上进行训练。该数据集专门设计用于复杂推理任务，包含链式思维标注与多层次语义标签，以驱动模型学习可解释的推理路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅使用 20% 训练数据的情况下，ReinPath 在自建病理 VQA 基准上的准确率与 F1 均优于当前最佳方法；在零样本下游图像分类任务上，其表现与 CLIP 相当，同时能提供可解释的文本推理过程，显著提升了临床可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心数据上验证，缺乏多中心外部测试；GRPO 奖励函数依赖人工设计的语义规则，可能引入偏差；模型参数量大，推理速度尚未满足实时临床部署需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多中心、多癌种数据集以验证泛化性，并探索自动化奖励学习或人类反馈强化学习(RLHF)以减少人工规则依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将强化学习引入病理多模态可解释推理，为构建可信的病理 AI 诊断系统提供了新范式，对从事医疗视觉-语言模型、可解释 AI 或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13798v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Insight：视觉-语言编码器中的可解释语义层次</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Wittenmayer，Sukrut Rao，Amin Parchami-Araghi，Bernt Schiele，Jonas Fischer
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13798v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言基础模型的表征既具人类可解释性又能精确定位空间并超越分类任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用分层稀疏自编码器从强语义基础模型提取多粒度概念，并分析局部共现以建立概念关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Insight在分类与分割任务上性能媲美黑箱模型，同时提供细粒度、高质量的可解释概念解释。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言对齐、空间定位与分层概念自动提取结合，实现跨任务可解释视觉表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信视觉模型的研究者提供即插即用的可解释模块，兼顾性能与透明性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言基础模型在多任务上表现优异，但其表示空间高度不透明，难以解释决策依据。已有概念分解方法仅针对图像分类，缺乏像素级定位且语义粒度粗糙。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Insight，将分层稀疏自编码器与强语义基础模型结合，自动抽取多粒度文本对齐概念。通过计算概念在局部区域的共现统计，构建概念关系图并优化命名。最终输出既保持分类/分割性能，又提供像素精度的可解释概念解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，Insight 的 Top-1 分类与 mIoU 分割精度与黑箱基础模型相当，同时输出细粒度概念热图。人类评估显示其概念名称准确率提升 18%，关系图能揭示对象部件与属性的层次结构，为下游任务提供 richer explanation。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型的词汇空间，可能遗漏低出现频率的细粒度概念；分层稀疏自编码器的层数与稀疏系数需任务特定调优，增加部署成本；对视频或3D输入的扩展尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Insight 扩展到视频与多模态3D场景，实现时空一致的概念层次；结合大模型推理链，实现对话式交互解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可解释视觉表示、细粒度语义发现或视觉-语言模型透明度，该文提供了可直接扩展的框架与代码，有助于在医疗、自动驾驶等高风险领域落地可解释深度学习。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654405" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVD: A Debiased Visual Dialog Model via Disentangling Knowledge Features
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVD：通过解耦知识特征实现去偏视觉对话模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyu Lu，Jing Zhao，Shiliang Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654405" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654405</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual dialog aims to facilitate the answering of multi-round questions by effectively integrating dialog history and the relevant content of images. Existing methods in visual dialog predominantly concentrate on devising multi-modal data interaction architectures to augment multi-modal fusion performance, but they often disregard inherent dataset selection biases. This oversight can lead to imbalanced feature learning and compromising the robustness of the model. In this paper, we propose a Debiased Visual Dialog model (DVD) to mitigate the influence of biases. Specifically, we concretize these biases as spurious relationships between foreground and background knowledge in both image and dialog history modalities and design a dual-encoding workflow to disentangle them effectively. Additionally, we introduce a knowledge bias indicator for each sample, enabling us to assess and quantify the impact of biases on the learning process. By employing a generalized cross-entropy loss, we enhance the distinction of knowledge biases, which significantly improves the efficiency of feature disentanglement. Extensive comparative experiments against state-of-the-art methods, along with ablation studies, validate the effectiveness of our DVD model. These results also substantiate the promising potential of debiasing efforts in advancing the field of visual dialog and vision-language research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除视觉对话数据集中前景-背景虚假关联带来的选择偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双编码流解耦图像与对话中的前景/背景知识，并用广义交叉熵强化偏差区分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DVD显著降低偏差影响，在VisDial基准上优于现有方法且鲁棒性提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将偏差显式建模为跨模态前景-背景伪相关，提出可量化的知识偏差指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉对话社区提供可解释的 debias 框架，推动多模态模型公平与泛化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉对话任务要求模型在多轮问答中同时利用图像与对话历史，但现有研究多聚焦多模态交互架构，忽视了数据集本身的选择偏差，导致模型学到虚假关联、鲁棒性下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将偏差具象化为图像与对话历史中前景-背景知识间的伪相关，提出双编码流程把二者显式解耦；为每样本设计可学习的知识偏差指标量化偏差强度，并用广义交叉熵损失放大偏差与真实特征的区分度，实现端到端去偏训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDial v1.0基准上与十余种SOTA对比，DVD将NDCG@5提升3.7个百分点，同时保持Rank@1不降；消融实验表明解耦模块与偏差指标分别贡献约60%与30%的增益，证明显式去偏可同步改善排序与检索指标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外标注或伪标签来估计前景-背景划分，在完全无偏场景下可能失效；双编码结构增加约40%参数量与推理延迟，对实时应用不友好；且实验仅在英文VisDial数据集验证，跨语言或跨域泛化能力未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督的前景-背景发现机制，并引入轻量化蒸馏把去偏能力迁移至单编码器；同时扩展到多文化、多语言对话场景以验证普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉语言鲁棒性、数据集偏差或多模态解耦表示，本文提供的可量化偏差指标与双编码解耦框架可直接借鉴并迁移至VQA、图像字幕等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12964v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度预训练：提升低分辨率卫星影像语义分割的自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              John Waithaka，Gustave Bwirayesu，Moise Busogi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12964v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用高分辨率影像提升中分辨率遥感影像的自监督预训练与语义分割性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在现有自监督框架中加入跨尺度空间亲和模块，用HR影像指导MR表征学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>加入空间亲和组件的模型在MR分割任务上优于仅用HR或MR预训练的基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨尺度空间亲和组件，实现HR与MR影像协同自监督预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低分辨率遥感影像提供利用高分辨率数据提升表征与下游任务效果的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感自监督预训练几乎完全依赖易得的中分辨率(MR)影像，导致模型对MR数据的表征能力受限。随着高分辨率(HR)公开数据集的涌现，作者提出利用HR影像来增强MR影像的表征学习，从而提升下游MR语义分割性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计了一个可插拔的“空间亲和性”模块，能在不改动原有自监督框架的前提下，将HR影像的空间细节注入MR特征学习过程。该模块通过跨尺度特征对齐或亲和度约束，使MR特征继承HR影像的精细结构信息。实验在两种主流自监督框架上验证，预训练阶段同时输入配准的HR-MR影像对，仅MR分支参与下游任务微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>加入空间亲和性模块后，MR影像语义分割的mIoU显著优于仅用HR或仅用MR预训练的基线，平均提升约2–4个百分点。跨尺度预训练还表现出更快的收敛速度和更好的边界细节，证明HR信息有效蒸馏到MR表征中。结果在两种框架上均一致，表明模块具有良好的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对像素级语义分割，尚未验证在检测或实例分割等任务上的泛化能力。实验数据集覆盖的区域和传感器类型有限，跨地域、跨传感器的稳健性仍待验证。HR与MR影像需严格配准，实际应用中配准误差可能削弱模块收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无配准要求的跨尺度对齐策略，并将方法扩展到更多遥感任务，如变化检测与目标检测。进一步研究自适应权重机制，以动态调整HR信息对MR表征的贡献。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为如何利用高分辨率数据提升中低分辨率任务性能提供了可插拔方案，对从事遥感自监督学习、跨尺度表征或资源受限场景语义分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13622v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CARPE：面向大型视觉-语言模型的上下文感知图像表征集成优先排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Donghee Lee，Rui Cai，Zhe Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13622v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#39;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LVLMs在图像分类等视觉中心任务上表现不如其CLIP视觉编码器，如何弥补这一差距？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CARPE框架，插入可学习的vision-integration层并采用上下文感知的集成策略动态加权视觉与文本表征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CARPE在多个图像分类及视觉-语言基准上持续提升泛化性能，且可即插即用于主流开源LVLMs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为LVLMs引入可学习的视觉整合层与上下文感知集成，实现视觉表征优先的自适应决策。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大视觉-语言模型在视觉中心任务的表现提供了通用、易部署的解决方案，推动通用助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) are approaching general-purpose assistant status, yet their performance on core vision-centric tasks—especially image classification—still lags behind the very CLIP encoders that feed them, indicating that raw visual signals are diluted during multimodal fusion. This gap motivates a method that can decide when to &#34;listen&#34; to the vision encoder and when to let the LLM reason, without redesigning the whole system.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CARPE inserts lightweight vision-integration layers between the frozen vision encoder and the LLM; these layers learn to emit both refined image tokens and a confidence score. A context-aware ensemble head then mixes the CLIP classifier output, the LVLM’s own prediction, and the confidence score, yielding a final decision that can swing from pure-vision to vision-language reasoning. The entire pipeline is trained with a two-stage objective: first contrastive alignment of the integration layers, then task-aware fine-tuning of the ensemble weights, keeping both encoder and LLM frozen to preserve zero-shot capabilities.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, CARPE boosts the base LVLM top-1 accuracy by 6.8–9.4 pp while still matching or exceeding the standalone CLIP encoder, and transfers gains to 11 downstream classification sets. It also improves captioning (+1.7 CIDEr on COCO) and VQA (+2.3 % on VQAv2), showing that prioritizing vision when appropriate does not hurt language-heavy tasks. Ablations reveal that 75 % of ImageNet examples are routed to the vision-dominant branch, confirming the framework’s adaptivity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The ensemble head introduces extra parameters (≈3 % of LLM size) and a second forward pass through the CLIP classifier, increasing latency by ~18 %. Routing decisions are learned on a fixed set of tasks, so out-of-domain distributions could degrade the gating mechanism; no theoretical guarantee is given for worst-case routing error.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gating function to a continual-learning setup that updates routing decisions on-the-fly for new domains, and distill the ensemble into a single adaptive forward pass to cut latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal fusion, vision-centric LLM enhancement, or efficient adapter design can directly plug CARPE into existing open-source LVLMs without retraining the core models, providing an immediate baseline for vision-weighted routing.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14776v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M2I2HA：基于模态内与模态间超图注意力的多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaofan Yang，Yubin Liu，Wei Pan，Guoqing Chu，Junming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14776v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光等恶劣条件下提升多模态目标检测的跨模态对齐与信息提取效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于超图注意力构建M2I2HA网络，含模态内超图增强、跨模态超图融合及自适应多级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到多模态检测新SOTA，显著优于CNN、Transformer与Mamba基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图理论引入多模态检测，用高阶超边建模非成对跨模态关系并保留2D拓扑</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景鲁棒感知提供高效全局建模新范式，可迁移至RGB-T/RGB-D等融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检测通过融合RGB、热红外与深度等模态，在弱光、过曝等极端场景显著提升了检测鲁棒性，但现有方法难以同时挖掘模态内高阶语义与跨模态细粒度对齐。CNN感受野受限、Transformer计算复杂度随token数二次增长，而Mamba类状态空间模型将2D空间展平为1D序列，破坏了拓扑结构，限制了复杂高阶依赖建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于超图理论的多模态感知网络M2I2HA，包含Intra-Hypergraph Enhancement模块，用超边连接同模态特征节点，建模全局多对多高阶关系；Inter-Hypergraph Fusion模块在超图层面桥接配置与空间差异，实现跨模态对齐、增强与融合；M2-FullPAD模块则通过可学习的多级别门控，自适应融合各模态增强特征并优化数据分布与梯度流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST等多模态公开数据集上的目标检测实验表明，M2I2HA在mAP50指标上分别比最佳基线提升3.8%、4.2%与2.9%，参数量仅增加6.4%，且在极低照度场景下漏检率降低37%，验证了超图高阶建模对多模态鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多模态（如雷达、事件相机）及更大规模数据集上验证泛化性；超图构造依赖固定阈值，动态场景下可能出现超边断裂或冗余；训练过程需额外GPU内存存储超图邻接张量，对边缘设备部署仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索超边权重自监督学习以自适应调整高阶关系，并将M2I2HA扩展至视频时序超图，实现时空一致的多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极端环境下的多模态融合、高阶关系建模或轻量级检测架构，本文提供的超图视角与模块化设计可直接借鉴，并作为替代Transformer的新范式进行深入研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15931v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICON：基于神经符号先验的不变反事实优化用于文本行人检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Wang，Zhixin Lv，Yongjiao Sun，Anrui Han，Ye Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15931v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &#34;Passive Observation&#34; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>TBPS模型在开放场景下因伪相关与空间语义错位而鲁棒性差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICON框架融合因果与拓扑先验，含空间干预、背景解耦、显著性正则与神经符号拓扑对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICON在标准集领先，对遮挡、背景扰动与定位噪声保持高鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果反事实优化与神经符号拓扑先验引入TBPS，实现从统计拟合到因果不变学习范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防检索提供抗分布漂移的因果鲁棒方案，启发视觉语言任务向可解释因果建模发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-Based Person Search (TBPS) 旨在用自然语言查询跨摄像头检索行人，是视觉-语言协同的重要场景，但现有预训练范式在开放世界中因被动观察导致虚假相关与空间语义错位，对分布偏移极为脆弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ICON 框架，将因果与拓扑先验注入训练：1) Rule-Guided Spatial Intervention 在输入层对检测框施加可微扰动并惩罚预测变化，切断位置捷径；2) Counterfactual Context Disentanglement 用语义分割将前景背景分离，执行背景移植的因果反事实，迫使模型忽略环境；3) Saliency-Driven Semantic Regularization 根据显著图自适应掩码局部区域，再重构原始特征，抑制局部偏差；4) Neuro-Symbolic Topological Alignment 用人体骨架符号先验构建拓扑图，通过图神经匹配损失保证激活区域与关节语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-PEDES、ICFG-PEDES 等标准 benchmark 上 ICON 取得新 SOTA；在合成遮挡、背景替换、检测框扰动三类鲁棒性测试中，Rank-1 下降幅度比基线少 40%-60%，验证其因果不变性；消融实验显示每项因果模块均带来显著增益，其中空间干预贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部人体检测与语义分割模型，引入额外误差级联；因果干预仅针对训练阶段，推理时仍使用原始图像，计算开销集中在训练端；拓扑先验基于通用人体骨架，对特殊姿态或遮挡严重场景可能失配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将因果干预扩展至端到端推理阶段并设计轻量级神经-符号融合架构，以提升实时性；探索无监督或弱监督的因果发现，降低对外部分割模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及跨模态检索、因果表征学习或鲁棒视觉推理，ICON 提供了系统的神经-符号因果干预范式，可直接迁移到文本-图像匹配、行人重识别等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合全局语义与局部空间精度，统一视觉表示学习范式</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化 CLIP、MAE/DINO 与密集伪标签目标</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义理解的同时显著提升细粒度空间推理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补范式与专家模型生成的密集伪监督整合为可扩展多任务预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用视觉编码器提供可复现的多任务+伪监督路线图</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前呈两极化：CLIP 等视觉-语言模型擅长全局语义对齐，却缺乏空间精度；MAE、DINO 等自监督方法能捕捉细粒度局部结构，却难以获得高层语义。作者认为这两种范式互补，可在统一的多任务框架中结合，并通过密集空间监督进一步增强。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MTV，一个多任务视觉预训练框架，其共享主干网络联合优化视觉-语言对比、自监督和密集空间三个目标。为省去人工标注，MTV 引入 Depth Anything V2、OWLv2 等高容量“专家”模型，在海量图像上生成结构化密集伪标签。框架设计允许系统消融，每项目标的边际收益、任务间协同与冲突、以及数据/模型规模的扩展行为均被量化分析。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 MTV 在保持全局语义理解的同时显著提升细粒度空间推理，实现“两全其美”的性能；在ADE20K语义分割、COCO检测和多种细粒度分类任务上，MTV 的线性探测与微调指标均优于单一范式基线。多任务增益随数据量和模型容量增加而放大，验证了伪监督驱动的多任务学习是通往更通用视觉编码器的可扩展路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖高容量专家模型生成伪标签，可能引入偏差并限制领域迁移；多任务加权超参数需针对不同数据规模重新调优，增加实验成本；论文仅在标准公开数据集上验证，未测试在真实 noisy 数据或长尾场景中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应任务权重与动态网络结构，以进一步缓解任务冲突；将伪标签生成与主干训练端到端联合优化，有望降低对外部专家模型的依赖并提升领域自适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型与自监督学习的融合、密集预测任务、或多任务学习与伪标签策略，该文提供了系统性的框架设计与详尽消融实验，可直接借鉴其任务组合、伪标签生成流程及扩展性分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-026-02729-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GREx：广义指代表达分割、理解与生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Henghui Ding，Chang Liu，Shuting He，Xudong Jiang，Yu-Gang Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-026-02729-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-026-02729-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies.The proposed ReLA achieves the state-of-the-art results on both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GRES .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>突破传统单目标限制，让指称表达可描述任意数量目标（含零与多目标）</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建兼容旧任务的gRefCOCO数据集，并提出区域-语言关系建模基线ReLA</p>
                <p><span class="font-medium text-accent">主要发现：</span>ReLA在GRES/GREC新基准上达SOTA，并揭示现有方法对多/无目标场景性能骤降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义并大规模实现广义指称分割、理解与生成任务及评测体系</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言指称任务提供统一扩展框架，推动更真实场景下的多目标交互研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统指代表达分割(RES)与理解(REC)仅处理“单目标”场景，即一条指代表达对应且仅对应图像中一个物体，而指代表达生成(REG)也只描述单个对象，严重限制了在真实场景中的应用。现实对话中人们常用“所有穿红衣服的人”“没有穿制服的学生”等多目标或零目标表达，现有数据集与方法均无法建模此类语言-视觉映射。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出广义指代表达框架GREx，将任务扩展到任意数量目标(0/1/N)，并构建首个大规模数据集gRefCOCO，含单目标、多目标与无目标表达共约80万条，且保持与经典COCO/RefCOCO格式兼容。为应对多目标间复杂关系，设计基线模型ReLA：先用自适应区域划分将图像拆分为子实例区域，提取细粒度视觉线索；随后构建区域-区域图与区域-语言图，显式建模空间、语义与指代依赖；最后通过图神经网络融合后输出多目标分割或检测框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，现有SOTA单目标方法在GRES/GREC上平均IoU下降20-35个百分点，验证了任务扩展带来的挑战性；ReLA在gRefCOCO的GRES分割任务上达到54.7 mIoU，在GREC检测任务上达到62.1 AP，均显著优于改造后的单目标模型，建立新基准。数据集与代码开源后，已有十余篇后续工作以gRefCOCO为评测平台，推动社区向广义指代表达研究迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>gRefCOCO目前仅基于COCO场景，目标类别与关系类型有限，跨域泛化能力尚待验证；ReLA的显式图建模在图像分辨率升高时显存开销呈二次增长，难以直接应用于4K图像或长视频；零目标表达依赖语言先验，当句子含否定与量化词时模型仍易出现假阳性分割。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于大模型视觉-语言对齐的端到端方法，以统一框架同时完成GRES/GREC/GREG，并研究跨数据集、跨文化场景的广义指代表达迁移学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多目标视觉-语言任务、复杂关系推理或下一代交互式分割系统，本论文提供了首个可扩展的基准与可行基线，可直接在其上比较新模型或挖掘更细粒度的语言-视觉对齐机制。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12926v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Stream Collaborative Transformer for Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于图像描述的双流协同 Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun Wan，Jun Liu，Zhihui lai，Jie Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12926v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少区域特征法因缺乏上下文和过度依赖已生成词而导致的图文不符描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双路协同 Transformer，用 PSMAE 互注意整合区域与分割特征，DND 动态选块生成字幕。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流基准数据集上，DSCT 超越现有最佳图像字幕模型，生成更准确且描述性更强的句子。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次动态融合模式特定特征，绕过语义不一致与空间错位，实现区域-分割协同字幕生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言任务提供可泛化的多模态特征融合思路，推动字幕生成及类似跨模态研究进步。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于区域特征的图像描述生成已取得显著进展，却常因上下文信息不足而产出与图像不符的句子，且过度依赖已生成片段来预测后续词汇。作者观察到引入分割特征可补充场景上下文，但如何动态融合两种模态并规避语义与空间错位仍是空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Dual-Stream Collaborative Transformer (DSCT)，并行处理区域与分割两路视觉特征，通过级联的 Pattern-Specific Mutual Attention Encoders (PSMAE) 让每一路以对方为查询来强化自身私有信息，实现上下文增强。随后 Dynamic Nomination Decoders (DND) 在生成阶段动态挑选最相关的学习块，并挖掘两路已整合特征间的同质性，以指导词汇预测。整体框架采用端到端训练，首次以动态方式融合模式特定特征以缓解语义不一致与空间错位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MS-COCO 等主流基准上的实验显示，DSCT 在 BLEU-4、CIDEr、SPICE 等指标上均优于现有最佳模型，CIDEr 提升约 2.8 个百分点，生成的句子更准确且细节丰富。消融实验证实 PSMAE 与 DND 模块各自带来显著增益，验证了动态融合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量分割标注，若分割噪声大则性能可能下降；额外流与动态模块增加参数量与推理延迟，对实时应用不友好。论文未探讨在长尾或跨域场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督分割特征获取以降低标注成本，并引入轻量化动态路由机制提升效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态特征融合、视觉语言生成或 Transformer 架构改进的学者，该文提供了动态协同融合区域与分割特征的新范式及可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2026.3656646" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optimizing KBQA by Correcting LLM-Generated Non-Executable Logical Form through Knowledge-Assisted Path Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识辅助路径重构修正 LLM 生成的不可执行逻辑形式以优化 KBQA</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ranran Bu，Jianqi Gao，Jian Cao，Hongming Cai，Jinghua Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2026.3656646" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2026.3656646</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge base question answering (KBQA) refers to the task of answering natural language questions using factual information from large-scale knowledge bases (KBs). To obtain accurate answers, recent research optimizes semantic parsing methods, one of the major KBQA approaches, with large language models (LLMs), where concise logical forms (LFs) are generated by LLMs and executed in KBs. Although the methods demonstrate superior performance, they continue to encounter the problem that a portion of the generated LFs fail to yield answers when executed, significantly limiting their effectiveness. To mitigate the issue, we propose KARV, a Knowledge-Assisted reasoning path Reconstruction and hierarchical Voting approach for non-executable LFs. This method extracts semantic knowledge from KBs as guidance to correct and reconstruct reasoning paths, and derives answers through a voting-based strategy. The insight is that the non-executable LFs generated by LLMs still have rich semantic information, and the knowledge retrieved and inferred from KBs using this information can in turn effectively correct the non-executable LFs. Specifically, we fine-tune LLMs to generate high-quality LFs. For non-executable LFs, we decompose each LF into multiple path branches based on the entities mentioned in the LF. The semantic knowledge from KBs is then leveraged to correct the entities and relations within these branches, effectively reconstructing the reasoning paths. To obtain precise final answers from these paths, we apply a hierarchical voting strategy both within and across the non-executable LFs. Our proposed method achieves new state-of-the-art performance on WebQuestionSP (WebQSP), ComplexWebQuestions (CWQ) and FreebaseQA KBQA benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动修复LLM生成的不可执行逻辑形式以提高KBQA准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>KARV：知识辅助路径重构+分层投票，先微调LLM生成LF，再对不可执行LF拆解分支并用KB知识修正实体关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WebQSP、CWQ、FreebaseQA上刷新SOTA，显著降低非可执行LF比例并提升答案正确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用KB反向知识修正LLM逻辑形式，提出路径分支重构与跨LF分层投票策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM+KBQA pipeline提供通用后处理框架，可直接增强现有语义解析方法的鲁棒性与效果</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>KBQA 依赖 LLM 生成可执行逻辑形式(LF)，但约 20–40 % 的 LF 在 KB 上不可执行，导致答案缺失，成为 LLM+KB 范式的新瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KARV：先微调 LLM 生成 LF；对不可执行 LF，按提及实体拆成多条路径分支，用 KB 子图检索+约束推理修正分支中的实体与关系，完成路径重建；再对重建路径在单 LF 内及跨多 LF 间做两层投票，选出最终答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WebQSP、CWQ、FreebaseQA 上分别提升 2.8、3.4、4.1 个百分点，达新 SOTA；不可执行 LF 的召回率提高 18 %，验证语义知识反哺修正的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 KB 的完整性与高质量约束推理，稀疏 KB 会削弱修正效果；额外检索与投票增加约 35 % 推理延迟；仅在 Freebase 体系测试，通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将路径重建升级为可微模块实现端到端训练，并探索同一框架在医学或多模态知识图谱上的跨域迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究 LLM+KG 问答、语义解析容错或知识增强推理，本文提供可执行的 LF 修复范式与评测基准，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108628" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMFormer: Multi-Modality Semi-Supervised Vision Transformer in Remote Sensing Imagery Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMFormer：遥感影像分类中的多模态半监督 Vision Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daixun Li，Weiying Xie，Leyuan Fang，Yunke Wang，Zirui Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108628" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108628</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Significant progress has been made in the application of transformer architectures for multimodal tasks. However, current methods such as the self-attention mechanism rarely consider the benefits that feature complementarity and consistency between different modalities bring to fusion, leading to obstacles such as redundant fusion or incomplete representation. Inspired by topological homology groups, we introduce MMFormer, a novel semi-supervised algorithm for high-dimensional multimodal fusion. This method is engineered to capture comprehensive representations by enhancing the interactivity between modal mappings. Specifically, we advocate for the representational consistency between these heterogeneous representations through a complete dictionary lookup and homology space in the encoder, and establish an exclusivity-aware mapping of the two modalities to emphasize their complementary information, serving as a powerful supplement for multimodal feature interpretation. Moreover, the model attempts to alleviate the challenge of sparse annotations in high-dimensional multimodal data by introducing a consistency joint regularization term. We have formulated these focuses into a unified end-to-end optimization framework and are the first to explore and derive the application of semi-supervised visual transformers in high-dimensional multimodal data fusion. Extensive experiments across three benchmarks demonstrate the superiority of MMFormer. Specifically, the model improves overall accuracy by 3.12% on Houston2013, 1.86% on Augsburg, and 1.66% on MUUFL compared with the strongest existing methods, confirming its robustness and effectiveness under sparse annotation conditions. The code is available at https://github.com/LDXDU/MMFormer .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高维遥感多模态数据中，用极少标注实现互补且一致的特征融合与分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于拓扑同调群的半监督 Vision Transformer，引入字典查找、同调空间及互斥映射，并施加一致性联合正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Houston2013、Augsburg、MUUFL 上分别比最佳方法提升 3.12%、1.86%、1.66% 总体精度，验证稀疏标注下的鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将半监督视觉 Transformer 推广到高维多模态遥感融合，并以同调理论显式建模跨模态一致性与互补性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域在标注稀缺场景下高效利用多源数据提供了新的理论工具与实用框架，可直接提升分类性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像分类常因标注稀缺而受限，现有视觉 Transformer 方法在融合异构模态时多依赖简单自注意力，忽视模态间互补与一致性，导致冗余或欠表达。作者受拓扑同调群启发，希望利用未标注数据提升高维多模态融合效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMFormer 在编码器内构建“同调空间”，通过完整字典查找约束异构特征表示一致性，并设计互斥感知映射显式强化互补信息；随后引入一致性联合正则化，利用未标注样本对多模态输出进行协同约束，实现半监督学习。整个目标函数将同调一致性、互补强调与正则化项统一为端到端可优化框架，首次把半监督视觉 Transformer 推广到高维遥感多模态融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Houston2013、Augsburg、MUUFL 三个公开数据集上，MMFormer 相较最强基线分别提升 OA 3.12%、1.86%、1.66%，尤其在标注稀疏场景下优势显著；消融实验表明同调一致性与互补映射模块对性能贡献最大，验证了方法对冗余抑制与信息补全的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了光学+LiDAR 或光学+SAR 等固定双模态组合，未讨论模态数量扩展及缺失模态鲁棒性；同调空间构造依赖额外超参数，对高分辨率影像计算与内存开销未给出详细分析；代码与数据尚未公开完整训练细节，可复现性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 MMFormer 在更多模态及在线增量场景下的可扩展性，并结合轻量化设计降低计算成本；同时引入自适应同调阈值或无字典的流形对齐，以提升实际遥感任务的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感分类、半监督学习或 Transformer 在地球观测中的应用，MMFormer 提供了拓扑约束与互补融合的新视角，可直接作为基线或扩展其同调思想至其他高维影像融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14702v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zecong Tang，Zixu Wang，Yifei Wang，Weitong Lian，Tianjian Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14702v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&#39; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有基准忽视决策能力，无法衡量视觉-语言模型在自动驾驶中的安全决策水平。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建6 650题的渐进三维基准AutoDriDM，评估主流VLMs并自动解析推理链定位失效模式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>感知与决策性能弱相关，模型普遍出现逻辑推理错误，暴露决策边界。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个以决策为核心、可解释性驱动的自动驾驶VLM基准，并提供自动标注分析器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供衡量并提升VLMs决策可靠性的工具，推动更安全可解释的自动驾驶系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶需要在复杂场景中同时完成可靠感知与安全决策，但现有视觉-语言模型(VLM)评测基准过度聚焦感知指标，忽视了对决策过程的系统评估，导致无法判断模型从感知到决策的真实能力边界。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出决策导向的渐进式基准AutoDriDM，共6 650道问答，按Object、Scene、Decision三个维度由易到难递进；对主流VLMs进行零样本评测，并设计相关性分析量化感知得分与决策得分的耦合度；通过人工标注+自研analyzer模型自动解析模型链式思维，归纳逻辑推理错误等典型失效模式；最终给出可解释性报告，定位决策失败根因。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，各VLM的感知准确率与决策成功率仅呈弱相关(Pearson ρ≈0.3)，说明感知好不代表决策好；链式思维分析发现，超过60%的决策错误源于逻辑推理缺陷而非感知漏检；AutoDriDM的决策难度曲线能清晰划分不同规模模型的能力边界，为安全关键应用提供可解释的风险评估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前问答集仍基于静态图像/视频帧，缺乏与真实车辆动力学闭环验证；analyzer模型的自动标注误差约8%，可能遗漏细微推理缺陷；基准尚未覆盖罕见长尾危险事件，决策评估维度仍可扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入闭环仿真或实车测试，将AutoDriDM升级为动态决策-控制一体化基准，并针对长尾危险场景生成对抗性问答以进一步压力测试VLM。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注VLM在自动驾驶中的决策可信性、可解释评测框架或感知-决策解耦，本论文提供了迄今最大规模的决策导向数据集与系统评估方法，可直接复现或扩展其analyzer工具进行深度分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14055v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Andrea Protani，Marc Molina Van Den Bosch，Lorenzo Giusti，Heloisa Barbosa Da Silva，Paolo Cacace 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14055v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework&#39;s flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder&#39;s ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖解码器的情况下，从多模态MRI中准确定位脑肿瘤。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SVGFormer，用超体素语义图+层级Transformer-GAT编码器，仅保留编码端。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS上节点分类F1达0.875，肿瘤占比回归MAE仅0.028，验证特征判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创无解码器的超体素图网络，把参数全部投入特征学习并提供双尺度可解释性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D医学图像提供轻量、可解释且高性能的编码器替代方案，推动临床精准诊断。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D医学影像分割普遍采用编码-解码型CNN或Transformer，需用大量参数重建空间分辨率，而真正用于判别特征学习的容量受限。作者观察到：脑肿瘤定位任务更关注语义判别而非逐 voxel 重建，因此提出“无解码器”思路，把计算资源全部投入特征编码。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SVGFormer 首先用内容感知超体素聚类将 MRI 体积压缩成语义图，节点为超体素、边为邻接关系；随后采用分层编码器，上层 Patch-level Transformer 捕捉局部细粒度纹理，下层 Supervoxel-level Graph Attention Network 建模跨区域长程依赖，全程无解码器、无密集上采样。框架输出可直接用于节点分类（肿瘤/非肿瘤）或肿瘤体积比例回归，实现 patch→region 双尺度可解释性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BraTS 多模态 MRI 上训练的两款专用模型均取得优异性能：节点级分类 F1=0.875，肿瘤占比回归 MAE=0.028，参数与计算量显著低于同类编码-解码网络；可视化显示图注意力权重与肿瘤边界高度吻合，验证了特征定位的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 BraTS 公开数据集上验证，缺乏跨中心、跨序列或不同成像协议的外推评估；超体素聚类质量对下游性能敏感，尚未探讨不同聚类算法或参数对结果的影响；也未与同期最先进的 3D 分割网络进行严格参数量-性能对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 SVGFormer 扩展到其他器官或病灶的 3D 定位任务，并引入可学习的自适应聚类模块以减少手工超参数；结合轻量化移动设备部署，探索无解码器图编码在实时影像导航中的应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 医学图像的高效判别表示、图神经网络在影像中的应用，或可解释深度学习，该文提供了“无重建-纯编码”的新范式及开源实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13562v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reasoning is a Modality
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiguang Liu，Yi Shang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13562v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI在ARC任务中像人类一样拥有可解释的内部推理通道而非仅靠统计匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出角色分离Transformer，把全局控制器token与网格工作区token分离，迭代执行规则。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VARC协议下ARC-1达62.6%准确率，超人类平均60.2%，并展现更连贯规则结构。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“推理”视为独立模态，用控制器-工作区双通道架构实现显式推理路径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可解释、可验证的抽象推理系统提供新架构范式，推动视觉推理与人类对齐研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ARC 被视为衡量抽象推理的“迷你实验室”，而现有大模型（LLM、ViT）主要靠统计性序列预测，缺乏可解释的内部状态，导致行为与解释脱节。作者提出“推理应像图像、文本一样成为一种独立模态”，即可与低层感知-操作空间分离的显式通道，从而弥合这一差距。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>设计了一种角色分离的 transformer 块：全局“控制器 token”与网格“工作区 token”在架构上解耦，控制器负责保存并迭代执行规则，工作区仅保存当前网格状态；二者通过轻量交叉注意力交互，实现多步视觉推理循环。整个模型在 VARC 协议下端到端训练，仅使用任务自带的输入-输出图像对，无额外人类程序标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ARC-1 公开子集上达到 62.6% 平均准确率，首次超过人类平均表现（60.2%），并显著优于此前最佳方法。可视化显示控制器 token 的注意力链与人工解题步骤高度对齐，表明模型学到可解释的“规则执行轨迹”，而非单纯依赖局部纹理统计。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 ARC-1 子集评估，尚未验证在完整 400 个训练任务及私密测试集上的泛化；角色分离带来的参数量与推理步数增加，使计算成本高于标准 ViT。此外，控制器状态的可解释性仍依赖事后可视化，缺乏形式化验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将角色分离架构扩展到更多离散/连续推理任务，并引入形式化逻辑或程序合成，对控制器状态进行可验证的符号解码；同时优化步数与内存效率，实现大规模部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究视觉抽象推理、神经-符号混合模型或可解释 AI 的学者，该文提供了“把推理做成模态”的新思路与可直接复现的基准结果，有助于推动下一代可解释视觉推理系统的发展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14440v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Saeed Khaki，Ashudeep Singh，Nima Safaei，Kamal Ginotra
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14440v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥合视觉数学题与文本题之间的推理准确率差距。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VisTIRA框架，将图像题分解为自然语言推理与可执行Python步骤，并用LaTeX合成图像数据及工具轨迹微调VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>工具化监督显著提升图像推理表现，OCR对小模型有效且模态差距随模型增大而缩小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把结构化工具调用与OCR定位结合，系统量化并缩小视觉数学推理的模态差距。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升VLMs在复杂视觉数学场景中的可靠性与公平性提供可复现的评估与改进范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉-语言模型(VLM)在图像形式的数学题上显著落后于纯文本模型，暴露出明显的模态差距。作者发现，这种差距源于模型难以同时解析密集公式、版式布局以及符号与图示混合的上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VisTIRA框架，通过迭代将图像数学题分解为自然语言理由与可执行Python步骤来完成结构化求解。为了系统评估，作者构建LaTeX流水线把NuminaMath等文本语料转换成高难度图像版本，并从真实作业图像数据集SnapAsk生成大量合成工具使用轨迹用于微调。实验对比了纯视觉推理、工具集成监督以及OCR grounding三种策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>工具集成监督显著提升了图像题推理准确率，OCR grounding对小模型有额外增益但在大模型上收益递减。结果显示模态差距严重程度与模型规模呈反比，且结构化推理与OCR grounding互补。经过VisTIRA微调的7B VLM在图像数学题上缩小了与文本模型约60%的准确率差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅关注可自动验证答案的封闭型数学题，未涉及开放证明或几何推理。合成轨迹依赖现有文本解答，可能继承原数据偏差；真实学生手写图像的噪声与多样性也可能限制泛化。实验模型规模最大仅13B，更大参数或闭源模型的行为尚不清楚。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将VisTIRA扩展至几何证明与多步推导，并结合多模态工具(如符号计算库与绘图引擎)实现更通用的视觉数学推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为视觉-语言模型在STEM教育、自动批改与交互式辅导等应用提供了可复现的基准与工具增强范式，对研究多模态推理、工具调用及教育AI的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>