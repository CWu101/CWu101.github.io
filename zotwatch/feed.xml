<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 16 Jan 2026 02:45:10 +0000</lastBuildDate><item><title>DVGBench: Implicit-to-explicit visual grounding benchmark in UAV imagery with large vision–language models</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.005</link><guid>10.1016/j.isprsjprs.2026.01.005</guid><pubDate>Wed, 14 Jan 2026 23:32:14 +0000</pubDate><dc:creator>Yue Zhou</dc:creator><dc:creator>Jue Chen</dc:creator><dc:creator>Zilun Zhang</dc:creator><dc:creator>Penghui Huang</dc:creator><dc:creator>Ran Ding</dc:creator><dc:creator>Zhentao Zou</dc:creator><dc:creator>PengFei Gao</dc:creator><dc:creator>Yuchen Wei</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Xue Jiang</dc:creator><dc:creator>Hongxin Yang</dc:creator><dc:creator>Jonathan Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.005</prism:doi><description>Remote sensing (RS) large vision–language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions – such as relative position, relative size, and color cues – thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench .
Published: 2026-01-14T23:32:14+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.608 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhou; Jue Chen; Zilun Zhang; Penghui Huang; Ran Ding; Zhentao Zou; PengFei Gao; Yuchen Wei; Ke Li; Xue Yang; Xue Jiang; Hongxin Yang; Jonathan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.005"&gt;10.1016/j.isprsjprs.2026.01.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.608 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing (RS) large vision–language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions – such as relative position, relative size, and color cues – thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench .&lt;/p&gt;</content:encoded></item><item><title>Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation</title><link>https://arxiv.org/abs/2601.08728v1</link><guid>http://arxiv.org/abs/2601.08728v1</guid><pubDate>Tue, 13 Jan 2026 16:57:09 +0000</pubDate><dc:creator>Runfeng Qu</dc:creator><dc:creator>Ole Hall</dc:creator><dc:creator>Pia K Bideau</dc:creator><dc:creator>Julie Ouerfelli-Ethier</dc:creator><dc:creator>Martin Rolfs</dc:creator><dc:creator>Klaus Obermayer</dc:creator><dc:creator>Olaf Hellwich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision
Published: 2026-01-13T16:57:09+00:00
Venue: arXiv
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runfeng Qu; Ole Hall; Pia K Bideau; Julie Ouerfelli-Ethier; Martin Rolfs; Klaus Obermayer; Olaf Hellwich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision&lt;/p&gt;</content:encoded></item><item><title>DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.09981v1</link><guid>http://arxiv.org/abs/2601.09981v1</guid><pubDate>Thu, 15 Jan 2026 01:48:45 +0000</pubDate><dc:creator>Yulin He</dc:creator><dc:creator>Wei Chen</dc:creator><dc:creator>Zhikang Jian</dc:creator><dc:creator>Tianhang Guo</dc:creator><dc:creator>Wenjuan Zhou</dc:creator><dc:creator>Minglong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.
Published: 2026-01-15T01:48:45+00:00
Venue: arXiv
Score: 0.571 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yulin He; Wei Chen; Zhikang Jian; Tianhang Guo; Wenjuan Zhou; Minglong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.571 (consider)&lt;/p&gt;
&lt;p&gt;Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.&lt;/p&gt;</content:encoded></item><item><title>Urban Socio-Semantic Segmentation with Vision-Language Reasoning</title><link>https://arxiv.org/abs/2601.10477v1</link><guid>http://arxiv.org/abs/2601.10477v1</guid><pubDate>Thu, 15 Jan 2026 15:00:36 +0000</pubDate><dc:creator>Yu Wang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Rui Dai</dc:creator><dc:creator>Yujie Wang</dc:creator><dc:creator>Kaikui Liu</dc:creator><dc:creator>Xiangxiang Chu</dc:creator><dc:creator>Yansheng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.
Published: 2026-01-15T15:00:36+00:00
Venue: arXiv
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Wang; Yi Wang; Rui Dai; Yujie Wang; Kaikui Liu; Xiangxiang Chu; Yansheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&amp;#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.&lt;/p&gt;</content:encoded></item><item><title>The Spatial Blindspot of Vision-Language Models</title><link>https://arxiv.org/abs/2601.09954v1</link><guid>http://arxiv.org/abs/2601.09954v1</guid><pubDate>Thu, 15 Jan 2026 00:30:34 +0000</pubDate><dc:creator>Nahid Alam</dc:creator><dc:creator>Leema Krishna Murali</dc:creator><dc:creator>Siddhant Bharadwaj</dc:creator><dc:creator>Patrick Liu</dc:creator><dc:creator>Timothy Chung</dc:creator><dc:creator>Drishti Sharma</dc:creator><dc:creator>Akshata A</dc:creator><dc:creator>Kranthi Kiran</dc:creator><dc:creator>Wesley Tam</dc:creator><dc:creator>Bala Krishna S Vegesna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.
Published: 2026-01-15T00:30:34+00:00
Venue: arXiv
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nahid Alam; Leema Krishna Murali; Siddhant Bharadwaj; Patrick Liu; Timothy Chung; Drishti Sharma; Akshata A; Kranthi Kiran; Wesley Tam; Bala Krishna S Vegesna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.&lt;/p&gt;</content:encoded></item><item><title>RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.10168v1</link><guid>http://arxiv.org/abs/2601.10168v1</guid><pubDate>Thu, 15 Jan 2026 08:15:01 +0000</pubDate><dc:creator>Yue Chang</dc:creator><dc:creator>Rufeng Chen</dc:creator><dc:creator>Zhaofan Zhang</dc:creator><dc:creator>Yi Chen</dc:creator><dc:creator>Sihong Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.
Published: 2026-01-15T08:15:01+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Chang; Rufeng Chen; Zhaofan Zhang; Yi Chen; Sihong Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.&lt;/p&gt;</content:encoded></item><item><title>LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning</title><link>https://arxiv.org/abs/2601.10129v1</link><guid>http://arxiv.org/abs/2601.10129v1</guid><pubDate>Thu, 15 Jan 2026 07:14:24 +0000</pubDate><dc:creator>Linquan Wu</dc:creator><dc:creator>Tianxiang Jiang</dc:creator><dc:creator>Yifei Dong</dc:creator><dc:creator>Haoyu Yang</dc:creator><dc:creator>Fengji Zhang</dc:creator><dc:creator>Shichaang Meng</dc:creator><dc:creator>Ai Xuan</dc:creator><dc:creator>Linqi Song</dc:creator><dc:creator>Jacky Keung</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.
Published: 2026-01-15T07:14:24+00:00
Venue: arXiv
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linquan Wu; Tianxiang Jiang; Yifei Dong; Haoyu Yang; Fengji Zhang; Shichaang Meng; Ai Xuan; Linqi Song; Jacky Keung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher&amp;#x27;s textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher&amp;#x27;s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.&lt;/p&gt;</content:encoded></item><item><title>EinsPT: Efficient Instance-Aware Pre-Training of Vision Foundation Models</title><link>https://doi.org/10.1109/tip.2026.3652371</link><guid>10.1109/tip.2026.3652371</guid><pubDate>Wed, 14 Jan 2026 20:41:45 +0000</pubDate><dc:creator>Zhaozhi Wang</dc:creator><dc:creator>Yunjie Tian</dc:creator><dc:creator>Lingxi Xie</dc:creator><dc:creator>Yaowei Wang</dc:creator><dc:creator>Qixiang Ye</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652371</prism:doi><description>In this study, we introduce EinsPT, an efficient instance-aware pre-training paradigm designed to reduce the transfer gap between vision foundation models and downstream instance-level tasks. Unlike conventional image-level pre-training that relies solely on unlabeled images, EinsPT leverages both image reconstruction and instance annotations to learn representations that are spatially coherent and instance discriminative. To achieve this efficiently, we propose a proxy–foundation architecture that decouples high-resolution and low-resolution learning: the foundation model processes masked low-resolution images for global semantics, while a lightweight proxy model operates on complete high-resolution images to preserve fine-grained details. The two branches are jointly optimized through reconstruction and instance-level prediction losses on fused features. Extensive experiments demonstrate that EinsPT consistently enhances recognition accuracy across various downstream tasks with substantially reduced computational cost, while qualitative results further reveal improved instance perception and completeness in visual representations. Code is available at github.com/feufhd/EinsPT.
Published: 2026-01-14T20:41:45+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaozhi Wang; Yunjie Tian; Lingxi Xie; Yaowei Wang; Qixiang Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652371"&gt;10.1109/tip.2026.3652371&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;In this study, we introduce EinsPT, an efficient instance-aware pre-training paradigm designed to reduce the transfer gap between vision foundation models and downstream instance-level tasks. Unlike conventional image-level pre-training that relies solely on unlabeled images, EinsPT leverages both image reconstruction and instance annotations to learn representations that are spatially coherent and instance discriminative. To achieve this efficiently, we propose a proxy–foundation architecture that decouples high-resolution and low-resolution learning: the foundation model processes masked low-resolution images for global semantics, while a lightweight proxy model operates on complete high-resolution images to preserve fine-grained details. The two branches are jointly optimized through reconstruction and instance-level prediction losses on fused features. Extensive experiments demonstrate that EinsPT consistently enhances recognition accuracy across various downstream tasks with substantially reduced computational cost, while qualitative results further reveal improved instance perception and completeness in visual representations. Code is available at github.com/feufhd/EinsPT.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Visual In-Context Learning by Multi-Faceted Fusion</title><link>https://arxiv.org/abs/2601.10107v1</link><guid>http://arxiv.org/abs/2601.10107v1</guid><pubDate>Thu, 15 Jan 2026 06:25:09 +0000</pubDate><dc:creator>Wenwen Liao</dc:creator><dc:creator>Jianbo Yu</dc:creator><dc:creator>Yuansong Wang</dc:creator><dc:creator>Qingchao Jiang</dc:creator><dc:creator>Xiaofeng Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.
Published: 2026-01-15T06:25:09+00:00
Venue: arXiv
Score: 0.550 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Liao; Jianbo Yu; Yuansong Wang; Qingchao Jiang; Xiaofeng Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.550 (consider)&lt;/p&gt;
&lt;p&gt;Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant &amp;quot;retrieve-then-prompt&amp;quot; approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model&amp;#x27;s reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.&lt;/p&gt;</content:encoded></item><item><title>CoGMoE: Sparse and Specialized Framework for Multi-Agent Collaborative Perception via Graph Mixture-of-Experts</title><link>https://doi.org/10.1016/j.knosys.2026.115329</link><guid>10.1016/j.knosys.2026.115329</guid><pubDate>Wed, 14 Jan 2026 16:15:10 +0000</pubDate><dc:creator>Xingpeng Li</dc:creator><dc:creator>Enwen Hu</dc:creator><dc:creator>Siyuan Jin</dc:creator><dc:creator>Baoding Zhou</dc:creator><dc:creator>Jingrong Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115329</prism:doi><description>Multi-agent collaborative perception significantly improves autonomous driving safety by sharing complementary information to overcome individual limitations owing to occlusions. A primary goal is to navigate the critical trade-off between perception performance and communication bandwidth. However, existing methods struggle to achieve this balance, treating all information equally without considering each agent’s specific situation. To address this issue, this study proposes CoGMoE, a novel collaborative perception method that models the V2V communication as a structured, hierarchical reasoning process. Specifically, CoGMoE provides three distinct advantages: i) it selects a sparse set of semantically salient keypoints from each vehicle, significantly reducing communication overhead while preserving important information; ii) it constructs a hierarchical communication graph that establishes direct alignment links between the corresponding position areas of different vehicles, explicitly separating them from the internal links used for context reasoning; and iii) it uses a graph mixture-of-experts (GraphMoE) architecture governed by multi-round expert deliberation to dynamically assign experts for each link type, achieving superior robustness using iterative feature refinement. Extensive experiments on both simulated and real-world datasets demonstrate that our proposed CoGMoE outperforms state-of-the-art collaborative perception methods in achieving detection accuracy and communication bandwidth trade-off.
Published: 2026-01-14T16:15:10+00:00
Venue: Knowledge-Based Systems
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingpeng Li; Enwen Hu; Siyuan Jin; Baoding Zhou; Jingrong Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115329"&gt;10.1016/j.knosys.2026.115329&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Multi-agent collaborative perception significantly improves autonomous driving safety by sharing complementary information to overcome individual limitations owing to occlusions. A primary goal is to navigate the critical trade-off between perception performance and communication bandwidth. However, existing methods struggle to achieve this balance, treating all information equally without considering each agent’s specific situation. To address this issue, this study proposes CoGMoE, a novel collaborative perception method that models the V2V communication as a structured, hierarchical reasoning process. Specifically, CoGMoE provides three distinct advantages: i) it selects a sparse set of semantically salient keypoints from each vehicle, significantly reducing communication overhead while preserving important information; ii) it constructs a hierarchical communication graph that establishes direct alignment links between the corresponding position areas of different vehicles, explicitly separating them from the internal links used for context reasoning; and iii) it uses a graph mixture-of-experts (GraphMoE) architecture governed by multi-round expert deliberation to dynamically assign experts for each link type, achieving superior robustness using iterative feature refinement. Extensive experiments on both simulated and real-world datasets demonstrate that our proposed CoGMoE outperforms state-of-the-art collaborative perception methods in achieving detection accuracy and communication bandwidth trade-off.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</title><link>https://arxiv.org/abs/2601.10551v1</link><guid>http://arxiv.org/abs/2601.10551v1</guid><pubDate>Thu, 15 Jan 2026 16:16:34 +0000</pubDate><dc:creator>Luxuan Fu</dc:creator><dc:creator>Chong Liu</dc:creator><dc:creator>Bisheng Yang</dc:creator><dc:creator>Zhen Dong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.
Published: 2026-01-15T16:16:34+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luxuan Fu; Chong Liu; Bisheng Yang; Zhen Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.&lt;/p&gt;</content:encoded></item><item><title>Unveiling the Unknown: A SAM Guided Open World Object Detection Method for Remote Sensing</title><link>https://doi.org/10.1109/tgrs.2026.3654387</link><guid>10.1109/tgrs.2026.3654387</guid><pubDate>Wed, 14 Jan 2026 20:40:24 +0000</pubDate><dc:creator>Mingtao Hu</dc:creator><dc:creator>Wenxin Yin</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Xin Gao</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654387</prism:doi><description>Despite the remarkable success of remote sensing object detection, these methods primarily operate under a closed-world paradigm, as they often misclassify or ignore novel objects in real world scenarios. To address this limitation, Open World Object Detection (OWOD) has emerged, enabling the discovery and incremental learning of new categories. However, existing OWOD approaches typically distinguish between known and unknown categories based on feature distance, overlooking the inherent challenge in remote sensing: large intra-class variation versus small inter-class variation. To bridge the gap, we propose a novel framework for SAM guided OWOD tailored for remote sensing imagery. Our approach is designed to leverage SAM’s capabilities for discovering new categories while systematically handling the noisy labels produced by SAM. We introduce four key components: (I) a Multi-scale Feature Fusion Perception (MFFP) module to enhance the detection of unknown objects across various scales in remote sensing; (II) a Cross-layer Cascaded Decoupling Decoder (CCDD) to alleviate the optimization conflicts between objectness and classification tasks for similar known and unknown classes in remote sensing images; (III) a Label Mapping Alignment (LMA) mechanism to adaptively filter background noisy proposals from SAM. And (IV) an Active Learning (AL) strategy is proposed to intelligently select exemplars for robust incremental learning. Extensive experiments on benchmark remote sensing datasets, including DIOR, DOTA, and NWPU demonstrate that our method significantly improves the recall of unknown objects while maintaining robust detection performance for known classes, demonstrating the effectiveness and potential of the SAM guided OWOD paradigm.
Published: 2026-01-14T20:40:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingtao Hu; Wenxin Yin; Wenhui Diao; Xin Gao; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654387"&gt;10.1109/tgrs.2026.3654387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Despite the remarkable success of remote sensing object detection, these methods primarily operate under a closed-world paradigm, as they often misclassify or ignore novel objects in real world scenarios. To address this limitation, Open World Object Detection (OWOD) has emerged, enabling the discovery and incremental learning of new categories. However, existing OWOD approaches typically distinguish between known and unknown categories based on feature distance, overlooking the inherent challenge in remote sensing: large intra-class variation versus small inter-class variation. To bridge the gap, we propose a novel framework for SAM guided OWOD tailored for remote sensing imagery. Our approach is designed to leverage SAM’s capabilities for discovering new categories while systematically handling the noisy labels produced by SAM. We introduce four key components: (I) a Multi-scale Feature Fusion Perception (MFFP) module to enhance the detection of unknown objects across various scales in remote sensing; (II) a Cross-layer Cascaded Decoupling Decoder (CCDD) to alleviate the optimization conflicts between objectness and classification tasks for similar known and unknown classes in remote sensing images; (III) a Label Mapping Alignment (LMA) mechanism to adaptively filter background noisy proposals from SAM. And (IV) an Active Learning (AL) strategy is proposed to intelligently select exemplars for robust incremental learning. Extensive experiments on benchmark remote sensing datasets, including DIOR, DOTA, and NWPU demonstrate that our method significantly improves the recall of unknown objects while maintaining robust detection performance for known classes, demonstrating the effectiveness and potential of the SAM guided OWOD paradigm.&lt;/p&gt;</content:encoded></item><item><title>CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval</title><link>https://arxiv.org/abs/2601.08175v1</link><guid>http://arxiv.org/abs/2601.08175v1</guid><pubDate>Tue, 13 Jan 2026 03:09:35 +0000</pubDate><dc:creator>Feiran Wang</dc:creator><dc:creator>Junyi Wu</dc:creator><dc:creator>Dawen Cai</dc:creator><dc:creator>Yuan Hong</dc:creator><dc:creator>Yan Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.
Published: 2026-01-13T03:09:35+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feiran Wang; Junyi Wu; Dawen Cai; Yuan Hong; Yan Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.&lt;/p&gt;</content:encoded></item><item><title>Semantic Misalignment in Vision-Language Models under Perceptual Degradation</title><link>https://arxiv.org/abs/2601.08355v2</link><guid>http://arxiv.org/abs/2601.08355v2</guid><pubDate>Tue, 13 Jan 2026 09:13:05 +0000</pubDate><dc:creator>Guo Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.
Published: 2026-01-13T09:13:05+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guo Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.&lt;/p&gt;</content:encoded></item><item><title>See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval</title><link>https://arxiv.org/abs/2601.09350v1</link><guid>http://arxiv.org/abs/2601.09350v1</guid><pubDate>Wed, 14 Jan 2026 10:28:11 +0000</pubDate><dc:creator>Mingyu Jeon</dc:creator><dc:creator>Sungjin Han</dc:creator><dc:creator>Jinkwon Hwang</dc:creator><dc:creator>Minchol Kwon</dc:creator><dc:creator>Jonghee Kim</dc:creator><dc:creator>Junyeong Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.
Published: 2026-01-14T10:28:11+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyu Jeon; Sungjin Han; Jinkwon Hwang; Minchol Kwon; Jonghee Kim; Junyeong Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.&lt;/p&gt;</content:encoded></item><item><title>VENUS: Visual Editing with Noise Inversion Using Scene Graphs</title><link>https://arxiv.org/abs/2601.07219v1</link><guid>http://arxiv.org/abs/2601.07219v1</guid><pubDate>Mon, 12 Jan 2026 05:24:58 +0000</pubDate><dc:creator>Thanh-Nhan Vo</dc:creator><dc:creator>Trong-Thuan Nguyen</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.
Published: 2026-01-12T05:24:58+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.&lt;/p&gt;</content:encoded></item><item><title>GeoCraft: A Diffusion Model-Based 3D Reconstruction Method Driven by Image and Point Cloud Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104149</link><guid>10.1016/j.inffus.2026.104149</guid><pubDate>Wed, 14 Jan 2026 00:29:37 +0000</pubDate><dc:creator>Weixuan Ma</dc:creator><dc:creator>Yamin Li</dc:creator><dc:creator>Chujin Liu</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Kansong Chen</dc:creator><dc:creator>Weixuan Gao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104149</prism:doi><description>With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .
Published: 2026-01-14T00:29:37+00:00
Venue: Information Fusion
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weixuan Ma; Yamin Li; Chujin Liu; Hao Zhang; Jie Li; Kansong Chen; Weixuan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104149"&gt;10.1016/j.inffus.2026.104149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .&lt;/p&gt;</content:encoded></item><item><title>MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</title><link>https://arxiv.org/abs/2601.08420v1</link><guid>http://arxiv.org/abs/2601.08420v1</guid><pubDate>Tue, 13 Jan 2026 10:44:37 +0000</pubDate><dc:creator>Aditya Chaudhary</dc:creator><dc:creator>Sneha Barman</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Girish Mishra</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.
Published: 2026-01-13T10:44:37+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aditya Chaudhary; Sneha Barman; Mainak Singha; Ankit Jha; Girish Mishra; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&amp;#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.&lt;/p&gt;</content:encoded></item><item><title>V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation</title><link>https://arxiv.org/abs/2601.10094v1</link><guid>http://arxiv.org/abs/2601.10094v1</guid><pubDate>Thu, 15 Jan 2026 05:47:43 +0000</pubDate><dc:creator>Han Wang</dc:creator><dc:creator>Yi Yang</dc:creator><dc:creator>Jingyuan Hu</dc:creator><dc:creator>Minfeng Zhu</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero
Published: 2026-01-15T05:47:43+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Wang; Yi Yang; Jingyuan Hu; Minfeng Zhu; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero&lt;/p&gt;</content:encoded></item><item><title>More Images, More Problems? A Controlled Analysis of VLM Failure Modes</title><link>https://arxiv.org/abs/2601.07812v1</link><guid>http://arxiv.org/abs/2601.07812v1</guid><pubDate>Mon, 12 Jan 2026 18:45:13 +0000</pubDate><dc:creator>Anurag Das</dc:creator><dc:creator>Adrian Bulat</dc:creator><dc:creator>Alberto Baldrati</dc:creator><dc:creator>Ioannis Maniadis Metaxas</dc:creator><dc:creator>Bernt Schiele</dc:creator><dc:creator>Georgios Tzimiropoulos</dc:creator><dc:creator>Brais Martinez</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.
Published: 2026-01-12T18:45:13+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anurag Das; Adrian Bulat; Alberto Baldrati; Ioannis Maniadis Metaxas; Bernt Schiele; Georgios Tzimiropoulos; Brais Martinez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.&lt;/p&gt;</content:encoded></item><item><title>Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</title><link>https://arxiv.org/abs/2601.07761v1</link><guid>http://arxiv.org/abs/2601.07761v1</guid><pubDate>Mon, 12 Jan 2026 17:46:10 +0000</pubDate><dc:creator>Yanxiang Huang</dc:creator><dc:creator>Guohua Gao</dc:creator><dc:creator>Zhaoyang Wei</dc:creator><dc:creator>Jianyuan Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.
Published: 2026-01-12T17:46:10+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanxiang Huang; Guohua Gao; Zhaoyang Wei; Jianyuan Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.&lt;/p&gt;</content:encoded></item><item><title>OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</title><link>https://arxiv.org/abs/2601.09575v1</link><guid>http://arxiv.org/abs/2601.09575v1</guid><pubDate>Wed, 14 Jan 2026 15:45:57 +0000</pubDate><dc:creator>Sheng-Yu Huang</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.
Published: 2026-01-14T15:45:57+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sheng-Yu Huang; Jaesung Choe; Yu-Chiang Frank Wang; Cheng Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.&lt;/p&gt;</content:encoded></item><item><title>Towards Generative Understanding: Incremental Few-shot Semantic Segmentation with Diffusion Models</title><link>https://doi.org/10.1109/tip.2026.3652357</link><guid>10.1109/tip.2026.3652357</guid><pubDate>Wed, 14 Jan 2026 20:41:45 +0000</pubDate><dc:creator>Qun Li</dc:creator><dc:creator>Lu Huang</dc:creator><dc:creator>Fu Xiao</dc:creator><dc:creator>Na Zhao</dc:creator><dc:creator>Bir Bhanu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652357</prism:doi><description>Incremental Few-shot Semantic Segmentation (iFSS) aims to learn novel classes with limited samples while preserving segmentation capability for base classes, addressing the challenge of continual learning of novel classes and catastrophic forgetting of previously seen classes. Existing methods mainly rely on techniques such as knowledge distillation and background learning, which, while partially effective, still suffer from issues such as feature drift and limited generalization to real-world novel classes, primarily due to a bidirectional coupling bottleneck between the learning of base classes and novel classes. To address these challenges, we propose, for the first time, a diffusion-based generative framework for iFSS. Specifically, we bridge the gap between generative and discriminative tasks through an innovative binary-to-RGB mask mapping mechanism, enabling pre-trained diffusion models to focus on target regions via class-specific semantic embedding optimization while sharpening foreground-background contrast with color embeddings. A lightweight post-processor then refines the generated images into high-quality binary masks. Crucially, by leveraging diffusion priors, our framework avoids complex training strategies. The optimization of class-specific semantic embeddings decouples the embedding spaces of base and novel classes, inherently preventing feature drift, mitigating catastrophic forgetting, and enabling rapid novel-class adaptation. Experimental results show that our method achieves state-of-the-art performance on the PASCAL-5i and COCO-20i datasets using much less data than other methods, and exhibiting competitive results in cross-domain few-shot segmentation tasks. Project page: https://ifss-diff.github.io/.
Published: 2026-01-14T20:41:45+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qun Li; Lu Huang; Fu Xiao; Na Zhao; Bir Bhanu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652357"&gt;10.1109/tip.2026.3652357&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Incremental Few-shot Semantic Segmentation (iFSS) aims to learn novel classes with limited samples while preserving segmentation capability for base classes, addressing the challenge of continual learning of novel classes and catastrophic forgetting of previously seen classes. Existing methods mainly rely on techniques such as knowledge distillation and background learning, which, while partially effective, still suffer from issues such as feature drift and limited generalization to real-world novel classes, primarily due to a bidirectional coupling bottleneck between the learning of base classes and novel classes. To address these challenges, we propose, for the first time, a diffusion-based generative framework for iFSS. Specifically, we bridge the gap between generative and discriminative tasks through an innovative binary-to-RGB mask mapping mechanism, enabling pre-trained diffusion models to focus on target regions via class-specific semantic embedding optimization while sharpening foreground-background contrast with color embeddings. A lightweight post-processor then refines the generated images into high-quality binary masks. Crucially, by leveraging diffusion priors, our framework avoids complex training strategies. The optimization of class-specific semantic embeddings decouples the embedding spaces of base and novel classes, inherently preventing feature drift, mitigating catastrophic forgetting, and enabling rapid novel-class adaptation. Experimental results show that our method achieves state-of-the-art performance on the PASCAL-5i and COCO-20i datasets using much less data than other methods, and exhibiting competitive results in cross-domain few-shot segmentation tasks. Project page: https://ifss-diff.github.io/.&lt;/p&gt;</content:encoded></item><item><title>Seeing Clearly and Detecting Precisely: Perceptual Enhancement and Focus Calibration for Small-Object Detection</title><link>https://doi.org/10.1109/tnnls.2026.3651289</link><guid>10.1109/tnnls.2026.3651289</guid><pubDate>Wed, 14 Jan 2026 20:40:49 +0000</pubDate><dc:creator>Zhiqin Zhu</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Guanqiu Qi</dc:creator><dc:creator>Shuang Li</dc:creator><dc:creator>Huafeng Li</dc:creator><dc:creator>Yu Liu</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2026.3651289</prism:doi><description>Small-object detection remains challenging due to limited pixel information, blurred boundaries, and weak semantic cues. Although recent advances in multiscale fusion and attention mechanisms have led to improved performance, existing methods still struggle to preserve high-frequency structural details and achieve precise localization—particularly in dense, cluttered, or low-resolution scenarios. These limitations are primarily caused by the loss of fine-grained features during downsampling and the absence of region-aware focus mechanisms. Inspired by the human visual strategy of “see clearly and detect precisely,” we propose PEFC-Net, a novel framework that enhances both perceptual clarity and localization accuracy for small-object detection. To mitigate structural degradation, we introduce the hybrid structural perception (HSP) module, which jointly encodes spatial gradients and localized frequency components through wavelet-based decomposition and edge-aware refinement. To further improve region-level focus, we design the axis-aligned focus calibration (AAFC) module, which captures long-range directional context via axis-sensitive pooling and adaptively refines attention with shape-aware calibration. Extensive experiments on four challenging benchmarks—VisDrone-2019, TT100K, NWPU VHR-10, and DIOR—demonstrate that PEFC-Net consistently outperforms state-of-the-art methods, delivering robust performance under occlusion, dense distribution, and scale variation.
Published: 2026-01-14T20:40:49+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiqin Zhu; Yang Yang; Guanqiu Qi; Shuang Li; Huafeng Li; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2026.3651289"&gt;10.1109/tnnls.2026.3651289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Small-object detection remains challenging due to limited pixel information, blurred boundaries, and weak semantic cues. Although recent advances in multiscale fusion and attention mechanisms have led to improved performance, existing methods still struggle to preserve high-frequency structural details and achieve precise localization—particularly in dense, cluttered, or low-resolution scenarios. These limitations are primarily caused by the loss of fine-grained features during downsampling and the absence of region-aware focus mechanisms. Inspired by the human visual strategy of “see clearly and detect precisely,” we propose PEFC-Net, a novel framework that enhances both perceptual clarity and localization accuracy for small-object detection. To mitigate structural degradation, we introduce the hybrid structural perception (HSP) module, which jointly encodes spatial gradients and localized frequency components through wavelet-based decomposition and edge-aware refinement. To further improve region-level focus, we design the axis-aligned focus calibration (AAFC) module, which captures long-range directional context via axis-sensitive pooling and adaptively refines attention with shape-aware calibration. Extensive experiments on four challenging benchmarks—VisDrone-2019, TT100K, NWPU VHR-10, and DIOR—demonstrate that PEFC-Net consistently outperforms state-of-the-art methods, delivering robust performance under occlusion, dense distribution, and scale variation.&lt;/p&gt;</content:encoded></item><item><title>Reconstruction Guided Few-shot Network For Remote Sensing Image Classification</title><link>https://arxiv.org/abs/2601.07335v1</link><guid>http://arxiv.org/abs/2601.07335v1</guid><pubDate>Mon, 12 Jan 2026 09:02:30 +0000</pubDate><dc:creator>Mohit Jaiswal</dc:creator><dc:creator>Naman Jain</dc:creator><dc:creator>Shivani Pathak</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Nikunja Bihari Kar</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.
Published: 2026-01-12T09:02:30+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohit Jaiswal; Naman Jain; Shivani Pathak; Mainak Singha; Nikunja Bihari Kar; Ankit Jha; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.&lt;/p&gt;</content:encoded></item><item><title>CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation</title><link>https://arxiv.org/abs/2601.08010v1</link><guid>http://arxiv.org/abs/2601.08010v1</guid><pubDate>Mon, 12 Jan 2026 21:24:45 +0000</pubDate><dc:creator>Chaoyu Li</dc:creator><dc:creator>Deeparghya Dutta Barua</dc:creator><dc:creator>Fei Tao</dc:creator><dc:creator>Pooyan Fazli</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.
Published: 2026-01-12T21:24:45+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaoyu Li; Deeparghya Dutta Barua; Fei Tao; Pooyan Fazli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.&lt;/p&gt;</content:encoded></item><item><title>Evaluating the encoding competence of visual language models using uncommon actions</title><link>https://arxiv.org/abs/2601.07737v1</link><guid>http://arxiv.org/abs/2601.07737v1</guid><pubDate>Mon, 12 Jan 2026 17:15:45 +0000</pubDate><dc:creator>Chen Ling</dc:creator><dc:creator>Nai Ding</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.
Published: 2026-01-12T17:15:45+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Ling; Nai Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model&amp;#x27;s competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage Fine-Tuning of Large Vision-Language Models with Hierarchical Prompting for Few-Shot Object Detection in Remote Sensing Images</title><link>https://doi.org/10.3390/rs18020266</link><guid>10.3390/rs18020266</guid><pubDate>Wed, 14 Jan 2026 15:12:04 +0000</pubDate><dc:creator>Yongqi Shi</dc:creator><dc:creator>Ruopeng Yang</dc:creator><dc:creator>Changsheng Yin</dc:creator><dc:creator>Yiwei Lu</dc:creator><dc:creator>Bo Huang</dc:creator><dc:creator>Yu Tao</dc:creator><dc:creator>Yihao Zhong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020266</prism:doi><description>Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.
Published: 2026-01-14T15:12:04+00:00
Venue: Remote Sensing
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongqi Shi; Ruopeng Yang; Changsheng Yin; Yiwei Lu; Bo Huang; Yu Tao; Yihao Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020266"&gt;10.3390/rs18020266&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.&lt;/p&gt;</content:encoded></item><item><title>Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</title><link>https://arxiv.org/abs/2601.09430v1</link><guid>http://arxiv.org/abs/2601.09430v1</guid><pubDate>Wed, 14 Jan 2026 12:24:47 +0000</pubDate><dc:creator>Rui Zhu</dc:creator><dc:creator>Xin Shen</dc:creator><dc:creator>Shuchen Wu</dc:creator><dc:creator>Chenxi Miao</dc:creator><dc:creator>Xin Yu</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Weikang Li</dc:creator><dc:creator>Deguo Xia</dc:creator><dc:creator>Jizhou Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.
Published: 2026-01-14T12:24:47+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Zhu; Xin Shen; Shuchen Wu; Chenxi Miao; Xin Yu; Yang Li; Weikang Li; Deguo Xia; Jizhou Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.&lt;/p&gt;</content:encoded></item><item><title>A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model</title><link>https://arxiv.org/abs/2601.07291v1</link><guid>http://arxiv.org/abs/2601.07291v1</guid><pubDate>Mon, 12 Jan 2026 07:55:13 +0000</pubDate><dc:creator>Qi Zheng</dc:creator><dc:creator>Shuliang Liu</dc:creator><dc:creator>Yu Huang</dc:creator><dc:creator>Sihang Jia</dc:creator><dc:creator>Jungang Li</dc:creator><dc:creator>Lyuhao Chen</dc:creator><dc:creator>Junhao Chen</dc:creator><dc:creator>Hanqian Li</dc:creator><dc:creator>Aiwei Liu</dc:creator><dc:creator>Yibo Yan</dc:creator><dc:creator>Xuming Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. In this paper, we propose the VIsual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual-Evidence Weights, which quantify the evidentiary support for candidate tokens based on the visual input. These weights guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity. Empirical results confirm that VISA-Mark outperforms conventional methods with a 7.8% improvement in visual consistency (Chair-I) and superior semantic fidelity. The framework maintains highly competitive detection accuracy (96.88% AUC) and robust attack resilience (99.3%) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multimodal watermarking.
Published: 2026-01-12T07:55:13+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qi Zheng; Shuliang Liu; Yu Huang; Sihang Jia; Jungang Li; Lyuhao Chen; Junhao Chen; Hanqian Li; Aiwei Liu; Yibo Yan; Xuming Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. In this paper, we propose the VIsual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual-Evidence Weights, which quantify the evidentiary support for candidate tokens based on the visual input. These weights guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity. Empirical results confirm that VISA-Mark outperforms conventional methods with a 7.8% improvement in visual consistency (Chair-I) and superior semantic fidelity. The framework maintains highly competitive detection accuracy (96.88% AUC) and robust attack resilience (99.3%) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multimodal watermarking.&lt;/p&gt;</content:encoded></item></channel></rss>