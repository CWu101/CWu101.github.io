<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 26 Dec 2025 02:39:14 +0000</lastBuildDate><item><title>Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning</title><link>https://doi.org/10.1109/tgrs.2025.3648057</link><guid>10.1109/tgrs.2025.3648057</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Lanxiao Wang</dc:creator><dc:creator>Heqian Qiu</dc:creator><dc:creator>Minjian Zhang</dc:creator><dc:creator>Fanman Meng</dc:creator><dc:creator>Qingbo Wu</dc:creator><dc:creator>Hongliang Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648057</prism:doi><description>Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lanxiao Wang; Heqian Qiu; Minjian Zhang; Fanman Meng; Qingbo Wu; Hongliang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648057"&gt;10.1109/tgrs.2025.3648057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Models for Person Re-identification: A Survey and Outlook</title><link>https://doi.org/10.1016/j.inffus.2025.104095</link><guid>10.1016/j.inffus.2025.104095</guid><pubDate>Wed, 24 Dec 2025 16:39:57 +0000</pubDate><dc:creator>Guorong Lin</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Zuoyong Li</dc:creator><dc:creator>Yao Lu</dc:creator><dc:creator>Xiaowen Ma</dc:creator><dc:creator>Zhenhua Huang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104095</prism:doi><description>Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.
Published: 2025-12-24T16:39:57+00:00
Venue: Information Fusion
Score: 0.569 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guorong Lin; Wei-Shi Zheng; Zuoyong Li; Yao Lu; Xiaowen Ma; Zhenhua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104095"&gt;10.1016/j.inffus.2025.104095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.569 (consider)&lt;/p&gt;
&lt;p&gt;Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.&lt;/p&gt;</content:encoded></item><item><title>Visual Dialog with Semantic Consistency: An External Knowledge-Driven Approach</title><link>https://doi.org/10.1016/j.neunet.2025.108523</link><guid>10.1016/j.neunet.2025.108523</guid><pubDate>Thu, 25 Dec 2025 00:11:00 +0000</pubDate><dc:creator>Shanshan Du</dc:creator><dc:creator>Hanli Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108523</prism:doi><description>As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.
Published: 2025-12-25T00:11:00+00:00
Venue: Neural Networks
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shanshan Du; Hanli Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108523"&gt;10.1016/j.neunet.2025.108523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion</title><link>https://doi.org/10.1109/tip.2025.3635475</link><guid>10.1109/tip.2025.3635475</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Haihong Xiao</dc:creator><dc:creator>Wenxiong Kang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Ying He</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635475</prism:doi><description>Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.563 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haihong Xiao; Wenxiong Kang; Yulan Guo; Hao Liu; Ying He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635475"&gt;10.1109/tip.2025.3635475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.563 (consider)&lt;/p&gt;
&lt;p&gt;Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.&lt;/p&gt;</content:encoded></item><item><title>UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</title><link>https://arxiv.org/abs/2512.21078v1</link><guid>http://arxiv.org/abs/2512.21078v1</guid><pubDate>Wed, 24 Dec 2025 09:55:16 +0000</pubDate><dc:creator>Tianchen Deng</dc:creator><dc:creator>Xun Chen</dc:creator><dc:creator>Ziming Li</dc:creator><dc:creator>Hongming Shen</dc:creator><dc:creator>Danwei Wang</dc:creator><dc:creator>Javier Civera</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.
Published: 2025-12-24T09:55:16+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianchen Deng; Xun Chen; Ziming Li; Hongming Shen; Danwei Wang; Javier Civera; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.&lt;/p&gt;</content:encoded></item><item><title>Vision Model Fine-tuning based on Two-level Prompts Fusion</title><link>https://doi.org/10.1016/j.knosys.2025.115185</link><guid>10.1016/j.knosys.2025.115185</guid><pubDate>Wed, 24 Dec 2025 00:12:37 +0000</pubDate><dc:creator>Keming Mao</dc:creator><dc:creator>Haoming Fang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115185</prism:doi><description>Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.
Published: 2025-12-24T00:12:37+00:00
Venue: Knowledge-Based Systems
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keming Mao; Haoming Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115185"&gt;10.1016/j.knosys.2025.115185&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.&lt;/p&gt;</content:encoded></item><item><title>Latent Implicit Visual Reasoning</title><link>https://arxiv.org/abs/2512.21218v1</link><guid>http://arxiv.org/abs/2512.21218v1</guid><pubDate>Wed, 24 Dec 2025 14:59:49 +0000</pubDate><dc:creator>Kelvin Li</dc:creator><dc:creator>Chuyi Shang</dc:creator><dc:creator>Leonid Karlinsky</dc:creator><dc:creator>Rogerio Feris</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Roei Herzig</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.
Published: 2025-12-24T14:59:49+00:00
Venue: arXiv
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kelvin Li; Chuyi Shang; Leonid Karlinsky; Rogerio Feris; Trevor Darrell; Roei Herzig&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &amp;quot;useful&amp;quot; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.&lt;/p&gt;</content:encoded></item><item><title>SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2512.20013v1</link><guid>http://arxiv.org/abs/2512.20013v1</guid><pubDate>Tue, 23 Dec 2025 03:10:17 +0000</pubDate><dc:creator>Zepeng Xin</dc:creator><dc:creator>Kaiyu Li</dc:creator><dc:creator>Luodi Chen</dc:creator><dc:creator>Wanchen Li</dc:creator><dc:creator>Yuchen Xiao</dc:creator><dc:creator>Hui Qiao</dc:creator><dc:creator>Weizhan Zhang</dc:creator><dc:creator>Deyu Meng</dc:creator><dc:creator>Xiangyong Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.
Published: 2025-12-23T03:10:17+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zepeng Xin; Kaiyu Li; Luodi Chen; Wanchen Li; Yuchen Xiao; Hui Qiao; Weizhan Zhang; Deyu Meng; Xiangyong Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model&amp;#x27;s effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</title><link>https://arxiv.org/abs/2512.21221v1</link><guid>http://arxiv.org/abs/2512.21221v1</guid><pubDate>Wed, 24 Dec 2025 15:02:33 +0000</pubDate><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Phu-Hoa Pham</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval
Published: 2025-12-24T15:02:33+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dao Sy Duy Minh; Huynh Trung Kiet; Nguyen Lam Phu Quy; Phu-Hoa Pham; Tran Chi Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval&lt;/p&gt;</content:encoded></item><item><title>Class-Domain Incremental Segmentation for Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648015</link><guid>10.1109/tgrs.2025.3648015</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Xingxing Weng</dc:creator><dc:creator>Chao Pang</dc:creator><dc:creator>Jiayu Li</dc:creator><dc:creator>Xiaoqian Sun</dc:creator><dc:creator>Gui-Song Xia</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648015</prism:doi><description>Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingxing Weng; Chao Pang; Jiayu Li; Xiaoqian Sun; Gui-Song Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648015"&gt;10.1109/tgrs.2025.3648015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.&lt;/p&gt;</content:encoded></item><item><title>Vehicle-centric Perception via Multimodal Structured Pre-training</title><link>https://arxiv.org/abs/2512.19934v1</link><guid>http://arxiv.org/abs/2512.19934v1</guid><pubDate>Mon, 22 Dec 2025 23:42:45 +0000</pubDate><dc:creator>Wentao Wu</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.
Published: 2025-12-22T23:42:45+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Wu; Xiao Wang; Chenglong Li; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model&amp;#x27;s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.&lt;/p&gt;</content:encoded></item><item><title>VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</title><link>https://arxiv.org/abs/2512.21194v1</link><guid>http://arxiv.org/abs/2512.21194v1</guid><pubDate>Wed, 24 Dec 2025 14:18:38 +0000</pubDate><dc:creator>Brigitta Malagurski Törtei</dc:creator><dc:creator>Yasser Dahou</dc:creator><dc:creator>Ngoc Dung Huynh</dc:creator><dc:creator>Wamiq Reyaz Para</dc:creator><dc:creator>Phúc H. Lê Khac</dc:creator><dc:creator>Ankit Singh</dc:creator><dc:creator>Sofian Chaybouti</dc:creator><dc:creator>Sanath Narayan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.
Published: 2025-12-24T14:18:38+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brigitta Malagurski Törtei; Yasser Dahou; Ngoc Dung Huynh; Wamiq Reyaz Para; Phúc H. Lê Khac; Ankit Singh; Sofian Chaybouti; Sanath Narayan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.&lt;/p&gt;</content:encoded></item><item><title>PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</title><link>https://arxiv.org/abs/2512.20907v1</link><guid>http://arxiv.org/abs/2512.20907v1</guid><pubDate>Wed, 24 Dec 2025 03:18:51 +0000</pubDate><dc:creator>Seongmin Jung</dc:creator><dc:creator>Seongho Choi</dc:creator><dc:creator>Gunwoo Jeon</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jongwoo Lim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.
Published: 2025-12-24T03:18:51+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seongmin Jung; Seongho Choi; Gunwoo Jeon; Minsu Cho; Jongwoo Lim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.&lt;/p&gt;</content:encoded></item><item><title>SAM-FuseNet: Segment Anything Guided Multi-Modal Fusion for RGB–Thermal Aerial Robotic Perception</title><link>https://doi.org/10.1109/tgrs.2025.3648127</link><guid>10.1109/tgrs.2025.3648127</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Chenyang Zhu</dc:creator><dc:creator>Jierui Wang</dc:creator><dc:creator>Lanlan Zhang</dc:creator><dc:creator>Jia Liang</dc:creator><dc:creator>Qianxiao Su</dc:creator><dc:creator>Baihua Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648127</prism:doi><description>Multi-modal semantic segmentation with RGB and Thermal (RGB–T) inputs is critical for robust aerial robotic perception, yet existing methods face challenges in effectively adapting pretrained representations and balancing complementary cues under variable illumination.To overcome these challenges, we introduce Segment Anything Guided Multi-Modal Fusion Network (SAM-FuseNet), a novel RGB–T framework that leverages modality-specific encoders built on the Segment Anything Model 2 (SAM2) Vision Transformer (ViT) backbone, enhanced with Low-Rank Adaptation (LoRA) modules for efficient modality-specific fine-tuning without full model retraining. Cross-modal fusion is achieved via a pixel-wise spatial attention mechanism combined with channel recalibration, allowing adaptive weighting of contributions from each modality. A dual-pathway segmentation head further improves prediction quality: the first pathway progressively refines coarse-to-fine masks under SAM2 guidance, while the second pathway performs hierarchical feature fusion to capture long-range contextual information. Extensive experiments on the Caltech RGB–T and PST900 benchmarks demonstrate the effectiveness of SAM-FuseNet, achieving improvements of up to 3.93% in mean Intersection-over-Union (mIoU) while reducing the trained model parameters by more than 50%. Qualitative results further demonstrate sharper boundaries, robustness under underexposed conditions, and resilience to modality degradation.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyang Zhu; Jierui Wang; Lanlan Zhang; Jia Liang; Qianxiao Su; Baihua Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648127"&gt;10.1109/tgrs.2025.3648127&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal semantic segmentation with RGB and Thermal (RGB–T) inputs is critical for robust aerial robotic perception, yet existing methods face challenges in effectively adapting pretrained representations and balancing complementary cues under variable illumination.To overcome these challenges, we introduce Segment Anything Guided Multi-Modal Fusion Network (SAM-FuseNet), a novel RGB–T framework that leverages modality-specific encoders built on the Segment Anything Model 2 (SAM2) Vision Transformer (ViT) backbone, enhanced with Low-Rank Adaptation (LoRA) modules for efficient modality-specific fine-tuning without full model retraining. Cross-modal fusion is achieved via a pixel-wise spatial attention mechanism combined with channel recalibration, allowing adaptive weighting of contributions from each modality. A dual-pathway segmentation head further improves prediction quality: the first pathway progressively refines coarse-to-fine masks under SAM2 guidance, while the second pathway performs hierarchical feature fusion to capture long-range contextual information. Extensive experiments on the Caltech RGB–T and PST900 benchmarks demonstrate the effectiveness of SAM-FuseNet, achieving improvements of up to 3.93% in mean Intersection-over-Union (mIoU) while reducing the trained model parameters by more than 50%. Qualitative results further demonstrate sharper boundaries, robustness under underexposed conditions, and resilience to modality degradation.&lt;/p&gt;</content:encoded></item><item><title>ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</title><link>https://arxiv.org/abs/2512.19354v1</link><guid>http://arxiv.org/abs/2512.19354v1</guid><pubDate>Mon, 22 Dec 2025 12:54:26 +0000</pubDate><dc:creator>Zhenyang Huang</dc:creator><dc:creator>Xiao Yu</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Decheng Wang</dc:creator><dc:creator>Hang Ruan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.
Published: 2025-12-22T12:54:26+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyang Huang; Xiao Yu; Yi Zhang; Decheng Wang; Hang Ruan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users&amp;#x27; CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users&amp;#x27; implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users&amp;#x27; implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.&lt;/p&gt;</content:encoded></item><item><title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title><link>https://arxiv.org/abs/2512.20557v1</link><guid>http://arxiv.org/abs/2512.20557v1</guid><pubDate>Tue, 23 Dec 2025 17:56:36 +0000</pubDate><dc:creator>Shengchao Zhou</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Yuying Ge</dc:creator><dc:creator>Wei Huang</dc:creator><dc:creator>Jiehong Lin</dc:creator><dc:creator>Ying Shan</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.
Published: 2025-12-23T17:56:36+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengchao Zhou; Yuxin Chen; Yuying Ge; Wei Huang; Jiehong Lin; Ying Shan; Xiaojuan Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.&lt;/p&gt;</content:encoded></item><item><title>BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</title><link>https://arxiv.org/abs/2512.20255v1</link><guid>http://arxiv.org/abs/2512.20255v1</guid><pubDate>Tue, 23 Dec 2025 11:13:01 +0000</pubDate><dc:creator>Jinghao Shi</dc:creator><dc:creator>Jianing Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.
Published: 2025-12-23T11:13:01+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinghao Shi; Jianing Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.&lt;/p&gt;</content:encoded></item><item><title>Dual Consistency Matching for Semi-Supervised Semantic Correspondence</title><link>https://doi.org/10.1007/s11263-025-02652-8</link><guid>10.1007/s11263-025-02652-8</guid><pubDate>Wed, 24 Dec 2025 17:57:36 +0000</pubDate><dc:creator>Hailong Jin</dc:creator><dc:creator>Huiying Li</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02652-8</prism:doi><description>Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.
Published: 2025-12-24T17:57:36+00:00
Venue: International Journal of Computer Vision
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hailong Jin; Huiying Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02652-8"&gt;10.1007/s11263-025-02652-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.&lt;/p&gt;</content:encoded></item><item><title>Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</title><link>https://arxiv.org/abs/2512.20042v1</link><guid>http://arxiv.org/abs/2512.20042v1</guid><pubDate>Tue, 23 Dec 2025 04:21:15 +0000</pubDate><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Pham Phu Hoa</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Nguyen Hoang Minh Ngoc</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding
Published: 2025-12-23T04:21:15+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nguyen Lam Phu Quy; Pham Phu Hoa; Tran Chi Nguyen; Dao Sy Duy Minh; Nguyen Hoang Minh Ngoc; Huynh Trung Kiet&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding&lt;/p&gt;</content:encoded></item><item><title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104097</link><guid>10.1016/j.inffus.2025.104097</guid><pubDate>Thu, 25 Dec 2025 16:05:23 +0000</pubDate><dc:creator>Xuanming Cao</dc:creator><dc:creator>Chengyu Tao</dc:creator><dc:creator>Yifeng Cheng</dc:creator><dc:creator>Juan Du</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104097</prism:doi><description>Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.
Published: 2025-12-25T16:05:23+00:00
Venue: Information Fusion
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanming Cao; Chengyu Tao; Yifeng Cheng; Juan Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104097"&gt;10.1016/j.inffus.2025.104097&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.&lt;/p&gt;</content:encoded></item><item><title>VL4Gaze: Unleashing Vision-Language Models for Gaze Following</title><link>https://arxiv.org/abs/2512.20735v1</link><guid>http://arxiv.org/abs/2512.20735v1</guid><pubDate>Tue, 23 Dec 2025 19:47:11 +0000</pubDate><dc:creator>Shijing Wang</dc:creator><dc:creator>Chaoqun Cui</dc:creator><dc:creator>Yaping Huang</dc:creator><dc:creator>Hyung Jin Chang</dc:creator><dc:creator>Yihua Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.
Published: 2025-12-23T19:47:11+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shijing Wang; Chaoqun Cui; Yaping Huang; Hyung Jin Chang; Yihua Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.&lt;/p&gt;</content:encoded></item><item><title>The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</title><link>https://arxiv.org/abs/2512.19693v1</link><guid>http://arxiv.org/abs/2512.19693v1</guid><pubDate>Mon, 22 Dec 2025 18:59:57 +0000</pubDate><dc:creator>Weichen Fan</dc:creator><dc:creator>Haiwen Diao</dc:creator><dc:creator>Quan Wang</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.
Published: 2025-12-22T18:59:57+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weichen Fan; Haiwen Diao; Quan Wang; Dahua Lin; Ziwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder&amp;#x27;s feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647855</link><guid>10.1109/tpami.2025.3647855</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Jingtao Sun</dc:creator><dc:creator>Yaonan Wang</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Chao Ding</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647855</prism:doi><description>Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingtao Sun; Yaonan Wang; Mingtao Feng; Chao Ding; Mike Zheng Shou; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647855"&gt;10.1109/tpami.2025.3647855&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.&lt;/p&gt;</content:encoded></item><item><title>TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104087</link><guid>10.1016/j.inffus.2025.104087</guid><pubDate>Thu, 25 Dec 2025 00:16:18 +0000</pubDate><dc:creator>Changbin Wang</dc:creator><dc:creator>Fengrui Ji</dc:creator><dc:creator>Baolin Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104087</prism:doi><description>Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.
Published: 2025-12-25T00:16:18+00:00
Venue: Information Fusion
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changbin Wang; Fengrui Ji; Baolin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104087"&gt;10.1016/j.inffus.2025.104087&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.&lt;/p&gt;</content:encoded></item><item><title>Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking</title><link>https://doi.org/10.1109/tpami.2025.3648020</link><guid>10.1109/tpami.2025.3648020</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hanzheng Wang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Xiang-Gen Xia</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648020</prism:doi><description>Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model's sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanzheng Wang; Wei Li; Xiang-Gen Xia; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648020"&gt;10.1109/tpami.2025.3648020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&amp;#x27;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.&lt;/p&gt;</content:encoded></item><item><title>Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647829</link><guid>10.1109/tpami.2025.3647829</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Yifei Shi</dc:creator><dc:creator>Boyan Wan</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647829</prism:doi><description>Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Shi; Boyan Wan; Xin Xu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647829"&gt;10.1109/tpami.2025.3647829&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&amp;#x27;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&amp;#x27;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&amp;#x27;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...&lt;/p&gt;</content:encoded></item><item><title>S
                    &lt;sup&gt;2&lt;/sup&gt;
                    EDL: Selective Semantic Efficient Distillation Learning for Large-Scale Remote Sensing Representation</title><link>https://doi.org/10.1109/jstars.2025.3647928</link><guid>10.1109/jstars.2025.3647928</guid><pubDate>Wed, 24 Dec 2025 18:46:08 +0000</pubDate><dc:creator>Wu Wen</dc:creator><dc:creator>Jinghui Luo</dc:creator><dc:creator>Lizhuang Tan</dc:creator><dc:creator>Konstantin Igorevich Kostromitin</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Peiying Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647928</prism:doi><description>Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.
Published: 2025-12-24T18:46:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wu Wen; Jinghui Luo; Lizhuang Tan; Konstantin Igorevich Kostromitin; Jian Wang; Peiying Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647928"&gt;10.1109/jstars.2025.3647928&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.&lt;/p&gt;</content:encoded></item><item><title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title><link>https://arxiv.org/abs/2512.18954v1</link><guid>http://arxiv.org/abs/2512.18954v1</guid><pubDate>Mon, 22 Dec 2025 02:05:45 +0000</pubDate><dc:creator>Zaidao Han</dc:creator><dc:creator>Risa Higashita</dc:creator><dc:creator>Jiang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.
Published: 2025-12-22T02:05:45+00:00
Venue: arXiv
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zaidao Han; Risa Higashita; Jiang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Fast SAM2 with Text-Driven Token Pruning</title><link>https://arxiv.org/abs/2512.21333v1</link><guid>http://arxiv.org/abs/2512.21333v1</guid><pubDate>Wed, 24 Dec 2025 18:59:05 +0000</pubDate><dc:creator>Avilasha Mandal</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Fachrina Dewi Puspitasari</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiaquan Zhang</dc:creator><dc:creator>Caiyan Qin</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.
Published: 2025-12-24T18:59:05+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Avilasha Mandal; Chaoning Zhang; Fachrina Dewi Puspitasari; Xudong Wang; Jiaquan Zhang; Caiyan Qin; Guoqing Wang; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation</title><link>https://arxiv.org/abs/2512.20936v1</link><guid>http://arxiv.org/abs/2512.20936v1</guid><pubDate>Wed, 24 Dec 2025 04:39:45 +0000</pubDate><dc:creator>Hongxing Fan</dc:creator><dc:creator>Shuyu Zhao</dc:creator><dc:creator>Jiayang Ao</dc:creator><dc:creator>Lu Sheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.
Published: 2025-12-24T04:39:45+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongxing Fan; Shuyu Zhao; Jiayang Ao; Lu Sheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.&lt;/p&gt;</content:encoded></item></channel></rss>