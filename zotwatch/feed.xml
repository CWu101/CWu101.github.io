<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 20 Jan 2026 02:46:39 +0000</lastBuildDate><item><title>An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition</title><link>https://doi.org/10.1016/j.inffus.2026.104157</link><guid>10.1016/j.inffus.2026.104157</guid><pubDate>Sun, 18 Jan 2026 04:47:28 +0000</pubDate><dc:creator>Xunqi Zhou</dc:creator><dc:creator>Zhenqi Zhang</dc:creator><dc:creator>Zifeng Wu</dc:creator><dc:creator>Qianming Wang</dc:creator><dc:creator>Jing Teng</dc:creator><dc:creator>Jinlong Liu</dc:creator><dc:creator>Yongjie Zhai</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104157</prism:doi><description>In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.
Published: 2026-01-18T04:47:28+00:00
Venue: Information Fusion
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xunqi Zhou; Zhenqi Zhang; Zifeng Wu; Qianming Wang; Jing Teng; Jinlong Liu; Yongjie Zhai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104157"&gt;10.1016/j.inffus.2026.104157&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</title><link>https://arxiv.org/abs/2601.11322v1</link><guid>http://arxiv.org/abs/2601.11322v1</guid><pubDate>Fri, 16 Jan 2026 14:16:38 +0000</pubDate><dc:creator>Pavana Pradeep</dc:creator><dc:creator>Krishna Kant</dc:creator><dc:creator>Suya Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.
Published: 2026-01-16T14:16:38+00:00
Venue: arXiv
Score: 0.558 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pavana Pradeep; Krishna Kant; Suya Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.558 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.&lt;/p&gt;</content:encoded></item><item><title>Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</title><link>https://arxiv.org/abs/2601.11442v1</link><guid>http://arxiv.org/abs/2601.11442v1</guid><pubDate>Fri, 16 Jan 2026 17:02:46 +0000</pubDate><dc:creator>Xiangjun Gao</dc:creator><dc:creator>Zhensong Zhang</dc:creator><dc:creator>Dave Zhenyu Chen</dc:creator><dc:creator>Songcen Xu</dc:creator><dc:creator>Long Quan</dc:creator><dc:creator>Eduardo Pérez-Pellitero</dc:creator><dc:creator>Youngkyoon Jang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.
Published: 2026-01-16T17:02:46+00:00
Venue: arXiv
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangjun Gao; Zhensong Zhang; Dave Zhenyu Chen; Songcen Xu; Long Quan; Eduardo Pérez-Pellitero; Youngkyoon Jang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.&lt;/p&gt;</content:encoded></item><item><title>Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning</title><link>https://arxiv.org/abs/2601.11109v1</link><guid>http://arxiv.org/abs/2601.11109v1</guid><pubDate>Fri, 16 Jan 2026 09:11:55 +0000</pubDate><dc:creator>Shaofeng Yin</dc:creator><dc:creator>Jiaxin Ge</dc:creator><dc:creator>Zora Zhiruo Wang</dc:creator><dc:creator>Xiuyu Li</dc:creator><dc:creator>Michael J. Black</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Angjoo Kanazawa</dc:creator><dc:creator>Haiwen Feng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren't able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn't require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn't require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.
Published: 2026-01-16T09:11:55+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shaofeng Yin; Jiaxin Ge; Zora Zhiruo Wang; Xiuyu Li; Michael J. Black; Trevor Darrell; Angjoo Kanazawa; Haiwen Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren&amp;#x27;t able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn&amp;#x27;t require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn&amp;#x27;t require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.&lt;/p&gt;</content:encoded></item><item><title>BECSL: boundary-enhanced consistency semi-supervised learning model for water extraction from remote sensing images</title><link>https://doi.org/10.1080/17538947.2026.2616983</link><guid>10.1080/17538947.2026.2616983</guid><pubDate>Sun, 18 Jan 2026 14:35:02 +0000</pubDate><dc:creator>Beibei Wu</dc:creator><dc:creator>Qun Sun</dc:creator><dc:creator>Anzhu Yu</dc:creator><dc:creator>Qing Xu</dc:creator><dc:creator>Longhao Wang</dc:creator><dc:creator>Xin Chen</dc:creator><prism:publicationName>International Journal of Digital Earth</prism:publicationName><prism:doi>10.1080/17538947.2026.2616983</prism:doi><description>Accurate water body identification and extraction presents a critical challenge in geographic information systems, particularly for boundary-sensitive applications. While deep learning offers promising solutions for automated geospatial analysis, most semi-supervised methods inadequately model edge information. This study introduces a Boundary-Enhanced Consistency Semi-supervised Learning (BECSL) framework to address this gap. Our approach integrates Segment Anything Model (SAM) with OpenStreetMap (OSM) data to create multi-modal training datasets. The framework employs a dual-decoder architecture combining reverse attention and boundary enhancement mechanisms. This design generates refined pseudo-labels for unlabeled data supervision. We further develop a self-contrast strategy targeting regions with boundary prediction inconsistencies. Comprehensive evaluation on 2024EarthVQA and custom datasets demonstrates our method's effectiveness. The framework achieves superior performance using merely 10% labeled data while maintaining precise boundary delineation. This work provides both theoretical and practical advances in resource-efficient water extraction from remote sensing images.
Published: 2026-01-18T14:35:02+00:00
Venue: International Journal of Digital Earth
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Beibei Wu; Qun Sun; Anzhu Yu; Qing Xu; Longhao Wang; Xin Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Digital Earth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/17538947.2026.2616983"&gt;10.1080/17538947.2026.2616983&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Accurate water body identification and extraction presents a critical challenge in geographic information systems, particularly for boundary-sensitive applications. While deep learning offers promising solutions for automated geospatial analysis, most semi-supervised methods inadequately model edge information. This study introduces a Boundary-Enhanced Consistency Semi-supervised Learning (BECSL) framework to address this gap. Our approach integrates Segment Anything Model (SAM) with OpenStreetMap (OSM) data to create multi-modal training datasets. The framework employs a dual-decoder architecture combining reverse attention and boundary enhancement mechanisms. This design generates refined pseudo-labels for unlabeled data supervision. We further develop a self-contrast strategy targeting regions with boundary prediction inconsistencies. Comprehensive evaluation on 2024EarthVQA and custom datasets demonstrates our method&amp;#x27;s effectiveness. The framework achieves superior performance using merely 10% labeled data while maintaining precise boundary delineation. This work provides both theoretical and practical advances in resource-efficient water extraction from remote sensing images.&lt;/p&gt;</content:encoded></item><item><title>Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104160</link><guid>10.1016/j.inffus.2026.104160</guid><pubDate>Sun, 18 Jan 2026 04:47:19 +0000</pubDate><dc:creator>Minghan Li</dc:creator><dc:creator>Ercong Nie</dc:creator><dc:creator>Huiping Huang</dc:creator><dc:creator>Xinxuan Lv</dc:creator><dc:creator>Guodong Zhou</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104160</prism:doi><description>Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.
Published: 2026-01-18T04:47:19+00:00
Venue: Information Fusion
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghan Li; Ercong Nie; Huiping Huang; Xinxuan Lv; Guodong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104160"&gt;10.1016/j.inffus.2026.104160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.&lt;/p&gt;</content:encoded></item><item><title>Autoregressive STG-based Diffusion Model for Spatiotemporal Trajectory Generation</title><link>https://doi.org/10.1145/3787975</link><guid>10.1145/3787975</guid><pubDate>Mon, 19 Jan 2026 10:06:38 +0000</pubDate><dc:creator>Tianru Xie</dc:creator><dc:creator>Pingfu Chao</dc:creator><dc:creator>Weizhu Qian</dc:creator><dc:creator>Junhua Fang</dc:creator><dc:creator>Jiajie Xu</dc:creator><prism:publicationName>ACM Transactions on Intelligent Systems and Technology</prism:publicationName><prism:doi>10.1145/3787975</prism:doi><description>The urban foundation model is critical for trajectory-based mobile applications, which require accurate synthesis of paths that adhere to spatial constraints (road networks) and contextual constraints (e.g., weather, traffic). However, existing methods predominantly rely on task-specific models, which fail to holistically capture and integrate diverse spatial patterns (e.g., connectivity) and temporal dynamics (e.g., periodicity, trends) within a cohesive framework, limiting their generalization across diverse prediction tasks. To bridge this gap, we propose AutoDiff, a diffusion-based model generating trajectories on spatial temporal graph (STG), which establishes a new paradigm for trajectory generation as a foundation model for sequential spatiotemporal data. Specifically, we disentangle complex spatiotemporal features as generalizable segment-wise time slices on road networks through autoregressive diffusion generation, which not only enforces realistic trajectory connectivity within road networks, but also enables knowledge transfer across tasks like trajectory recovery and travel time prediction. Besides, we design a confidence-based early-exiting mechanism to eliminate redundant denoising steps without sacrificing quality, enabling scalable applications in mobility analytics. Extensive experiments on three real-world urban trajectory datasets demonstrate the superior performance of AutoDiff in path prediction, trajectory recovery and time estimation tasks, outperforming task-specific baselines while maintaining computational efficiency.
Published: 2026-01-19T10:06:38+00:00
Venue: ACM Transactions on Intelligent Systems and Technology
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianru Xie; Pingfu Chao; Weizhu Qian; Junhua Fang; Jiajie Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Intelligent Systems and Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3787975"&gt;10.1145/3787975&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;The urban foundation model is critical for trajectory-based mobile applications, which require accurate synthesis of paths that adhere to spatial constraints (road networks) and contextual constraints (e.g., weather, traffic). However, existing methods predominantly rely on task-specific models, which fail to holistically capture and integrate diverse spatial patterns (e.g., connectivity) and temporal dynamics (e.g., periodicity, trends) within a cohesive framework, limiting their generalization across diverse prediction tasks. To bridge this gap, we propose AutoDiff, a diffusion-based model generating trajectories on spatial temporal graph (STG), which establishes a new paradigm for trajectory generation as a foundation model for sequential spatiotemporal data. Specifically, we disentangle complex spatiotemporal features as generalizable segment-wise time slices on road networks through autoregressive diffusion generation, which not only enforces realistic trajectory connectivity within road networks, but also enables knowledge transfer across tasks like trajectory recovery and travel time prediction. Besides, we design a confidence-based early-exiting mechanism to eliminate redundant denoising steps without sacrificing quality, enabling scalable applications in mobility analytics. Extensive experiments on three real-world urban trajectory datasets demonstrate the superior performance of AutoDiff in path prediction, trajectory recovery and time estimation tasks, outperforming task-specific baselines while maintaining computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>Multi-scale steering of large vision language models via visual information intervention</title><link>https://doi.org/10.1016/j.neucom.2026.132780</link><guid>10.1016/j.neucom.2026.132780</guid><pubDate>Mon, 19 Jan 2026 07:24:14 +0000</pubDate><dc:creator>Dongliang Zhao</dc:creator><dc:creator>Bo Sun</dc:creator><dc:creator>Jun He</dc:creator><dc:creator>Yinghui Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132780</prism:doi><description>Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.
Published: 2026-01-19T07:24:14+00:00
Venue: Neurocomputing
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongliang Zhao; Bo Sun; Jun He; Yinghui Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132780"&gt;10.1016/j.neucom.2026.132780&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.&lt;/p&gt;</content:encoded></item><item><title>SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction</title><link>https://arxiv.org/abs/2601.11396v1</link><guid>http://arxiv.org/abs/2601.11396v1</guid><pubDate>Fri, 16 Jan 2026 16:07:38 +0000</pubDate><dc:creator>Hanlin Wu</dc:creator><dc:creator>Pengfei Lin</dc:creator><dc:creator>Ehsan Javanmardi</dc:creator><dc:creator>Nanren Bao</dc:creator><dc:creator>Bo Qian</dc:creator><dc:creator>Hao Si</dc:creator><dc:creator>Manabu Tsukada</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.
Published: 2026-01-16T16:07:38+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanlin Wu; Pengfei Lin; Ehsan Javanmardi; Nanren Bao; Bo Qian; Hao Si; Manabu Tsukada&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.&lt;/p&gt;</content:encoded></item><item><title>AMS-Former: Adaptive multi-scale transformer for multi-modal image matching</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.021</link><guid>10.1016/j.isprsjprs.2026.01.021</guid><pubDate>Mon, 19 Jan 2026 08:34:48 +0000</pubDate><dc:creator>Jiahao Rao</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Jianjun Guan</dc:creator><dc:creator>Xin Tian</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.021</prism:doi><description>Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .
Published: 2026-01-19T08:34:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Rao; Rui Liu; Jianjun Guan; Xin Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021"&gt;10.1016/j.isprsjprs.2026.01.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .&lt;/p&gt;</content:encoded></item><item><title>Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification</title><link>https://arxiv.org/abs/2601.11243v1</link><guid>http://arxiv.org/abs/2601.11243v1</guid><pubDate>Fri, 16 Jan 2026 12:45:01 +0000</pubDate><dc:creator>Zhiqi Pang</dc:creator><dc:creator>Lingling Zhao</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Chunyu Wang</dc:creator><dc:creator>Gaurav Sharma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.
Published: 2026-01-16T12:45:01+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiqi Pang; Lingling Zhao; Yang Liu; Chunyu Wang; Gaurav Sharma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.&lt;/p&gt;</content:encoded></item><item><title>Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning</title><link>https://arxiv.org/abs/2601.11393v1</link><guid>http://arxiv.org/abs/2601.11393v1</guid><pubDate>Fri, 16 Jan 2026 16:05:49 +0000</pubDate><dc:creator>Haomiao Tang</dc:creator><dc:creator>Jinpeng Wang</dc:creator><dc:creator>Minyi Zhao</dc:creator><dc:creator>Guanghao Meng</dc:creator><dc:creator>Ruisheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Shu-Tao Xia</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.
Published: 2026-01-16T16:05:49+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haomiao Tang; Jinpeng Wang; Minyi Zhao; Guanghao Meng; Ruisheng Luo; Long Chen; Shu-Tao Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model&amp;#x27;s robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG&amp;#x27;s effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.&lt;/p&gt;</content:encoded></item><item><title>PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131194</link><guid>10.1016/j.eswa.2026.131194</guid><pubDate>Sun, 18 Jan 2026 06:31:10 +0000</pubDate><dc:creator>Shuming Lin</dc:creator><dc:creator>Sang Fyeng</dc:creator><dc:creator>Jinyi Liang</dc:creator><dc:creator>Junnan Tan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131194</prism:doi><description>Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.
Published: 2026-01-18T06:31:10+00:00
Venue: Expert Systems with Applications
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuming Lin; Sang Fyeng; Jinyi Liang; Junnan Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131194"&gt;10.1016/j.eswa.2026.131194&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.&lt;/p&gt;</content:encoded></item><item><title>Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval</title><link>https://arxiv.org/abs/2601.11248v1</link><guid>http://arxiv.org/abs/2601.11248v1</guid><pubDate>Fri, 16 Jan 2026 12:55:41 +0000</pubDate><dc:creator>Fangke Chen</dc:creator><dc:creator>Tianhao Dong</dc:creator><dc:creator>Sirry Chen</dc:creator><dc:creator>Guobin Zhang</dc:creator><dc:creator>Yishu Zhang</dc:creator><dc:creator>Yining Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.
Published: 2026-01-16T12:55:41+00:00
Venue: arXiv
Score: 0.508 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fangke Chen; Tianhao Dong; Sirry Chen; Guobin Zhang; Yishu Zhang; Yining Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.508 (consider)&lt;/p&gt;
&lt;p&gt;Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.&lt;/p&gt;</content:encoded></item><item><title>Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning</title><link>https://arxiv.org/abs/2601.11252v1</link><guid>http://arxiv.org/abs/2601.11252v1</guid><pubDate>Fri, 16 Jan 2026 13:00:42 +0000</pubDate><dc:creator>Qianyue Wang</dc:creator><dc:creator>Jinwu Hu</dc:creator><dc:creator>Yufeng Wang</dc:creator><dc:creator>Huanxiang Lin</dc:creator><dc:creator>Bolin Chen</dc:creator><dc:creator>Zhiquan Wen</dc:creator><dc:creator>Yaofo Chen</dc:creator><dc:creator>Mingkui Tan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.
Published: 2026-01-16T13:00:42+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianyue Wang; Jinwu Hu; Yufeng Wang; Huanxiang Lin; Bolin Chen; Zhiquan Wen; Yaofo Chen; Mingkui Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.&lt;/p&gt;</content:encoded></item><item><title>面向遥感图像解译的参数高效微调研究综述</title><link>https://doi.org/10.11834/jig.250105</link><guid>10.11834/jig.250105</guid><pubDate>Mon, 19 Jan 2026 01:26:13 +0000</pubDate><dc:creator>Chen Shiqi</dc:creator><dc:creator>Yang Xue</dc:creator><dc:creator>Zhu Rongqiang</dc:creator><dc:creator>Liao Ning</dc:creator><dc:creator>Zhao Weiwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250105</prism:doi><description>海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。
Published: 2026-01-19T01:26:13+00:00
Venue: Journal of Image and Graphics
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Shiqi; Yang Xue; Zhu Rongqiang; Liao Ning; Zhao Weiwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250105"&gt;10.11834/jig.250105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。&lt;/p&gt;</content:encoded></item><item><title>Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</title><link>https://arxiv.org/abs/2601.11359v1</link><guid>http://arxiv.org/abs/2601.11359v1</guid><pubDate>Fri, 16 Jan 2026 15:14:04 +0000</pubDate><dc:creator>Wenhui Tan</dc:creator><dc:creator>Ruihua Song</dc:creator><dc:creator>Jiaze Li</dc:creator><dc:creator>Jianzhong Ju</dc:creator><dc:creator>Zhenbo Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.
Published: 2026-01-16T15:14:04+00:00
Venue: arXiv
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhui Tan; Ruihua Song; Jiaze Li; Jianzhong Ju; Zhenbo Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.&lt;/p&gt;</content:encoded></item><item><title>Data Fusion for Low-Cost Sensors: A Systematic Literature Review</title><link>https://doi.org/10.1016/j.inffus.2026.104124</link><guid>10.1016/j.inffus.2026.104124</guid><pubDate>Sun, 18 Jan 2026 15:46:20 +0000</pubDate><dc:creator>Gabriel Oduori</dc:creator><dc:creator>Chaira Cocco</dc:creator><dc:creator>Payam Sajadi</dc:creator><dc:creator>Francesco Pilla</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104124</prism:doi><description>Data fusion (DF) addresses the challenge of integrating heterogeneous data sources to improve decision-making and inference. Although DF has been widely explored, no prior systematic review has specifically focused on its application to low-cost sensor (LCS) data in environmental monitoring. To address this gap, we conduct a systematic literature review (SLR) following the PRISMA framework, synthesising findings from 82 peer-reviewed articles. The review addresses three key questions: (1) What fusion methodologies are employed in conjunction with LCS data? (2) In what environmental contexts are these methods applied? (3) What are the methodological challenges and research gaps? Our analysis reveals that geostatistical and machine learning approaches dominate current practice, with air quality monitoring emerging as the primary application domain. Additionally, artificial intelligence (AI)-based methods are increasingly used to integrate spatial, temporal, and multimodal data. However, limitations persist in uncertainty quantification, validation standards, and the generalisability of fusion frameworks. This review provides a comprehensive synthesis of current techniques and outlines key directions for future research, including the development of robust, uncertainty-aware fusion methods and broader application to less-studied environmental variables.
Published: 2026-01-18T15:46:20+00:00
Venue: Information Fusion
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gabriel Oduori; Chaira Cocco; Payam Sajadi; Francesco Pilla&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104124"&gt;10.1016/j.inffus.2026.104124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Data fusion (DF) addresses the challenge of integrating heterogeneous data sources to improve decision-making and inference. Although DF has been widely explored, no prior systematic review has specifically focused on its application to low-cost sensor (LCS) data in environmental monitoring. To address this gap, we conduct a systematic literature review (SLR) following the PRISMA framework, synthesising findings from 82 peer-reviewed articles. The review addresses three key questions: (1) What fusion methodologies are employed in conjunction with LCS data? (2) In what environmental contexts are these methods applied? (3) What are the methodological challenges and research gaps? Our analysis reveals that geostatistical and machine learning approaches dominate current practice, with air quality monitoring emerging as the primary application domain. Additionally, artificial intelligence (AI)-based methods are increasingly used to integrate spatial, temporal, and multimodal data. However, limitations persist in uncertainty quantification, validation standards, and the generalisability of fusion frameworks. This review provides a comprehensive synthesis of current techniques and outlines key directions for future research, including the development of robust, uncertainty-aware fusion methods and broader application to less-studied environmental variables.&lt;/p&gt;</content:encoded></item><item><title>Spatial-X fusion for multi-source satellite imageries</title><link>https://doi.org/10.1016/j.rse.2025.115214</link><guid>10.1016/j.rse.2025.115214</guid><pubDate>Mon, 19 Jan 2026 11:34:50 +0000</pubDate><dc:creator>Jiang He</dc:creator><dc:creator>Liupeng Lin</dc:creator><dc:creator>Zhuo Zheng</dc:creator><dc:creator>Qiangqiang Yuan</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:creator>Xiao xiang Zhu</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115214</prism:doi><description>Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.
Published: 2026-01-19T11:34:50+00:00
Venue: Remote Sensing of Environment
Score: 0.500 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiang He; Liupeng Lin; Zhuo Zheng; Qiangqiang Yuan; Jie Li; Liangpei Zhang; Xiao xiang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115214"&gt;10.1016/j.rse.2025.115214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.500 (consider)&lt;/p&gt;
&lt;p&gt;Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.&lt;/p&gt;</content:encoded></item><item><title>SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes</title><link>https://doi.org/10.1007/s11263-025-02581-6</link><guid>10.1007/s11263-025-02581-6</guid><pubDate>Mon, 19 Jan 2026 08:28:15 +0000</pubDate><dc:creator>Chenhui Shi</dc:creator><dc:creator>Fulin Tang</dc:creator><dc:creator>Hao Wei</dc:creator><dc:creator>Yihong Wu</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02581-6</prism:doi><description>We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.
Published: 2026-01-19T08:28:15+00:00
Venue: International Journal of Computer Vision
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenhui Shi; Fulin Tang; Hao Wei; Yihong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02581-6"&gt;10.1007/s11263-025-02581-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.&lt;/p&gt;</content:encoded></item><item><title>Focus on Primary: Differential Diverse Data Augmentation for Generalization in Visual Reinforcement Learning</title><link>https://doi.org/10.1016/j.eswa.2026.131231</link><guid>10.1016/j.eswa.2026.131231</guid><pubDate>Sun, 18 Jan 2026 03:11:22 +0000</pubDate><dc:creator>Junhong Wu</dc:creator><dc:creator>Jie Liu</dc:creator><dc:creator>Xi Xiong</dc:creator><dc:creator>Daolong An</dc:creator><dc:creator>Shuai</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131231</prism:doi><description>It is common for the agent in reinforcement learning to overfit the training environment, making generalization to unseen environments extremely challenging. Visual reinforcement learning that relies on observed images as input is particularly constrained by generalization. To address these challenges, various data augmentation methods are consistently attempted to improve the generalization capability and reduce the training cost. However, the naive use of data augmentation can often lead to suboptimal policies. In this paper, we propose two novel approaches: Diverse Data Augmentation (DDA) and Differential Diverse Data Augmentation (D3A). Leveraging a pre-trained model, we segment primary pixels to avoid inappropriate data augmentation affecting semantic information. DDA improves the generalization capability of the agent in complex environments through the consistency of encoding. D3A uses proper data augmentation for primary pixels to further improve generalization while satisfying semantic-invariant state transformation. We extensively evaluate our methods on 2 challenging benchmarks for generalization. The results demonstrate that our methods significantly improve the generalization performance of the agent in unseen environments.
Published: 2026-01-18T03:11:22+00:00
Venue: Expert Systems with Applications
Score: 0.498 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junhong Wu; Jie Liu; Xi Xiong; Daolong An; Shuai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131231"&gt;10.1016/j.eswa.2026.131231&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.498 (consider)&lt;/p&gt;
&lt;p&gt;It is common for the agent in reinforcement learning to overfit the training environment, making generalization to unseen environments extremely challenging. Visual reinforcement learning that relies on observed images as input is particularly constrained by generalization. To address these challenges, various data augmentation methods are consistently attempted to improve the generalization capability and reduce the training cost. However, the naive use of data augmentation can often lead to suboptimal policies. In this paper, we propose two novel approaches: Diverse Data Augmentation (DDA) and Differential Diverse Data Augmentation (D3A). Leveraging a pre-trained model, we segment primary pixels to avoid inappropriate data augmentation affecting semantic information. DDA improves the generalization capability of the agent in complex environments through the consistency of encoding. D3A uses proper data augmentation for primary pixels to further improve generalization while satisfying semantic-invariant state transformation. We extensively evaluate our methods on 2 challenging benchmarks for generalization. The results demonstrate that our methods significantly improve the generalization performance of the agent in unseen environments.&lt;/p&gt;</content:encoded></item><item><title>视觉语言模型驱动的目标计数</title><link>https://doi.org/10.11834/jig.250119</link><guid>10.11834/jig.250119</guid><pubDate>Mon, 19 Jan 2026 01:26:11 +0000</pubDate><dc:creator>Cao Feng</dc:creator><dc:creator>Zhang Xiaowen</dc:creator><dc:creator>Yue Zijie</dc:creator><dc:creator>Li Li</dc:creator><dc:creator>Shi Miaojing</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250119</prism:doi><description>目的大型视觉语言模型的进展给解决基于文本提示的目标计数问题带来新的思路。然而，现有方法仍面临类别语义错位与解码器架构局限两大挑战。前者导致模型易将相似背景或无关类别误检为目标，后者依赖单一卷积神经网络（convolutional neural network，CNN）架构的局部特征提取，可能引发全局语义与局部细节的割裂，严重制约复杂场景下的计数鲁棒性。针对上述问题，提出跨分支协作对齐网络（cross-branch cooperative alignment network，CANet）。方法其核心包括：1）双分支解码器架构：通过并行Transformer分支（建模全局上下文依赖）与CNN分支（提取细粒度局部特征），结合信息互馈模块实现跨分支的特征交互和密度图预测；2）视觉—文本类别对齐损失：通过约束图像与文本特征的跨模态对齐，迫使模型区分目标与干扰语义，实现对类别的准确检测。结果在5个基准数据集上与先进的4种基于文本的目标计数方法进行比较实验。在FSC-147（few-shot counting-147）数据集上， CANet相较于性能第2的模型，在测试集上的平均绝对误差（mean absolute error，MAE）和均方根误差（root mean squared error，RMSE）分别降低1.22和8.45；在CARPK（car parking lot dataset）和PUCPR+（Pontifical Catholic University of Parana+ dataset）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低0.08和3.58；在SHA（ShanghaiTech part-A）和SHB（ShanghaiTech part-B）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低了47.0和9.8。同时也在FSC-147数据集上进行丰富的消融实验以验证算法的有效性，消融实验结果表明提出的方法针对两个问题做出了有效改进。结论本文方法能够解决现有方法所面临的两个问题，使计数结果更加准确。本文方法在4个数据集的交叉验证实验均取得SOTA（state-of-the-art）的性能，表明了CANet在零样本目标计数任务中的强大泛化能力。
Published: 2026-01-19T01:26:11+00:00
Venue: Journal of Image and Graphics
Score: 0.496 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cao Feng; Zhang Xiaowen; Yue Zijie; Li Li; Shi Miaojing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250119"&gt;10.11834/jig.250119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.496 (consider)&lt;/p&gt;
&lt;p&gt;目的大型视觉语言模型的进展给解决基于文本提示的目标计数问题带来新的思路。然而，现有方法仍面临类别语义错位与解码器架构局限两大挑战。前者导致模型易将相似背景或无关类别误检为目标，后者依赖单一卷积神经网络（convolutional neural network，CNN）架构的局部特征提取，可能引发全局语义与局部细节的割裂，严重制约复杂场景下的计数鲁棒性。针对上述问题，提出跨分支协作对齐网络（cross-branch cooperative alignment network，CANet）。方法其核心包括：1）双分支解码器架构：通过并行Transformer分支（建模全局上下文依赖）与CNN分支（提取细粒度局部特征），结合信息互馈模块实现跨分支的特征交互和密度图预测；2）视觉—文本类别对齐损失：通过约束图像与文本特征的跨模态对齐，迫使模型区分目标与干扰语义，实现对类别的准确检测。结果在5个基准数据集上与先进的4种基于文本的目标计数方法进行比较实验。在FSC-147（few-shot counting-147）数据集上， CANet相较于性能第2的模型，在测试集上的平均绝对误差（mean absolute error，MAE）和均方根误差（root mean squared error，RMSE）分别降低1.22和8.45；在CARPK（car parking lot dataset）和PUCPR+（Pontifical Catholic University of Parana+ dataset）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低0.08和3.58；在SHA（ShanghaiTech part-A）和SHB（ShanghaiTech part-B）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低了47.0和9.8。同时也在FSC-147数据集上进行丰富的消融实验以验证算法的有效性，消融实验结果表明提出的方法针对两个问题做出了有效改进。结论本文方法能够解决现有方法所面临的两个问题，使计数结果更加准确。本文方法在4个数据集的交叉验证实验均取得SOTA（state-of-the-art）的性能，表明了CANet在零样本目标计数任务中的强大泛化能力。&lt;/p&gt;</content:encoded></item><item><title>WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.020</link><guid>10.1016/j.isprsjprs.2026.01.020</guid><pubDate>Mon, 19 Jan 2026 11:01:56 +0000</pubDate><dc:creator>Wenxia Gan</dc:creator><dc:creator>Yu Feng</dc:creator><dc:creator>Jianhao Miao</dc:creator><dc:creator>Xinghua Li</dc:creator><dc:creator>Huanfeng Shen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.020</prism:doi><description>The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .
Published: 2026-01-19T11:01:56+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.496 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxia Gan; Yu Feng; Jianhao Miao; Xinghua Li; Huanfeng Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020"&gt;10.1016/j.isprsjprs.2026.01.020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.496 (consider)&lt;/p&gt;
&lt;p&gt;The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .&lt;/p&gt;</content:encoded></item><item><title>PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis</title><link>https://arxiv.org/abs/2601.10945v1</link><guid>http://arxiv.org/abs/2601.10945v1</guid><pubDate>Fri, 16 Jan 2026 02:18:29 +0000</pubDate><dc:creator>K Lokesh</dc:creator><dc:creator>Abhirama Subramanyam Penamakuri</dc:creator><dc:creator>Uday Agarwal</dc:creator><dc:creator>Apoorva Challa</dc:creator><dc:creator>Shreya K Gowda</dc:creator><dc:creator>Somesh Gupta</dc:creator><dc:creator>Anand Mishra</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.
Published: 2026-01-16T02:18:29+00:00
Venue: arXiv
Score: 0.495 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; K Lokesh; Abhirama Subramanyam Penamakuri; Uday Agarwal; Apoorva Challa; Shreya K Gowda; Somesh Gupta; Anand Mishra&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.495 (consider)&lt;/p&gt;
&lt;p&gt;Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.&lt;/p&gt;</content:encoded></item><item><title>MDGC-Net: Multi-Dimensional Geometric Feature and Adaptive Geometric Consistency for Enhanced Point Cloud Segmentation</title><link>https://doi.org/10.1016/j.patcog.2026.113110</link><guid>10.1016/j.patcog.2026.113110</guid><pubDate>Sun, 18 Jan 2026 22:46:51 +0000</pubDate><dc:creator>Kangzhe Hu</dc:creator><dc:creator>Haijun Huang</dc:creator><dc:creator>Jie Sun</dc:creator><dc:creator>Dingcheng Huang</dc:creator><dc:creator>Maomao Fan</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113110</prism:doi><description>Existing point cloud segmentation methods often struggle to capture fine geometric variations and maintain blurred object boundaries. We propose MDGC-Net, an efficient encoder-decoder network, to address these limitations through two innovations. (1) Our Multi-Dimensional Geometric Feature (MDGF) module processes relative coordinates, normal differences, and curvature cues in parallel. It introduces a novel branch attention mechanism to adaptively weight each cue’s contribution, forming a robust geometric descriptor. (2) Our Adaptive Geometric Consistency Regularization (AGCR) loss directly improves boundary sharpness. The loss is boundary-aware, automatically reducing penalties at sharp edges, and robust to noisy neighbors using a Huber-based formulation. This design enforces feature consistency primarily on smooth surfaces. A novel Local Feature Aggregation (LFA) block integrates these features using lightweight graph message passing (GMP). Experiments on S3DIS and ScanNetV2 show MDGC-Net achieves highly competitive performance. Crucially, quantitative analysis using boundary-specific metrics confirms enhanced sharpness, and a complexity analysis verifies a favorable balance between performance and efficiency.
Published: 2026-01-18T22:46:51+00:00
Venue: Pattern Recognition
Score: 0.494 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kangzhe Hu; Haijun Huang; Jie Sun; Dingcheng Huang; Maomao Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113110"&gt;10.1016/j.patcog.2026.113110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.494 (consider)&lt;/p&gt;
&lt;p&gt;Existing point cloud segmentation methods often struggle to capture fine geometric variations and maintain blurred object boundaries. We propose MDGC-Net, an efficient encoder-decoder network, to address these limitations through two innovations. (1) Our Multi-Dimensional Geometric Feature (MDGF) module processes relative coordinates, normal differences, and curvature cues in parallel. It introduces a novel branch attention mechanism to adaptively weight each cue’s contribution, forming a robust geometric descriptor. (2) Our Adaptive Geometric Consistency Regularization (AGCR) loss directly improves boundary sharpness. The loss is boundary-aware, automatically reducing penalties at sharp edges, and robust to noisy neighbors using a Huber-based formulation. This design enforces feature consistency primarily on smooth surfaces. A novel Local Feature Aggregation (LFA) block integrates these features using lightweight graph message passing (GMP). Experiments on S3DIS and ScanNetV2 show MDGC-Net achieves highly competitive performance. Crucially, quantitative analysis using boundary-specific metrics confirms enhanced sharpness, and a complexity analysis verifies a favorable balance between performance and efficiency.&lt;/p&gt;</content:encoded></item><item><title>Combating Spurious Correlations in Graph Interpretability via Self-Reflection</title><link>https://arxiv.org/abs/2601.11021v1</link><guid>http://arxiv.org/abs/2601.11021v1</guid><pubDate>Fri, 16 Jan 2026 06:31:16 +0000</pubDate><dc:creator>Kecheng Cai</dc:creator><dc:creator>Chenyang Xu</dc:creator><dc:creator>Chao Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.
Published: 2026-01-16T06:31:16+00:00
Venue: arXiv
Score: 0.493 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kecheng Cai; Chenyang Xu; Chao Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.493 (consider)&lt;/p&gt;
&lt;p&gt;Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.&lt;/p&gt;</content:encoded></item><item><title>预训练混合架构模型的电影场景分割方法</title><link>https://doi.org/10.11834/jig.240532</link><guid>10.11834/jig.240532</guid><pubDate>Mon, 19 Jan 2026 01:26:15 +0000</pubDate><dc:creator>Zhao Xiaolei</dc:creator><dc:creator>Zhao Xin</dc:creator><dc:creator>Zheng Ke</dc:creator><dc:creator>Yu Haoyang</dc:creator><dc:creator>Sun Xu</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.240532</prism:doi><description>目的随着电影内容的复杂化与多样化，电影场景分割成为理解影片结构和支持多媒体应用的重要任务。为提升镜头特征提取和特征关联的有效性，增强镜头序列的上下文感知能力，提出一种混合架构电影场景分割方法（hybrid architecture scene segmentation network， HASSNet）。方法首先，采用预训练结合微调策略，在大量无场景标签的电影数据上进行无监督预训练，使模型学习有效的镜头特征表示和关联特性，然后在有场景标签的数据上进行微调训练，进一步提升模型性能；其次，模型架构上混合了状态空间模型和自注意力机制模型，分别设计Shot Mamba镜头特征提取模块和Scene Transformer特征关联模块，Shot Mamba通过对镜头图像分块建模提取有效特征表示，Scene Transformer则通过注意力机制对不同镜头特征进行关联建模；最后，采用3种无监督损失函数进行预训练，提升模型在镜头特征提取和关联上的性能，并使用Focal Loss损失函数进行微调，以改善由于类别不平衡导致的精度不足问题。结果实验结果表明，HASSNet在3个数据集上显著提升了场景分割的精度，在典型电影场景分割数据集MovieNet中，与先进的场景分割方法相比，AP（average precision）、mIoU（mean intersection over union）、AUC-ROC（area under the receiver operating characteristic curve）和F1分别提升1.66%、10.54%、0.21%和16.83%，验证了本文提出的HASSNet方法可以有效提升场景边界定位的准确性。结论本文提出的HASSNet方法有效结合了预训练与微调策略，借助混合状态空间模型和自注意力机制模型的特点，增强了镜头的上下文感知能力，使电影场景分割的结果更加准确。
Published: 2026-01-19T01:26:15+00:00
Venue: Journal of Image and Graphics
Score: 0.492 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Xiaolei; Zhao Xin; Zheng Ke; Yu Haoyang; Sun Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.240532"&gt;10.11834/jig.240532&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.492 (consider)&lt;/p&gt;
&lt;p&gt;目的随着电影内容的复杂化与多样化，电影场景分割成为理解影片结构和支持多媒体应用的重要任务。为提升镜头特征提取和特征关联的有效性，增强镜头序列的上下文感知能力，提出一种混合架构电影场景分割方法（hybrid architecture scene segmentation network， HASSNet）。方法首先，采用预训练结合微调策略，在大量无场景标签的电影数据上进行无监督预训练，使模型学习有效的镜头特征表示和关联特性，然后在有场景标签的数据上进行微调训练，进一步提升模型性能；其次，模型架构上混合了状态空间模型和自注意力机制模型，分别设计Shot Mamba镜头特征提取模块和Scene Transformer特征关联模块，Shot Mamba通过对镜头图像分块建模提取有效特征表示，Scene Transformer则通过注意力机制对不同镜头特征进行关联建模；最后，采用3种无监督损失函数进行预训练，提升模型在镜头特征提取和关联上的性能，并使用Focal Loss损失函数进行微调，以改善由于类别不平衡导致的精度不足问题。结果实验结果表明，HASSNet在3个数据集上显著提升了场景分割的精度，在典型电影场景分割数据集MovieNet中，与先进的场景分割方法相比，AP（average precision）、mIoU（mean intersection over union）、AUC-ROC（area under the receiver operating characteristic curve）和F1分别提升1.66%、10.54%、0.21%和16.83%，验证了本文提出的HASSNet方法可以有效提升场景边界定位的准确性。结论本文提出的HASSNet方法有效结合了预训练与微调策略，借助混合状态空间模型和自注意力机制模型的特点，增强了镜头的上下文感知能力，使电影场景分割的结果更加准确。&lt;/p&gt;</content:encoded></item><item><title>VARDiff: vision-augmented retrieval-guided diffusion for stock forecasting</title><link>https://doi.org/10.1016/j.ins.2026.123113</link><guid>10.1016/j.ins.2026.123113</guid><pubDate>Mon, 19 Jan 2026 07:20:29 +0000</pubDate><dc:creator>Thi-Thu Nguyen</dc:creator><dc:creator>Xuan-Thong Truong</dc:creator><dc:creator>Thai-Binh Nguyen</dc:creator><dc:creator>Nhat-Hai Nguyen</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2026.123113</prism:doi><description>Stock price forecasting is a critical yet inherently difficult task in quantitative finance due to the volatile and non-stationary nature of financial time series. While diffusion models have emerged as promising tools for capturing predictive uncertainty, their effectiveness is often limited by insufficient data and the absence of informative guidance during generation. To address these challenges, we propose VARDiff, a diffusion forecasting architecture conditioned on visual-semantic references retrieved from a historical database. Our core novelty is a cross-attention-based denoising network that operates on delay embedding (DE) image representations of time series, fusing the target trajectory with its visually similar historical counterparts retrieved via a GAF-based visual encoding pipeline using a pre-trained VGG backbone to provide structured guidance during iterative denoising. VARDiff transforms historical price sequences into image representations and extracts semantic embeddings using a pre-trained vision encoder. These embeddings facilitate the retrieval of visually similar historical trajectories, which serve as external references to guide the denoising process of the diffusion model. Extensive experiments on nine benchmark stock datasets show that VARDiff reduces forecasting errors by an average of 16.27% (MSE) and 8.12% (MAE) compared to state-of-the-art baselines. The results underscore the effectiveness of integrating vision-based retrieval into diffusion forecasting, leading to more robust and data-efficient financial prediction.
Published: 2026-01-19T07:20:29+00:00
Venue: Information Sciences
Score: 0.492 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thi-Thu Nguyen; Xuan-Thong Truong; Thai-Binh Nguyen; Nhat-Hai Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2026.123113"&gt;10.1016/j.ins.2026.123113&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.492 (consider)&lt;/p&gt;
&lt;p&gt;Stock price forecasting is a critical yet inherently difficult task in quantitative finance due to the volatile and non-stationary nature of financial time series. While diffusion models have emerged as promising tools for capturing predictive uncertainty, their effectiveness is often limited by insufficient data and the absence of informative guidance during generation. To address these challenges, we propose VARDiff, a diffusion forecasting architecture conditioned on visual-semantic references retrieved from a historical database. Our core novelty is a cross-attention-based denoising network that operates on delay embedding (DE) image representations of time series, fusing the target trajectory with its visually similar historical counterparts retrieved via a GAF-based visual encoding pipeline using a pre-trained VGG backbone to provide structured guidance during iterative denoising. VARDiff transforms historical price sequences into image representations and extracts semantic embeddings using a pre-trained vision encoder. These embeddings facilitate the retrieval of visually similar historical trajectories, which serve as external references to guide the denoising process of the diffusion model. Extensive experiments on nine benchmark stock datasets show that VARDiff reduces forecasting errors by an average of 16.27% (MSE) and 8.12% (MAE) compared to state-of-the-art baselines. The results underscore the effectiveness of integrating vision-based retrieval into diffusion forecasting, leading to more robust and data-efficient financial prediction.&lt;/p&gt;</content:encoded></item><item><title>Context-Aware Semantic Segmentation via Stage-Wise Attention</title><link>https://arxiv.org/abs/2601.11310v1</link><guid>http://arxiv.org/abs/2601.11310v1</guid><pubDate>Fri, 16 Jan 2026 14:06:46 +0000</pubDate><dc:creator>Antoine Carreaud</dc:creator><dc:creator>Elias Naha</dc:creator><dc:creator>Arthur Chansel</dc:creator><dc:creator>Nina Lahellec</dc:creator><dc:creator>Jan Skaloud</dc:creator><dc:creator>Adrien Gressin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.
Published: 2026-01-16T14:06:46+00:00
Venue: arXiv
Score: 0.490 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Antoine Carreaud; Elias Naha; Arthur Chansel; Nina Lahellec; Jan Skaloud; Adrien Gressin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.490 (consider)&lt;/p&gt;
&lt;p&gt;Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.&lt;/p&gt;</content:encoded></item><item><title>PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs</title><link>https://arxiv.org/abs/2601.11451v1</link><guid>http://arxiv.org/abs/2601.11451v1</guid><pubDate>Fri, 16 Jan 2026 17:16:26 +0000</pubDate><dc:creator>Oishee Bintey Hoque</dc:creator><dc:creator>Nibir Chandra Mandal</dc:creator><dc:creator>Kyle Luong</dc:creator><dc:creator>Amanda Wilson</dc:creator><dc:creator>Samarth Swarup</dc:creator><dc:creator>Madhav Marathe</dc:creator><dc:creator>Abhijin Adiga</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho
Published: 2026-01-16T17:16:26+00:00
Venue: arXiv
Score: 0.489 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Oishee Bintey Hoque; Nibir Chandra Mandal; Kyle Luong; Amanda Wilson; Samarth Swarup; Madhav Marathe; Abhijin Adiga&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.489 (consider)&lt;/p&gt;
&lt;p&gt;Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho&lt;/p&gt;</content:encoded></item></channel></rss>