<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 01 Feb 2026 03:43:43 +0000</lastBuildDate><item><title>Set-CVGL: A new perspective on cross-view geo-localization with unordered ground-view image sets</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.037</link><guid>10.1016/j.isprsjprs.2026.01.037</guid><pubDate>Fri, 30 Jan 2026 10:57:03 +0000</pubDate><dc:creator>Qiong Wu</dc:creator><dc:creator>Panwang Xia</dc:creator><dc:creator>Lei Yu</dc:creator><dc:creator>Yi Liu</dc:creator><dc:creator>Mingtao Xiong</dc:creator><dc:creator>Liheng Zhong</dc:creator><dc:creator>Jingdong Chen</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:creator>Yi Wan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.037</prism:doi><description>Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;#xD7; " role="presentation"&gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .
Published: 2026-01-30T10:57:03+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiong Wu; Panwang Xia; Lei Yu; Yi Liu; Mingtao Xiong; Liheng Zhong; Jingdong Chen; Ming Yang; Yongjun Zhang; Yi Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037"&gt;10.1016/j.isprsjprs.2026.01.037&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;amp;#xD7; &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .&lt;/p&gt;</content:encoded></item><item><title>SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</title><link>https://arxiv.org/abs/2601.21498v1</link><guid>http://arxiv.org/abs/2601.21498v1</guid><pubDate>Thu, 29 Jan 2026 10:15:55 +0000</pubDate><dc:creator>Thanh-Nhan Vo</dc:creator><dc:creator>Trong-Thuan Nguyen</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.
Published: 2026-01-29T10:15:55+00:00
Venue: arXiv
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>DeepSeek-OCR 2: Visual Causal Flow</title><link>https://arxiv.org/abs/2601.20552v1</link><guid>http://arxiv.org/abs/2601.20552v1</guid><pubDate>Wed, 28 Jan 2026 12:46:07 +0000</pubDate><dc:creator>Haoran Wei</dc:creator><dc:creator>Yaofeng Sun</dc:creator><dc:creator>Yukun Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.
Published: 2026-01-28T12:46:07+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Wei; Yaofeng Sun; Yukun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.&lt;/p&gt;</content:encoded></item><item><title>Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.21159v1</link><guid>http://arxiv.org/abs/2601.21159v1</guid><pubDate>Thu, 29 Jan 2026 01:46:03 +0000</pubDate><dc:creator>Jianzheng Wang</dc:creator><dc:creator>Huan Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.
Published: 2026-01-29T01:46:03+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianzheng Wang; Huan Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using &amp;quot;one-way injection&amp;quot; and &amp;quot;shallow post-processing&amp;quot; strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.&lt;/p&gt;</content:encoded></item><item><title>FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</title><link>https://arxiv.org/abs/2601.21187v1</link><guid>http://arxiv.org/abs/2601.21187v1</guid><pubDate>Thu, 29 Jan 2026 02:36:19 +0000</pubDate><dc:creator>Chenyu Huang</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Xudong Tan</dc:creator><dc:creator>Jinhan Mu</dc:creator><dc:creator>Shenghe Zheng</dc:creator><dc:creator>Li Shen</dc:creator><dc:creator>Tao Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.
Published: 2026-01-29T02:36:19+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyu Huang; Peng Ye; Xudong Tan; Jinhan Mu; Shenghe Zheng; Li Shen; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&amp;#x27;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Disentangle to Fuse: Towards Content Preservation and Cross-Modality Consistency for Multi-Modality Image Fusion</title><link>https://doi.org/10.1109/tip.2026.3657183</link><guid>10.1109/tip.2026.3657183</guid><pubDate>Fri, 30 Jan 2026 21:05:59 +0000</pubDate><dc:creator>Xinran Qin</dc:creator><dc:creator>Yuning Cui</dc:creator><dc:creator>Shangquan Sun</dc:creator><dc:creator>Ruoyu Chen</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Alois Knoll</dc:creator><dc:creator>Xiaochun Cao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657183</prism:doi><description>Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.
Published: 2026-01-30T21:05:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinran Qin; Yuning Cui; Shangquan Sun; Ruoyu Chen; Wenqi Ren; Alois Knoll; Xiaochun Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657183"&gt;10.1109/tip.2026.3657183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.&lt;/p&gt;</content:encoded></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://doi.org/10.1109/tpami.2026.3659598</link><guid>10.1109/tpami.2026.3659598</guid><pubDate>Fri, 30 Jan 2026 21:03:08 +0000</pubDate><dc:creator>Jiaming Zhang</dc:creator><dc:creator>Xin Wang</dc:creator><dc:creator>Xingjun Ma</dc:creator><dc:creator>Lingyu Qiu</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><dc:creator>Jitao Sang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3659598</prism:doi><description>Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...
Published: 2026-01-30T21:03:08+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaming Zhang; Xin Wang; Xingjun Ma; Lingyu Qiu; Yu-Gang Jiang; Jitao Sang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3659598"&gt;10.1109/tpami.2026.3659598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Model with Siamese Bilateral Difference Network and Text-Guided Image Feature Enhancement for Acute Ischemic Stroke Outcome Prediction on CT Angiography</title><link>https://doi.org/10.1016/j.inffus.2026.104195</link><guid>10.1016/j.inffus.2026.104195</guid><pubDate>Fri, 30 Jan 2026 00:38:27 +0000</pubDate><dc:creator>Hulin Kuang</dc:creator><dc:creator>Bin Hu</dc:creator><dc:creator>Shuai Yang</dc:creator><dc:creator>Dongcui Wang</dc:creator><dc:creator>Guanghua Luo</dc:creator><dc:creator>Weihua Liao</dc:creator><dc:creator>Wu Qiu</dc:creator><dc:creator>Shulin Liu</dc:creator><dc:creator>Jianxin Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104195</prism:doi><description>Acute ischemic stroke (AIS) outcome prediction is crucial for treatment decisions. However, AIS outcome prediction is challenging due to the combined influence of lesion characteristics, vascular status, and other health conditions. In this study, we introduce a vision-language model with a Siamese bilateral difference network and a text-guided image feature enhancement module for predicting AIS outcome (e.g., modified Rankin Scale, mRS) on CT angiography. In the Siamese bilateral difference network, based on fine-tuning the foundation model LVM-Med, we design an interactive Transformer fine-tuning encoder and a vision question answering guided bilateral difference awareness module, which generates bilateral difference text via image-text pair question answering as a prompt to enhance the extracted brain vascular difference features. Additionally, in the text-guided image feature enhancement module, we propose a text feature extraction module to extract patient phrase-level and inter-phrase embeddings from clinical notes, and employ a multi-scale image-text interaction module to obtain fine-grained phrase-enhanced image attention feature and coarse-grained phrase context-aware image attention feature. We validate our model on the public ISLES2024 dataset, a private dataset A, and an external AIS dataset. It achieves accuracies of 81.11%, 83.05%, and 80.00% and AUCs of 80.06%, 85.48% and 82.62% for 90-day mRS prediction on the 3 datasets, respectively, outperforming several state-of-the-art methods and demonstrating its generalization ability. Moreover, the proposed method can be effectively extended to glaucoma visual field progression prediction, which is also related to vascular differences and clinical notes.
Published: 2026-01-30T00:38:27+00:00
Venue: Information Fusion
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hulin Kuang; Bin Hu; Shuai Yang; Dongcui Wang; Guanghua Luo; Weihua Liao; Wu Qiu; Shulin Liu; Jianxin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104195"&gt;10.1016/j.inffus.2026.104195&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Acute ischemic stroke (AIS) outcome prediction is crucial for treatment decisions. However, AIS outcome prediction is challenging due to the combined influence of lesion characteristics, vascular status, and other health conditions. In this study, we introduce a vision-language model with a Siamese bilateral difference network and a text-guided image feature enhancement module for predicting AIS outcome (e.g., modified Rankin Scale, mRS) on CT angiography. In the Siamese bilateral difference network, based on fine-tuning the foundation model LVM-Med, we design an interactive Transformer fine-tuning encoder and a vision question answering guided bilateral difference awareness module, which generates bilateral difference text via image-text pair question answering as a prompt to enhance the extracted brain vascular difference features. Additionally, in the text-guided image feature enhancement module, we propose a text feature extraction module to extract patient phrase-level and inter-phrase embeddings from clinical notes, and employ a multi-scale image-text interaction module to obtain fine-grained phrase-enhanced image attention feature and coarse-grained phrase context-aware image attention feature. We validate our model on the public ISLES2024 dataset, a private dataset A, and an external AIS dataset. It achieves accuracies of 81.11%, 83.05%, and 80.00% and AUCs of 80.06%, 85.48% and 82.62% for 90-day mRS prediction on the 3 datasets, respectively, outperforming several state-of-the-art methods and demonstrating its generalization ability. Moreover, the proposed method can be effectively extended to glaucoma visual field progression prediction, which is also related to vascular differences and clinical notes.&lt;/p&gt;</content:encoded></item><item><title>Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</title><link>https://arxiv.org/abs/2601.21193v1</link><guid>http://arxiv.org/abs/2601.21193v1</guid><pubDate>Thu, 29 Jan 2026 02:49:33 +0000</pubDate><dc:creator>Zecheng Zhao</dc:creator><dc:creator>Zhi Chen</dc:creator><dc:creator>Zi Huang</dc:creator><dc:creator>Shazia Sadiq</dc:creator><dc:creator>Tong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.
Published: 2026-01-29T02:49:33+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zecheng Zhao; Zhi Chen; Zi Huang; Shazia Sadiq; Tong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.&lt;/p&gt;</content:encoded></item><item><title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.22060v1</link><guid>http://arxiv.org/abs/2601.22060v1</guid><pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate><dc:creator>Wenxuan Huang</dc:creator><dc:creator>Yu Zeng</dc:creator><dc:creator>Qiuchen Wang</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Shaosheng Cao</dc:creator><dc:creator>Zheng Chu</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Zhenfei Yin</dc:creator><dc:creator>Lin Chen</dc:creator><dc:creator>Zehui Chen</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Philip Torr</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Wanli Ouyang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.
Published: 2026-01-29T17:58:40+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxuan Huang; Yu Zeng; Qiuchen Wang; Zhen Fang; Shaosheng Cao; Zheng Chu; Qingyu Yin; Shuang Chen; Zhenfei Yin; Lin Chen; Zehui Chen; Yao Hu; Philip Torr; Feng Zhao; Wanli Ouyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&amp;#x27;&amp;#x27; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.&lt;/p&gt;</content:encoded></item><item><title>Thinker: A vision-language foundation model for embodied intelligence</title><link>https://arxiv.org/abs/2601.21199v1</link><guid>http://arxiv.org/abs/2601.21199v1</guid><pubDate>Thu, 29 Jan 2026 02:52:08 +0000</pubDate><dc:creator>Baiyu Pan</dc:creator><dc:creator>Daqin Luo</dc:creator><dc:creator>Junpeng Yang</dc:creator><dc:creator>Jiyuan Wang</dc:creator><dc:creator>Yixuan Zhang</dc:creator><dc:creator>Hailin Shi</dc:creator><dc:creator>Jichao Jiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.
Published: 2026-01-29T02:52:08+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baiyu Pan; Daqin Luo; Junpeng Yang; Jiyuan Wang; Yixuan Zhang; Hailin Shi; Jichao Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model&amp;#x27;s capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.&lt;/p&gt;</content:encoded></item><item><title>PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization</title><link>https://arxiv.org/abs/2601.21617v1</link><guid>http://arxiv.org/abs/2601.21617v1</guid><pubDate>Thu, 29 Jan 2026 12:21:16 +0000</pubDate><dc:creator>Songhan Jiang</dc:creator><dc:creator>Fengchun Liu</dc:creator><dc:creator>Ziyue Wang</dc:creator><dc:creator>Linghan Cai</dc:creator><dc:creator>Yongbing Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.
Published: 2026-01-29T12:21:16+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Jiang; Fengchun Liu; Ziyue Wang; Linghan Cai; Yongbing Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.&lt;/p&gt;</content:encoded></item><item><title>CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization</title><link>https://arxiv.org/abs/2601.20355v1</link><guid>http://arxiv.org/abs/2601.20355v1</guid><pubDate>Wed, 28 Jan 2026 08:15:56 +0000</pubDate><dc:creator>Yue Liang</dc:creator><dc:creator>Jiatong Du</dc:creator><dc:creator>Ziyi Yang</dc:creator><dc:creator>Yanjun Huang</dc:creator><dc:creator>Hong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.
Published: 2026-01-28T08:15:56+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Liang; Jiatong Du; Ziyi Yang; Yanjun Huang; Hong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.&lt;/p&gt;</content:encoded></item><item><title>Deep learning-based interactive segmentation in remote sensing</title><link>https://doi.org/10.1109/jstars.2026.3659488</link><guid>10.1109/jstars.2026.3659488</guid><pubDate>Fri, 30 Jan 2026 21:03:44 +0000</pubDate><dc:creator>Zhe Wang</dc:creator><dc:creator>Shoukun Sun</dc:creator><dc:creator>Xiang Que</dc:creator><dc:creator>Xiaogang Ma</dc:creator><dc:creator>Carmen Galaz García</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659488</prism:doi><description>Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.
Published: 2026-01-30T21:03:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhe Wang; Shoukun Sun; Xiang Que; Xiaogang Ma; Carmen Galaz García&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659488"&gt;10.1109/jstars.2026.3659488&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.&lt;/p&gt;</content:encoded></item><item><title>SVRMAE: Enhancing surveillance video super-resolution through separation masking and MAE pretraining</title><link>https://doi.org/10.1016/j.neucom.2026.132834</link><guid>10.1016/j.neucom.2026.132834</guid><pubDate>Fri, 30 Jan 2026 16:26:18 +0000</pubDate><dc:creator>Zhifeng Liu</dc:creator><dc:creator>Zheng He</dc:creator><dc:creator>Gang Ye</dc:creator><dc:creator>Wenqian Zhu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132834</prism:doi><description>We innovatively integrate the masked auto-encoder (MAE) structure with the separate-process-merge paradigm, named Surveillance Video Restoration Masked Auto-Encoder (SVRMAE), to address the challenge of surveillance video super-resolution (SVSR). Given the shared need for spatiotemporal information extraction in MAE pre-training and VSR tasks, SVRMAE integrates the MAE pre-training strategy, enabling the model to deeply comprehend low-level visual statistics and improve video super-resolution performance. To the best of our knowledge, our work represents the first successful attempt to combine MAE pre-training with the VSR task, and SVRMAE is widely adaptable to the majority of existing VSR models. Additionally, we devised a novel separation masking strategy tailored for the separate-process-merge framework, which distinctly emphasizes the super-resolution of foreground and background elements and, when integrated with our backbone architecture, effectively enhances the extraction of unique semantic features from both. Extensive experiments demonstrate that our SVRMAE method excels in super-resolving surveillance videos, outperforming other state-of-the-art models.
Published: 2026-01-30T16:26:18+00:00
Venue: Neurocomputing
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhifeng Liu; Zheng He; Gang Ye; Wenqian Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132834"&gt;10.1016/j.neucom.2026.132834&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;We innovatively integrate the masked auto-encoder (MAE) structure with the separate-process-merge paradigm, named Surveillance Video Restoration Masked Auto-Encoder (SVRMAE), to address the challenge of surveillance video super-resolution (SVSR). Given the shared need for spatiotemporal information extraction in MAE pre-training and VSR tasks, SVRMAE integrates the MAE pre-training strategy, enabling the model to deeply comprehend low-level visual statistics and improve video super-resolution performance. To the best of our knowledge, our work represents the first successful attempt to combine MAE pre-training with the VSR task, and SVRMAE is widely adaptable to the majority of existing VSR models. Additionally, we devised a novel separation masking strategy tailored for the separate-process-merge framework, which distinctly emphasizes the super-resolution of foreground and background elements and, when integrated with our backbone architecture, effectively enhances the extraction of unique semantic features from both. Extensive experiments demonstrate that our SVRMAE method excels in super-resolving surveillance videos, outperforming other state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</title><link>https://arxiv.org/abs/2601.20433v2</link><guid>http://arxiv.org/abs/2601.20433v2</guid><pubDate>Wed, 28 Jan 2026 09:44:31 +0000</pubDate><dc:creator>Wenbo Xu</dc:creator><dc:creator>Wei Lu</dc:creator><dc:creator>Xiangyang Luo</dc:creator><dc:creator>Jiantao Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.
Published: 2026-01-28T09:44:31+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenbo Xu; Wei Lu; Xiangyang Luo; Jiantao Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.&lt;/p&gt;</content:encoded></item><item><title>Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction</title><link>https://arxiv.org/abs/2601.20720v1</link><guid>http://arxiv.org/abs/2601.20720v1</guid><pubDate>Wed, 28 Jan 2026 15:53:32 +0000</pubDate><dc:creator>Matej Halinkovic</dc:creator><dc:creator>Nina Masarykova</dc:creator><dc:creator>Alexey Vinel</dc:creator><dc:creator>Marek Galinski</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.
Published: 2026-01-28T15:53:32+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matej Halinkovic; Nina Masarykova; Alexey Vinel; Marek Galinski&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.&lt;/p&gt;</content:encoded></item><item><title>Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation</title><link>https://arxiv.org/abs/2601.21406v1</link><guid>http://arxiv.org/abs/2601.21406v1</guid><pubDate>Thu, 29 Jan 2026 08:42:25 +0000</pubDate><dc:creator>Zihan Su</dc:creator><dc:creator>Hongyang Wei</dc:creator><dc:creator>Kangrui Cen</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Guanhua Chen</dc:creator><dc:creator>Chun Yuan</dc:creator><dc:creator>Xiangxiang Chu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.
Published: 2026-01-29T08:42:25+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihan Su; Hongyang Wei; Kangrui Cen; Yong Wang; Guanhua Chen; Chun Yuan; Xiangxiang Chu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.&lt;/p&gt;</content:encoded></item><item><title>Lite-BSSNet: A Lightweight Blueprint-Guided Visual State Space Network for Remote Sensing Imagery Segmentation</title><link>https://doi.org/10.3390/rs18030441</link><guid>10.3390/rs18030441</guid><pubDate>Fri, 30 Jan 2026 17:38:34 +0000</pubDate><dc:creator>Jiaxin Yan</dc:creator><dc:creator>Yuxiang Xie</dc:creator><dc:creator>Yan Chen</dc:creator><dc:creator>Yanming Guo</dc:creator><dc:creator>Wenzhe Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030441</prism:doi><description>Remote sensing image segmentation requires balancing global context and local detail across multi-scale objects. However, convolutional neural network (CNN)-based methods struggle to model long-range dependencies, while transformer-based approaches suffer from quadratic complexity and become inefficient for high-resolution remote sensing scenarios. In addition, the semantic gap between deep and shallow features can cause misalignment during cross-layer aggregation, and information loss in upsampling tends to break thin continuous structures, such as roads and roof edges, introducing pronounced structural noise. To address these issues, we propose lightweight Lite-BSSNet (Blueprint-Guided State Space Network). First, a Structural Blueprint Generator (SBG) converts high-level semantics into an edge-enhanced structural blueprint that provides a topological prior. Then, a Visual State Space Bridge (VSS-Bridge) aligns multi-level features and projects axially aggregated features into a linear-complexity visual state space, smoothing high-gradient edge signals for sequential scanning. Finally, a Structural Repair Block (SRB) enlarges the effective receptive field via dilated convolutions and uses spatial/channel gating to suppress upsampling artifacts and reconnect thin structures. Experiments on the ISPRS Vaihingen and Potsdam datasets show that Lite-BSSNet achieves the highest segmentation accuracy among the compared lightweight models, with mIoU of 83.9% and 86.7%, respectively, while requiring only 45.4 GFLOPs, thus achieving a favorable trade-off between accuracy and efficiency.
Published: 2026-01-30T17:38:34+00:00
Venue: Remote Sensing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxin Yan; Yuxiang Xie; Yan Chen; Yanming Guo; Wenzhe Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030441"&gt;10.3390/rs18030441&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image segmentation requires balancing global context and local detail across multi-scale objects. However, convolutional neural network (CNN)-based methods struggle to model long-range dependencies, while transformer-based approaches suffer from quadratic complexity and become inefficient for high-resolution remote sensing scenarios. In addition, the semantic gap between deep and shallow features can cause misalignment during cross-layer aggregation, and information loss in upsampling tends to break thin continuous structures, such as roads and roof edges, introducing pronounced structural noise. To address these issues, we propose lightweight Lite-BSSNet (Blueprint-Guided State Space Network). First, a Structural Blueprint Generator (SBG) converts high-level semantics into an edge-enhanced structural blueprint that provides a topological prior. Then, a Visual State Space Bridge (VSS-Bridge) aligns multi-level features and projects axially aggregated features into a linear-complexity visual state space, smoothing high-gradient edge signals for sequential scanning. Finally, a Structural Repair Block (SRB) enlarges the effective receptive field via dilated convolutions and uses spatial/channel gating to suppress upsampling artifacts and reconnect thin structures. Experiments on the ISPRS Vaihingen and Potsdam datasets show that Lite-BSSNet achieves the highest segmentation accuracy among the compared lightweight models, with mIoU of 83.9% and 86.7%, respectively, while requiring only 45.4 GFLOPs, thus achieving a favorable trade-off between accuracy and efficiency.&lt;/p&gt;</content:encoded></item><item><title>Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning</title><link>https://arxiv.org/abs/2601.21037v1</link><guid>http://arxiv.org/abs/2601.21037v1</guid><pubDate>Wed, 28 Jan 2026 20:57:55 +0000</pubDate><dc:creator>Chengzu Li</dc:creator><dc:creator>Zanyi Wang</dc:creator><dc:creator>Jiaang Li</dc:creator><dc:creator>Yi Xu</dc:creator><dc:creator>Han Zhou</dc:creator><dc:creator>Huanyu Zhang</dc:creator><dc:creator>Ruichuan An</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Zhaochong An</dc:creator><dc:creator>Ivan Vulić</dc:creator><dc:creator>Serge Belongie</dc:creator><dc:creator>Anna Korhonen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.
Published: 2026-01-28T20:57:55+00:00
Venue: arXiv
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chengzu Li; Zanyi Wang; Jiaang Li; Yi Xu; Han Zhou; Huanyu Zhang; Ruichuan An; Dengyang Jiang; Zhaochong An; Ivan Vulić; Serge Belongie; Anna Korhonen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.&lt;/p&gt;</content:encoded></item><item><title>OCSA-FN: A Fusion Network with Orthogonality-Constrained Spatial Attention for Hyperspectral and Land Surface Temperature Data Classification</title><link>https://doi.org/10.1109/tgrs.2026.3659827</link><guid>10.1109/tgrs.2026.3659827</guid><pubDate>Fri, 30 Jan 2026 21:03:13 +0000</pubDate><dc:creator>Enyu Zhao</dc:creator><dc:creator>Yongfang Su</dc:creator><dc:creator>Nianxin Qu</dc:creator><dc:creator>Yulei Wang</dc:creator><dc:creator>Yongguang Zhao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3659827</prism:doi><description>With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.
Published: 2026-01-30T21:03:13+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Enyu Zhao; Yongfang Su; Nianxin Qu; Yulei Wang; Yongguang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3659827"&gt;10.1109/tgrs.2026.3659827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.&lt;/p&gt;</content:encoded></item><item><title>CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation</title><link>https://doi.org/10.1109/tip.2026.3657240</link><guid>10.1109/tip.2026.3657240</guid><pubDate>Fri, 30 Jan 2026 21:05:59 +0000</pubDate><dc:creator>Shilong Zou</dc:creator><dc:creator>Yuhang Huang</dc:creator><dc:creator>Renjiao Yi</dc:creator><dc:creator>Chenyang Zhu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657240</prism:doi><description>We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...
Published: 2026-01-30T21:05:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shilong Zou; Yuhang Huang; Renjiao Yi; Chenyang Zhu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657240"&gt;10.1109/tip.2026.3657240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...&lt;/p&gt;</content:encoded></item><item><title>GeoRC: A Benchmark for Geolocation Reasoning Chains</title><link>https://arxiv.org/abs/2601.21278v1</link><guid>http://arxiv.org/abs/2601.21278v1</guid><pubDate>Thu, 29 Jan 2026 05:18:40 +0000</pubDate><dc:creator>Mohit Talreja</dc:creator><dc:creator>Joshua Diao</dc:creator><dc:creator>Jim Thannikary James</dc:creator><dc:creator>Radu Casapu</dc:creator><dc:creator>Tejas Santanam</dc:creator><dc:creator>Ethan Mendes</dc:creator><dc:creator>Alan Ritter</dc:creator><dc:creator>Wei Xu</dc:creator><dc:creator>James Hays</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.
Published: 2026-01-29T05:18:40+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohit Talreja; Joshua Diao; Jim Thannikary James; Radu Casapu; Tejas Santanam; Ethan Mendes; Alan Ritter; Wei Xu; James Hays&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation</title><link>https://arxiv.org/abs/2601.21751v1</link><guid>http://arxiv.org/abs/2601.21751v1</guid><pubDate>Thu, 29 Jan 2026 14:06:23 +0000</pubDate><dc:creator>Jiankun Peng</dc:creator><dc:creator>Jianyuan Guo</dc:creator><dc:creator>Ying Xu</dc:creator><dc:creator>Yue Liu</dc:creator><dc:creator>Jiashuang Yan</dc:creator><dc:creator>Xuanwei Ye</dc:creator><dc:creator>Houhua Li</dc:creator><dc:creator>Xiaoming Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a "Granularity Rigidity" problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling "densification on demand" in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav.
Published: 2026-01-29T14:06:23+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiankun Peng; Jianyuan Guo; Ying Xu; Yue Liu; Jiashuang Yan; Xuanwei Ye; Houhua Li; Xiaoming Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &amp;quot;Granularity Rigidity&amp;quot; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &amp;quot;densification on demand&amp;quot; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav.&lt;/p&gt;</content:encoded></item><item><title>Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization</title><link>https://arxiv.org/abs/2601.21078v1</link><guid>http://arxiv.org/abs/2601.21078v1</guid><pubDate>Wed, 28 Jan 2026 22:03:46 +0000</pubDate><dc:creator>Jiaqi Li</dc:creator><dc:creator>Guangming Wang</dc:creator><dc:creator>Shuntian Zheng</dc:creator><dc:creator>Minzhe Ni</dc:creator><dc:creator>Xiaoman Lu</dc:creator><dc:creator>Guanghui Ye</dc:creator><dc:creator>Yu Guan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.
Published: 2026-01-28T22:03:46+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Li; Guangming Wang; Shuntian Zheng; Minzhe Ni; Xiaoman Lu; Guanghui Ye; Yu Guan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.&lt;/p&gt;</content:encoded></item><item><title>Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning</title><link>https://arxiv.org/abs/2601.21904v1</link><guid>http://arxiv.org/abs/2601.21904v1</guid><pubDate>Thu, 29 Jan 2026 16:00:12 +0000</pubDate><dc:creator>Hanmo Chen</dc:creator><dc:creator>Guangtao Lyu</dc:creator><dc:creator>Chenghao Xu</dc:creator><dc:creator>Jiexi Yan</dc:creator><dc:creator>Xu Yang</dc:creator><dc:creator>Cheng Deng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.
Published: 2026-01-29T16:00:12+00:00
Venue: arXiv
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanmo Chen; Guangtao Lyu; Chenghao Xu; Jiexi Yan; Xu Yang; Cheng Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</title><link>https://arxiv.org/abs/2601.21468v1</link><guid>http://arxiv.org/abs/2601.21468v1</guid><pubDate>Thu, 29 Jan 2026 09:47:17 +0000</pubDate><dc:creator>Yaorui Shi</dc:creator><dc:creator>Shugui Liu</dc:creator><dc:creator>Yu Yang</dc:creator><dc:creator>Wenyu Mao</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Qi GU</dc:creator><dc:creator>Hui Su</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Xiang Wang</dc:creator><dc:creator>An Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.
Published: 2026-01-29T09:47:17+00:00
Venue: arXiv
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaorui Shi; Shugui Liu; Yu Yang; Wenyu Mao; Yuxin Chen; Qi GU; Hui Su; Xunliang Cai; Xiang Wang; An Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.&lt;/p&gt;</content:encoded></item><item><title>MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</title><link>https://arxiv.org/abs/2601.21821v1</link><guid>http://arxiv.org/abs/2601.21821v1</guid><pubDate>Thu, 29 Jan 2026 15:07:28 +0000</pubDate><dc:creator>Honglin Lin</dc:creator><dc:creator>Zheng Liu</dc:creator><dc:creator>Yun Zhu</dc:creator><dc:creator>Chonghan Qin</dc:creator><dc:creator>Juekai Lin</dc:creator><dc:creator>Xiaoran Shang</dc:creator><dc:creator>Conghui He</dc:creator><dc:creator>Wentao Zhang</dc:creator><dc:creator>Lijun Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a "less is more" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.
Published: 2026-01-29T15:07:28+00:00
Venue: arXiv
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Honglin Lin; Zheng Liu; Yun Zhu; Chonghan Qin; Juekai Lin; Xiaoran Shang; Conghui He; Wentao Zhang; Lijun Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a &amp;quot;less is more&amp;quot; phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.&lt;/p&gt;</content:encoded></item><item><title>RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent</title><link>https://doi.org/10.1109/tpami.2026.3651958</link><guid>10.1109/tpami.2026.3651958</guid><pubDate>Fri, 30 Jan 2026 21:03:08 +0000</pubDate><dc:creator>Yijie Deng</dc:creator><dc:creator>Lei Han</dc:creator><dc:creator>Tianpeng Lin</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Jinzhi Zhang</dc:creator><dc:creator>Lu Fang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651958</prism:doi><description>With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.
Published: 2026-01-30T21:03:08+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yijie Deng; Lei Han; Tianpeng Lin; Lin Li; Jinzhi Zhang; Lu Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651958"&gt;10.1109/tpami.2026.3651958&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.&lt;/p&gt;</content:encoded></item><item><title>DiCo: Disentangled concept representation for text-to-image person re-identification</title><link>https://doi.org/10.1016/j.neucom.2026.132885</link><guid>10.1016/j.neucom.2026.132885</guid><pubDate>Fri, 30 Jan 2026 07:46:11 +0000</pubDate><dc:creator>Giyeol Kim</dc:creator><dc:creator>Chanho Eom</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132885</prism:doi><description>Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes ( e.g. , color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.
Published: 2026-01-30T07:46:11+00:00
Venue: Neurocomputing
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Giyeol Kim; Chanho Eom&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132885"&gt;10.1016/j.neucom.2026.132885&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes ( e.g. , color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.&lt;/p&gt;</content:encoded></item></channel></rss>