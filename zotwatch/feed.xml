<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 27 Dec 2025 02:34:29 +0000</lastBuildDate><item><title>SAM-I2V++: Efficiently Upgrading SAM for Promptable Video Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3648863</link><guid>10.1109/tpami.2025.3648863</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Haiyang Mei</dc:creator><dc:creator>Pengyu Zhang</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648863</prism:doi><description>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM's static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2's performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.596 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Mei; Pengyu Zhang; Mike Zheng Shou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648863"&gt;10.1109/tpami.2025.3648863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.596 (consider)&lt;/p&gt;
&lt;p&gt;Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM&amp;#x27;s static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2&amp;#x27;s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.&lt;/p&gt;</content:encoded></item><item><title>Visual Dialog with Semantic Consistency: An External Knowledge-Driven Approach</title><link>https://doi.org/10.1016/j.neunet.2025.108523</link><guid>10.1016/j.neunet.2025.108523</guid><pubDate>Thu, 25 Dec 2025 00:11:00 +0000</pubDate><dc:creator>Shanshan Du</dc:creator><dc:creator>Hanli Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108523</prism:doi><description>As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.
Published: 2025-12-25T00:11:00+00:00
Venue: Neural Networks
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shanshan Du; Hanli Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108523"&gt;10.1016/j.neunet.2025.108523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;As a crucial subfield of intelligent human-machine interaction, visual dialog involves answering multi-turn questions based on visual content and history dialog, presenting significant technical challenges. Although recent works have made steady progress in visual dialog, several issues remain to be addressed. First, there are bias issues in fine-grained multimodal modeling, including information asymmetry and representation inconsistency, which lead to incomplete information understanding and decision-making biases during question answering. Second, previous visual dialog models relying on external knowledge suffer from poor knowledge quality and insufficient knowledge diversity, which introduce noise into the model and undermine the accuracy and coherence of the question responses. In this work, a novel semantic consistency visual dialog model enhanced by external knowledge (SCVD+) is proposed to cope with these challenges. Specifically, fine-grained structured visual and textual scene graphs are constructed to mitigate the issue of information asymmetry, which equally prioritize both linguistic and visual elements, ensuring a comprehensive capture of object relationships in images and word associations in dialog history. Furthermore, beneficial external knowledge sourced from a commonsense knowledge base is integrated to alleviate the representation inconsistency in multimodal scene graphs and to promote the model’s interpretability. Finally, implicit clues are derived from pre-trained large models and integrated with explicit information from scene graphs using a proposed dual-level knowledge fusion and reasoning strategy, which ensures the diversity of external knowledge and enhances the model’s reasoning capability in complex scenarios. Experimental results demonstrate the effectiveness of our method on the public datasets VisDial v0.9, VisDial v1.0, and OpenVisDial 2.0.&lt;/p&gt;</content:encoded></item><item><title>UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</title><link>https://arxiv.org/abs/2512.21078v1</link><guid>http://arxiv.org/abs/2512.21078v1</guid><pubDate>Wed, 24 Dec 2025 09:55:16 +0000</pubDate><dc:creator>Tianchen Deng</dc:creator><dc:creator>Xun Chen</dc:creator><dc:creator>Ziming Li</dc:creator><dc:creator>Hongming Shen</dc:creator><dc:creator>Danwei Wang</dc:creator><dc:creator>Javier Civera</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.
Published: 2025-12-24T09:55:16+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianchen Deng; Xun Chen; Ziming Li; Hongming Shen; Danwei Wang; Javier Civera; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.&lt;/p&gt;</content:encoded></item><item><title>Latent Implicit Visual Reasoning</title><link>https://arxiv.org/abs/2512.21218v1</link><guid>http://arxiv.org/abs/2512.21218v1</guid><pubDate>Wed, 24 Dec 2025 14:59:49 +0000</pubDate><dc:creator>Kelvin Li</dc:creator><dc:creator>Chuyi Shang</dc:creator><dc:creator>Leonid Karlinsky</dc:creator><dc:creator>Rogerio Feris</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Roei Herzig</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.
Published: 2025-12-24T14:59:49+00:00
Venue: arXiv
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kelvin Li; Chuyi Shang; Leonid Karlinsky; Rogerio Feris; Trevor Darrell; Roei Herzig&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &amp;quot;useful&amp;quot; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.&lt;/p&gt;</content:encoded></item><item><title>SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2512.20013v1</link><guid>http://arxiv.org/abs/2512.20013v1</guid><pubDate>Tue, 23 Dec 2025 03:10:17 +0000</pubDate><dc:creator>Zepeng Xin</dc:creator><dc:creator>Kaiyu Li</dc:creator><dc:creator>Luodi Chen</dc:creator><dc:creator>Wanchen Li</dc:creator><dc:creator>Yuchen Xiao</dc:creator><dc:creator>Hui Qiao</dc:creator><dc:creator>Weizhan Zhang</dc:creator><dc:creator>Deyu Meng</dc:creator><dc:creator>Xiangyong Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.
Published: 2025-12-23T03:10:17+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zepeng Xin; Kaiyu Li; Luodi Chen; Wanchen Li; Yuchen Xiao; Hui Qiao; Weizhan Zhang; Deyu Meng; Xiangyong Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model&amp;#x27;s effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.&lt;/p&gt;</content:encoded></item><item><title>Language Embedded 3D Gaussians for Open-Vocabulary Scene Querying</title><link>https://doi.org/10.1109/tpami.2025.3648837</link><guid>10.1109/tpami.2025.3648837</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Miao Wang</dc:creator><dc:creator>Jin-Chuan Shi</dc:creator><dc:creator>Shao-Hua Guan</dc:creator><dc:creator>Hao-Bin Duan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648837</prism:doi><description>Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miao Wang; Jin-Chuan Shi; Shao-Hua Guan; Hao-Bin Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648837"&gt;10.1109/tpami.2025.3648837&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</title><link>https://arxiv.org/abs/2512.21221v1</link><guid>http://arxiv.org/abs/2512.21221v1</guid><pubDate>Wed, 24 Dec 2025 15:02:33 +0000</pubDate><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Phu-Hoa Pham</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval
Published: 2025-12-24T15:02:33+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dao Sy Duy Minh; Huynh Trung Kiet; Nguyen Lam Phu Quy; Phu-Hoa Pham; Tran Chi Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval&lt;/p&gt;</content:encoded></item><item><title>VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</title><link>https://arxiv.org/abs/2512.21194v1</link><guid>http://arxiv.org/abs/2512.21194v1</guid><pubDate>Wed, 24 Dec 2025 14:18:38 +0000</pubDate><dc:creator>Brigitta Malagurski Törtei</dc:creator><dc:creator>Yasser Dahou</dc:creator><dc:creator>Ngoc Dung Huynh</dc:creator><dc:creator>Wamiq Reyaz Para</dc:creator><dc:creator>Phúc H. Lê Khac</dc:creator><dc:creator>Ankit Singh</dc:creator><dc:creator>Sofian Chaybouti</dc:creator><dc:creator>Sanath Narayan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.
Published: 2025-12-24T14:18:38+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brigitta Malagurski Törtei; Yasser Dahou; Ngoc Dung Huynh; Wamiq Reyaz Para; Phúc H. Lê Khac; Ankit Singh; Sofian Chaybouti; Sanath Narayan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.&lt;/p&gt;</content:encoded></item><item><title>PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</title><link>https://arxiv.org/abs/2512.20907v1</link><guid>http://arxiv.org/abs/2512.20907v1</guid><pubDate>Wed, 24 Dec 2025 03:18:51 +0000</pubDate><dc:creator>Seongmin Jung</dc:creator><dc:creator>Seongho Choi</dc:creator><dc:creator>Gunwoo Jeon</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jongwoo Lim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.
Published: 2025-12-24T03:18:51+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seongmin Jung; Seongho Choi; Gunwoo Jeon; Minsu Cho; Jongwoo Lim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.&lt;/p&gt;</content:encoded></item><item><title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title><link>https://arxiv.org/abs/2512.20557v1</link><guid>http://arxiv.org/abs/2512.20557v1</guid><pubDate>Tue, 23 Dec 2025 17:56:36 +0000</pubDate><dc:creator>Shengchao Zhou</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Yuying Ge</dc:creator><dc:creator>Wei Huang</dc:creator><dc:creator>Jiehong Lin</dc:creator><dc:creator>Ying Shan</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.
Published: 2025-12-23T17:56:36+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengchao Zhou; Yuxin Chen; Yuying Ge; Wei Huang; Jiehong Lin; Ying Shan; Xiaojuan Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.&lt;/p&gt;</content:encoded></item><item><title>SciceVPR: Stable cross-image correlation enhanced model for visual place recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132539</link><guid>10.1016/j.neucom.2025.132539</guid><pubDate>Fri, 26 Dec 2025 07:36:24 +0000</pubDate><dc:creator>Shanshan Wan</dc:creator><dc:creator>Yingmei Wei</dc:creator><dc:creator>Lai Kang</dc:creator><dc:creator>Tianrui Shen</dc:creator><dc:creator>Haixuan Wang</dc:creator><dc:creator>Yee-Hong Yang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132539</prism:doi><description>Visual Place Recognition (VPR) is a major challenge for robotics and autonomous systems, with the goal of predicting the location of an image based solely on its visual features. State-of-the-art (SOTA) models extract global descriptors using the powerful foundation model DINOv2 as backbone. These models either explore the cross-image correlation or propose a time-consuming two-stage re-ranking strategy to achieve better performance. However, existing works only utilize the final output of DINOv2, and the current cross-image correlation causes unstable retrieval results. To produce both discriminative and stable global descriptors, this paper proposes a s table c ross- i mage c orrelation e nhanced model for VPR called SciceVPR. This model explores the full potential of DINOv2 in providing useful feature representations that implicitly encode valuable contextual knowledge. Specifically, SciceVPR first uses a multi-layer feature fusion module to capture increasingly detailed task-relevant channel and spatial information from the multi-layer output of DINOv2. The designed module not only enhances local feature quality but also reduces feature extraction time by approximately 50 % compared to existing adaptations. Secondly, SciceVPR considers the invariant correlation between images within a batch as valuable knowledge to be distilled into the proposed invariant feature projector. After knowledge distillation, SciceVPR is able to produce a self-enhanced global descriptor from a single input that achieves performance comparable to a multi-frame correlated global descriptor. These two innovations enables SciceVPR to produce fairly robust global features regardless of domain shifts (e.g., changes in illumination, weather and viewpoint between pictures taken in the same place). Experimental results demonstrate that the base variant, SciceVPR-B, achieves SOTA performance on several challenging urban benchmarks. The large variant, SciceVPR-L, performs on par with SOTA two-stage models, scoring over 3 % higher in Recall@1 compared to existing models on the challenging Tokyo24/7 dataset. Our code will be released at https://github.com/shuimushan/SciceVPR .
Published: 2025-12-26T07:36:24+00:00
Venue: Neurocomputing
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shanshan Wan; Yingmei Wei; Lai Kang; Tianrui Shen; Haixuan Wang; Yee-Hong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132539"&gt;10.1016/j.neucom.2025.132539&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) is a major challenge for robotics and autonomous systems, with the goal of predicting the location of an image based solely on its visual features. State-of-the-art (SOTA) models extract global descriptors using the powerful foundation model DINOv2 as backbone. These models either explore the cross-image correlation or propose a time-consuming two-stage re-ranking strategy to achieve better performance. However, existing works only utilize the final output of DINOv2, and the current cross-image correlation causes unstable retrieval results. To produce both discriminative and stable global descriptors, this paper proposes a s table c ross- i mage c orrelation e nhanced model for VPR called SciceVPR. This model explores the full potential of DINOv2 in providing useful feature representations that implicitly encode valuable contextual knowledge. Specifically, SciceVPR first uses a multi-layer feature fusion module to capture increasingly detailed task-relevant channel and spatial information from the multi-layer output of DINOv2. The designed module not only enhances local feature quality but also reduces feature extraction time by approximately 50 % compared to existing adaptations. Secondly, SciceVPR considers the invariant correlation between images within a batch as valuable knowledge to be distilled into the proposed invariant feature projector. After knowledge distillation, SciceVPR is able to produce a self-enhanced global descriptor from a single input that achieves performance comparable to a multi-frame correlated global descriptor. These two innovations enables SciceVPR to produce fairly robust global features regardless of domain shifts (e.g., changes in illumination, weather and viewpoint between pictures taken in the same place). Experimental results demonstrate that the base variant, SciceVPR-B, achieves SOTA performance on several challenging urban benchmarks. The large variant, SciceVPR-L, performs on par with SOTA two-stage models, scoring over 3 % higher in Recall@1 compared to existing models on the challenging Tokyo24/7 dataset. Our code will be released at https://github.com/shuimushan/SciceVPR .&lt;/p&gt;</content:encoded></item><item><title>BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</title><link>https://arxiv.org/abs/2512.20255v1</link><guid>http://arxiv.org/abs/2512.20255v1</guid><pubDate>Tue, 23 Dec 2025 11:13:01 +0000</pubDate><dc:creator>Jinghao Shi</dc:creator><dc:creator>Jianing Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.
Published: 2025-12-23T11:13:01+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinghao Shi; Jianing Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Guided Denoising Network for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648408</link><guid>10.1109/tgrs.2025.3648408</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Xin Li</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Jue Zhang</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Xin Lyu</dc:creator><dc:creator>Fan Liu</dc:creator><dc:creator>Hongmin Gao</dc:creator><dc:creator>André Kaup</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648408</prism:doi><description>Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Li; Feng Xu; Jue Zhang; Hongsheng Zhang; Xin Lyu; Fan Liu; Hongmin Gao; André Kaup&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648408"&gt;10.1109/tgrs.2025.3648408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.&lt;/p&gt;</content:encoded></item><item><title>Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</title><link>https://arxiv.org/abs/2512.20042v1</link><guid>http://arxiv.org/abs/2512.20042v1</guid><pubDate>Tue, 23 Dec 2025 04:21:15 +0000</pubDate><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Pham Phu Hoa</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Nguyen Hoang Minh Ngoc</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding
Published: 2025-12-23T04:21:15+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nguyen Lam Phu Quy; Pham Phu Hoa; Tran Chi Nguyen; Dao Sy Duy Minh; Nguyen Hoang Minh Ngoc; Huynh Trung Kiet&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding&lt;/p&gt;</content:encoded></item><item><title>Data-Driven Bidirectional Spatial-Adaptive Network for Weakly Supervised Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tpami.2025.3646464</link><guid>10.1109/tpami.2025.3646464</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Zebin Wu</dc:creator><dc:creator>Shangdong Zheng</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Le Wang</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Gang Hua</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646464</prism:doi><description>Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zebin Wu; Shangdong Zheng; Yang Xu; Le Wang; Zhihui Wei; Gang Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646464"&gt;10.1109/tpami.2025.3646464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.&lt;/p&gt;</content:encoded></item><item><title>A Memory and Retrieval Transformer-based Unsupervised Learning Model for Anomaly Detection and Segmentation</title><link>https://doi.org/10.1016/j.patcog.2025.113004</link><guid>10.1016/j.patcog.2025.113004</guid><pubDate>Fri, 26 Dec 2025 07:33:15 +0000</pubDate><dc:creator>Jiawei Guo</dc:creator><dc:creator>Ge Song</dc:creator><dc:creator>Yi Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113004</prism:doi><description>Unsupervised learning models have recently advanced anomaly detection, especially for complex vision-based datasets, but most still yield coarse anomaly masks with vague shapes and locations. To address these limitations, this paper presents U nsupervised S egmentation and A nomaly G radient I nterpretation ( USAGI ), a novel transformer-based unsupervised learning approach designed for accurate anomaly detection and segmentation. We propose two novel components, the Memory Transformer and Retrieval Transformer: the former builds a memory bank from normal features during training, while the latter retrieves and compares features during testing to enable fine-grained anomaly reconstruction and segmentation. USAGI achieves salient performance on the MVTec AD dataset with an AUROC of 98.2% and on the VisA dataset (Visual Anomaly Dataset) with an AUROC of 99.5%, 98.8% on Real-IAD, and 96.4% on MANTA, demonstrating its superior performance in anomaly detection and segmentation.
Published: 2025-12-26T07:33:15+00:00
Venue: Pattern Recognition
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Guo; Ge Song; Yi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113004"&gt;10.1016/j.patcog.2025.113004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Unsupervised learning models have recently advanced anomaly detection, especially for complex vision-based datasets, but most still yield coarse anomaly masks with vague shapes and locations. To address these limitations, this paper presents U nsupervised S egmentation and A nomaly G radient I nterpretation ( USAGI ), a novel transformer-based unsupervised learning approach designed for accurate anomaly detection and segmentation. We propose two novel components, the Memory Transformer and Retrieval Transformer: the former builds a memory bank from normal features during training, while the latter retrieves and compares features during testing to enable fine-grained anomaly reconstruction and segmentation. USAGI achieves salient performance on the MVTec AD dataset with an AUROC of 98.2% and on the VisA dataset (Visual Anomaly Dataset) with an AUROC of 99.5%, 98.8% on Real-IAD, and 96.4% on MANTA, demonstrating its superior performance in anomaly detection and segmentation.&lt;/p&gt;</content:encoded></item><item><title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104097</link><guid>10.1016/j.inffus.2025.104097</guid><pubDate>Thu, 25 Dec 2025 16:05:23 +0000</pubDate><dc:creator>Xuanming Cao</dc:creator><dc:creator>Chengyu Tao</dc:creator><dc:creator>Yifeng Cheng</dc:creator><dc:creator>Juan Du</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104097</prism:doi><description>Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.
Published: 2025-12-25T16:05:23+00:00
Venue: Information Fusion
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanming Cao; Chengyu Tao; Yifeng Cheng; Juan Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104097"&gt;10.1016/j.inffus.2025.104097&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.&lt;/p&gt;</content:encoded></item><item><title>VL4Gaze: Unleashing Vision-Language Models for Gaze Following</title><link>https://arxiv.org/abs/2512.20735v1</link><guid>http://arxiv.org/abs/2512.20735v1</guid><pubDate>Tue, 23 Dec 2025 19:47:11 +0000</pubDate><dc:creator>Shijing Wang</dc:creator><dc:creator>Chaoqun Cui</dc:creator><dc:creator>Yaping Huang</dc:creator><dc:creator>Hyung Jin Chang</dc:creator><dc:creator>Yihua Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.
Published: 2025-12-23T19:47:11+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shijing Wang; Chaoqun Cui; Yaping Huang; Hyung Jin Chang; Yihua Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.&lt;/p&gt;</content:encoded></item><item><title>TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104087</link><guid>10.1016/j.inffus.2025.104087</guid><pubDate>Thu, 25 Dec 2025 00:16:18 +0000</pubDate><dc:creator>Changbin Wang</dc:creator><dc:creator>Fengrui Ji</dc:creator><dc:creator>Baolin Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104087</prism:doi><description>Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.
Published: 2025-12-25T00:16:18+00:00
Venue: Information Fusion
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changbin Wang; Fengrui Ji; Baolin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104087"&gt;10.1016/j.inffus.2025.104087&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.&lt;/p&gt;</content:encoded></item><item><title>VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment</title><link>https://doi.org/10.1016/j.inffus.2025.104092</link><guid>10.1016/j.inffus.2025.104092</guid><pubDate>Thu, 25 Dec 2025 00:15:59 +0000</pubDate><dc:creator>Shaina Raza</dc:creator><dc:creator>Ashmal Vayani</dc:creator><dc:creator>Aditya Jain</dc:creator><dc:creator>Aravind Narayanan</dc:creator><dc:creator>Vahid Reza Khazaie</dc:creator><dc:creator>Syed Raza Bashir</dc:creator><dc:creator>Elham Dolatabadi</dc:creator><dc:creator>Gias Uddin</dc:creator><dc:creator>Christos Emmanouilidis</dc:creator><dc:creator>Rizwan Qureshi</dc:creator><dc:creator>Mubarak Shah</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104092</prism:doi><description>Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI-safety benchmarks focus on single-modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news; remains largely unaddressed. In this work, we introduce the V ision- L anguage D isinformation Detection Bench mark ( VLDBench ), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluation of state-of-the-art LLMs and VLMs on VLDBench shows that adding visual cues improves detection accuracy, with gains ranging from 5 points for strong baselines (e.g., LLaMA-3.2-11B-Vision 74.82% vs. LLaMA-3.2-1B-Instruct 70.29%) to 25-30 points for smaller families (e.g., LLaVA-v1.5-Vicuna7B 72.32% vs. Vicuna-7B-v1.5 55.21%), reflecting complementary evidence from images (e.g., meme-like visuals, image-text consistency) that text alone cannot capture. We provide data and code for evaluation, fine-tuning and robustness tests to support disinformation analysis. Developed in alignment with the AI Goverance frameworks (MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media. Project: https://vectorinstitute.github.io/VLDBench/ Data: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench
Published: 2025-12-25T00:15:59+00:00
Venue: Information Fusion
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shaina Raza; Ashmal Vayani; Aditya Jain; Aravind Narayanan; Vahid Reza Khazaie; Syed Raza Bashir; Elham Dolatabadi; Gias Uddin; Christos Emmanouilidis; Rizwan Qureshi; Mubarak Shah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104092"&gt;10.1016/j.inffus.2025.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI-safety benchmarks focus on single-modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news; remains largely unaddressed. In this work, we introduce the V ision- L anguage D isinformation Detection Bench mark ( VLDBench ), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluation of state-of-the-art LLMs and VLMs on VLDBench shows that adding visual cues improves detection accuracy, with gains ranging from 5 points for strong baselines (e.g., LLaMA-3.2-11B-Vision 74.82% vs. LLaMA-3.2-1B-Instruct 70.29%) to 25-30 points for smaller families (e.g., LLaVA-v1.5-Vicuna7B 72.32% vs. Vicuna-7B-v1.5 55.21%), reflecting complementary evidence from images (e.g., meme-like visuals, image-text consistency) that text alone cannot capture. We provide data and code for evaluation, fine-tuning and robustness tests to support disinformation analysis. Developed in alignment with the AI Goverance frameworks (MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media. Project: https://vectorinstitute.github.io/VLDBench/ Data: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench&lt;/p&gt;</content:encoded></item><item><title>DDCL-GAN: a novel dual-domain contrastive learning generative adversarial network for unsupervised multimodal remote sensing image change detection</title><link>https://doi.org/10.1016/j.eswa.2025.130995</link><guid>10.1016/j.eswa.2025.130995</guid><pubDate>Fri, 26 Dec 2025 16:37:16 +0000</pubDate><dc:creator>Zhifu Zhu</dc:creator><dc:creator>Xiping Yuan</dc:creator><dc:creator>Shu Gan</dc:creator><dc:creator>Raobo Li</dc:creator><dc:creator>Weidong Luo</dc:creator><dc:creator>Cheng Chen</dc:creator><dc:creator>Rui Bi</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130995</prism:doi><description>Change detection (CD), as a core task in remote sensing image analysis, plays a crucial role in disaster monitoring, urban planning, and environmental assessment. Compared to traditional CD methods, multimodal CD breaks the dependency limitations on homogeneous sensor data, demonstrating higher timeliness and flexibility in disaster emergency response. However, significant imaging differences between different sensors lead to a lack of comparability in multimodal images, severely hindering their practical application. Existing multimodal CD research has effectively alleviated this issue by introducing Generative adversarial networks (GANs) to transform multimodal images into the same image domain. Nevertheless, these methods generally overlook the negative impact of intrinsic change regions on image transformation quality and the enhancement of CD performance through complementary features between different modalities, resulting in limited change recognition accuracy. To address these shortcomings, we propose a novel dual-domain contrastive learning GAN (DDCL-GAN) for unsupervised multimodal CD. This network effectively suppresses interference from change regions through a carefully designed non-local spatial correlation patch contrastive learning strategy, thereby ensuring semantic consistency at the content level between transformed and original images. Additionally, we develop a multi-scale dual residual module and a semantic cross-flow alignment module to enhance the model’s ability to express multi-scale semantic information and fine-grained features, respectively. Finally, we construct a dual-domain difference markov random field (DDMRF) detection model, which effectively improves detection performance by simultaneously considering dual-domain image information. To validate the effectiveness of the proposed method, we conducted comparative experiments with twelve mainstream unsupervised multimodal CD methods on nine typical datasets. Experimental results demonstrate that the proposed method achieves optimal performance across multiple average quantitative metrics, particularly in terms of mean IoU and κ coefficient, showing improvements of at least 2.9% and 2.3%, respectively, compared to comparative methods.
Published: 2025-12-26T16:37:16+00:00
Venue: Expert Systems with Applications
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhifu Zhu; Xiping Yuan; Shu Gan; Raobo Li; Weidong Luo; Cheng Chen; Rui Bi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130995"&gt;10.1016/j.eswa.2025.130995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Change detection (CD), as a core task in remote sensing image analysis, plays a crucial role in disaster monitoring, urban planning, and environmental assessment. Compared to traditional CD methods, multimodal CD breaks the dependency limitations on homogeneous sensor data, demonstrating higher timeliness and flexibility in disaster emergency response. However, significant imaging differences between different sensors lead to a lack of comparability in multimodal images, severely hindering their practical application. Existing multimodal CD research has effectively alleviated this issue by introducing Generative adversarial networks (GANs) to transform multimodal images into the same image domain. Nevertheless, these methods generally overlook the negative impact of intrinsic change regions on image transformation quality and the enhancement of CD performance through complementary features between different modalities, resulting in limited change recognition accuracy. To address these shortcomings, we propose a novel dual-domain contrastive learning GAN (DDCL-GAN) for unsupervised multimodal CD. This network effectively suppresses interference from change regions through a carefully designed non-local spatial correlation patch contrastive learning strategy, thereby ensuring semantic consistency at the content level between transformed and original images. Additionally, we develop a multi-scale dual residual module and a semantic cross-flow alignment module to enhance the model’s ability to express multi-scale semantic information and fine-grained features, respectively. Finally, we construct a dual-domain difference markov random field (DDMRF) detection model, which effectively improves detection performance by simultaneously considering dual-domain image information. To validate the effectiveness of the proposed method, we conducted comparative experiments with twelve mainstream unsupervised multimodal CD methods on nine typical datasets. Experimental results demonstrate that the proposed method achieves optimal performance across multiple average quantitative metrics, particularly in terms of mean IoU and κ coefficient, showing improvements of at least 2.9% and 2.3%, respectively, compared to comparative methods.&lt;/p&gt;</content:encoded></item><item><title>Fast SAM2 with Text-Driven Token Pruning</title><link>https://arxiv.org/abs/2512.21333v1</link><guid>http://arxiv.org/abs/2512.21333v1</guid><pubDate>Wed, 24 Dec 2025 18:59:05 +0000</pubDate><dc:creator>Avilasha Mandal</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Fachrina Dewi Puspitasari</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiaquan Zhang</dc:creator><dc:creator>Caiyan Qin</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.
Published: 2025-12-24T18:59:05+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Avilasha Mandal; Chaoning Zhang; Fachrina Dewi Puspitasari; Xudong Wang; Jiaquan Zhang; Caiyan Qin; Guoqing Wang; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation</title><link>https://arxiv.org/abs/2512.20936v1</link><guid>http://arxiv.org/abs/2512.20936v1</guid><pubDate>Wed, 24 Dec 2025 04:39:45 +0000</pubDate><dc:creator>Hongxing Fan</dc:creator><dc:creator>Shuyu Zhao</dc:creator><dc:creator>Jiayang Ao</dc:creator><dc:creator>Lu Sheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.
Published: 2025-12-24T04:39:45+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongxing Fan; Shuyu Zhao; Jiayang Ao; Lu Sheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.&lt;/p&gt;</content:encoded></item><item><title>Multi-stage Group Interaction and Cross-domain Fusion Network for Real-time Smoke Segmentation</title><link>https://doi.org/10.1109/tip.2025.3646455</link><guid>10.1109/tip.2025.3646455</guid><pubDate>Thu, 25 Dec 2025 18:28:37 +0000</pubDate><dc:creator>Kang Li</dc:creator><dc:creator>Feiniu Yuan</dc:creator><dc:creator>Chunmei Wang</dc:creator><dc:creator>Chunli Meng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646455</prism:doi><description>Lightweight smoke image segmentation is essential for fire warning systems, particularly on mobile devices. In recent years, although numerous high-precision, large-scale smoke segmentation models have been developed, there are few lightweight solutions specifically designed for mobile applications. Therefore, we propose a Multi-stage Group Interaction and Cross-domain Fusion Network (MGICFN) with low computational complexity for real-time smoke segmentation. To improve the model’s ability to effectively analyze smoke features, we incorporate a Cross-domain Interaction Attention Module (CIAM) to merge spatial and frequency domain features for creating a lightweight smoke encoder. To alleviate the loss of critical information from small smoke objects during downsampling, we design a Multi-stage Group Interaction Module (MGIM). The MGIM calibrates the information discrepancies between high and low-dimensional features. To enhance the boundary information of smoke targets, we introduce an Edge Enhancement Module (EEM), which utilizes predicted target boundaries as advanced guidance to refine lower-level smoke features. Furthermore, we implement a Group Convolutional Block Attention Module (GCBAM) and a Group Fusion Module (GFM) to connect the encoder and decoder efficiently. Experimental results demonstrate that MGICFN achieves an 88.70% Dice coefficient (Dice), an 81.16% mean Intersection over Union (mIoU), and a 91.93% accuracy (Acc) on the SFS3K dataset. It also achieves an 87.30% Dice, a 78.68% mIoU, and a 92.95% Acc on the SYN70K test dataset. Our MGICFN model has 0.73M parameters and requires 0.3G FLOPs.
Published: 2025-12-25T18:28:37+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kang Li; Feiniu Yuan; Chunmei Wang; Chunli Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646455"&gt;10.1109/tip.2025.3646455&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Lightweight smoke image segmentation is essential for fire warning systems, particularly on mobile devices. In recent years, although numerous high-precision, large-scale smoke segmentation models have been developed, there are few lightweight solutions specifically designed for mobile applications. Therefore, we propose a Multi-stage Group Interaction and Cross-domain Fusion Network (MGICFN) with low computational complexity for real-time smoke segmentation. To improve the model’s ability to effectively analyze smoke features, we incorporate a Cross-domain Interaction Attention Module (CIAM) to merge spatial and frequency domain features for creating a lightweight smoke encoder. To alleviate the loss of critical information from small smoke objects during downsampling, we design a Multi-stage Group Interaction Module (MGIM). The MGIM calibrates the information discrepancies between high and low-dimensional features. To enhance the boundary information of smoke targets, we introduce an Edge Enhancement Module (EEM), which utilizes predicted target boundaries as advanced guidance to refine lower-level smoke features. Furthermore, we implement a Group Convolutional Block Attention Module (GCBAM) and a Group Fusion Module (GFM) to connect the encoder and decoder efficiently. Experimental results demonstrate that MGICFN achieves an 88.70% Dice coefficient (Dice), an 81.16% mean Intersection over Union (mIoU), and a 91.93% accuracy (Acc) on the SFS3K dataset. It also achieves an 87.30% Dice, a 78.68% mIoU, and a 92.95% Acc on the SYN70K test dataset. Our MGICFN model has 0.73M parameters and requires 0.3G FLOPs.&lt;/p&gt;</content:encoded></item><item><title>BACF: Boundary-Aware Collaborative Framework for Weakly Supervised Semantic Segmentation</title><link>https://doi.org/10.1016/j.patcog.2025.112989</link><guid>10.1016/j.patcog.2025.112989</guid><pubDate>Thu, 25 Dec 2025 22:52:49 +0000</pubDate><dc:creator>Yanshuang Lu</dc:creator><dc:creator>Jiahua Zhang</dc:creator><dc:creator>Delong Kong</dc:creator><dc:creator>Xin Zheng</dc:creator><dc:creator>Xiaopeng Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112989</prism:doi><description>Single-stage Weakly Supervised Semantic Segmentation (WSSS) methods based on image-level labels have attracted increasing attention recently due to their simplicity and efficiency compared with more complex multi-stage pipelines. However, most existing approaches focus primarily on generating accurate and complete Class Activation Maps (CAMs) as pseudo-labels, often overlooking the optimization of the segmentation network itself. A significant discrepancy is frequently observed between the quality of pseudo-labels and the final segmentation performance, with most errors concentrated around object boundaries. To address this issue, we propose a novel Boundary-Aware Collaborative Framework (BACF) designed to improve both boundary precision and segmentation reliability in WSSS. Specifically, BACF employs a multi-view dual-branch architecture that facilitates cross-branch feature interaction through a similarity-guided fusion mechanism and interaction loss, thereby enhancing semantic consistency and feature representation. Furthermore, BACF incorporates a reliable dynamic learning strategy that adaptively adjusts feature fusion strength and supervision weight based on prediction confidence. To further refine boundary localization and semantic understanding, a multi-scale perceptual field and a boundary refinement module are also integrated. Extensive experiments on the PASCAL VOC 2012 and MS COCO datasets demonstrate that BACF consistently outperforms existing single-stage WSSS methods, validating its effectiveness and robustness.
Published: 2025-12-25T22:52:49+00:00
Venue: Pattern Recognition
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanshuang Lu; Jiahua Zhang; Delong Kong; Xin Zheng; Xiaopeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112989"&gt;10.1016/j.patcog.2025.112989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Single-stage Weakly Supervised Semantic Segmentation (WSSS) methods based on image-level labels have attracted increasing attention recently due to their simplicity and efficiency compared with more complex multi-stage pipelines. However, most existing approaches focus primarily on generating accurate and complete Class Activation Maps (CAMs) as pseudo-labels, often overlooking the optimization of the segmentation network itself. A significant discrepancy is frequently observed between the quality of pseudo-labels and the final segmentation performance, with most errors concentrated around object boundaries. To address this issue, we propose a novel Boundary-Aware Collaborative Framework (BACF) designed to improve both boundary precision and segmentation reliability in WSSS. Specifically, BACF employs a multi-view dual-branch architecture that facilitates cross-branch feature interaction through a similarity-guided fusion mechanism and interaction loss, thereby enhancing semantic consistency and feature representation. Furthermore, BACF incorporates a reliable dynamic learning strategy that adaptively adjusts feature fusion strength and supervision weight based on prediction confidence. To further refine boundary localization and semantic understanding, a multi-scale perceptual field and a boundary refinement module are also integrated. Extensive experiments on the PASCAL VOC 2012 and MS COCO datasets demonstrate that BACF consistently outperforms existing single-stage WSSS methods, validating its effectiveness and robustness.&lt;/p&gt;</content:encoded></item><item><title>DF2Net: A Differential-Feature Dynamic-Fusion Network for Building Change Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648359</link><guid>10.1109/tgrs.2025.3648359</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Like Zhao</dc:creator><dc:creator>Mingwang Yang</dc:creator><dc:creator>Yiran Zhao</dc:creator><dc:creator>Yuqing Liu</dc:creator><dc:creator>Huawei Jiang</dc:creator><dc:creator>Zhen Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648359</prism:doi><description>With the continuous advancement of urbanization, the importance of building change detection in urban planning, disaster management, and other fields has become increasingly prominent. However, existing change detection methods still have significant limitations in spatial domain difference feature representation and multi-scale difference capture. To address this, we propose a novel Differential-Feature Dynamic-Fusion Network (DF2Net), which aims to improve the accuracy of building change detection in remote sensing images. DF2Net achieves efficient detection of change areas through the collaborative design of three key modules: Differential-Feature Frequency-domain Attention Module (DFAM), Global-Local Feature Dynamic-Fusion Module (GFDM), and Pseudo-label Deep-supervision Module (PDM). Specifically, DFAM combines Fourier transform and self-attention mechanisms to extract key information from the frequency domain, thereby improving the feature representation of change areas. GFDM dynamically fuses global and local features to effectively capture multi-scale change information. PDM optimizes feature learning in the middle layer of the decoder by introducing pseudo-label supervision. Experimental results on three mainstream building change detection datasets, LEVIR-CD, WHU-CD, and HRCUS-CD, show that DF2Net significantly outperforms existing mainstream methods in both F1-score and IoU, demonstrating superior performance. This indicates that DF2Net has broad application prospects in handling complex scenarios and multi-scale change detection tasks. The code available from https://github.com/Mw-yang/DF2Net.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Like Zhao; Mingwang Yang; Yiran Zhao; Yuqing Liu; Huawei Jiang; Zhen Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648359"&gt;10.1109/tgrs.2025.3648359&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;With the continuous advancement of urbanization, the importance of building change detection in urban planning, disaster management, and other fields has become increasingly prominent. However, existing change detection methods still have significant limitations in spatial domain difference feature representation and multi-scale difference capture. To address this, we propose a novel Differential-Feature Dynamic-Fusion Network (DF2Net), which aims to improve the accuracy of building change detection in remote sensing images. DF2Net achieves efficient detection of change areas through the collaborative design of three key modules: Differential-Feature Frequency-domain Attention Module (DFAM), Global-Local Feature Dynamic-Fusion Module (GFDM), and Pseudo-label Deep-supervision Module (PDM). Specifically, DFAM combines Fourier transform and self-attention mechanisms to extract key information from the frequency domain, thereby improving the feature representation of change areas. GFDM dynamically fuses global and local features to effectively capture multi-scale change information. PDM optimizes feature learning in the middle layer of the decoder by introducing pseudo-label supervision. Experimental results on three mainstream building change detection datasets, LEVIR-CD, WHU-CD, and HRCUS-CD, show that DF2Net significantly outperforms existing mainstream methods in both F1-score and IoU, demonstrating superior performance. This indicates that DF2Net has broad application prospects in handling complex scenarios and multi-scale change detection tasks. The code available from https://github.com/Mw-yang/DF2Net.&lt;/p&gt;</content:encoded></item><item><title>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</title><link>https://arxiv.org/abs/2512.20174v1</link><guid>http://arxiv.org/abs/2512.20174v1</guid><pubDate>Tue, 23 Dec 2025 09:14:16 +0000</pubDate><dc:creator>Hao Guo</dc:creator><dc:creator>Xugong Qin</dc:creator><dc:creator>Jun Jie Ou Yang</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Gangyan Zeng</dc:creator><dc:creator>Yubo Li</dc:creator><dc:creator>Hailun Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.
Published: 2025-12-23T09:14:16+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Guo; Xugong Qin; Jun Jie Ou Yang; Peng Zhang; Gangyan Zeng; Yubo Li; Hailun Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.&lt;/p&gt;</content:encoded></item><item><title>CAHN-Net: Content-Adaptive Hierarchical Network for Cross-Modal Remote Sensing Object Detection</title><link>https://doi.org/10.1109/lgrs.2025.3648658</link><guid>10.1109/lgrs.2025.3648658</guid><pubDate>Thu, 25 Dec 2025 18:29:06 +0000</pubDate><dc:creator>Han Wang</dc:creator><dc:creator>Yiqing Li</dc:creator><dc:creator>Wen Zhou</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648658</prism:doi><description>Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.
Published: 2025-12-25T18:29:06+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Wang; Yiqing Li; Wen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648658"&gt;10.1109/lgrs.2025.3648658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.&lt;/p&gt;</content:encoded></item><item><title>LongVideoAgent: Multi-Agent Reasoning with Long Videos</title><link>https://arxiv.org/abs/2512.20618v1</link><guid>http://arxiv.org/abs/2512.20618v1</guid><pubDate>Tue, 23 Dec 2025 18:59:49 +0000</pubDate><dc:creator>Runtao Liu</dc:creator><dc:creator>Ziyi Liu</dc:creator><dc:creator>Jiaqi Tang</dc:creator><dc:creator>Yue Ma</dc:creator><dc:creator>Renjie Pi</dc:creator><dc:creator>Jipeng Zhang</dc:creator><dc:creator>Qifeng Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.
Published: 2025-12-23T18:59:49+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runtao Liu; Ziyi Liu; Jiaqi Tang; Yue Ma; Renjie Pi; Jipeng Zhang; Qifeng Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.&lt;/p&gt;</content:encoded></item><item><title>Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</title><link>https://arxiv.org/abs/2512.20556v1</link><guid>http://arxiv.org/abs/2512.20556v1</guid><pubDate>Tue, 23 Dec 2025 17:55:35 +0000</pubDate><dc:creator>Mingwei Tang</dc:creator><dc:creator>Jiahao Nie</dc:creator><dc:creator>Guang Yang</dc:creator><dc:creator>Ziqing Cui</dc:creator><dc:creator>Jie Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.
Published: 2025-12-23T17:55:35+00:00
Venue: arXiv
Score: 0.508 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingwei Tang; Jiahao Nie; Guang Yang; Ziqing Cui; Jie Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.508 (consider)&lt;/p&gt;
&lt;p&gt;Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.&lt;/p&gt;</content:encoded></item></channel></rss>