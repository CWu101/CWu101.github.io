<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 23 Jan 2026 02:50:45 +0000</lastBuildDate><item><title>Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts</title><link>https://doi.org/10.1109/tip.2026.3654473</link><guid>10.1109/tip.2026.3654473</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Zhong Ji</dc:creator><dc:creator>Rongshuai Wei</dc:creator><dc:creator>Jingren Liu</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Jungong Han</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654473</prism:doi><description>Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhong Ji; Rongshuai Wei; Jingren Liu; Yanwei Pang; Jungong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654473"&gt;10.1109/tip.2026.3654473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.&lt;/p&gt;</content:encoded></item><item><title>Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation</title><link>https://arxiv.org/abs/2601.14438v1</link><guid>http://arxiv.org/abs/2601.14438v1</guid><pubDate>Tue, 20 Jan 2026 19:50:42 +0000</pubDate><dc:creator>Danial Sadrian Zadeh</dc:creator><dc:creator>Otman A. Basir</dc:creator><dc:creator>Behzad Moshiri</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.
Published: 2026-01-20T19:50:42+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Danial Sadrian Zadeh; Otman A. Basir; Behzad Moshiri&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.&lt;/p&gt;</content:encoded></item><item><title>HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</title><link>https://arxiv.org/abs/2601.16155v1</link><guid>http://arxiv.org/abs/2601.16155v1</guid><pubDate>Thu, 22 Jan 2026 17:57:42 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Sihang Cai</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.
Published: 2026-01-22T17:57:42+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Xin Liu; Boyun Zhang; Yuxiao Lin; Sihang Cai; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &amp;quot;blind&amp;quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition</title><link>https://arxiv.org/abs/2601.12729v1</link><guid>http://arxiv.org/abs/2601.12729v1</guid><pubDate>Mon, 19 Jan 2026 05:19:56 +0000</pubDate><dc:creator>Hanyu Zhu</dc:creator><dc:creator>Zhihao Zhan</dc:creator><dc:creator>Yuhang Ming</dc:creator><dc:creator>Liang Li</dc:creator><dc:creator>Dibo Hou</dc:creator><dc:creator>Javier Civera</dc:creator><dc:creator>Wanzeng Kong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.
Published: 2026-01-19T05:19:56+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanyu Zhu; Zhihao Zhan; Yuhang Ming; Liang Li; Dibo Hou; Javier Civera; Wanzeng Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.&lt;/p&gt;</content:encoded></item><item><title>SigMa: Semantic Similarity-Guided Semi-Dense Feature Matching</title><link>https://doi.org/10.1109/tip.2026.3654367</link><guid>10.1109/tip.2026.3654367</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Xiang Fang</dc:creator><dc:creator>Zizhuo Li</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654367</prism:doi><description>Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Fang; Zizhuo Li; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654367"&gt;10.1109/tip.2026.3654367&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.&lt;/p&gt;</content:encoded></item><item><title>Video Decoupling Networks for Accurate, Efficient, Generalizable, and Robust Video Object Segmentation</title><link>https://doi.org/10.1109/tip.2025.3649360</link><guid>10.1109/tip.2025.3649360</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Jisheng Dang</dc:creator><dc:creator>Huicheng Zheng</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Jianhuang Lai</dc:creator><dc:creator>Bin Hu</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649360</prism:doi><description>object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Dang; Huicheng Zheng; Yulan Guo; Jianhuang Lai; Bin Hu; Tat-Seng Chua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649360"&gt;10.1109/tip.2025.3649360&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.&lt;/p&gt;</content:encoded></item><item><title>Retrieval-augmented Pseudo-image Guided Alignment and Text Domain-aware Memory Recall for Continual Zero-shot Captioning</title><link>https://doi.org/10.1109/tcsvt.2026.3656817</link><guid>10.1109/tcsvt.2026.3656817</guid><pubDate>Wed, 21 Jan 2026 21:12:03 +0000</pubDate><dc:creator>Bing Liu</dc:creator><dc:creator>Wenjie Yang</dc:creator><dc:creator>Mingming Liu</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Peng Liu</dc:creator><dc:creator>Yong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656817</prism:doi><description>Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.
Published: 2026-01-21T21:12:03+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bing Liu; Wenjie Yang; Mingming Liu; Hao Liu; Peng Liu; Yong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656817"&gt;10.1109/tcsvt.2026.3656817&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.&lt;/p&gt;</content:encoded></item><item><title>Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval</title><link>https://arxiv.org/abs/2601.12768v1</link><guid>http://arxiv.org/abs/2601.12768v1</guid><pubDate>Mon, 19 Jan 2026 06:55:33 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.
Published: 2026-01-19T06:55:33+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Boyun Zhang; Yuxiao Lin; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&amp;#x27;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.&lt;/p&gt;</content:encoded></item><item><title>VTFusion: A Vision–Text Multimodal Fusion Network for Few-Shot Anomaly Detection</title><link>https://doi.org/10.1109/tcyb.2026.3651630</link><guid>10.1109/tcyb.2026.3651630</guid><pubDate>Wed, 21 Jan 2026 21:10:54 +0000</pubDate><dc:creator>Yuxin Jiang</dc:creator><dc:creator>Yunkang Cao</dc:creator><dc:creator>Yuqi Cheng</dc:creator><dc:creator>Yiheng Zhang</dc:creator><dc:creator>Weiming Shen</dc:creator><prism:publicationName>IEEE Transactions on Cybernetics</prism:publicationName><prism:doi>10.1109/tcyb.2026.3651630</prism:doi><description>Few-shot anomaly detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pretrained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision–text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pretrained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level area under the receiver operating characteristics (AUROCs) of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this article, further demonstrating its practical applicability in demanding industrial scenarios.
Published: 2026-01-21T21:10:54+00:00
Venue: IEEE Transactions on Cybernetics
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Jiang; Yunkang Cao; Yuqi Cheng; Yiheng Zhang; Weiming Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Cybernetics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcyb.2026.3651630"&gt;10.1109/tcyb.2026.3651630&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot anomaly detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pretrained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision–text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pretrained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level area under the receiver operating characteristics (AUROCs) of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this article, further demonstrating its practical applicability in demanding industrial scenarios.&lt;/p&gt;</content:encoded></item><item><title>DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities</title><link>https://arxiv.org/abs/2601.13502v1</link><guid>http://arxiv.org/abs/2601.13502v1</guid><pubDate>Tue, 20 Jan 2026 01:33:54 +0000</pubDate><dc:creator>Nhi Kieu</dc:creator><dc:creator>Kien Nguyen</dc:creator><dc:creator>Arnold Wiliem</dc:creator><dc:creator>Clinton Fookes</dc:creator><dc:creator>Sridha Sridharan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.
Published: 2026-01-20T01:33:54+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nhi Kieu; Kien Nguyen; Arnold Wiliem; Clinton Fookes; Sridha Sridharan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Multi-Patch Perception and Knowledge-Guided Network for Semantic Segmentation of Large-Scale Remote Sensing Image</title><link>https://doi.org/10.1109/tgrs.2026.3656649</link><guid>10.1109/tgrs.2026.3656649</guid><pubDate>Wed, 21 Jan 2026 21:09:42 +0000</pubDate><dc:creator>Yijie Zhang</dc:creator><dc:creator>Jian Cheng</dc:creator><dc:creator>Ziying Xia</dc:creator><dc:creator>Wenqi Yin</dc:creator><dc:creator>Siyu Liu</dc:creator><dc:creator>Nyima Tashi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3656649</prism:doi><description>Semantic segmentation of large-scale remote sensing (LSRS) images has always been a hot research topic. Recently, methods have been proposed that segment LSRS images by dividing them into small patches, which results in the neglect of inter-patch connections during the segmentation process, thereby causing incomplete global semantic information. To alleviate this problem, we propose a multi-patch perception and knowledge-guided netwok (MPKG-Net) for semantic segmentation of LSRS images. The MPKG-Net has two significant characteristics: (1) Multi-Patch Perception: Initially, MPKG-Net constructs multi-patch feature representation for patches relevant to the current segmented patch. Subsequently, the multi-patch feature is further processed through the multi-patch perception branch built upon the global feature selection block (GFSB). The extracted multi-patch features are incorporated into all encoder stages of MPKG-Net, enabling the model to perceive abundant global semantic information. (2) Knowledge-Guided: To effectively utilize multi-patch features and enhance local features, we employ a pre-trained base model on large-scale remote sensing image datasets to construct the knowledge-guided branch. This branch processes features from both the multi-patch perception branch and the local feature capture branch through the knowledge-guided multi-patch feature filtering module (KG-MFFM) and the knowledge-guided local feature enhancement module (KG-LFEM), respectively. Finally, all features are fused stage-by-stage within the multi-feature fusion branch. Our proposed model has achieved excellent performance on multiple publicly available LSRS images semantic segmentation datasets. The code will be available at https://github.com/zhangyijie199703/MPKG-Net.
Published: 2026-01-21T21:09:42+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yijie Zhang; Jian Cheng; Ziying Xia; Wenqi Yin; Siyu Liu; Nyima Tashi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3656649"&gt;10.1109/tgrs.2026.3656649&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of large-scale remote sensing (LSRS) images has always been a hot research topic. Recently, methods have been proposed that segment LSRS images by dividing them into small patches, which results in the neglect of inter-patch connections during the segmentation process, thereby causing incomplete global semantic information. To alleviate this problem, we propose a multi-patch perception and knowledge-guided netwok (MPKG-Net) for semantic segmentation of LSRS images. The MPKG-Net has two significant characteristics: (1) Multi-Patch Perception: Initially, MPKG-Net constructs multi-patch feature representation for patches relevant to the current segmented patch. Subsequently, the multi-patch feature is further processed through the multi-patch perception branch built upon the global feature selection block (GFSB). The extracted multi-patch features are incorporated into all encoder stages of MPKG-Net, enabling the model to perceive abundant global semantic information. (2) Knowledge-Guided: To effectively utilize multi-patch features and enhance local features, we employ a pre-trained base model on large-scale remote sensing image datasets to construct the knowledge-guided branch. This branch processes features from both the multi-patch perception branch and the local feature capture branch through the knowledge-guided multi-patch feature filtering module (KG-MFFM) and the knowledge-guided local feature enhancement module (KG-LFEM), respectively. Finally, all features are fused stage-by-stage within the multi-feature fusion branch. Our proposed model has achieved excellent performance on multiple publicly available LSRS images semantic segmentation datasets. The code will be available at https://github.com/zhangyijie199703/MPKG-Net.&lt;/p&gt;</content:encoded></item><item><title>Natural Language-Driven Global Mapping of Martian Landforms</title><link>https://arxiv.org/abs/2601.15949v1</link><guid>http://arxiv.org/abs/2601.15949v1</guid><pubDate>Thu, 22 Jan 2026 13:38:13 +0000</pubDate><dc:creator>Yiran Wang</dc:creator><dc:creator>Shuoyuan Wang</dc:creator><dc:creator>Zhaoran Wei</dc:creator><dc:creator>Jiannan Zhao</dc:creator><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Zejian Xie</dc:creator><dc:creator>Songxin Zhang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Bingyi Jing</dc:creator><dc:creator>Hongxin Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.
Published: 2026-01-22T13:38:13+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiran Wang; Shuoyuan Wang; Zhaoran Wei; Jiannan Zhao; Zhonghua Yao; Zejian Xie; Songxin Zhang; Jun Huang; Bingyi Jing; Hongxin Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.&lt;/p&gt;</content:encoded></item><item><title>Unknown Category Classification by Transferring Knowledge from Known</title><link>https://doi.org/10.1109/tgrs.2026.3656512</link><guid>10.1109/tgrs.2026.3656512</guid><pubDate>Wed, 21 Jan 2026 21:09:42 +0000</pubDate><dc:creator>Tengfei Gong</dc:creator><dc:creator>Xianqi Liao</dc:creator><dc:creator>Yaxiong Chen</dc:creator><dc:creator>Yingchao Feng</dc:creator><dc:creator>Ning Li</dc:creator><dc:creator>Shengwu Xiong</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3656512</prism:doi><description>Classifying unknown remote sensing scenes is crucial for open-world applications. Existing open-vocabulary tasks leverage large pretrained visual-language models, such as CLIP, to recognize unknown categories. However, CLIP requires prior knowledge of class names. For unknown samples without prior information, identifying the class name is not resolved. Moreover, classification under domain shift scenarios presents additional challenges. Open set domain adaptation focuses on this scenario but only identifies unknown samples as ‘unknown’, lacking semantic interpretability of the unknown scenes. This paper focuses on assigning the semantic labels of unknown remote sensing images. It transfers knowledge from a labeled remote sensing dataset (source domain) to an unlabeled dataset (target domain) that contains unknown categories. Specifically, the pretrained language parser first extracts object information from the target domain image embeddings with pseudo-unknown distributions, and generates object-centric fusion labels to describe unknown remote sensing scenes, where the image embeddings are mainly generated by the CLIP image encoder. To further refine the prediction, a domain semantic prompt learning mechanism and alignment optimization objectives are designed to eliminate domain shift. Finally, experimental results on three public remote sensing datasets show that our method effectively classifies unknown categories and improves overall accuracy.
Published: 2026-01-21T21:09:42+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tengfei Gong; Xianqi Liao; Yaxiong Chen; Yingchao Feng; Ning Li; Shengwu Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3656512"&gt;10.1109/tgrs.2026.3656512&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Classifying unknown remote sensing scenes is crucial for open-world applications. Existing open-vocabulary tasks leverage large pretrained visual-language models, such as CLIP, to recognize unknown categories. However, CLIP requires prior knowledge of class names. For unknown samples without prior information, identifying the class name is not resolved. Moreover, classification under domain shift scenarios presents additional challenges. Open set domain adaptation focuses on this scenario but only identifies unknown samples as ‘unknown’, lacking semantic interpretability of the unknown scenes. This paper focuses on assigning the semantic labels of unknown remote sensing images. It transfers knowledge from a labeled remote sensing dataset (source domain) to an unlabeled dataset (target domain) that contains unknown categories. Specifically, the pretrained language parser first extracts object information from the target domain image embeddings with pseudo-unknown distributions, and generates object-centric fusion labels to describe unknown remote sensing scenes, where the image embeddings are mainly generated by the CLIP image encoder. To further refine the prediction, a domain semantic prompt learning mechanism and alignment optimization objectives are designed to eliminate domain shift. Finally, experimental results on three public remote sensing datasets show that our method effectively classifies unknown categories and improves overall accuracy.&lt;/p&gt;</content:encoded></item><item><title>ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</title><link>https://arxiv.org/abs/2601.14757v1</link><guid>http://arxiv.org/abs/2601.14757v1</guid><pubDate>Wed, 21 Jan 2026 08:21:35 +0000</pubDate><dc:creator>Kangcheng Zhou</dc:creator><dc:creator>Jun Jiang</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Shuang Zheng</dc:creator><dc:creator>Qingli Li</dc:creator><dc:creator>Shugong Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.
Published: 2026-01-21T08:21:35+00:00
Venue: arXiv
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kangcheng Zhou; Jun Jiang; Qing Zhang; Shuang Zheng; Qingli Li; Shugong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.&lt;/p&gt;</content:encoded></item><item><title>Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders</title><link>https://arxiv.org/abs/2601.13798v1</link><guid>http://arxiv.org/abs/2601.13798v1</guid><pubDate>Tue, 20 Jan 2026 09:57:26 +0000</pubDate><dc:creator>Kai Wittenmayer</dc:creator><dc:creator>Sukrut Rao</dc:creator><dc:creator>Amin Parchami-Araghi</dc:creator><dc:creator>Bernt Schiele</dc:creator><dc:creator>Jonas Fischer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.
Published: 2026-01-20T09:57:26+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Wittenmayer; Sukrut Rao; Amin Parchami-Araghi; Bernt Schiele; Jonas Fischer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.&lt;/p&gt;</content:encoded></item><item><title>DVD: A Debiased Visual Dialog Model via Disentangling Knowledge Features</title><link>https://doi.org/10.1109/tmm.2026.3654405</link><guid>10.1109/tmm.2026.3654405</guid><pubDate>Wed, 21 Jan 2026 21:10:50 +0000</pubDate><dc:creator>Chenyu Lu</dc:creator><dc:creator>Jing Zhao</dc:creator><dc:creator>Shiliang Sun</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654405</prism:doi><description>Visual dialog aims to facilitate the answering of multi-round questions by effectively integrating dialog history and the relevant content of images. Existing methods in visual dialog predominantly concentrate on devising multi-modal data interaction architectures to augment multi-modal fusion performance, but they often disregard inherent dataset selection biases. This oversight can lead to imbalanced feature learning and compromising the robustness of the model. In this paper, we propose a Debiased Visual Dialog model (DVD) to mitigate the influence of biases. Specifically, we concretize these biases as spurious relationships between foreground and background knowledge in both image and dialog history modalities and design a dual-encoding workflow to disentangle them effectively. Additionally, we introduce a knowledge bias indicator for each sample, enabling us to assess and quantify the impact of biases on the learning process. By employing a generalized cross-entropy loss, we enhance the distinction of knowledge biases, which significantly improves the efficiency of feature disentanglement. Extensive comparative experiments against state-of-the-art methods, along with ablation studies, validate the effectiveness of our DVD model. These results also substantiate the promising potential of debiasing efforts in advancing the field of visual dialog and vision-language research.
Published: 2026-01-21T21:10:50+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyu Lu; Jing Zhao; Shiliang Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654405"&gt;10.1109/tmm.2026.3654405&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Visual dialog aims to facilitate the answering of multi-round questions by effectively integrating dialog history and the relevant content of images. Existing methods in visual dialog predominantly concentrate on devising multi-modal data interaction architectures to augment multi-modal fusion performance, but they often disregard inherent dataset selection biases. This oversight can lead to imbalanced feature learning and compromising the robustness of the model. In this paper, we propose a Debiased Visual Dialog model (DVD) to mitigate the influence of biases. Specifically, we concretize these biases as spurious relationships between foreground and background knowledge in both image and dialog history modalities and design a dual-encoding workflow to disentangle them effectively. Additionally, we introduce a knowledge bias indicator for each sample, enabling us to assess and quantify the impact of biases on the learning process. By employing a generalized cross-entropy loss, we enhance the distinction of knowledge biases, which significantly improves the efficiency of feature disentanglement. Extensive comparative experiments against state-of-the-art methods, along with ablation studies, validate the effectiveness of our DVD model. These results also substantiate the promising potential of debiasing efforts in advancing the field of visual dialog and vision-language research.&lt;/p&gt;</content:encoded></item><item><title>Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation</title><link>https://arxiv.org/abs/2601.12964v1</link><guid>http://arxiv.org/abs/2601.12964v1</guid><pubDate>Mon, 19 Jan 2026 11:21:19 +0000</pubDate><dc:creator>John Waithaka</dc:creator><dc:creator>Gustave Bwirayesu</dc:creator><dc:creator>Moise Busogi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.
Published: 2026-01-19T11:21:19+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; John Waithaka; Gustave Bwirayesu; Moise Busogi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.&lt;/p&gt;</content:encoded></item><item><title>CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.13622v1</link><guid>http://arxiv.org/abs/2601.13622v1</guid><pubDate>Tue, 20 Jan 2026 05:44:33 +0000</pubDate><dc:creator>Donghee Lee</dc:creator><dc:creator>Rui Cai</dc:creator><dc:creator>Zhe Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.
Published: 2026-01-20T05:44:33+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Donghee Lee; Rui Cai; Zhe Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&amp;#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.&lt;/p&gt;</content:encoded></item><item><title>Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</title><link>https://arxiv.org/abs/2601.15780v1</link><guid>http://arxiv.org/abs/2601.15780v1</guid><pubDate>Thu, 22 Jan 2026 09:14:11 +0000</pubDate><dc:creator>Pascal Benschop</dc:creator><dc:creator>Justin Dauwels</dc:creator><dc:creator>Jan van Gemert</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.
Published: 2026-01-22T09:14:11+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pascal Benschop; Justin Dauwels; Jan van Gemert&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.&lt;/p&gt;</content:encoded></item><item><title>M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention</title><link>https://arxiv.org/abs/2601.14776v1</link><guid>http://arxiv.org/abs/2601.14776v1</guid><pubDate>Wed, 21 Jan 2026 08:55:07 +0000</pubDate><dc:creator>Xiaofan Yang</dc:creator><dc:creator>Yubin Liu</dc:creator><dc:creator>Wei Pan</dc:creator><dc:creator>Guoqing Chu</dc:creator><dc:creator>Junming Zhang</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Zhuoqi Man</dc:creator><dc:creator>Xuanming Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.
Published: 2026-01-21T08:55:07+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaofan Yang; Yubin Liu; Wei Pan; Guoqing Chu; Junming Zhang; Jie Zhao; Zhuoqi Man; Xuanming Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search</title><link>https://arxiv.org/abs/2601.15931v1</link><guid>http://arxiv.org/abs/2601.15931v1</guid><pubDate>Thu, 22 Jan 2026 13:09:22 +0000</pubDate><dc:creator>Xiangyu Wang</dc:creator><dc:creator>Zhixin Lv</dc:creator><dc:creator>Yongjiao Sun</dc:creator><dc:creator>Anrui Han</dc:creator><dc:creator>Ye Yuan</dc:creator><dc:creator>Hangxu Ji</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.
Published: 2026-01-22T13:09:22+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Wang; Zhixin Lv; Yongjiao Sun; Anrui Han; Ye Yuan; Hangxu Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &amp;quot;Passive Observation&amp;quot; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Multi-Task Visual Representation Learning</title><link>https://arxiv.org/abs/2601.13886v1</link><guid>http://arxiv.org/abs/2601.13886v1</guid><pubDate>Tue, 20 Jan 2026 11:59:19 +0000</pubDate><dc:creator>Shangzhe Di</dc:creator><dc:creator>Zhonghua Zhai</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.
Published: 2026-01-20T11:59:19+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangzhe Di; Zhonghua Zhai; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &amp;quot;expert&amp;quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &amp;quot;best-of-both-worlds&amp;quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.&lt;/p&gt;</content:encoded></item><item><title>GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</title><link>https://doi.org/10.1007/s11263-026-02729-y</link><guid>10.1007/s11263-026-02729-y</guid><pubDate>Wed, 21 Jan 2026 11:26:30 +0000</pubDate><dc:creator>Henghui Ding</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Shuting He</dc:creator><dc:creator>Xudong Jiang</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02729-y</prism:doi><description>Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies.The proposed ReLA achieves the state-of-the-art results on both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GRES .
Published: 2026-01-21T11:26:30+00:00
Venue: International Journal of Computer Vision
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Henghui Ding; Chang Liu; Shuting He; Xudong Jiang; Yu-Gang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02729-y"&gt;10.1007/s11263-026-02729-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies.The proposed ReLA achieves the state-of-the-art results on both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GRES .&lt;/p&gt;</content:encoded></item><item><title>Dual-Stream Collaborative Transformer for Image Captioning</title><link>https://arxiv.org/abs/2601.12926v1</link><guid>http://arxiv.org/abs/2601.12926v1</guid><pubDate>Mon, 19 Jan 2026 10:28:56 +0000</pubDate><dc:creator>Jun Wan</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Zhihui lai</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.
Published: 2026-01-19T10:28:56+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun Wan; Jun Liu; Zhihui lai; Jie Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.&lt;/p&gt;</content:encoded></item><item><title>Optimizing KBQA by Correcting LLM-Generated Non-Executable Logical Form through Knowledge-Assisted Path Reconstruction</title><link>https://doi.org/10.1109/tkde.2026.3656646</link><guid>10.1109/tkde.2026.3656646</guid><pubDate>Wed, 21 Jan 2026 21:11:50 +0000</pubDate><dc:creator>Ranran Bu</dc:creator><dc:creator>Jianqi Gao</dc:creator><dc:creator>Jian Cao</dc:creator><dc:creator>Hongming Cai</dc:creator><dc:creator>Jinghua Tang</dc:creator><dc:creator>Yonggang Zhang</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2026.3656646</prism:doi><description>Knowledge base question answering (KBQA) refers to the task of answering natural language questions using factual information from large-scale knowledge bases (KBs). To obtain accurate answers, recent research optimizes semantic parsing methods, one of the major KBQA approaches, with large language models (LLMs), where concise logical forms (LFs) are generated by LLMs and executed in KBs. Although the methods demonstrate superior performance, they continue to encounter the problem that a portion of the generated LFs fail to yield answers when executed, significantly limiting their effectiveness. To mitigate the issue, we propose KARV, a Knowledge-Assisted reasoning path Reconstruction and hierarchical Voting approach for non-executable LFs. This method extracts semantic knowledge from KBs as guidance to correct and reconstruct reasoning paths, and derives answers through a voting-based strategy. The insight is that the non-executable LFs generated by LLMs still have rich semantic information, and the knowledge retrieved and inferred from KBs using this information can in turn effectively correct the non-executable LFs. Specifically, we fine-tune LLMs to generate high-quality LFs. For non-executable LFs, we decompose each LF into multiple path branches based on the entities mentioned in the LF. The semantic knowledge from KBs is then leveraged to correct the entities and relations within these branches, effectively reconstructing the reasoning paths. To obtain precise final answers from these paths, we apply a hierarchical voting strategy both within and across the non-executable LFs. Our proposed method achieves new state-of-the-art performance on WebQuestionSP (WebQSP), ComplexWebQuestions (CWQ) and FreebaseQA KBQA benchmarks.
Published: 2026-01-21T21:11:50+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ranran Bu; Jianqi Gao; Jian Cao; Hongming Cai; Jinghua Tang; Yonggang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2026.3656646"&gt;10.1109/tkde.2026.3656646&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge base question answering (KBQA) refers to the task of answering natural language questions using factual information from large-scale knowledge bases (KBs). To obtain accurate answers, recent research optimizes semantic parsing methods, one of the major KBQA approaches, with large language models (LLMs), where concise logical forms (LFs) are generated by LLMs and executed in KBs. Although the methods demonstrate superior performance, they continue to encounter the problem that a portion of the generated LFs fail to yield answers when executed, significantly limiting their effectiveness. To mitigate the issue, we propose KARV, a Knowledge-Assisted reasoning path Reconstruction and hierarchical Voting approach for non-executable LFs. This method extracts semantic knowledge from KBs as guidance to correct and reconstruct reasoning paths, and derives answers through a voting-based strategy. The insight is that the non-executable LFs generated by LLMs still have rich semantic information, and the knowledge retrieved and inferred from KBs using this information can in turn effectively correct the non-executable LFs. Specifically, we fine-tune LLMs to generate high-quality LFs. For non-executable LFs, we decompose each LF into multiple path branches based on the entities mentioned in the LF. The semantic knowledge from KBs is then leveraged to correct the entities and relations within these branches, effectively reconstructing the reasoning paths. To obtain precise final answers from these paths, we apply a hierarchical voting strategy both within and across the non-executable LFs. Our proposed method achieves new state-of-the-art performance on WebQuestionSP (WebQSP), ComplexWebQuestions (CWQ) and FreebaseQA KBQA benchmarks.&lt;/p&gt;</content:encoded></item><item><title>MMFormer: Multi-Modality Semi-Supervised Vision Transformer in Remote Sensing Imagery Classification</title><link>https://doi.org/10.1016/j.neunet.2026.108628</link><guid>10.1016/j.neunet.2026.108628</guid><pubDate>Wed, 21 Jan 2026 16:00:23 +0000</pubDate><dc:creator>Daixun Li</dc:creator><dc:creator>Weiying Xie</dc:creator><dc:creator>Leyuan Fang</dc:creator><dc:creator>Yunke Wang</dc:creator><dc:creator>Zirui Li</dc:creator><dc:creator>Mingxiang Cao</dc:creator><dc:creator>Jitao Ma</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Chang Xu</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108628</prism:doi><description>Significant progress has been made in the application of transformer architectures for multimodal tasks. However, current methods such as the self-attention mechanism rarely consider the benefits that feature complementarity and consistency between different modalities bring to fusion, leading to obstacles such as redundant fusion or incomplete representation. Inspired by topological homology groups, we introduce MMFormer, a novel semi-supervised algorithm for high-dimensional multimodal fusion. This method is engineered to capture comprehensive representations by enhancing the interactivity between modal mappings. Specifically, we advocate for the representational consistency between these heterogeneous representations through a complete dictionary lookup and homology space in the encoder, and establish an exclusivity-aware mapping of the two modalities to emphasize their complementary information, serving as a powerful supplement for multimodal feature interpretation. Moreover, the model attempts to alleviate the challenge of sparse annotations in high-dimensional multimodal data by introducing a consistency joint regularization term. We have formulated these focuses into a unified end-to-end optimization framework and are the first to explore and derive the application of semi-supervised visual transformers in high-dimensional multimodal data fusion. Extensive experiments across three benchmarks demonstrate the superiority of MMFormer. Specifically, the model improves overall accuracy by 3.12% on Houston2013, 1.86% on Augsburg, and 1.66% on MUUFL compared with the strongest existing methods, confirming its robustness and effectiveness under sparse annotation conditions. The code is available at https://github.com/LDXDU/MMFormer .
Published: 2026-01-21T16:00:23+00:00
Venue: Neural Networks
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daixun Li; Weiying Xie; Leyuan Fang; Yunke Wang; Zirui Li; Mingxiang Cao; Jitao Ma; Yunsong Li; Chang Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108628"&gt;10.1016/j.neunet.2026.108628&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Significant progress has been made in the application of transformer architectures for multimodal tasks. However, current methods such as the self-attention mechanism rarely consider the benefits that feature complementarity and consistency between different modalities bring to fusion, leading to obstacles such as redundant fusion or incomplete representation. Inspired by topological homology groups, we introduce MMFormer, a novel semi-supervised algorithm for high-dimensional multimodal fusion. This method is engineered to capture comprehensive representations by enhancing the interactivity between modal mappings. Specifically, we advocate for the representational consistency between these heterogeneous representations through a complete dictionary lookup and homology space in the encoder, and establish an exclusivity-aware mapping of the two modalities to emphasize their complementary information, serving as a powerful supplement for multimodal feature interpretation. Moreover, the model attempts to alleviate the challenge of sparse annotations in high-dimensional multimodal data by introducing a consistency joint regularization term. We have formulated these focuses into a unified end-to-end optimization framework and are the first to explore and derive the application of semi-supervised visual transformers in high-dimensional multimodal data fusion. Extensive experiments across three benchmarks demonstrate the superiority of MMFormer. Specifically, the model improves overall accuracy by 3.12% on Houston2013, 1.86% on Augsburg, and 1.66% on MUUFL compared with the strongest existing methods, confirming its robustness and effectiveness under sparse annotation conditions. The code is available at https://github.com/LDXDU/MMFormer .&lt;/p&gt;</content:encoded></item><item><title>AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.14702v1</link><guid>http://arxiv.org/abs/2601.14702v1</guid><pubDate>Wed, 21 Jan 2026 06:29:09 +0000</pubDate><dc:creator>Zecong Tang</dc:creator><dc:creator>Zixu Wang</dc:creator><dc:creator>Yifei Wang</dc:creator><dc:creator>Weitong Lian</dc:creator><dc:creator>Tianjian Gao</dc:creator><dc:creator>Haoran Li</dc:creator><dc:creator>Tengju Ru</dc:creator><dc:creator>Lingyi Meng</dc:creator><dc:creator>Zhejun Cui</dc:creator><dc:creator>Yichen Zhu</dc:creator><dc:creator>Qi Kang</dc:creator><dc:creator>Kaixuan Wang</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.
Published: 2026-01-21T06:29:09+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zecong Tang; Zixu Wang; Yifei Wang; Weitong Lian; Tianjian Gao; Haoran Li; Tengju Ru; Lingyi Meng; Zhejun Cui; Yichen Zhu; Qi Kang; Kaixuan Wang; Yu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&amp;#x27; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI</title><link>https://arxiv.org/abs/2601.14055v1</link><guid>http://arxiv.org/abs/2601.14055v1</guid><pubDate>Tue, 20 Jan 2026 15:13:04 +0000</pubDate><dc:creator>Andrea Protani</dc:creator><dc:creator>Marc Molina Van Den Bosch</dc:creator><dc:creator>Lorenzo Giusti</dc:creator><dc:creator>Heloisa Barbosa Da Silva</dc:creator><dc:creator>Paolo Cacace</dc:creator><dc:creator>Albert Sund Aillet</dc:creator><dc:creator>Miguel Angel Gonzalez Ballester</dc:creator><dc:creator>Friedhelm Hummel</dc:creator><dc:creator>Luigi Serio</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.
Published: 2026-01-20T15:13:04+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Andrea Protani; Marc Molina Van Den Bosch; Lorenzo Giusti; Heloisa Barbosa Da Silva; Paolo Cacace; Albert Sund Aillet; Miguel Angel Gonzalez Ballester; Friedhelm Hummel; Luigi Serio&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework&amp;#x27;s flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder&amp;#x27;s ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.&lt;/p&gt;</content:encoded></item><item><title>Reasoning is a Modality</title><link>https://arxiv.org/abs/2601.13562v1</link><guid>http://arxiv.org/abs/2601.13562v1</guid><pubDate>Tue, 20 Jan 2026 03:37:17 +0000</pubDate><dc:creator>Zhiguang Liu</dc:creator><dc:creator>Yi Shang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.
Published: 2026-01-20T03:37:17+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiguang Liu; Yi Shang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.&lt;/p&gt;</content:encoded></item><item><title>VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration</title><link>https://arxiv.org/abs/2601.14440v1</link><guid>http://arxiv.org/abs/2601.14440v1</guid><pubDate>Tue, 20 Jan 2026 19:54:49 +0000</pubDate><dc:creator>Saeed Khaki</dc:creator><dc:creator>Ashudeep Singh</dc:creator><dc:creator>Nima Safaei</dc:creator><dc:creator>Kamal Ginotra</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.
Published: 2026-01-20T19:54:49+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Saeed Khaki; Ashudeep Singh; Nima Safaei; Kamal Ginotra&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.&lt;/p&gt;</content:encoded></item></channel></rss>