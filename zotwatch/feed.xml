<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 29 Dec 2025 02:55:13 +0000</lastBuildDate><item><title>Pythia-RAG: Retrieval-Augmented Generation over a Unified Multimodal Knowledge Graph for Enhanced QA</title><link>https://doi.org/10.1016/j.knosys.2025.115200</link><guid>10.1016/j.knosys.2025.115200</guid><pubDate>Sat, 27 Dec 2025 00:06:50 +0000</pubDate><dc:creator>Zafar Ali</dc:creator><dc:creator>Yi Huang</dc:creator><dc:creator>Asad Khan</dc:creator><dc:creator>Guilin Qi</dc:creator><dc:creator>Yuxin Zhang</dc:creator><dc:creator>Junlan Feng</dc:creator><dc:creator>Chao Deng</dc:creator><dc:creator>Pavlos Kefalas</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115200</prism:doi><description>The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.
Published: 2025-12-27T00:06:50+00:00
Venue: Knowledge-Based Systems
Score: 0.582 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zafar Ali; Yi Huang; Asad Khan; Guilin Qi; Yuxin Zhang; Junlan Feng; Chao Deng; Pavlos Kefalas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115200"&gt;10.1016/j.knosys.2025.115200&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.582 (consider)&lt;/p&gt;
&lt;p&gt;The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.&lt;/p&gt;</content:encoded></item><item><title>See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</title><link>https://arxiv.org/abs/2512.22120v1</link><guid>http://arxiv.org/abs/2512.22120v1</guid><pubDate>Fri, 26 Dec 2025 18:59:47 +0000</pubDate><dc:creator>Shuoshuo Zhang</dc:creator><dc:creator>Yizhen Zhang</dc:creator><dc:creator>Jingjing Fu</dc:creator><dc:creator>Lei Song</dc:creator><dc:creator>Jiang Bian</dc:creator><dc:creator>Yujiu Yang</dc:creator><dc:creator>Rui Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.
Published: 2025-12-26T18:59:47+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuoshuo Zhang; Yizhen Zhang; Jingjing Fu; Lei Song; Jiang Bian; Yujiu Yang; Rui Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.&lt;/p&gt;</content:encoded></item><item><title>Trajectory semantics-based graph convolutional network for taxi demand forecasting</title><link>https://doi.org/10.1016/j.ins.2025.123051</link><guid>10.1016/j.ins.2025.123051</guid><pubDate>Sat, 27 Dec 2025 00:00:32 +0000</pubDate><dc:creator>Chaolong Jia</dc:creator><dc:creator>Siyan Huang</dc:creator><dc:creator>Wenxiao Zhang</dc:creator><dc:creator>Wenjing Zhang</dc:creator><dc:creator>Rong Wang</dc:creator><dc:creator>Yunpeng Xiao</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123051</prism:doi><description>Taxi demand forecasting is vital for improving the efficiency of urban transportation systems and supporting intelligent mobility management. However, most existing machine learning approaches either focus solely on temporal sequence modeling or spatial dependency learning, while overlooking the latent semantic correlations embedded in human mobility trajectories, which limits their generalization and interpretability. Inspired by natural language processing’s contextual semantic analysis, this study models the start and end points of trajectory data as nodes, with a collection of these nodes analogous to words or articles. We propose a Trajectory Semantics-based Graph Convolutional Network (TSGCN) for taxi demand prediction. First, a Node Context Matrix (NC-Matrix) is constructed to model potential spatiotemporal correlations among nodes, similar to contextual dependencies in textual semantics. Next, we introduce two graph convolutional modules, the local spatiotemporal graph convolution module (LSTGCM) and the global spatiotemporal graph convolution module (GSTGCM), to extract multi-scale spatiotemporal features. Finally, inspired by the idea that articles with similar keywords often describe related topics, a demand pattern frequency algorithm is designed to identify functional similarities between distant regions. Experiments on the New York City taxi dataset confirm that TSGCN achieves superior performance over existing baselines by capturing both spatial–temporal and semantic dependencies.
Published: 2025-12-27T00:00:32+00:00
Venue: Information Sciences
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaolong Jia; Siyan Huang; Wenxiao Zhang; Wenjing Zhang; Rong Wang; Yunpeng Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123051"&gt;10.1016/j.ins.2025.123051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Taxi demand forecasting is vital for improving the efficiency of urban transportation systems and supporting intelligent mobility management. However, most existing machine learning approaches either focus solely on temporal sequence modeling or spatial dependency learning, while overlooking the latent semantic correlations embedded in human mobility trajectories, which limits their generalization and interpretability. Inspired by natural language processing’s contextual semantic analysis, this study models the start and end points of trajectory data as nodes, with a collection of these nodes analogous to words or articles. We propose a Trajectory Semantics-based Graph Convolutional Network (TSGCN) for taxi demand prediction. First, a Node Context Matrix (NC-Matrix) is constructed to model potential spatiotemporal correlations among nodes, similar to contextual dependencies in textual semantics. Next, we introduce two graph convolutional modules, the local spatiotemporal graph convolution module (LSTGCM) and the global spatiotemporal graph convolution module (GSTGCM), to extract multi-scale spatiotemporal features. Finally, inspired by the idea that articles with similar keywords often describe related topics, a demand pattern frequency algorithm is designed to identify functional similarities between distant regions. Experiments on the New York City taxi dataset confirm that TSGCN achieves superior performance over existing baselines by capturing both spatial–temporal and semantic dependencies.&lt;/p&gt;</content:encoded></item><item><title>Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</title><link>https://arxiv.org/abs/2512.21778v1</link><guid>http://arxiv.org/abs/2512.21778v1</guid><pubDate>Thu, 25 Dec 2025 20:31:36 +0000</pubDate><dc:creator>Nimrod Berman</dc:creator><dc:creator>Adam Botach</dc:creator><dc:creator>Emanuel Ben-Baruch</dc:creator><dc:creator>Shunit Haviv Hakimi</dc:creator><dc:creator>Asaf Gendler</dc:creator><dc:creator>Ilan Naiman</dc:creator><dc:creator>Erez Yosef</dc:creator><dc:creator>Igor Kviatkovsky</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.
Published: 2025-12-25T20:31:36+00:00
Venue: arXiv
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nimrod Berman; Adam Botach; Emanuel Ben-Baruch; Shunit Haviv Hakimi; Asaf Gendler; Ilan Naiman; Erez Yosef; Igor Kviatkovsky&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.&lt;/p&gt;</content:encoded></item><item><title>Multi-Perspective Information Fusion Network for Remote Sensing Segmentation</title><link>https://doi.org/10.3390/rs18010100</link><guid>10.3390/rs18010100</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Jianchao Liu</dc:creator><dc:creator>Shuli Cheng</dc:creator><dc:creator>Anyu Du</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010100</prism:doi><description>Remote sensing acquires Earth surface information without physical contact through sensors operating at diverse spatial, spectral, and temporal resolutions. In high-resolution remote sensing imagery, objects often exhibit large scale variation, complex spatial distributions, and strong inter-class similarity, posing persistent challenges for accurate semantic segmentation. Existing methods still struggle to simultaneously preserve fine boundary details and model long-range spatial dependencies, and lack explicit mechanisms to decouple low-frequency semantic context from high-frequency structural information. To address these limitations, we propose the Multi-Perspective Information Fusion Network (MPIFNet) for remote sensing semantic segmentation, motivated by the need to integrate global context, local structures, and multi-frequency information into a unified framework. MPIFNet employs a Global and Local Mamba Block Self-Attention (GLMBSA) module to capture long-range dependencies while preserving local details, and a Double-Branch Haar Wavelet Transform (DBHWT) module to separate and enhance low- and high-frequency features. By fusing spatial, hierarchical, and frequency representations, MPIFNet learns more discriminative and robust features. Evaluations on the Vaihingen, Potsdam, and LoveDA datasets through ablation and comparative studies highlight the strong generalization of our model, yielding mIoU results of 86.03%, 88.36%, and 55.76%.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianchao Liu; Shuli Cheng; Anyu Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010100"&gt;10.3390/rs18010100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing acquires Earth surface information without physical contact through sensors operating at diverse spatial, spectral, and temporal resolutions. In high-resolution remote sensing imagery, objects often exhibit large scale variation, complex spatial distributions, and strong inter-class similarity, posing persistent challenges for accurate semantic segmentation. Existing methods still struggle to simultaneously preserve fine boundary details and model long-range spatial dependencies, and lack explicit mechanisms to decouple low-frequency semantic context from high-frequency structural information. To address these limitations, we propose the Multi-Perspective Information Fusion Network (MPIFNet) for remote sensing semantic segmentation, motivated by the need to integrate global context, local structures, and multi-frequency information into a unified framework. MPIFNet employs a Global and Local Mamba Block Self-Attention (GLMBSA) module to capture long-range dependencies while preserving local details, and a Double-Branch Haar Wavelet Transform (DBHWT) module to separate and enhance low- and high-frequency features. By fusing spatial, hierarchical, and frequency representations, MPIFNet learns more discriminative and robust features. Evaluations on the Vaihingen, Potsdam, and LoveDA datasets through ablation and comparative studies highlight the strong generalization of our model, yielding mIoU results of 86.03%, 88.36%, and 55.76%.&lt;/p&gt;</content:encoded></item><item><title>Text-to-Graph Query Using Semantic Subgraph Retrieval</title><link>https://doi.org/10.1016/j.knosys.2025.115211</link><guid>10.1016/j.knosys.2025.115211</guid><pubDate>Sat, 27 Dec 2025 16:14:39 +0000</pubDate><dc:creator>Yongzhe Jia</dc:creator><dc:creator>Yiming Lei</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Xin Wang</dc:creator><dc:creator>Dawei Xu</dc:creator><dc:creator>Xiangyu Ji</dc:creator><dc:creator>Jianguo Wei</dc:creator><dc:creator>Muhammad Tahir</dc:creator><dc:creator>Yurong Qian</dc:creator><dc:creator>Wushour Silamu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115211</prism:doi><description>The advent of advanced artificial intelligence has underscored the significance of graph databases for managing complex relationships and semantic data. Natural Language to Graph Query Language (NL2GQL) aims to bridge human interaction with graph databases through natural language, emerging as a pivotal research area. Existing methods, however, encounter challenges such as insufficient availability of high-quality training datasets, suboptimal accuracy in query translation, limited cross-domain generalization capabilities, and inadequate support for semantic retrieval by graph databases. This paper introduces a novel framework that leverages Large Language Models (LLMs) to enhance NL2GQL dataset generation and improve the semantic accuracy of queries. Additionally, we propose an innovative graph database architecture incorporating vector-based retrieval for efficient semantic subgraph queries. The experimental results demonstrate that the proposed method achieves 90% accuracy in query generation, representing a 30% improvement over state-of-the-art benchmarks in cross-domain tasks through semantic similarity retrieval. A unified vector-graph paradigm enhances query throughput by an order of magnitude (10 × ) compared to conventional graph engines. The code is available at https://github.com/anxiaozu/TGQ-SSR.git .
Published: 2025-12-27T16:14:39+00:00
Venue: Knowledge-Based Systems
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongzhe Jia; Yiming Lei; Yang Liu; Xin Wang; Dawei Xu; Xiangyu Ji; Jianguo Wei; Muhammad Tahir; Yurong Qian; Wushour Silamu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115211"&gt;10.1016/j.knosys.2025.115211&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;The advent of advanced artificial intelligence has underscored the significance of graph databases for managing complex relationships and semantic data. Natural Language to Graph Query Language (NL2GQL) aims to bridge human interaction with graph databases through natural language, emerging as a pivotal research area. Existing methods, however, encounter challenges such as insufficient availability of high-quality training datasets, suboptimal accuracy in query translation, limited cross-domain generalization capabilities, and inadequate support for semantic retrieval by graph databases. This paper introduces a novel framework that leverages Large Language Models (LLMs) to enhance NL2GQL dataset generation and improve the semantic accuracy of queries. Additionally, we propose an innovative graph database architecture incorporating vector-based retrieval for efficient semantic subgraph queries. The experimental results demonstrate that the proposed method achieves 90% accuracy in query generation, representing a 30% improvement over state-of-the-art benchmarks in cross-domain tasks through semantic similarity retrieval. A unified vector-graph paradigm enhances query throughput by an order of magnitude (10 × ) compared to conventional graph engines. The code is available at https://github.com/anxiaozu/TGQ-SSR.git .&lt;/p&gt;</content:encoded></item><item><title>LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis</title><link>https://arxiv.org/abs/2512.21482v1</link><guid>http://arxiv.org/abs/2512.21482v1</guid><pubDate>Thu, 25 Dec 2025 03:02:27 +0000</pubDate><dc:creator>Fanwei Zeng</dc:creator><dc:creator>Changtao Miao</dc:creator><dc:creator>Jing Huang</dc:creator><dc:creator>Zhiya Tan</dc:creator><dc:creator>Shutao Gong</dc:creator><dc:creator>Xiaoming Yu</dc:creator><dc:creator>Yang Wang</dc:creator><dc:creator>Huazhe Tan</dc:creator><dc:creator>Weibin Yao</dc:creator><dc:creator>Jianshu Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.
Published: 2025-12-25T03:02:27+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fanwei Zeng; Changtao Miao; Jing Huang; Zhiya Tan; Shutao Gong; Xiaoming Yu; Yang Wang; Huazhe Tan; Weibin Yao; Jianshu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer</title><link>https://arxiv.org/abs/2512.21883v1</link><guid>http://arxiv.org/abs/2512.21883v1</guid><pubDate>Fri, 26 Dec 2025 06:12:17 +0000</pubDate><dc:creator>Tianchen Deng</dc:creator><dc:creator>Wenhua Wu</dc:creator><dc:creator>Kunzhen Wu</dc:creator><dc:creator>Guangming Wang</dc:creator><dc:creator>Siting Zhu</dc:creator><dc:creator>Shenghai Yuan</dc:creator><dc:creator>Xun Chen</dc:creator><dc:creator>Guole Shen</dc:creator><dc:creator>Zhe Liu</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.
Published: 2025-12-26T06:12:17+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianchen Deng; Wenhua Wu; Kunzhen Wu; Guangming Wang; Siting Zhu; Shenghai Yuan; Xun Chen; Guole Shen; Zhe Liu; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.&lt;/p&gt;</content:encoded></item><item><title>Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.21683v1</link><guid>http://arxiv.org/abs/2512.21683v1</guid><pubDate>Thu, 25 Dec 2025 14:00:17 +0000</pubDate><dc:creator>Yuntian Bo</dc:creator><dc:creator>Tao Zhou</dc:creator><dc:creator>Zechao Li</dc:creator><dc:creator>Haofeng Zhang</dc:creator><dc:creator>Ling Shao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-domain few-shot medical image segmentation (CD-FSMIS) offers a promising and data-efficient solution for medical applications where annotations are severely scarce and multimodal analysis is required. However, existing methods typically filter out domain-specific information to improve generalization, which inadvertently limits cross-domain performance and degrades source-domain accuracy. To address this, we present Contrastive Graph Modeling (C-Graph), a framework that leverages the structural consistency of medical images as a reliable domain-transferable prior. We represent image features as graphs, with pixels as nodes and semantic affinities as edges. A Structural Prior Graph (SPG) layer is proposed to capture and transfer target-category node dependencies and enable global structure modeling through explicit node interactions. Building upon SPG layers, we introduce a Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction. Furthermore, we design a Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity by contrastively enhancing node discriminability in the graph space. Our method significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.
Published: 2025-12-25T14:00:17+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuntian Bo; Tao Zhou; Zechao Li; Haofeng Zhang; Ling Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Cross-domain few-shot medical image segmentation (CD-FSMIS) offers a promising and data-efficient solution for medical applications where annotations are severely scarce and multimodal analysis is required. However, existing methods typically filter out domain-specific information to improve generalization, which inadvertently limits cross-domain performance and degrades source-domain accuracy. To address this, we present Contrastive Graph Modeling (C-Graph), a framework that leverages the structural consistency of medical images as a reliable domain-transferable prior. We represent image features as graphs, with pixels as nodes and semantic affinities as edges. A Structural Prior Graph (SPG) layer is proposed to capture and transfer target-category node dependencies and enable global structure modeling through explicit node interactions. Building upon SPG layers, we introduce a Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction. Furthermore, we design a Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity by contrastively enhancing node discriminability in the graph space. Our method significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.&lt;/p&gt;</content:encoded></item><item><title>Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</title><link>https://arxiv.org/abs/2512.22046v1</link><guid>http://arxiv.org/abs/2512.22046v1</guid><pubDate>Fri, 26 Dec 2025 14:48:58 +0000</pubDate><dc:creator>Zongmin Zhang</dc:creator><dc:creator>Zhen Sun</dc:creator><dc:creator>Yifan Liao</dc:creator><dc:creator>Wenhan Dong</dc:creator><dc:creator>Xinlei He</dc:creator><dc:creator>Xingshuo Han</dc:creator><dc:creator>Shengmin Xu</dc:creator><dc:creator>Xinyi Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.
Published: 2025-12-26T14:48:58+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongmin Zhang; Zhen Sun; Yifan Liao; Wenhan Dong; Xinlei He; Xingshuo Han; Shengmin Xu; Xinyi Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.&lt;/p&gt;</content:encoded></item><item><title>Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution</title><link>https://doi.org/10.1007/s11263-025-02668-0</link><guid>10.1007/s11263-025-02668-0</guid><pubDate>Sat, 27 Dec 2025 07:49:37 +0000</pubDate><dc:creator>Yang Zou</dc:creator><dc:creator>Zhixin Chen</dc:creator><dc:creator>Zhipeng Zhang</dc:creator><dc:creator>Xingyuan Li</dc:creator><dc:creator>Long Ma</dc:creator><dc:creator>Jinyuan Liu</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02668-0</prism:doi><description>Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .
Published: 2025-12-27T07:49:37+00:00
Venue: International Journal of Computer Vision
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Zou; Zhixin Chen; Zhipeng Zhang; Xingyuan Li; Long Ma; Jinyuan Liu; Peng Wang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02668-0"&gt;10.1007/s11263-025-02668-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .&lt;/p&gt;</content:encoded></item><item><title>Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</title><link>https://arxiv.org/abs/2512.21916v1</link><guid>http://arxiv.org/abs/2512.21916v1</guid><pubDate>Fri, 26 Dec 2025 08:17:10 +0000</pubDate><dc:creator>Zeyu Liang</dc:creator><dc:creator>Hailun Xia</dc:creator><dc:creator>Naichuan Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.
Published: 2025-12-26T08:17:10+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Liang; Hailun Xia; Naichuan Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Visual Prompt Meets Low-Light Saliency Detection</title><link>https://doi.org/10.1016/j.patcog.2025.113008</link><guid>10.1016/j.patcog.2025.113008</guid><pubDate>Sat, 27 Dec 2025 07:16:40 +0000</pubDate><dc:creator>Nana Yu</dc:creator><dc:creator>Jie Wang</dc:creator><dc:creator>Yahong Han</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113008</prism:doi><description>The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.
Published: 2025-12-27T07:16:40+00:00
Venue: Pattern Recognition
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nana Yu; Jie Wang; Yahong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113008"&gt;10.1016/j.patcog.2025.113008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.&lt;/p&gt;</content:encoded></item><item><title>GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104090</link><guid>10.1016/j.inffus.2025.104090</guid><pubDate>Sat, 27 Dec 2025 16:17:57 +0000</pubDate><dc:creator>Qinglong Zhou</dc:creator><dc:creator>Xing Wang</dc:creator><dc:creator>Jiahao Fang</dc:creator><dc:creator>Wenbo Wu</dc:creator><dc:creator>Bingxian Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104090</prism:doi><description>To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.
Published: 2025-12-27T16:17:57+00:00
Venue: Information Fusion
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qinglong Zhou; Xing Wang; Jiahao Fang; Wenbo Wu; Bingxian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104090"&gt;10.1016/j.inffus.2025.104090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.&lt;/p&gt;</content:encoded></item><item><title>Scoping Review of Multimodal Sentiment Analysis and Summarization: State of the Art, Challenges and Future Directions</title><link>https://doi.org/10.1016/j.inffus.2025.104082</link><guid>10.1016/j.inffus.2025.104082</guid><pubDate>Sat, 27 Dec 2025 07:23:47 +0000</pubDate><dc:creator>Magaly Lika Fujimoto</dc:creator><dc:creator>Ricardo Marcondes Marcacini</dc:creator><dc:creator>Solange Oliveira Rezende</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104082</prism:doi><description>In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.
Published: 2025-12-27T07:23:47+00:00
Venue: Information Fusion
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Magaly Lika Fujimoto; Ricardo Marcondes Marcacini; Solange Oliveira Rezende&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104082"&gt;10.1016/j.inffus.2025.104082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.&lt;/p&gt;</content:encoded></item><item><title>Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection</title><link>https://arxiv.org/abs/2512.21856v1</link><guid>http://arxiv.org/abs/2512.21856v1</guid><pubDate>Fri, 26 Dec 2025 04:37:49 +0000</pubDate><dc:creator>Lupiao Hu</dc:creator><dc:creator>Fasheng Wang</dc:creator><dc:creator>Fangmei Chen</dc:creator><dc:creator>Fuming Sun</dc:creator><dc:creator>Haojie Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing RGB-T salient object detection methods predominantly rely on manually aligned and annotated datasets, struggling to handle real-world scenarios with raw, unaligned RGB-T image pairs. In practical applications, due to significant cross-modal disparities such as spatial misalignment, scale variations, and viewpoint shifts, the performance of current methods drastically deteriorates on unaligned datasets. To address this issue, we propose an efficient RGB-T SOD method for real-world unaligned image pairs, termed Thin-Plate Spline-driven Semantic Correlation Learning Network (TPS-SCL). We employ a dual-stream MobileViT as the encoder, combined with efficient Mamba scanning mechanisms, to effectively model correlations between the two modalities while maintaining low parameter counts and computational overhead. To suppress interference from redundant background information during alignment, we design a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features. Furthermore, we introduce a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between modalities. Additionally, a Cross-Modal Correlation Module (CMCM) is incorporated to fully explore and integrate inter-modal dependencies, enhancing detection performance. Extensive experiments on various datasets demonstrate that TPS-SCL attains state-of-the-art (SOTA) performance among existing lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.
Published: 2025-12-26T04:37:49+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lupiao Hu; Fasheng Wang; Fangmei Chen; Fuming Sun; Haojie Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Existing RGB-T salient object detection methods predominantly rely on manually aligned and annotated datasets, struggling to handle real-world scenarios with raw, unaligned RGB-T image pairs. In practical applications, due to significant cross-modal disparities such as spatial misalignment, scale variations, and viewpoint shifts, the performance of current methods drastically deteriorates on unaligned datasets. To address this issue, we propose an efficient RGB-T SOD method for real-world unaligned image pairs, termed Thin-Plate Spline-driven Semantic Correlation Learning Network (TPS-SCL). We employ a dual-stream MobileViT as the encoder, combined with efficient Mamba scanning mechanisms, to effectively model correlations between the two modalities while maintaining low parameter counts and computational overhead. To suppress interference from redundant background information during alignment, we design a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features. Furthermore, we introduce a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between modalities. Additionally, a Cross-Modal Correlation Module (CMCM) is incorporated to fully explore and integrate inter-modal dependencies, enhancing detection performance. Extensive experiments on various datasets demonstrate that TPS-SCL attains state-of-the-art (SOTA) performance among existing lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.&lt;/p&gt;</content:encoded></item><item><title>UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</title><link>https://arxiv.org/abs/2512.21675v1</link><guid>http://arxiv.org/abs/2512.21675v1</guid><pubDate>Thu, 25 Dec 2025 13:35:52 +0000</pubDate><dc:creator>Shuo Cao</dc:creator><dc:creator>Jiayang Li</dc:creator><dc:creator>Xiaohui Li</dc:creator><dc:creator>Yuandong Pu</dc:creator><dc:creator>Kaiwen Zhu</dc:creator><dc:creator>Yuanting Gao</dc:creator><dc:creator>Siqi Luo</dc:creator><dc:creator>Yi Xin</dc:creator><dc:creator>Qi Qin</dc:creator><dc:creator>Yu Zhou</dc:creator><dc:creator>Xiangyu Chen</dc:creator><dc:creator>Wenlong Zhang</dc:creator><dc:creator>Bin Fu</dc:creator><dc:creator>Yu Qiao</dc:creator><dc:creator>Yihao Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.
Published: 2025-12-25T13:35:52+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Cao; Jiayang Li; Xiaohui Li; Yuandong Pu; Kaiwen Zhu; Yuanting Gao; Siqi Luo; Yi Xin; Qi Qin; Yu Zhou; Xiangyu Chen; Wenlong Zhang; Bin Fu; Yu Qiao; Yihao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.&lt;/p&gt;</content:encoded></item><item><title>PMM3D: a transformer-based monocular 3D detector with parallel multi-time inquiry and mixup enhancement</title><link>https://doi.org/10.1016/j.eswa.2025.131014</link><guid>10.1016/j.eswa.2025.131014</guid><pubDate>Sat, 27 Dec 2025 16:14:50 +0000</pubDate><dc:creator>Chao Lin</dc:creator><dc:creator>Tongzhou Zhang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yiou Wang</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Gang Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131014</prism:doi><description>Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.
Published: 2025-12-27T16:14:50+00:00
Venue: Expert Systems with Applications
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Lin; Tongzhou Zhang; Wei Zhou; Yiou Wang; Wei Zhang; Gang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131014"&gt;10.1016/j.eswa.2025.131014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.&lt;/p&gt;</content:encoded></item><item><title>FSKGE: Fuzzy spatio-temporal knowledge graph embedding</title><link>https://doi.org/10.1016/j.neucom.2025.132537</link><guid>10.1016/j.neucom.2025.132537</guid><pubDate>Sat, 27 Dec 2025 07:20:51 +0000</pubDate><dc:creator>Xinhai Hu</dc:creator><dc:creator>Ming Sun</dc:creator><dc:creator>Chao Yang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132537</prism:doi><description>This paper introduces the Fuzzy Spatio-Temporal Knowledge Graph Embedding (FSKGE) model, a novel and highly adaptable framework that addresses the limitations of conventional methods by systematically coordinating fuzzy semantics with continuous spatio-temporal modeling. The core technical innovation of FSKGE is a novel descriptive paradigm that employs geometric operations, such as projection and rotation, to embed complex spatio-temporal information into a composite vector space. The model further introduces an element-level fuzziness embedding mechanism. By leveraging the magnitude of anisotropic vectors, it infuses fine-grained fuzziness into each element of the spatio-temporal quintuple. Furthermore, to capture the intricate interplay between clear static knowledge and fuzzy spatio-temporal knowledge, the model incorporates a spatio-temporal sensitivity capture mechanism. By dynamically learning and weighting the importance of temporal and spatial dimensions, the model achieves more precise reasoning. To validate the model’s effectiveness, we constructe corresponding fuzzy spatio-temporal knowledge graphs based on four benchmark datasets (WikiData53k, YAGO33k, DBpedia34k, ICEWS10k) and conducted comprehensive experimental evaluations. Experiments demonstrate that FSKGE significantly outperforms state-of-the-art baseline models when handling complex fuzzy spatio-temporal knowledge tasks.
Published: 2025-12-27T07:20:51+00:00
Venue: Neurocomputing
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinhai Hu; Ming Sun; Chao Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132537"&gt;10.1016/j.neucom.2025.132537&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;This paper introduces the Fuzzy Spatio-Temporal Knowledge Graph Embedding (FSKGE) model, a novel and highly adaptable framework that addresses the limitations of conventional methods by systematically coordinating fuzzy semantics with continuous spatio-temporal modeling. The core technical innovation of FSKGE is a novel descriptive paradigm that employs geometric operations, such as projection and rotation, to embed complex spatio-temporal information into a composite vector space. The model further introduces an element-level fuzziness embedding mechanism. By leveraging the magnitude of anisotropic vectors, it infuses fine-grained fuzziness into each element of the spatio-temporal quintuple. Furthermore, to capture the intricate interplay between clear static knowledge and fuzzy spatio-temporal knowledge, the model incorporates a spatio-temporal sensitivity capture mechanism. By dynamically learning and weighting the importance of temporal and spatial dimensions, the model achieves more precise reasoning. To validate the model’s effectiveness, we constructe corresponding fuzzy spatio-temporal knowledge graphs based on four benchmark datasets (WikiData53k, YAGO33k, DBpedia34k, ICEWS10k) and conducted comprehensive experimental evaluations. Experiments demonstrate that FSKGE significantly outperforms state-of-the-art baseline models when handling complex fuzzy spatio-temporal knowledge tasks.&lt;/p&gt;</content:encoded></item><item><title>Correlation-Guided Calibration of Query Dependency for Video Temporal Grounding</title><link>https://doi.org/10.1016/j.patcog.2025.112984</link><guid>10.1016/j.patcog.2025.112984</guid><pubDate>Sat, 27 Dec 2025 16:08:59 +0000</pubDate><dc:creator>Wonjun Moon</dc:creator><dc:creator>Sangeek Hyun</dc:creator><dc:creator>Subeen Lee</dc:creator><dc:creator>Jae-Pil Heo</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112984</prism:doi><description>tTemporal grounding is to identify specific moments or highlights from a video corresponding to textual descriptions. Typical approaches in temporal grounding treat all video clips equally during the encoding process regardless of their semantic relevance with the text query. Therefore, we propose Correlation-Guided DEtection TRansformer (CG-DETR), exploring to provide clues for query-associated video clips within the cross-modal attention. First, we design an adaptive cross-attention with dummy tokens. Dummy tokens conditioned by text query take portions of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all words equally inherit the text query’s correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e ., moment and sentence level, and inferring the clip-word correlation. Lastly, we exploit the moment-specific characteristics and combine them with the context of each video to form a moment-adaptive saliency detector. By exploiting the degrees of text engagement in each video clip, it precisely measures the highlightness of each clip. CG-DETR achieves remarkable gains on various benchmarks for temporal grounding.
Published: 2025-12-27T16:08:59+00:00
Venue: Pattern Recognition
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wonjun Moon; Sangeek Hyun; Subeen Lee; Jae-Pil Heo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112984"&gt;10.1016/j.patcog.2025.112984&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;tTemporal grounding is to identify specific moments or highlights from a video corresponding to textual descriptions. Typical approaches in temporal grounding treat all video clips equally during the encoding process regardless of their semantic relevance with the text query. Therefore, we propose Correlation-Guided DEtection TRansformer (CG-DETR), exploring to provide clues for query-associated video clips within the cross-modal attention. First, we design an adaptive cross-attention with dummy tokens. Dummy tokens conditioned by text query take portions of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all words equally inherit the text query’s correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e ., moment and sentence level, and inferring the clip-word correlation. Lastly, we exploit the moment-specific characteristics and combine them with the context of each video to form a moment-adaptive saliency detector. By exploiting the degrees of text engagement in each video clip, it precisely measures the highlightness of each clip. CG-DETR achieves remarkable gains on various benchmarks for temporal grounding.&lt;/p&gt;</content:encoded></item><item><title>LVLM-Aided Alignment of Task-Specific Vision Models</title><link>https://arxiv.org/abs/2512.21985v1</link><guid>http://arxiv.org/abs/2512.21985v1</guid><pubDate>Fri, 26 Dec 2025 11:11:25 +0000</pubDate><dc:creator>Alexander Koebler</dc:creator><dc:creator>Lukas Kuhn</dc:creator><dc:creator>Ingo Thon</dc:creator><dc:creator>Florian Buettner</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.
Published: 2025-12-26T11:11:25+00:00
Venue: arXiv
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alexander Koebler; Lukas Kuhn; Ingo Thon; Florian Buettner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model&amp;#x27;s dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.&lt;/p&gt;</content:encoded></item><item><title>End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</title><link>https://arxiv.org/abs/2512.21831v1</link><guid>http://arxiv.org/abs/2512.21831v1</guid><pubDate>Fri, 26 Dec 2025 02:20:22 +0000</pubDate><dc:creator>Zhenwei Yang</dc:creator><dc:creator>Yibo Ai</dc:creator><dc:creator>Weidong Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.
Published: 2025-12-26T02:20:22+00:00
Venue: arXiv
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenwei Yang; Yibo Ai; Weidong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.&lt;/p&gt;</content:encoded></item><item><title>A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning</title><link>https://arxiv.org/abs/2512.21583v1</link><guid>http://arxiv.org/abs/2512.21583v1</guid><pubDate>Thu, 25 Dec 2025 09:01:06 +0000</pubDate><dc:creator>Zelin Zang</dc:creator><dc:creator>Wenyi Gu</dc:creator><dc:creator>Siqi Ma</dc:creator><dc:creator>Dan Yang</dc:creator><dc:creator>Yue Shen</dc:creator><dc:creator>Zhu Zhang</dc:creator><dc:creator>Guohui Fan</dc:creator><dc:creator>Wing-Kuen Ling</dc:creator><dc:creator>Fuji Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.
Published: 2025-12-25T09:01:06+00:00
Venue: arXiv
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zelin Zang; Wenyi Gu; Siqi Ma; Dan Yang; Yue Shen; Zhu Zhang; Guohui Fan; Wing-Kuen Ling; Fuji Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.&lt;/p&gt;</content:encoded></item><item><title>TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</title><link>https://arxiv.org/abs/2512.21641v1</link><guid>http://arxiv.org/abs/2512.21641v1</guid><pubDate>Thu, 25 Dec 2025 12:02:56 +0000</pubDate><dc:creator>Jiahong Yu</dc:creator><dc:creator>Ziqi Wang</dc:creator><dc:creator>Hailiang Zhao</dc:creator><dc:creator>Wei Zhai</dc:creator><dc:creator>Xueqiang Yan</dc:creator><dc:creator>Shuiguang Deng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.
Published: 2025-12-25T12:02:56+00:00
Venue: arXiv
Score: 0.500 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahong Yu; Ziqi Wang; Hailiang Zhao; Wei Zhai; Xueqiang Yan; Shuiguang Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.500 (consider)&lt;/p&gt;
&lt;p&gt;Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.&lt;/p&gt;</content:encoded></item><item><title>Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</title><link>https://arxiv.org/abs/2512.21860v1</link><guid>http://arxiv.org/abs/2512.21860v1</guid><pubDate>Fri, 26 Dec 2025 04:51:23 +0000</pubDate><dc:creator>Masayuki Kawarada</dc:creator><dc:creator>Kosuke Yamada</dc:creator><dc:creator>Antonio Tejero-de-Pablos</dc:creator><dc:creator>Naoto Inoue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.
Published: 2025-12-26T04:51:23+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Masayuki Kawarada; Kosuke Yamada; Antonio Tejero-de-Pablos; Naoto Inoue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM&amp;#x27;s last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.&lt;/p&gt;</content:encoded></item><item><title>Causality-inspired Learning Semantic Segmentation in Unseen Domain</title><link>https://doi.org/10.1016/j.patcog.2025.113006</link><guid>10.1016/j.patcog.2025.113006</guid><pubDate>Sat, 27 Dec 2025 00:00:55 +0000</pubDate><dc:creator>Pei He</dc:creator><dc:creator>Lingling Li</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Ronghua Shang</dc:creator><dc:creator>Yuwei Guo</dc:creator><dc:creator>Puhua Chen</dc:creator><dc:creator>Shuyuan Yang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113006</prism:doi><description>Semantic segmentation in unseen domain is a critical challenge. Previous approaches rely on neural networks that employ statistical models to learn the correlation patterns within the source domain. However, the generalization of source domain correlations is limited for domain shifts. In this paper, a novel causality-inspired learning method is proposed, which explores how to learn the causal properties to effectively improve generalization in semantic segmentation. Firstly, Consistent Embedding Representation (CER) is proposed to learn the causal completeness that ensures feature causal sufficiency and avoids overfitting the source domain. CER constructs a consistent embedding representation that is not inclined to fit the correlation and updates it to a sufficient prototype representation, which contains enough latent causal information for pixel classification. Secondly, Causal Prototype Learning (CPL) is proposed to learn causal independence. CPL improves the causal factors in confounding information through causal consistent regularization and adaptive learning, which encourages the model to classify according to the causal factors, thus improving the generalization of segmentation in unknown domain. Experiments on four domain generalized scene segmentation benchmarks demonstrate the effectiveness of the proposed approach. The code will be available at https://github.com/ChicalH/CLSS .
Published: 2025-12-27T00:00:55+00:00
Venue: Pattern Recognition
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pei He; Lingling Li; Licheng Jiao; Xu Liu; Fang Liu; Ronghua Shang; Yuwei Guo; Puhua Chen; Shuyuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113006"&gt;10.1016/j.patcog.2025.113006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation in unseen domain is a critical challenge. Previous approaches rely on neural networks that employ statistical models to learn the correlation patterns within the source domain. However, the generalization of source domain correlations is limited for domain shifts. In this paper, a novel causality-inspired learning method is proposed, which explores how to learn the causal properties to effectively improve generalization in semantic segmentation. Firstly, Consistent Embedding Representation (CER) is proposed to learn the causal completeness that ensures feature causal sufficiency and avoids overfitting the source domain. CER constructs a consistent embedding representation that is not inclined to fit the correlation and updates it to a sufficient prototype representation, which contains enough latent causal information for pixel classification. Secondly, Causal Prototype Learning (CPL) is proposed to learn causal independence. CPL improves the causal factors in confounding information through causal consistent regularization and adaptive learning, which encourages the model to classify according to the causal factors, thus improving the generalization of segmentation in unknown domain. Experiments on four domain generalized scene segmentation benchmarks demonstrate the effectiveness of the proposed approach. The code will be available at https://github.com/ChicalH/CLSS .&lt;/p&gt;</content:encoded></item><item><title>Compositional Concept Extraction with Multimodal Large Models: A Unified Framework with Thought Chain Optimization</title><link>https://doi.org/10.1016/j.eswa.2025.130925</link><guid>10.1016/j.eswa.2025.130925</guid><pubDate>Sun, 28 Dec 2025 15:06:17 +0000</pubDate><dc:creator>Yuxin Wu</dc:creator><dc:creator>Zichen Song</dc:creator><dc:creator>Sitan Huang</dc:creator><dc:creator>Zhongfeng Kang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130925</prism:doi><description>Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.
Published: 2025-12-28T15:06:17+00:00
Venue: Expert Systems with Applications
Score: 0.497 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Wu; Zichen Song; Sitan Huang; Zhongfeng Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130925"&gt;10.1016/j.eswa.2025.130925&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.497 (consider)&lt;/p&gt;
&lt;p&gt;Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.&lt;/p&gt;</content:encoded></item><item><title>FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.21695v1</link><guid>http://arxiv.org/abs/2512.21695v1</guid><pubDate>Thu, 25 Dec 2025 14:38:39 +0000</pubDate><dc:creator>Md. Zahid Hossain</dc:creator><dc:creator>Most. Sharmin Sultana Samu</dc:creator><dc:creator>Md. Kamrozzaman Bhuiyan</dc:creator><dc:creator>Farhad Uz Zaman</dc:creator><dc:creator>Md. Rakibul Islam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.
Published: 2025-12-25T14:38:39+00:00
Venue: arXiv
Score: 0.494 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md. Zahid Hossain; Most. Sharmin Sultana Samu; Md. Kamrozzaman Bhuiyan; Farhad Uz Zaman; Md. Rakibul Islam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.494 (consider)&lt;/p&gt;
&lt;p&gt;The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP&amp;#x27;s Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.&lt;/p&gt;</content:encoded></item><item><title>All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115239</link><guid>10.1016/j.knosys.2025.115239</guid><pubDate>Sun, 28 Dec 2025 07:00:46 +0000</pubDate><dc:creator>Yu Fu</dc:creator><dc:creator>Weichao Yi</dc:creator><dc:creator>Liquan Dong</dc:creator><dc:creator>Ming Liu</dc:creator><dc:creator>Lingqin Kong</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115239</prism:doi><description>Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .
Published: 2025-12-28T07:00:46+00:00
Venue: Knowledge-Based Systems
Score: 0.493 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Fu; Weichao Yi; Liquan Dong; Ming Liu; Lingqin Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115239"&gt;10.1016/j.knosys.2025.115239&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.493 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .&lt;/p&gt;</content:encoded></item><item><title>Dual-Level Attention Relearning for Cross-Modality Rotated Object Detection in UAV RGB–Thermal Imagery</title><link>https://doi.org/10.3390/rs18010107</link><guid>10.3390/rs18010107</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Zhuqiang Li</dc:creator><dc:creator>Zhijun Zhen</dc:creator><dc:creator>Shengbo Chen</dc:creator><dc:creator>Liqiang Zhang</dc:creator><dc:creator>Lisai Cao</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010107</prism:doi><description>Effectively leveraging multi-source unmanned aerial vehicle (UAV) observations for reliable object recognition is often compromised by environmental extremes (e.g., occlusion and low illumination) and the inherent physical discrepancies between modalities. To overcome these limitations, we propose DLANet, a lightweight, rotation-aware multimodal object detection framework that introduces a dual-level attention relearning strategy to maximize complementary information from visible (RGB) and thermal infrared (TIR) imagery. DLANet integrates two novel components: the Implicit Fine-Grained Fusion Module (IF2M), which facilitates deep cross-modal interaction by jointly modeling channel and spatial dependencies at intermediate stages, and the Adaptive Branch Feature Weighting (ABFW) module, which dynamically recalibrates modality contributions at higher levels to suppress noise and pseudo-targets. This synergistic approach allows the network to relearn feature importance based on real-time scene conditions. To support industrial applications, we construct the OilLeak dataset, a dedicated benchmark for onshore oil-spill detection. The experimental results demonstrate that DLANet achieves state-of-the-art performance, recording an mAP0.5 of 0.858 on the public DroneVehicle dataset while maintaining high efficiency, with 39.04 M parameters and 72.69 GFLOPs, making it suitable for real-time edge deployment.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.487 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuqiang Li; Zhijun Zhen; Shengbo Chen; Liqiang Zhang; Lisai Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010107"&gt;10.3390/rs18010107&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.487 (consider)&lt;/p&gt;
&lt;p&gt;Effectively leveraging multi-source unmanned aerial vehicle (UAV) observations for reliable object recognition is often compromised by environmental extremes (e.g., occlusion and low illumination) and the inherent physical discrepancies between modalities. To overcome these limitations, we propose DLANet, a lightweight, rotation-aware multimodal object detection framework that introduces a dual-level attention relearning strategy to maximize complementary information from visible (RGB) and thermal infrared (TIR) imagery. DLANet integrates two novel components: the Implicit Fine-Grained Fusion Module (IF2M), which facilitates deep cross-modal interaction by jointly modeling channel and spatial dependencies at intermediate stages, and the Adaptive Branch Feature Weighting (ABFW) module, which dynamically recalibrates modality contributions at higher levels to suppress noise and pseudo-targets. This synergistic approach allows the network to relearn feature importance based on real-time scene conditions. To support industrial applications, we construct the OilLeak dataset, a dedicated benchmark for onshore oil-spill detection. The experimental results demonstrate that DLANet achieves state-of-the-art performance, recording an mAP0.5 of 0.858 on the public DroneVehicle dataset while maintaining high efficiency, with 39.04 M parameters and 72.69 GFLOPs, making it suitable for real-time edge deployment.&lt;/p&gt;</content:encoded></item></channel></rss>