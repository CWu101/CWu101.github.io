<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 30 Jan 2026 03:30:31 +0000</lastBuildDate><item><title>Vocabulary-free Image Classification and Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2026.3657989</link><guid>10.1109/tpami.2026.3657989</guid><pubDate>Wed, 28 Jan 2026 20:59:22 +0000</pubDate><dc:creator>Alessandro Conti</dc:creator><dc:creator>Enrico Fini</dc:creator><dc:creator>Massimiliano Mancini</dc:creator><dc:creator>Paolo Rota</dc:creator><dc:creator>Yiming Wang</dc:creator><dc:creator>Elisa Ricci</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657989</prism:doi><description>Large vision-language models revolutionized image classification and semantic segmentation paradigms. However, they typically assume a pre-defined set of categories, or vocabulary, at test time for composing textual prompts. This assumption is impractical in scenarios with unknown or evolving semantic context. Here, we address this issue and introduce the Vocabulary-free Image Classification (VIC) task, which aims to assign a class from an unconstrained language-induced semantic space to an input image without needing a known vocabulary. VIC is challenging due to the vastness of the semantic space, which contains millions of concepts, including fine-grained categories. To address VIC, we propose Category Search from External Databases (CaSED), a training-free method that leverages a pre-trained vision-language model and an external database. CaSED first extracts the set of candidate categories from the most semantically similar captions in the database and then assigns the image to the best-matching candidate category according to the same vision-language model. Furthermore, we demonstrate that CaSED can be applied locally to generate a coarse segmentation mask that classifies image regions, introducing the task of Vocabulary-free Semantic Segmentation. CaSED and its variants outperform other more complex vision-language models, on classification and semantic segmentation benchmarks, while using much fewer parameters.
Published: 2026-01-28T20:59:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.599 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alessandro Conti; Enrico Fini; Massimiliano Mancini; Paolo Rota; Yiming Wang; Elisa Ricci&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657989"&gt;10.1109/tpami.2026.3657989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.599 (consider)&lt;/p&gt;
&lt;p&gt;Large vision-language models revolutionized image classification and semantic segmentation paradigms. However, they typically assume a pre-defined set of categories, or vocabulary, at test time for composing textual prompts. This assumption is impractical in scenarios with unknown or evolving semantic context. Here, we address this issue and introduce the Vocabulary-free Image Classification (VIC) task, which aims to assign a class from an unconstrained language-induced semantic space to an input image without needing a known vocabulary. VIC is challenging due to the vastness of the semantic space, which contains millions of concepts, including fine-grained categories. To address VIC, we propose Category Search from External Databases (CaSED), a training-free method that leverages a pre-trained vision-language model and an external database. CaSED first extracts the set of candidate categories from the most semantically similar captions in the database and then assigns the image to the best-matching candidate category according to the same vision-language model. Furthermore, we demonstrate that CaSED can be applied locally to generate a coarse segmentation mask that classifies image regions, introducing the task of Vocabulary-free Semantic Segmentation. CaSED and its variants outperform other more complex vision-language models, on classification and semantic segmentation benchmarks, while using much fewer parameters.&lt;/p&gt;</content:encoded></item><item><title>RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning</title><link>https://arxiv.org/abs/2601.21634v1</link><guid>http://arxiv.org/abs/2601.21634v1</guid><pubDate>Thu, 29 Jan 2026 12:35:57 +0000</pubDate><dc:creator>Shiqi Huang</dc:creator><dc:creator>Shuting He</dc:creator><dc:creator>Bihan Wen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.
Published: 2026-01-29T12:35:57+00:00
Venue: arXiv
Score: 0.585 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiqi Huang; Shuting He; Bihan Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.585 (consider)&lt;/p&gt;
&lt;p&gt;Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.&lt;/p&gt;</content:encoded></item><item><title>Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models</title><link>https://arxiv.org/abs/2601.19060v1</link><guid>http://arxiv.org/abs/2601.19060v1</guid><pubDate>Tue, 27 Jan 2026 00:46:08 +0000</pubDate><dc:creator>Jeonghwan Kim</dc:creator><dc:creator>Renjie Tao</dc:creator><dc:creator>Sanat Sharma</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Kai Sun</dc:creator><dc:creator>Zhaojiang Lin</dc:creator><dc:creator>Seungwhan Moon</dc:creator><dc:creator>Lambert Mathias</dc:creator><dc:creator>Anuj Kumar</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Xin Luna Dong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &lt;search&gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.
Published: 2026-01-27T00:46:08+00:00
Venue: arXiv
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeonghwan Kim; Renjie Tao; Sanat Sharma; Jiaqi Wang; Kai Sun; Zhaojiang Lin; Seungwhan Moon; Lambert Mathias; Anuj Kumar; Heng Ji; Xin Luna Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &amp;lt;search&amp;gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.&lt;/p&gt;</content:encoded></item><item><title>QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding</title><link>https://arxiv.org/abs/2601.18195v1</link><guid>http://arxiv.org/abs/2601.18195v1</guid><pubDate>Mon, 26 Jan 2026 06:27:03 +0000</pubDate><dc:creator>Linhan Cao</dc:creator><dc:creator>Wei Sun</dc:creator><dc:creator>Weixia Zhang</dc:creator><dc:creator>Xiangyang Zhu</dc:creator><dc:creator>Kaiwei Zhang</dc:creator><dc:creator>Jun Jia</dc:creator><dc:creator>Dandan Zhu</dc:creator><dc:creator>Guangtao Zhai</dc:creator><dc:creator>Xiongkuo Min</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.
Published: 2026-01-26T06:27:03+00:00
Venue: arXiv
Score: 0.568 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linhan Cao; Wei Sun; Weixia Zhang; Xiangyang Zhu; Kaiwei Zhang; Jun Jia; Dandan Zhu; Guangtao Zhai; Xiongkuo Min&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.568 (consider)&lt;/p&gt;
&lt;p&gt;Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.&lt;/p&gt;</content:encoded></item><item><title>bi-modal textual prompt learning for vision-language models in remote sensing</title><link>https://arxiv.org/abs/2601.20675v1</link><guid>http://arxiv.org/abs/2601.20675v1</guid><pubDate>Wed, 28 Jan 2026 14:58:14 +0000</pubDate><dc:creator>Pankhi Kashyap</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.
Published: 2026-01-28T14:58:14+00:00
Venue: arXiv
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pankhi Kashyap; Mainak Singha; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.&lt;/p&gt;</content:encoded></item><item><title>DeSPAR: Depth-Guided Semantic-Prompted Adaptive Refinement for ORSI Salient Object Detection</title><link>https://doi.org/10.1109/tgrs.2026.3658823</link><guid>10.1109/tgrs.2026.3658823</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Xiaoli Zhang</dc:creator><dc:creator>Ping Liufu</dc:creator><dc:creator>Xihang Hu</dc:creator><dc:creator>Xiongfei Li</dc:creator><dc:creator>Chuanmin Jia</dc:creator><dc:creator>Siwei Ma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658823</prism:doi><description>Optical remote sensing image salient object detection (ORSI-SOD) currently faces two major challenges: (1) existing RGB-based methods primarily depend on color and texture cues, which makes it difficult to obtain robust representations of object spatial structure under extreme imaging conditions; (2) significant morphological variations across object categories lead to semantic feature confusion in existing methods. To address these issues, we propose Depth-Guided Semantic-Prompted Adaptive Refinement (DeSPAR), a progressive refinement framework with geometric–semantic decoupling. To avoid excessive coupling between geometric and semantic signals in an end-to-end architecture, which would cause semantic priors to interfere too early with the construction of generic geometric representations, DeSPAR adopts a two-stage design for feature learning. In Stage 1, Depth-Guided Geometric Learning (DGL) employs a novel lightweight Depth-Guided Refiner (DGR) to build a generic geometric foundation. DGR utilizes RGB features to guide pseudo-depth denoising and injects geometric cues from pseudo-depth to enhance spatial feature representations. In Stage 2, Depth-Guided Semantic-Adaptive Refinement (DSR) inherits the encoder weights from DGL and introduces category-specific constraints. Under the guidance of a Semantic Prompt Bank constructed from DGL, DSR adaptively optimizes the representations of different categories through a prompt-guided mechanism. Experimental results demonstrate that DeSPAR surpasses 22 state-of-the-art methods on three public ORSI-SOD benchmarks, achieving superior performance with only 26.4M parameters while attaining an inference speed of 161 FPS.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoli Zhang; Ping Liufu; Xihang Hu; Xiongfei Li; Chuanmin Jia; Siwei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658823"&gt;10.1109/tgrs.2026.3658823&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;Optical remote sensing image salient object detection (ORSI-SOD) currently faces two major challenges: (1) existing RGB-based methods primarily depend on color and texture cues, which makes it difficult to obtain robust representations of object spatial structure under extreme imaging conditions; (2) significant morphological variations across object categories lead to semantic feature confusion in existing methods. To address these issues, we propose Depth-Guided Semantic-Prompted Adaptive Refinement (DeSPAR), a progressive refinement framework with geometric–semantic decoupling. To avoid excessive coupling between geometric and semantic signals in an end-to-end architecture, which would cause semantic priors to interfere too early with the construction of generic geometric representations, DeSPAR adopts a two-stage design for feature learning. In Stage 1, Depth-Guided Geometric Learning (DGL) employs a novel lightweight Depth-Guided Refiner (DGR) to build a generic geometric foundation. DGR utilizes RGB features to guide pseudo-depth denoising and injects geometric cues from pseudo-depth to enhance spatial feature representations. In Stage 2, Depth-Guided Semantic-Adaptive Refinement (DSR) inherits the encoder weights from DGL and introduces category-specific constraints. Under the guidance of a Semantic Prompt Bank constructed from DGL, DSR adaptively optimizes the representations of different categories through a prompt-guided mechanism. Experimental results demonstrate that DeSPAR surpasses 22 state-of-the-art methods on three public ORSI-SOD benchmarks, achieving superior performance with only 26.4M parameters while attaining an inference speed of 161 FPS.&lt;/p&gt;</content:encoded></item><item><title>SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</title><link>https://arxiv.org/abs/2601.21498v1</link><guid>http://arxiv.org/abs/2601.21498v1</guid><pubDate>Thu, 29 Jan 2026 10:15:55 +0000</pubDate><dc:creator>Thanh-Nhan Vo</dc:creator><dc:creator>Trong-Thuan Nguyen</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.
Published: 2026-01-29T10:15:55+00:00
Venue: arXiv
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>ThinkMatter: Panoramic-Aware Instructional Semantics for Monocular Vision-and-Language Navigation</title><link>https://doi.org/10.1109/tip.2026.3652003</link><guid>10.1109/tip.2026.3652003</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Guangzhao Dai</dc:creator><dc:creator>Shuo Wang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:creator>Bin Zhu</dc:creator><dc:creator>Qianru Sun</dc:creator><dc:creator>Xiangbo Shu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652003</prism:doi><description>Vision-and-Language Navigation in continuous environments (VLN-CE) requires an embodied robot to navigate the target destination following the natural language instruction. Most existing methods use panoramic RGB-D cameras for 360° observation of environments. However, these methods struggle in real-world applications because of the higher cost of panoramic RGB-D cameras. This paper studies a low-cost and practical VLN-CE setting, e.g., using monocular cameras of limited field of view, which means “Look Less” for visual observations and environment semantics. In this paper, we propose a ThinkMatter framework for monocular VLN-CE, where we motivate monocular robots to “Think More” by 1) generating novel views and 2) integrating instruction semantics. Specifically, we achieve the former by the proposed 3DGS-based panoramic generation to render novel views at each step, based on past observation collections. We achieve the latter by the proposed enhancement of the occupancy-instruction semantics, which integrates the spatial semantics of occupancy maps with the textual semantics of language instructions. These operations promote monocular robots with wider environment perceptions as well as transparent semantic connections with the instruction. Both extensive experiments in the simulators and real-world environments demonstrate the effectiveness of ThinkMatter, providing a promising practice for real-world navigation.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangzhao Dai; Shuo Wang; Hao Zhao; Bin Zhu; Qianru Sun; Xiangbo Shu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652003"&gt;10.1109/tip.2026.3652003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Vision-and-Language Navigation in continuous environments (VLN-CE) requires an embodied robot to navigate the target destination following the natural language instruction. Most existing methods use panoramic RGB-D cameras for 360° observation of environments. However, these methods struggle in real-world applications because of the higher cost of panoramic RGB-D cameras. This paper studies a low-cost and practical VLN-CE setting, e.g., using monocular cameras of limited field of view, which means “Look Less” for visual observations and environment semantics. In this paper, we propose a ThinkMatter framework for monocular VLN-CE, where we motivate monocular robots to “Think More” by 1) generating novel views and 2) integrating instruction semantics. Specifically, we achieve the former by the proposed 3DGS-based panoramic generation to render novel views at each step, based on past observation collections. We achieve the latter by the proposed enhancement of the occupancy-instruction semantics, which integrates the spatial semantics of occupancy maps with the textual semantics of language instructions. These operations promote monocular robots with wider environment perceptions as well as transparent semantic connections with the instruction. Both extensive experiments in the simulators and real-world environments demonstrate the effectiveness of ThinkMatter, providing a promising practice for real-world navigation.&lt;/p&gt;</content:encoded></item><item><title>A Few-Shot Class Incremental Learning Method Using Graph Neural Networks</title><link>https://doi.org/10.1109/tip.2026.3657170</link><guid>10.1109/tip.2026.3657170</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yuqian Ma</dc:creator><dc:creator>Youfa Liu</dc:creator><dc:creator>Bo Du</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657170</prism:doi><description>Few-shot class incremental learning (FSCIL) aims to continuously learn new classes from limited training samples while retaining previously acquired knowledge. Existing approaches are not fully capable of balancing stability and plasticity in dynamic scenarios. To overcome this limitation, we introduce a novel FSCIL framework that leverages graph neural networks (GNNs) to model interdependencies between different categories and enhance cross-modal alignment. Our framework incorporates three key components: (1) a Graph Isomorphism Network (GIN) to propagate contextual relationships among prompts; (2) a Hamiltonian Graph Network with Energy Conservation (HGN-EC) to stabilize training dynamics via energy conservation constraints; and (3) an Adversarially Constrained Graph Autoencoder (ACGA) to enforce latent space consistency. By integrating these components with a parameter-efficient CLIP backbone, our method dynamically adapts graph structures to model semantic correlations between textual and visual modalities. Additionally, contrastive learning with energy-based regularization is employed to mitigate catastrophic forgetting and improve generalization. Comprehensive experiments on benchmark datasets validate the framework’s incremental accuracy and stability compared to state-of-the-art baselines. This work advances FSCIL by unifying graph-based relational reasoning with physics-inspired optimization, offering a scalable and interpretable framework.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqian Ma; Youfa Liu; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657170"&gt;10.1109/tip.2026.3657170&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot class incremental learning (FSCIL) aims to continuously learn new classes from limited training samples while retaining previously acquired knowledge. Existing approaches are not fully capable of balancing stability and plasticity in dynamic scenarios. To overcome this limitation, we introduce a novel FSCIL framework that leverages graph neural networks (GNNs) to model interdependencies between different categories and enhance cross-modal alignment. Our framework incorporates three key components: (1) a Graph Isomorphism Network (GIN) to propagate contextual relationships among prompts; (2) a Hamiltonian Graph Network with Energy Conservation (HGN-EC) to stabilize training dynamics via energy conservation constraints; and (3) an Adversarially Constrained Graph Autoencoder (ACGA) to enforce latent space consistency. By integrating these components with a parameter-efficient CLIP backbone, our method dynamically adapts graph structures to model semantic correlations between textual and visual modalities. Additionally, contrastive learning with energy-based regularization is employed to mitigate catastrophic forgetting and improve generalization. Comprehensive experiments on benchmark datasets validate the framework’s incremental accuracy and stability compared to state-of-the-art baselines. This work advances FSCIL by unifying graph-based relational reasoning with physics-inspired optimization, offering a scalable and interpretable framework.&lt;/p&gt;</content:encoded></item><item><title>Dissecting RGB-D Learning for Improved Multi-modal Fusion</title><link>https://doi.org/10.1109/tip.2026.3657171</link><guid>10.1109/tip.2026.3657171</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Hao Chen</dc:creator><dc:creator>Haoran Zhou</dc:creator><dc:creator>Yunshu Zhang</dc:creator><dc:creator>Zheng Lin</dc:creator><dc:creator>Yongjian Deng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657171</prism:doi><description>In the RGB-D vision community, extensive research has been focused on designing multi-modal learning strategies and fusion structures. However, the complementary and fusion mechanisms in RGB-D models remain a black box. In this paper, we present an analytical framework and a novel score to dissect the RGB-D vision community. Our approach involves measuring proposed semantic variance and feature similarity across modalities and levels, conducting visual and quantitative analyzes on multi-modal learning through comprehensive experiments. Specifically, we investigate the consistency and specialty of features across modalities, evolution rules within each modality, and the collaboration logic used when optimizing a RGB-D model. Our studies reveal/verify several important findings, such as the discrepancy in cross-modal features and the hybrid multi-modal cooperation rule, which highlights consistency and specialty simultaneously for complementary inference. We also showcase the versatility of the proposed RGB-D dissection method and introduce a straightforward fusion strategy based on our findings, which delivers significant enhancements across various tasks and even other multi-modal data.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Chen; Haoran Zhou; Yunshu Zhang; Zheng Lin; Yongjian Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657171"&gt;10.1109/tip.2026.3657171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;In the RGB-D vision community, extensive research has been focused on designing multi-modal learning strategies and fusion structures. However, the complementary and fusion mechanisms in RGB-D models remain a black box. In this paper, we present an analytical framework and a novel score to dissect the RGB-D vision community. Our approach involves measuring proposed semantic variance and feature similarity across modalities and levels, conducting visual and quantitative analyzes on multi-modal learning through comprehensive experiments. Specifically, we investigate the consistency and specialty of features across modalities, evolution rules within each modality, and the collaboration logic used when optimizing a RGB-D model. Our studies reveal/verify several important findings, such as the discrepancy in cross-modal features and the hybrid multi-modal cooperation rule, which highlights consistency and specialty simultaneously for complementary inference. We also showcase the versatility of the proposed RGB-D dissection method and introduce a straightforward fusion strategy based on our findings, which delivers significant enhancements across various tasks and even other multi-modal data.&lt;/p&gt;</content:encoded></item><item><title>Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</title><link>https://doi.org/10.1016/j.inffus.2026.104193</link><guid>10.1016/j.inffus.2026.104193</guid><pubDate>Thu, 29 Jan 2026 07:44:09 +0000</pubDate><dc:creator>Shunlei Li</dc:creator><dc:creator>Longsen Gao</dc:creator><dc:creator>Jin Wang</dc:creator><dc:creator>Chang Che</dc:creator><dc:creator>Xi Xiao</dc:creator><dc:creator>Jiuwen Cao</dc:creator><dc:creator>Yingbai Hu</dc:creator><dc:creator>Hamid Reza Karimi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104193</prism:doi><description>Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB(-D) human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95% graph accuracy and 93% subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94% grasp success, 89% placement accuracy, and 90% overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.
Published: 2026-01-29T07:44:09+00:00
Venue: Information Fusion
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shunlei Li; Longsen Gao; Jin Wang; Chang Che; Xi Xiao; Jiuwen Cao; Yingbai Hu; Hamid Reza Karimi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104193"&gt;10.1016/j.inffus.2026.104193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB(-D) human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95% graph accuracy and 93% subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94% grasp success, 89% placement accuracy, and 90% overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.&lt;/p&gt;</content:encoded></item><item><title>Think Twice Before Determining: Towards Scene-aware Visual Reasoning for Mirror Detection</title><link>https://doi.org/10.1109/tcsvt.2026.3657574</link><guid>10.1109/tcsvt.2026.3657574</guid><pubDate>Wed, 28 Jan 2026 21:01:12 +0000</pubDate><dc:creator>Mingfeng Zha</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yunqiang Pei</dc:creator><dc:creator>Tianyu Li</dc:creator><dc:creator>Xiongxin Tang</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657574</prism:doi><description>Mirror detection (MD) aims to overcome interference caused by reflections and locate mirror regions. Existing methods focus on designing components to explicitly establish the associations between physical entities and corresponding imagings, or utilizing rotation to construct symmetric consistency. We observe that: a) incomplete and incorrect correspondence between entities and imagings; b) other physical materials (e.g., glass) exhibit characteristics partially similar to mirrors, causing confusion when they co-occur; c) complex interfering factors (e.g., occlusion) and reflection mechanisms may expand vector space several times over. To address these issues in a unified manner, we formulate the scene-aware visual reasoning network (SVRNet) based on visual prompts. Specifically, we construct the prototype-guided prompt chain reasoning (PPCR) that generates a mixed chain of thought reasoning based on maximal difference heterogeneous prototypes to construct comprehensive spatial location and semantic perception. Noise may accumulate gradually through the chain, and crucial clues may also disappear. Therefore, we design the prompt evolution (PE) to filter out noise and enhance the coupling between prompts. We further develop the mixture of prompt injection expert (MPIE) to dynamically select the optimal injection strategy in the low-rank space based on specific scene. Due to reflection interference and random parameter space introducing potential ambiguity, we formulate the three-way evidence-aware (TEA) loss to quantify the uncertainty, thereby providing reliable predictions. To leverage historical knowledge and further disentangle representations, we propose the frequency prototype contrastive (FPC) loss for learning more generalizable features across images. Finally, we relabel 25,828 images and formulate the first point-supervised MD framework. Extensive experiments conducted on four mirror benchmarks under three settings demonstrate that our method surpasses sta...
Published: 2026-01-28T21:01:12+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingfeng Zha; Guoqing Wang; Yunqiang Pei; Tianyu Li; Xiongxin Tang; Jiayi Ma; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657574"&gt;10.1109/tcsvt.2026.3657574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Mirror detection (MD) aims to overcome interference caused by reflections and locate mirror regions. Existing methods focus on designing components to explicitly establish the associations between physical entities and corresponding imagings, or utilizing rotation to construct symmetric consistency. We observe that: a) incomplete and incorrect correspondence between entities and imagings; b) other physical materials (e.g., glass) exhibit characteristics partially similar to mirrors, causing confusion when they co-occur; c) complex interfering factors (e.g., occlusion) and reflection mechanisms may expand vector space several times over. To address these issues in a unified manner, we formulate the scene-aware visual reasoning network (SVRNet) based on visual prompts. Specifically, we construct the prototype-guided prompt chain reasoning (PPCR) that generates a mixed chain of thought reasoning based on maximal difference heterogeneous prototypes to construct comprehensive spatial location and semantic perception. Noise may accumulate gradually through the chain, and crucial clues may also disappear. Therefore, we design the prompt evolution (PE) to filter out noise and enhance the coupling between prompts. We further develop the mixture of prompt injection expert (MPIE) to dynamically select the optimal injection strategy in the low-rank space based on specific scene. Due to reflection interference and random parameter space introducing potential ambiguity, we formulate the three-way evidence-aware (TEA) loss to quantify the uncertainty, thereby providing reliable predictions. To leverage historical knowledge and further disentangle representations, we propose the frequency prototype contrastive (FPC) loss for learning more generalizable features across images. Finally, we relabel 25,828 images and formulate the first point-supervised MD framework. Extensive experiments conducted on four mirror benchmarks under three settings demonstrate that our method surpasses sta...&lt;/p&gt;</content:encoded></item><item><title>Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</title><link>https://arxiv.org/abs/2601.18190v1</link><guid>http://arxiv.org/abs/2601.18190v1</guid><pubDate>Mon, 26 Jan 2026 06:16:53 +0000</pubDate><dc:creator>Yifan Li</dc:creator><dc:creator>Shiying Wang</dc:creator><dc:creator>Jianqiang Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.
Published: 2026-01-26T06:16:53+00:00
Venue: arXiv
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Li; Shiying Wang; Jianqiang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Conditioned Reasoning in Long-Egocentric Videos</title><link>https://arxiv.org/abs/2601.18100v1</link><guid>http://arxiv.org/abs/2601.18100v1</guid><pubDate>Mon, 26 Jan 2026 03:21:35 +0000</pubDate><dc:creator>James Tribble</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Si-En Hong</dc:creator><dc:creator>Chaoyi Zhou</dc:creator><dc:creator>Ashish Bastola</dc:creator><dc:creator>Siyu Huang</dc:creator><dc:creator>Abolfazl Razi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.
Published: 2026-01-26T03:21:35+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James Tribble; Hao Wang; Si-En Hong; Chaoyi Zhou; Ashish Bastola; Siyu Huang; Abolfazl Razi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.&lt;/p&gt;</content:encoded></item><item><title>DeepSeek-OCR 2: Visual Causal Flow</title><link>https://arxiv.org/abs/2601.20552v1</link><guid>http://arxiv.org/abs/2601.20552v1</guid><pubDate>Wed, 28 Jan 2026 12:46:07 +0000</pubDate><dc:creator>Haoran Wei</dc:creator><dc:creator>Yaofeng Sun</dc:creator><dc:creator>Yukun Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.
Published: 2026-01-28T12:46:07+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Wei; Yaofeng Sun; Yukun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.&lt;/p&gt;</content:encoded></item><item><title>Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.21159v1</link><guid>http://arxiv.org/abs/2601.21159v1</guid><pubDate>Thu, 29 Jan 2026 01:46:03 +0000</pubDate><dc:creator>Jianzheng Wang</dc:creator><dc:creator>Huan Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.
Published: 2026-01-29T01:46:03+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianzheng Wang; Huan Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using &amp;quot;one-way injection&amp;quot; and &amp;quot;shallow post-processing&amp;quot; strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.&lt;/p&gt;</content:encoded></item><item><title>FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</title><link>https://arxiv.org/abs/2601.21187v1</link><guid>http://arxiv.org/abs/2601.21187v1</guid><pubDate>Thu, 29 Jan 2026 02:36:19 +0000</pubDate><dc:creator>Chenyu Huang</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Xudong Tan</dc:creator><dc:creator>Jinhan Mu</dc:creator><dc:creator>Shenghe Zheng</dc:creator><dc:creator>Li Shen</dc:creator><dc:creator>Tao Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.
Published: 2026-01-29T02:36:19+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyu Huang; Peng Ye; Xudong Tan; Jinhan Mu; Shenghe Zheng; Li Shen; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&amp;#x27;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.&lt;/p&gt;</content:encoded></item><item><title>m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</title><link>https://arxiv.org/abs/2601.19099v1</link><guid>http://arxiv.org/abs/2601.19099v1</guid><pubDate>Tue, 27 Jan 2026 02:01:56 +0000</pubDate><dc:creator>Yosub Shin</dc:creator><dc:creator>Michael Buriek</dc:creator><dc:creator>Igor Molybog</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.
Published: 2026-01-27T02:01:56+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yosub Shin; Michael Buriek; Igor Molybog&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.&lt;/p&gt;</content:encoded></item><item><title>All-in-One Transformer for Image Restoration Under Adverse Weather Degradations</title><link>https://doi.org/10.1109/tpami.2026.3658598</link><guid>10.1109/tpami.2026.3658598</guid><pubDate>Wed, 28 Jan 2026 20:59:22 +0000</pubDate><dc:creator>Jiawei Mao</dc:creator><dc:creator>Yu Yang</dc:creator><dc:creator>Xuesong Yin</dc:creator><dc:creator>Ling Shao</dc:creator><dc:creator>Hao Tang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658598</prism:doi><description>Severe weather restoration models often face the simultaneous interaction of multiple degradations in real-world scenarios. Existing approaches typically handle single or composite degradations based on scene descriptors derived from text or image embeddings. However, due to the varying proportions of different degradations within an image, these scene descriptors may not accurately differentiate between degradations, leading to suboptimal restoration in practical applications. To address this issue, we propose a novel Transformer-based restoration framework, AllRestorer, for dealing with four physical severe weather impairments: low-light, haze, rain, and snow. In AllRestorer, we enable the model to adaptively consider all weather impairments, thereby avoiding errors from scene descriptor misdirection. Specifically, we introduce the All-in-One Transformer Block (AiOTB), the core innovation of which is the ability to adaptively handle multiple degradations in a single image, beyond the limitation of existing Transformers that can only handle one type of degradation at a time. To accurately address different variations potentially present within the same type of degradation and minimize ambiguity, AiOTB utilizes a Composite Scene Embedding consisting of both image and text embeddings to define the degradation. Moreover, AiOTB includes an adaptive weight for each degradation, allowing for precise control of the restoration intensity. By leveraging AiOTB, AllRestorer avoids misdirection caused by inaccurate scene descriptors, achieving a 5.00 dB increase in PSNR compared to the baseline on the CDD-11 dataset.
Published: 2026-01-28T20:59:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Mao; Yu Yang; Xuesong Yin; Ling Shao; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658598"&gt;10.1109/tpami.2026.3658598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Severe weather restoration models often face the simultaneous interaction of multiple degradations in real-world scenarios. Existing approaches typically handle single or composite degradations based on scene descriptors derived from text or image embeddings. However, due to the varying proportions of different degradations within an image, these scene descriptors may not accurately differentiate between degradations, leading to suboptimal restoration in practical applications. To address this issue, we propose a novel Transformer-based restoration framework, AllRestorer, for dealing with four physical severe weather impairments: low-light, haze, rain, and snow. In AllRestorer, we enable the model to adaptively consider all weather impairments, thereby avoiding errors from scene descriptor misdirection. Specifically, we introduce the All-in-One Transformer Block (AiOTB), the core innovation of which is the ability to adaptively handle multiple degradations in a single image, beyond the limitation of existing Transformers that can only handle one type of degradation at a time. To accurately address different variations potentially present within the same type of degradation and minimize ambiguity, AiOTB utilizes a Composite Scene Embedding consisting of both image and text embeddings to define the degradation. Moreover, AiOTB includes an adaptive weight for each degradation, allowing for precise control of the restoration intensity. By leveraging AiOTB, AllRestorer avoids misdirection caused by inaccurate scene descriptors, achieving a 5.00 dB increase in PSNR compared to the baseline on the CDD-11 dataset.&lt;/p&gt;</content:encoded></item><item><title>Data-Efficient Generalization for Zero-shot Composed Image Retrieval</title><link>https://doi.org/10.1016/j.patcog.2026.113187</link><guid>10.1016/j.patcog.2026.113187</guid><pubDate>Wed, 28 Jan 2026 16:04:06 +0000</pubDate><dc:creator>Zining Chen</dc:creator><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Fei Su</dc:creator><dc:creator>Shijian Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113187</prism:doi><description>Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic Sample Pool (SSP) module. The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The SSP module exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.
Published: 2026-01-28T16:04:06+00:00
Venue: Pattern Recognition
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zining Chen; Zhicheng Zhao; Fei Su; Shijian Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113187"&gt;10.1016/j.patcog.2026.113187&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic Sample Pool (SSP) module. The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The SSP module exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Interpretable Image Recognition Network via Language-Guided Global-Local Collaboratively Alignment</title><link>https://doi.org/10.1016/j.knosys.2026.115422</link><guid>10.1016/j.knosys.2026.115422</guid><pubDate>Wed, 28 Jan 2026 07:55:11 +0000</pubDate><dc:creator>Sulan Zhang</dc:creator><dc:creator>Peijun Zhang</dc:creator><dc:creator>Lihua Hu</dc:creator><dc:creator>Xin Wen</dc:creator><dc:creator>Jifu Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115422</prism:doi><description>Interpretability is crucial for establishing user trust in image recognition models in high-risk domains such as medical diagnosis and autonomous driving. Recent studies have enhanced model interpretability through visual-language alignment.However, existing methods mostly rely on coarse-grained alignment between overall image representations and semantic concepts, making it difficult to achieve fine-grained interactions between local visual regions and semantic concepts. This limitation restricts further improvements in practical interpretability and recognition performance. To address this issue, we propose a Language-Guided Global-Local Collaboratively Aligned Multimodal Interpretable Image Recognition Network (LGLCA-Net), which uses text concepts generated by large language models (LLMs) to guide the collaborative alignment of images and text in terms of global and local semantics.The network first designs a concept de-redundancy and visual recognizability verification strategy, driving the large language model to generate high-quality and visually relevant semantic concepts as the foundation for language guidance. Subsequently, we utilize the multimodal space provided by the CLIP model to construct a dual-branch alignment structure for global semantics and local visual semantics.In the local visual semantics branch, we introduce a visual prompting mechanism that extracts discriminative local regions to achieve fine-grained alignment with semantic concepts. Finally, we design a learnable dynamic weighting mechanism to adaptively fuse the alignment information from both branches, achieving collaborative alignment of global-local semantics and semantic concepts. Extensive experiments show that our method not only provides finer-grained and more trustworthy visual explanations but also improves recognition performance.
Published: 2026-01-28T07:55:11+00:00
Venue: Knowledge-Based Systems
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sulan Zhang; Peijun Zhang; Lihua Hu; Xin Wen; Jifu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115422"&gt;10.1016/j.knosys.2026.115422&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is crucial for establishing user trust in image recognition models in high-risk domains such as medical diagnosis and autonomous driving. Recent studies have enhanced model interpretability through visual-language alignment.However, existing methods mostly rely on coarse-grained alignment between overall image representations and semantic concepts, making it difficult to achieve fine-grained interactions between local visual regions and semantic concepts. This limitation restricts further improvements in practical interpretability and recognition performance. To address this issue, we propose a Language-Guided Global-Local Collaboratively Aligned Multimodal Interpretable Image Recognition Network (LGLCA-Net), which uses text concepts generated by large language models (LLMs) to guide the collaborative alignment of images and text in terms of global and local semantics.The network first designs a concept de-redundancy and visual recognizability verification strategy, driving the large language model to generate high-quality and visually relevant semantic concepts as the foundation for language guidance. Subsequently, we utilize the multimodal space provided by the CLIP model to construct a dual-branch alignment structure for global semantics and local visual semantics.In the local visual semantics branch, we introduce a visual prompting mechanism that extracts discriminative local regions to achieve fine-grained alignment with semantic concepts. Finally, we design a learnable dynamic weighting mechanism to adaptively fuse the alignment information from both branches, achieving collaborative alignment of global-local semantics and semantic concepts. Extensive experiments show that our method not only provides finer-grained and more trustworthy visual explanations but also improves recognition performance.&lt;/p&gt;</content:encoded></item><item><title>Domain-aware Adversarial Domain Augmentation Network for Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2026.3657203</link><guid>10.1109/tip.2026.3657203</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yi Huang</dc:creator><dc:creator>Jiangtao Peng</dc:creator><dc:creator>Weiwei Sun</dc:creator><dc:creator>Na Chen</dc:creator><dc:creator>Zhijing Ye</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657203</prism:doi><description>Classifying hyperspectral remote sensing images across different scenes has recently emerged as a significant challenge. When only historical labeled images (source domain, SD) are available, it is crucial to leverage these images effectively to train a model with strong generalization ability that can be directly applied to classify unseen samples (target domain, TD). To address these challenges, this paper proposes a novel single-domain generalization (SDG) network, termed the domain-aware adversarial domain augmentation network (DADAnet) for cross-scene hyperspectral image classification (HSIC). DADAnet involves two stages: adversarial domain augmentation (ADA) and task-specific training. ADA employs a progressive adversarial generation strategy to construct an augmented domain (AD). To enhance variability in both spatial and spectral dimensions, a domain-aware spatial-spectral mask (DSSM) encoder is constructed to increase the diversity of the generated adversarial samples. Furthermore, a two-level contrastive loss (TCC) is designed and incorporated into the ADA to ensure both the diversity and effectiveness of AD samples. Finally, DADAnet performs supervised learning jointly on the SD and AD during the task-specific training stage. Experimental results on two public hyperspectral image datasets and a new Hangzhouwan (HZW) dataset demonstrate that the proposed DADAnet outperforms existing domain adaptation (DA) and domain generalization (DG) methods, achieving overall accuracies of 80.69%, 63.75%, and 87.61% on three datasets, respectively.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Huang; Jiangtao Peng; Weiwei Sun; Na Chen; Zhijing Ye; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657203"&gt;10.1109/tip.2026.3657203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Classifying hyperspectral remote sensing images across different scenes has recently emerged as a significant challenge. When only historical labeled images (source domain, SD) are available, it is crucial to leverage these images effectively to train a model with strong generalization ability that can be directly applied to classify unseen samples (target domain, TD). To address these challenges, this paper proposes a novel single-domain generalization (SDG) network, termed the domain-aware adversarial domain augmentation network (DADAnet) for cross-scene hyperspectral image classification (HSIC). DADAnet involves two stages: adversarial domain augmentation (ADA) and task-specific training. ADA employs a progressive adversarial generation strategy to construct an augmented domain (AD). To enhance variability in both spatial and spectral dimensions, a domain-aware spatial-spectral mask (DSSM) encoder is constructed to increase the diversity of the generated adversarial samples. Furthermore, a two-level contrastive loss (TCC) is designed and incorporated into the ADA to ensure both the diversity and effectiveness of AD samples. Finally, DADAnet performs supervised learning jointly on the SD and AD during the task-specific training stage. Experimental results on two public hyperspectral image datasets and a new Hangzhouwan (HZW) dataset demonstrate that the proposed DADAnet outperforms existing domain adaptation (DA) and domain generalization (DG) methods, achieving overall accuracies of 80.69%, 63.75%, and 87.61% on three datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>AutoRoadSAM: Multimodal Remote Sensing Road Extraction with Structure-Semantic Awareness via Auto-Prompting Vision Foundation Models</title><link>https://doi.org/10.1109/tgrs.2026.3658664</link><guid>10.1109/tgrs.2026.3658664</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Jiayuan Li</dc:creator><dc:creator>Zhen Wang</dc:creator><dc:creator>Xiao Sun</dc:creator><dc:creator>Zhiyong Lv</dc:creator><dc:creator>Nan Xu</dc:creator><dc:creator>Zhuhong You</dc:creator><dc:creator>Deshuang Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658664</prism:doi><description>The integration of multimodal data holds great promise for advancing road extraction in remote sensing. However, existing approaches are limited by the lack of unified end-to-end frameworks for diverse modality combinations, suboptimal multimodal feature fusion, and challenges in capturing the slender, winding, and complex topological structures of roads. In this paper, we propose AutoRoadSAM, a novel end-to-end framework for multimodal road extraction that fully exploits the powerful visual representation capabilities of the Segment Anything Model (SAM) and, for the first time, introduces an Auto-Prompting Mechanism via a Dynamic Snake Convolution-based Decoder. This decoder adaptively generates task-specific prompts by capturing fine-grained local geometric features from auxiliary modality branches, enabling precise alignment with complex road structures. To further enhance multimodal feature fusion and topological perception, we design the Cross-Modal Information Interaction (CMII) module, which facilitates global context modeling and cross-modal interaction, while strengthening the representation of intricate road topology through multidirectional snake scanning. Moreover, we incorporate a Mask Decoder with Cross Polarity-aware Linear Attention to boost decoding efficiency and effectively address pixel imbalance. Together, these innovations enable AutoRoadSAM to achieve superior structure- and semantic-aware road extraction across diverse modality combinations. Extensive experiments on six public datasets and four modality combinations demonstrate that AutoRoadSAM consistently outperforms state-of-the-art methods, validating the effectiveness and generalization capability of each proposed component. The code is available at https: //github.com/NWPUFranklee/AutoRoadSAM.git.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayuan Li; Zhen Wang; Xiao Sun; Zhiyong Lv; Nan Xu; Zhuhong You; Deshuang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658664"&gt;10.1109/tgrs.2026.3658664&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;The integration of multimodal data holds great promise for advancing road extraction in remote sensing. However, existing approaches are limited by the lack of unified end-to-end frameworks for diverse modality combinations, suboptimal multimodal feature fusion, and challenges in capturing the slender, winding, and complex topological structures of roads. In this paper, we propose AutoRoadSAM, a novel end-to-end framework for multimodal road extraction that fully exploits the powerful visual representation capabilities of the Segment Anything Model (SAM) and, for the first time, introduces an Auto-Prompting Mechanism via a Dynamic Snake Convolution-based Decoder. This decoder adaptively generates task-specific prompts by capturing fine-grained local geometric features from auxiliary modality branches, enabling precise alignment with complex road structures. To further enhance multimodal feature fusion and topological perception, we design the Cross-Modal Information Interaction (CMII) module, which facilitates global context modeling and cross-modal interaction, while strengthening the representation of intricate road topology through multidirectional snake scanning. Moreover, we incorporate a Mask Decoder with Cross Polarity-aware Linear Attention to boost decoding efficiency and effectively address pixel imbalance. Together, these innovations enable AutoRoadSAM to achieve superior structure- and semantic-aware road extraction across diverse modality combinations. Extensive experiments on six public datasets and four modality combinations demonstrate that AutoRoadSAM consistently outperforms state-of-the-art methods, validating the effectiveness and generalization capability of each proposed component. The code is available at https: //github.com/NWPUFranklee/AutoRoadSAM.git.&lt;/p&gt;</content:encoded></item><item><title>Agentic Very Long Video Understanding</title><link>https://arxiv.org/abs/2601.18157v1</link><guid>http://arxiv.org/abs/2601.18157v1</guid><pubDate>Mon, 26 Jan 2026 05:20:47 +0000</pubDate><dc:creator>Aniket Rege</dc:creator><dc:creator>Arka Sadhu</dc:creator><dc:creator>Yuliang Li</dc:creator><dc:creator>Kejie Li</dc:creator><dc:creator>Ramya Korlakai Vinayak</dc:creator><dc:creator>Yuning Chai</dc:creator><dc:creator>Yong Jae Lee</dc:creator><dc:creator>Hyo Jin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.
Published: 2026-01-26T05:20:47+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aniket Rege; Arka Sadhu; Yuliang Li; Kejie Li; Ramya Korlakai Vinayak; Yuning Chai; Yong Jae Lee; Hyo Jin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.&lt;/p&gt;</content:encoded></item><item><title>Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</title><link>https://arxiv.org/abs/2601.21193v1</link><guid>http://arxiv.org/abs/2601.21193v1</guid><pubDate>Thu, 29 Jan 2026 02:49:33 +0000</pubDate><dc:creator>Zecheng Zhao</dc:creator><dc:creator>Zhi Chen</dc:creator><dc:creator>Zi Huang</dc:creator><dc:creator>Shazia Sadiq</dc:creator><dc:creator>Tong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.
Published: 2026-01-29T02:49:33+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zecheng Zhao; Zhi Chen; Zi Huang; Shazia Sadiq; Tong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.&lt;/p&gt;</content:encoded></item><item><title>LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge</title><link>https://arxiv.org/abs/2601.19155v1</link><guid>http://arxiv.org/abs/2601.19155v1</guid><pubDate>Tue, 27 Jan 2026 03:40:03 +0000</pubDate><dc:creator>Qiujun Li</dc:creator><dc:creator>Zijin Xiao</dc:creator><dc:creator>Xulin Wang</dc:creator><dc:creator>Zhidan Ma</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Haifeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.
Published: 2026-01-27T03:40:03+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiujun Li; Zijin Xiao; Xulin Wang; Zhidan Ma; Cheng Yang; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.&lt;/p&gt;</content:encoded></item><item><title>SFSR: Spectral Fusion Super-Resolution for Multi-Sensor Remote Sensing with Degraded References</title><link>https://doi.org/10.1109/tgrs.2026.3658541</link><guid>10.1109/tgrs.2026.3658541</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Seunghyun Gwak</dc:creator><dc:creator>Sooyoung Yang</dc:creator><dc:creator>Myungjoo Kang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658541</prism:doi><description>Reference-based super-resolution (RefSR) aims to enhance low-resolution (LR) imagery by leveraging auxiliary reference observations. While effective under controlled settings, most existing RefSR methods implicitly assume that reference images are clean, well-aligned, and geometrically consistent with the target input. In real-world remote sensing systems, however, reference observations are frequently degraded by sensor noise, atmospheric blur, and geometric inconsistencies caused by different viewing angles and acquisition times. Moreover, due to the inherent resolution gap between LR and reference images, strict spectral consistency is difficult to guarantee in practice. These factors substantially reduce the reliability of reference cues and limit the applicability of RefSR in multi-sensor satellite imaging scenarios. To address these challenges, we propose Spectral Fusion Super-Resolution (SFSR), a diffusion-based RefSR framework designed to operate robustly under degraded reference conditions. At its core, SFSR introduces the Spectral Swin Cross-Attention Module (S2CAM), which enables frequency-aware reference utilization and integrates the refined reference features as conditional guidance within the reverse diffusion process. By explicitly redistributing spectral components and suppressing unreliable high-frequency responses introduced by noise, SFSR enables stable and effective use of reference information that conventional RefSR methods struggle to exploit. Extensive experiments on synthetic and benchmark satellite datasets demonstrate that SFSR consistently outperforms state-of-the-art RefSR approaches in terms of PSNR, SSIM, and perceptual metrics, while maintaining high visual fidelity under severe degradation. In addition, evaluations on downstream tasks such as object detection and semantic segmentation show that SFSR leads to clear performance improvements, confirming its robustness and practical value for real-world multi-sensor remote sensing appli...
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seunghyun Gwak; Sooyoung Yang; Myungjoo Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658541"&gt;10.1109/tgrs.2026.3658541&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Reference-based super-resolution (RefSR) aims to enhance low-resolution (LR) imagery by leveraging auxiliary reference observations. While effective under controlled settings, most existing RefSR methods implicitly assume that reference images are clean, well-aligned, and geometrically consistent with the target input. In real-world remote sensing systems, however, reference observations are frequently degraded by sensor noise, atmospheric blur, and geometric inconsistencies caused by different viewing angles and acquisition times. Moreover, due to the inherent resolution gap between LR and reference images, strict spectral consistency is difficult to guarantee in practice. These factors substantially reduce the reliability of reference cues and limit the applicability of RefSR in multi-sensor satellite imaging scenarios. To address these challenges, we propose Spectral Fusion Super-Resolution (SFSR), a diffusion-based RefSR framework designed to operate robustly under degraded reference conditions. At its core, SFSR introduces the Spectral Swin Cross-Attention Module (S2CAM), which enables frequency-aware reference utilization and integrates the refined reference features as conditional guidance within the reverse diffusion process. By explicitly redistributing spectral components and suppressing unreliable high-frequency responses introduced by noise, SFSR enables stable and effective use of reference information that conventional RefSR methods struggle to exploit. Extensive experiments on synthetic and benchmark satellite datasets demonstrate that SFSR consistently outperforms state-of-the-art RefSR approaches in terms of PSNR, SSIM, and perceptual metrics, while maintaining high visual fidelity under severe degradation. In addition, evaluations on downstream tasks such as object detection and semantic segmentation show that SFSR leads to clear performance improvements, confirming its robustness and practical value for real-world multi-sensor remote sensing appli...&lt;/p&gt;</content:encoded></item><item><title>DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.20064v1</link><guid>http://arxiv.org/abs/2601.20064v1</guid><pubDate>Tue, 27 Jan 2026 21:15:10 +0000</pubDate><dc:creator>Zhen Yao</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Taotao Jing</dc:creator><dc:creator>Shuai Zhang</dc:creator><dc:creator>Mooi Choo Chuah</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.
Published: 2026-01-27T21:15:10+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Yao; Xin Li; Taotao Jing; Shuai Zhang; Mooi Choo Chuah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</title><link>https://arxiv.org/abs/2601.19433v1</link><guid>http://arxiv.org/abs/2601.19433v1</guid><pubDate>Tue, 27 Jan 2026 10:10:55 +0000</pubDate><dc:creator>Jisheng Chu</dc:creator><dc:creator>Wenrui Li</dc:creator><dc:creator>Rui Zhao</dc:creator><dc:creator>Wangmeng Zuo</dc:creator><dc:creator>Shifeng Chen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.
Published: 2026-01-27T10:10:55+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Chu; Wenrui Li; Rui Zhao; Wangmeng Zuo; Shifeng Chen; Xiaopeng Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.&lt;/p&gt;</content:encoded></item><item><title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.22060v1</link><guid>http://arxiv.org/abs/2601.22060v1</guid><pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate><dc:creator>Wenxuan Huang</dc:creator><dc:creator>Yu Zeng</dc:creator><dc:creator>Qiuchen Wang</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Shaosheng Cao</dc:creator><dc:creator>Zheng Chu</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Zhenfei Yin</dc:creator><dc:creator>Lin Chen</dc:creator><dc:creator>Zehui Chen</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Philip Torr</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Wanli Ouyang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.
Published: 2026-01-29T17:58:40+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxuan Huang; Yu Zeng; Qiuchen Wang; Zhen Fang; Shaosheng Cao; Zheng Chu; Qingyu Yin; Shuang Chen; Zhenfei Yin; Lin Chen; Zehui Chen; Yao Hu; Philip Torr; Feng Zhao; Wanli Ouyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&amp;#x27;&amp;#x27; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.&lt;/p&gt;</content:encoded></item></channel></rss>