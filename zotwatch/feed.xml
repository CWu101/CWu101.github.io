<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 06 Jan 2026 02:56:19 +0000</lastBuildDate><item><title>Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing</title><link>https://doi.org/10.1109/tiv.2025.3650682</link><guid>10.1109/tiv.2025.3650682</guid><pubDate>Mon, 05 Jan 2026 18:40:39 +0000</pubDate><dc:creator>Sicen Guo</dc:creator><dc:creator>Tianyou Wen</dc:creator><dc:creator>Chuang-Wei Liu</dc:creator><dc:creator>Qijun Chen</dc:creator><dc:creator>Rui Fan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2025.3650682</prism:doi><description>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.
Published: 2026-01-05T18:40:39+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sicen Guo; Tianyou Wen; Chuang-Wei Liu; Qijun Chen; Rui Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2025.3650682"&gt;10.1109/tiv.2025.3650682&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.&lt;/p&gt;</content:encoded></item><item><title>A Size-Aware Graph Embedding Approach to Remote Sensing Image Captioning with Object Relative Size Information</title><link>https://doi.org/10.1109/tgrs.2026.3650788</link><guid>10.1109/tgrs.2026.3650788</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Zihao Ni</dc:creator><dc:creator>Yinghao Xu</dc:creator><dc:creator>Weibo Zhang</dc:creator><dc:creator>Zhaoyun Zong</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650788</prism:doi><description>Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Ni; Yinghao Xu; Weibo Zhang; Zhaoyun Zong; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650788"&gt;10.1109/tgrs.2026.3650788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction</title><link>https://doi.org/10.1109/tpami.2025.3650478</link><guid>10.1109/tpami.2025.3650478</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Bohan Li</dc:creator><dc:creator>Jiajun Deng</dc:creator><dc:creator>Yasheng Sun</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Xin Jin</dc:creator><dc:creator>Wenjun Zeng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650478</prism:doi><description>Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bohan Li; Jiajun Deng; Yasheng Sun; Xiaofeng Wang; Xin Jin; Wenjun Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650478"&gt;10.1109/tpami.2025.3650478&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp;amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.&lt;/p&gt;</content:encoded></item><item><title>MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.021</link><guid>10.1016/j.isprsjprs.2025.12.021</guid><pubDate>Mon, 05 Jan 2026 11:18:47 +0000</pubDate><dc:creator>Weipeng Jing</dc:creator><dc:creator>Peilun Kang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Lei Fan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.021</prism:doi><description>Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.
Published: 2026-01-05T11:18:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.554 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weipeng Jing; Peilun Kang; Donglin Di; Jian Wang; Yang Song; Chao Li; Lei Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021"&gt;10.1016/j.isprsjprs.2025.12.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.554 (consider)&lt;/p&gt;
&lt;p&gt;Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.&lt;/p&gt;</content:encoded></item><item><title>BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding</title><link>https://arxiv.org/abs/2601.01526v1</link><guid>http://arxiv.org/abs/2601.01526v1</guid><pubDate>Sun, 04 Jan 2026 13:30:06 +0000</pubDate><dc:creator>Hongbing Li</dc:creator><dc:creator>Linhui Xiao</dc:creator><dc:creator>Zihan Zhao</dc:creator><dc:creator>Qi Shen</dc:creator><dc:creator>Yixiang Huang</dc:creator><dc:creator>Bo Xiao</dc:creator><dc:creator>Zhanyu Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.
Published: 2026-01-04T13:30:06+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbing Li; Linhui Xiao; Zihan Zhao; Qi Shen; Yixiang Huang; Bo Xiao; Zhanyu Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.&lt;/p&gt;</content:encoded></item><item><title>Cross-view and Multi-step Interaction for Change Captioning</title><link>https://doi.org/10.1109/tmm.2026.3651125</link><guid>10.1109/tmm.2026.3651125</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Tiantao Xian</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Delu Zeng</dc:creator><dc:creator>Bo Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651125</prism:doi><description>Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tiantao Xian; Zhiheng Zhou; Wenlve Zhou; Delu Zeng; Bo Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651125"&gt;10.1109/tmm.2026.3651125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI&lt;/p&gt;</content:encoded></item><item><title>AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval</title><link>https://arxiv.org/abs/2601.01416v1</link><guid>http://arxiv.org/abs/2601.01416v1</guid><pubDate>Sun, 04 Jan 2026 07:38:51 +0000</pubDate><dc:creator>Yue Zhou</dc:creator><dc:creator>Ran Ding</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Xue Jiang</dc:creator><dc:creator>Xingzhao Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot
Published: 2026-01-04T07:38:51+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhou; Ran Ding; Xue Yang; Xue Jiang; Xingzhao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot&lt;/p&gt;</content:encoded></item><item><title>FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.01513v1</link><guid>http://arxiv.org/abs/2601.01513v1</guid><pubDate>Sun, 04 Jan 2026 12:46:35 +0000</pubDate><dc:creator>Gen Li</dc:creator><dc:creator>Peiyu Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.
Published: 2026-01-04T12:46:35+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gen Li; Peiyu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.&lt;/p&gt;</content:encoded></item><item><title>GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</title><link>https://arxiv.org/abs/2601.00584v1</link><guid>http://arxiv.org/abs/2601.00584v1</guid><pubDate>Fri, 02 Jan 2026 06:04:58 +0000</pubDate><dc:creator>Mingyu Jeon</dc:creator><dc:creator>Sunjae Yoon</dc:creator><dc:creator>Jonghee Kim</dc:creator><dc:creator>Junyeoung Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.
Published: 2026-01-02T06:04:58+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyu Jeon; Sunjae Yoon; Jonghee Kim; Junyeoung Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality&amp;#x27;s representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.&lt;/p&gt;</content:encoded></item><item><title>Text-Injected Discriminative Model for Remote Sensing Visual Grounding</title><link>https://doi.org/10.3390/rs18010161</link><guid>10.3390/rs18010161</guid><pubDate>Mon, 05 Jan 2026 08:40:53 +0000</pubDate><dc:creator>Minhan Hu</dc:creator><dc:creator>Keke Yang</dc:creator><dc:creator>Jing Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010161</prism:doi><description>Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.
Published: 2026-01-05T08:40:53+00:00
Venue: Remote Sensing
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minhan Hu; Keke Yang; Jing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010161"&gt;10.3390/rs18010161&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.&lt;/p&gt;</content:encoded></item><item><title>PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3650671</link><guid>10.1109/tcsvt.2025.3650671</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ke Wang</dc:creator><dc:creator>Weilin Gao</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Tianyi Shao</dc:creator><dc:creator>Liyang Li</dc:creator><dc:creator>Tianqiang Zhou</dc:creator><dc:creator>Jianbo Lu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3650671</prism:doi><description>To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Wang; Weilin Gao; Kai Chen; Tianyi Shao; Liyang Li; Tianqiang Zhou; Jianbo Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3650671"&gt;10.1109/tcsvt.2025.3650671&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.&lt;/p&gt;</content:encoded></item><item><title>Excluding the Interference for Open-Vocabulary Semantic Segmentation</title><link>https://doi.org/10.1109/tcsvt.2026.3650803</link><guid>10.1109/tcsvt.2026.3650803</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Shuai Shao</dc:creator><dc:creator>Shiyuan Zhao</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Baodi Liu</dc:creator><dc:creator>Weifeng Liu</dc:creator><dc:creator>Yicong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650803</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Shao; Shiyuan Zhao; Rui Xu; Yan Wang; Baodi Liu; Weifeng Liu; Yicong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650803"&gt;10.1109/tcsvt.2026.3650803&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.&lt;/p&gt;</content:encoded></item><item><title>A Cascaded Information Interaction Network for Precise Image Segmentation</title><link>https://arxiv.org/abs/2601.00562v1</link><guid>http://arxiv.org/abs/2601.00562v1</guid><pubDate>Fri, 02 Jan 2026 04:33:03 +0000</pubDate><dc:creator>Hewen Xiao</dc:creator><dc:creator>Jie Mei</dc:creator><dc:creator>Guangfu Ma</dc:creator><dc:creator>Weiren Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.
Published: 2026-01-02T04:33:03+00:00
Venue: arXiv
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hewen Xiao; Jie Mei; Guangfu Ma; Weiren Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.&lt;/p&gt;</content:encoded></item><item><title>Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models</title><link>https://doi.org/10.1109/tpami.2026.3650761</link><guid>10.1109/tpami.2026.3650761</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>YiFan Zhang</dc:creator><dc:creator>Qingsong Wen</dc:creator><dc:creator>Chaoyou Fu</dc:creator><dc:creator>Kun Wang</dc:creator><dc:creator>Xue Wang</dc:creator><dc:creator>Zhang Zhang</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Rong Jin</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650761</prism:doi><description>Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YiFan Zhang; Qingsong Wen; Chaoyou Fu; Kun Wang; Xue Wang; Zhang Zhang; Liang Wang; Rong Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650761"&gt;10.1109/tpami.2026.3650761&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.&lt;/p&gt;</content:encoded></item><item><title>LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting</title><link>https://doi.org/10.1109/tpami.2026.3650769</link><guid>10.1109/tpami.2026.3650769</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Yuchen Su</dc:creator><dc:creator>Zhineng Chen</dc:creator><dc:creator>Yongkun Du</dc:creator><dc:creator>Zuxuan Wu</dc:creator><dc:creator>Hongtao Xie</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650769</prism:doi><description>End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Su; Zhineng Chen; Yongkun Du; Zuxuan Wu; Hongtao Xie; Yu-Gang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650769"&gt;10.1109/tpami.2026.3650769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.&lt;/p&gt;</content:encoded></item><item><title>MarineSeg: A CNN–Transformer Hybrid Architecture with Feature Voting Decoder for Robust Semantic Segmentation in USV-Captured Images</title><link>https://doi.org/10.1016/j.neucom.2025.132597</link><guid>10.1016/j.neucom.2025.132597</guid><pubDate>Mon, 05 Jan 2026 16:47:33 +0000</pubDate><dc:creator>Qingyang Gu</dc:creator><dc:creator>Baoyuan Deng</dc:creator><dc:creator>Yunze He</dc:creator><dc:creator>Yongjie Zhang</dc:creator><dc:creator>Liang Cheng</dc:creator><dc:creator>Yaonan Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132597</prism:doi><description>Semantic segmentation is essential for scene perception in unmanned surface vehicles (USVs), particularly in complex and dynamic navigation environments. Although recent Transformer models have outperformed convolutional neural networks (CNNs) in segmentation tasks, their high computational costs and large training data requirements limit their applicability to real-time navigation scenarios. To address these challenges in complex waterway environments, MarineSeg is proposed, a CNN-Transformer hybrid semantic segmentation architecture that achieves high performance while maintaining competitive efficiency. MarineSeg’s backbone integrates sparse self-attention and multi-scale pooling into CoAtNet, which stacks convolutional blocks for capturing local features and attention blocks for modeling global features. To enhance the integration of local convolutional and global attention features, a feature voting module with ensemble mechanism is introduced, facilitating adaptive fusion based on semantic importance. By extracting and integrating multi-level features, MarineSeg achieves state-of-the-art performance among pure image models on the YZ-PLUS, GBA, MODS and MODD2 datasets across key metrics while maintaining competitive real-time capabilities.
Published: 2026-01-05T16:47:33+00:00
Venue: Neurocomputing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyang Gu; Baoyuan Deng; Yunze He; Yongjie Zhang; Liang Cheng; Yaonan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132597"&gt;10.1016/j.neucom.2025.132597&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation is essential for scene perception in unmanned surface vehicles (USVs), particularly in complex and dynamic navigation environments. Although recent Transformer models have outperformed convolutional neural networks (CNNs) in segmentation tasks, their high computational costs and large training data requirements limit their applicability to real-time navigation scenarios. To address these challenges in complex waterway environments, MarineSeg is proposed, a CNN-Transformer hybrid semantic segmentation architecture that achieves high performance while maintaining competitive efficiency. MarineSeg’s backbone integrates sparse self-attention and multi-scale pooling into CoAtNet, which stacks convolutional blocks for capturing local features and attention blocks for modeling global features. To enhance the integration of local convolutional and global attention features, a feature voting module with ensemble mechanism is introduced, facilitating adaptive fusion based on semantic importance. By extracting and integrating multi-level features, MarineSeg achieves state-of-the-art performance among pure image models on the YZ-PLUS, GBA, MODS and MODD2 datasets across key metrics while maintaining competitive real-time capabilities.&lt;/p&gt;</content:encoded></item><item><title>DPS-Net: Direction-Aware Pseudo-Stereo Network for Accurate Road Surface Reconstruction</title><link>https://doi.org/10.1109/tcsvt.2026.3650947</link><guid>10.1109/tcsvt.2026.3650947</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Shiyuan Han</dc:creator><dc:creator>Yidan Pei</dc:creator><dc:creator>Rui Wang</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650947</prism:doi><description>The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyuan Han; Yidan Pei; Rui Wang; Tong Zhang; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650947"&gt;10.1109/tcsvt.2026.3650947&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.&lt;/p&gt;</content:encoded></item><item><title>Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.01781v1</link><guid>http://arxiv.org/abs/2601.01781v1</guid><pubDate>Mon, 05 Jan 2026 04:28:49 +0000</pubDate><dc:creator>Lakshay Sharma</dc:creator><dc:creator>Alex Marin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.
Published: 2026-01-05T04:28:49+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lakshay Sharma; Alex Marin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.&lt;/p&gt;</content:encoded></item><item><title>A SAM Fine-Tuning Framework with Frequency-Domain Interactive LoRA for Remote Sensing Change Detection</title><link>https://doi.org/10.1109/tgrs.2026.3650952</link><guid>10.1109/tgrs.2026.3650952</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Junqing Huang</dc:creator><dc:creator>Shucheng Ji</dc:creator><dc:creator>Yapeng Wang</dc:creator><dc:creator>Min Xia</dc:creator><dc:creator>Xiaochen Yuan</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650952</prism:doi><description>Achieving high-accuracy remote sensing change detection (RSCD) algorithms requires high-quality semantic feature extraction from remote sensing images (RSIs). Due to its powerful general-purpose feature extraction capability, the Segment Anything Model (SAM) has found wide application across diverse fields. However, SAM may not be optimally suited for RSIs. To address this limitation, we propose a Frequency-domain Interactive LoRA Fine-tuning Architecture (FILFArch) to enhance the performance of SAM in RSCD tasks. Based on FILFArch, we then develop two task-specific algorithms, the FILFBCD for Binary Change Detection (BCD), and the FILFSCD for Semantic Change Detection (SCD). To enhance the capability of SAM in capturing bi-temporal RSIs feature relationship, the Bi-temporal Feature Interactive LoRA (BIF-LoRA) is designed with Siamese architecture. Within BIF-LoRA, Frequency-Domain Feature Interaction (FDFI) utilizes Fast Fourier Transform Block (FFTB) to fuse bi-temporal frequency-domain features. This enables cross-temporal frequency-domain interaction, effectively discriminating spatio-temporal feature differences. Additionally, we use a shared BCD Decoder to serves as the binary change detector for both FILFBCD and FILFSCD. The BCD Decoder first applies a Coarse Difference Feature Extraction (CDFE) to coarsely fuse deep semantic features, yielding a coarse-grained change feature map. Subsequently, a Frequency-Domain Feature Enhancement (FDFE) refines these abstract features to generate a fine-grained change map. In FILFSCD, FDFE is further utilized to recover semantic change information of each temporal RSIs. Experimental results demonstrate that FILFBCD achieves the highest F1 scores of 83.53%, 66.75%, and 83.79% on BCD datasets MLCD, S2Looking, and SYSU-CD, respectively. Meanwhile, FILFSCD achieves the highest F1 scores of 64.05% and 87.02% on SCD datasets SECOND, and DSCD, respectively. These results demonstrate the effectiveness and versatility of the propos...
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junqing Huang; Shucheng Ji; Yapeng Wang; Min Xia; Xiaochen Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650952"&gt;10.1109/tgrs.2026.3650952&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Achieving high-accuracy remote sensing change detection (RSCD) algorithms requires high-quality semantic feature extraction from remote sensing images (RSIs). Due to its powerful general-purpose feature extraction capability, the Segment Anything Model (SAM) has found wide application across diverse fields. However, SAM may not be optimally suited for RSIs. To address this limitation, we propose a Frequency-domain Interactive LoRA Fine-tuning Architecture (FILFArch) to enhance the performance of SAM in RSCD tasks. Based on FILFArch, we then develop two task-specific algorithms, the FILFBCD for Binary Change Detection (BCD), and the FILFSCD for Semantic Change Detection (SCD). To enhance the capability of SAM in capturing bi-temporal RSIs feature relationship, the Bi-temporal Feature Interactive LoRA (BIF-LoRA) is designed with Siamese architecture. Within BIF-LoRA, Frequency-Domain Feature Interaction (FDFI) utilizes Fast Fourier Transform Block (FFTB) to fuse bi-temporal frequency-domain features. This enables cross-temporal frequency-domain interaction, effectively discriminating spatio-temporal feature differences. Additionally, we use a shared BCD Decoder to serves as the binary change detector for both FILFBCD and FILFSCD. The BCD Decoder first applies a Coarse Difference Feature Extraction (CDFE) to coarsely fuse deep semantic features, yielding a coarse-grained change feature map. Subsequently, a Frequency-Domain Feature Enhancement (FDFE) refines these abstract features to generate a fine-grained change map. In FILFSCD, FDFE is further utilized to recover semantic change information of each temporal RSIs. Experimental results demonstrate that FILFBCD achieves the highest F1 scores of 83.53%, 66.75%, and 83.79% on BCD datasets MLCD, S2Looking, and SYSU-CD, respectively. Meanwhile, FILFSCD achieves the highest F1 scores of 64.05% and 87.02% on SCD datasets SECOND, and DSCD, respectively. These results demonstrate the effectiveness and versatility of the propos...&lt;/p&gt;</content:encoded></item><item><title>MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs</title><link>https://doi.org/10.1109/tpami.2025.3650545</link><guid>10.1109/tpami.2025.3650545</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Jianjian Cao</dc:creator><dc:creator>Chong Yu</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Tao Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650545</prism:doi><description>Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianjian Cao; Chong Yu; Peng Ye; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650545"&gt;10.1109/tpami.2025.3650545&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation</title><link>https://doi.org/10.1109/tcsvt.2026.3651056</link><guid>10.1109/tcsvt.2026.3651056</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ming Li</dc:creator><dc:creator>Xuejiao Hu</dc:creator><dc:creator>Zihang Gao</dc:creator><dc:creator>Sidan Du</dc:creator><dc:creator>Yang Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651056</prism:doi><description>Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Li; Xuejiao Hu; Zihang Gao; Sidan Du; Yang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651056"&gt;10.1109/tcsvt.2026.3651056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification</title><link>https://doi.org/10.1007/s11263-025-02584-3</link><guid>10.1007/s11263-025-02584-3</guid><pubDate>Sun, 04 Jan 2026 02:39:12 +0000</pubDate><dc:creator>Mingyao Hong</dc:creator><dc:creator>Xinfeng Zhang</dc:creator><dc:creator>Guorong Li</dc:creator><dc:creator>Qingming Huang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02584-3</prism:doi><description>Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.
Published: 2026-01-04T02:39:12+00:00
Venue: International Journal of Computer Vision
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyao Hong; Xinfeng Zhang; Guorong Li; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02584-3"&gt;10.1007/s11263-025-02584-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.&lt;/p&gt;</content:encoded></item><item><title>Parameter-Aware Mamba Model for Multitask Dense Prediction</title><link>https://doi.org/10.1109/tcyb.2025.3634359</link><guid>10.1109/tcyb.2025.3634359</guid><pubDate>Mon, 05 Jan 2026 18:39:27 +0000</pubDate><dc:creator>Xinzhuo Yu</dc:creator><dc:creator>Yunzhi Zhuge</dc:creator><dc:creator>Sitong Gong</dc:creator><dc:creator>Lu Zhang</dc:creator><dc:creator>Pingping Zhang</dc:creator><dc:creator>Huchuan Lu</dc:creator><prism:publicationName>IEEE Transactions on Cybernetics</prism:publicationName><prism:doi>10.1109/tcyb.2025.3634359</prism:doi><description>Understanding the inter-relations and interactions between tasks is crucial for multitask dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, parameter-aware Mamba model (PAMM), specifically designed for dense prediction in multitask learning (MTL) setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state-space models (SSMs) to enhance task interconnectivity. It features dual state-space parameter experts (PEs) that integrate and set task-specific parameter priors (PPs), capturing the intrinsic properties of each task. This approach not only facilitates precise multitask interactions but also allows for the global integration of task priors through the structured state-space sequence (S4) model. Furthermore, we employ the multidirectional Hilbert scanning (MDHS) method to construct multiangle feature sequences, thereby enhancing the sequence model’s perceptual capabilities for 2-D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM
Published: 2026-01-05T18:39:27+00:00
Venue: IEEE Transactions on Cybernetics
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinzhuo Yu; Yunzhi Zhuge; Sitong Gong; Lu Zhang; Pingping Zhang; Huchuan Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Cybernetics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcyb.2025.3634359"&gt;10.1109/tcyb.2025.3634359&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Understanding the inter-relations and interactions between tasks is crucial for multitask dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, parameter-aware Mamba model (PAMM), specifically designed for dense prediction in multitask learning (MTL) setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state-space models (SSMs) to enhance task interconnectivity. It features dual state-space parameter experts (PEs) that integrate and set task-specific parameter priors (PPs), capturing the intrinsic properties of each task. This approach not only facilitates precise multitask interactions but also allows for the global integration of task priors through the structured state-space sequence (S4) model. Furthermore, we employ the multidirectional Hilbert scanning (MDHS) method to construct multiangle feature sequences, thereby enhancing the sequence model’s perceptual capabilities for 2-D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM&lt;/p&gt;</content:encoded></item><item><title>ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval</title><link>https://arxiv.org/abs/2601.01024v1</link><guid>http://arxiv.org/abs/2601.01024v1</guid><pubDate>Sat, 03 Jan 2026 01:19:36 +0000</pubDate><dc:creator>Tien-Huy Nguyen</dc:creator><dc:creator>Huu-Loc Tran</dc:creator><dc:creator>Thanh Duc Ngo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself
Published: 2026-01-03T01:19:36+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tien-Huy Nguyen; Huu-Loc Tran; Thanh Duc Ngo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model&amp;#x27;s own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself&lt;/p&gt;</content:encoded></item><item><title>Efficient Human Feature Refinement for Weakly Supervised Group Activity Recognition</title><link>https://doi.org/10.1109/tmm.2026.3651017</link><guid>10.1109/tmm.2026.3651017</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Yihui Zhou</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Zhanzhou Feng</dc:creator><dc:creator>Shiliang Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651017</prism:doi><description>Weakly supervised group activity recognition (WSGAR) aims to identify the joint activity of a group of people without relying on hand-annotated human bounding boxes. Existing WSGAR methods typically acquire coarse human-level features by pooling from detected bounding boxes or applying human queries with cross attentions. These approaches focus on learning human relations from the acquired features. However, discriminative person-specific clues might be confused with irrelevant backgrounds, hindering the effectiveness of downstream human relation learning. To address this limitation, we propose a Human Feature Refinement framework that enhances human-level information with graph convolutional networks and self-attention. We define in-box regions as tokens and learn their spatial correspondence through GCN and self-attention. By explicitly extracting in-box details and suppressing irrelevant regions, our method acquires more discriminative human-level features for relation learning and group activity prediction. We further propose a Graph-based Token Merging algorithm to reduce the computation cost of Human Feature Refinement, while minimizing information loss and overfitting risk. Experiments show that our method outperforms previous WSGAR methods on Volleyball, NBA and JRDB-PAR benchmarks, with reduced computation cost.
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihui Zhou; Hao Chen; Zhanzhou Feng; Shiliang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651017"&gt;10.1109/tmm.2026.3651017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised group activity recognition (WSGAR) aims to identify the joint activity of a group of people without relying on hand-annotated human bounding boxes. Existing WSGAR methods typically acquire coarse human-level features by pooling from detected bounding boxes or applying human queries with cross attentions. These approaches focus on learning human relations from the acquired features. However, discriminative person-specific clues might be confused with irrelevant backgrounds, hindering the effectiveness of downstream human relation learning. To address this limitation, we propose a Human Feature Refinement framework that enhances human-level information with graph convolutional networks and self-attention. We define in-box regions as tokens and learn their spatial correspondence through GCN and self-attention. By explicitly extracting in-box details and suppressing irrelevant regions, our method acquires more discriminative human-level features for relation learning and group activity prediction. We further propose a Graph-based Token Merging algorithm to reduce the computation cost of Human Feature Refinement, while minimizing information loss and overfitting risk. Experiments show that our method outperforms previous WSGAR methods on Volleyball, NBA and JRDB-PAR benchmarks, with reduced computation cost.&lt;/p&gt;</content:encoded></item><item><title>Describing Land Cover Changes via Multi-Temporal Remote Sensing Image Captioning Using LLM, ViT, and LoRA</title><link>https://doi.org/10.3390/rs18010166</link><guid>10.3390/rs18010166</guid><pubDate>Mon, 05 Jan 2026 08:40:53 +0000</pubDate><dc:creator>Javier Lamar Léon</dc:creator><dc:creator>Vitor Nogueira</dc:creator><dc:creator>Pedro Salgueiro</dc:creator><dc:creator>Paulo Quaresma</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010166</prism:doi><description>Describing land cover changes from multi-temporal remote sensing imagery requires capturing both visual transformations and their semantic meaning in natural language. Existing methods often struggle to balance visual accuracy with descriptive coherence. We propose MVLT-LoRA-CC (Multi-modal Vision Language Transformer with Low-Rank Adaptation for Change Captioning), a framework that integrates a Vision Transformer (ViT), a Large Language Model (LLM), and Low-Rank Adaptation (LoRA) for efficient multi-modal learning. The model processes paired temporal images through patch embeddings and transformer blocks, aligning visual and textual representations via a multi-modal adapter. To improve efficiency and avoid unnecessary parameter growth, LoRA modules are selectively inserted only into the attention projection layers and cross-modal adapter blocks rather than being uniformly applied to all linear layers. This targeted design preserves general linguistic knowledge while enabling effective adaptation to remote sensing change description. To assess performance, we introduce the Complementary Consistency Score (CCS) framework, which evaluates both descriptive fidelity for change instances and classification accuracy for no change cases. Experiments on the LEVIR-CC test set demonstrate that MVLT-LoRA-CC generates semantically accurate captions, surpassing prior methods in both descriptive richness and temporal change recognition. The approach establishes a scalable solution for multi-modal land cover change description in remote sensing applications.
Published: 2026-01-05T08:40:53+00:00
Venue: Remote Sensing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Javier Lamar Léon; Vitor Nogueira; Pedro Salgueiro; Paulo Quaresma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010166"&gt;10.3390/rs18010166&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Describing land cover changes from multi-temporal remote sensing imagery requires capturing both visual transformations and their semantic meaning in natural language. Existing methods often struggle to balance visual accuracy with descriptive coherence. We propose MVLT-LoRA-CC (Multi-modal Vision Language Transformer with Low-Rank Adaptation for Change Captioning), a framework that integrates a Vision Transformer (ViT), a Large Language Model (LLM), and Low-Rank Adaptation (LoRA) for efficient multi-modal learning. The model processes paired temporal images through patch embeddings and transformer blocks, aligning visual and textual representations via a multi-modal adapter. To improve efficiency and avoid unnecessary parameter growth, LoRA modules are selectively inserted only into the attention projection layers and cross-modal adapter blocks rather than being uniformly applied to all linear layers. This targeted design preserves general linguistic knowledge while enabling effective adaptation to remote sensing change description. To assess performance, we introduce the Complementary Consistency Score (CCS) framework, which evaluates both descriptive fidelity for change instances and classification accuracy for no change cases. Experiments on the LEVIR-CC test set demonstrate that MVLT-LoRA-CC generates semantically accurate captions, surpassing prior methods in both descriptive richness and temporal change recognition. The approach establishes a scalable solution for multi-modal land cover change description in remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery</title><link>https://doi.org/10.3390/rs18010176</link><guid>10.3390/rs18010176</guid><pubDate>Mon, 05 Jan 2026 12:38:56 +0000</pubDate><dc:creator>Wenhao Xu</dc:creator><dc:creator>You Yang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010176</prism:doi><description>Multimodal remote sensing imagery, such as visible and infrared data, offers crucial complementary information that is vital for time-sensitive emergency applications like search and rescue or disaster monitoring, where robust detection under adverse conditions is essential. However, existing methods’ object detection performance is often suboptimal due to task-independent fusion and inherent modality inconsistency. To address this issue, we propose a joint fusion and detection approach for multimodal remote sensing imagery (JFDet). First, a gradient-enhanced residual module (GERM) is introduced to combine dense feature connections with gradient residual pathways, effectively enhancing structural representation and fine-grained texture details in fused images. For robust detection, we introduce a second-order channel attention (SOCA) mechanism and design a multi-scale contextual feature-encoding (MCFE) module to capture higher-order semantic dependencies, enrich multi-scale contextual information, and thereby improve the recognition of small and variably scaled objects. Furthermore, a dual-loss feedback strategy propagates detection loss to the fusion network, enabling adaptive synergy between low-level fusion and high-level detection. Experiments on the VEDAI and FLIR-ADAS datasets demonstrate that the proposed detection-driven fusion framework significantly improves both fusion quality and detection accuracy compared with state-of-the-art methods, highlighting its effectiveness and high potential for mission-critical multimodal remote sensing and time-sensitive application.
Published: 2026-01-05T12:38:56+00:00
Venue: Remote Sensing
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhao Xu; You Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010176"&gt;10.3390/rs18010176&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal remote sensing imagery, such as visible and infrared data, offers crucial complementary information that is vital for time-sensitive emergency applications like search and rescue or disaster monitoring, where robust detection under adverse conditions is essential. However, existing methods’ object detection performance is often suboptimal due to task-independent fusion and inherent modality inconsistency. To address this issue, we propose a joint fusion and detection approach for multimodal remote sensing imagery (JFDet). First, a gradient-enhanced residual module (GERM) is introduced to combine dense feature connections with gradient residual pathways, effectively enhancing structural representation and fine-grained texture details in fused images. For robust detection, we introduce a second-order channel attention (SOCA) mechanism and design a multi-scale contextual feature-encoding (MCFE) module to capture higher-order semantic dependencies, enrich multi-scale contextual information, and thereby improve the recognition of small and variably scaled objects. Furthermore, a dual-loss feedback strategy propagates detection loss to the fusion network, enabling adaptive synergy between low-level fusion and high-level detection. Experiments on the VEDAI and FLIR-ADAS datasets demonstrate that the proposed detection-driven fusion framework significantly improves both fusion quality and detection accuracy compared with state-of-the-art methods, highlighting its effectiveness and high potential for mission-critical multimodal remote sensing and time-sensitive application.&lt;/p&gt;</content:encoded></item><item><title>Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios</title><link>https://arxiv.org/abs/2601.00537v1</link><guid>http://arxiv.org/abs/2601.00537v1</guid><pubDate>Fri, 02 Jan 2026 02:42:04 +0000</pubDate><dc:creator>Guangqian Guo</dc:creator><dc:creator>Pengfei Chen</dc:creator><dc:creator>Yong Guo</dc:creator><dc:creator>Huafeng Chen</dc:creator><dc:creator>Boqiang Zhang</dc:creator><dc:creator>Shan Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.
Published: 2026-01-02T02:42:04+00:00
Venue: arXiv
Score: 0.508 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangqian Guo; Pengfei Chen; Yong Guo; Huafeng Chen; Boqiang Zhang; Shan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.508 (consider)&lt;/p&gt;
&lt;p&gt;Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM&amp;#x27;s perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM&amp;#x27;s low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model&amp;#x27;s segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.&lt;/p&gt;</content:encoded></item><item><title>MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images</title><link>https://doi.org/10.1109/tgrs.2026.3650963</link><guid>10.1109/tgrs.2026.3650963</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Jian Zhang</dc:creator><dc:creator>Jiarong Lv</dc:creator><dc:creator>Heng Zhang</dc:creator><dc:creator>Ming Li</dc:creator><dc:creator>Meng Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650963</prism:doi><description>In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Zhang; Jiarong Lv; Heng Zhang; Ming Li; Meng Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650963"&gt;10.1109/tgrs.2026.3650963&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.&lt;/p&gt;</content:encoded></item><item><title>A Lightweight Hybrid Gabor Deep Learning Approach and its Application to Medical Image Classification</title><link>https://doi.org/10.1007/s11263-025-02658-2</link><guid>10.1007/s11263-025-02658-2</guid><pubDate>Mon, 05 Jan 2026 16:47:16 +0000</pubDate><dc:creator>Rayyan Ahmed</dc:creator><dc:creator>Hamza Baali</dc:creator><dc:creator>Abdesselam Bouzerdoum</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02658-2</prism:doi><description>Abstract Deep learning has revolutionized image analysis, but its applications are limited by the need for large datasets and high computational resources. Hybrid approaches that combine domain-specific, universal feature extractor with learnable neural networks offer a promising balance of efficiency and accuracy. This paper presents a hybrid model integrating a Gabor filter bank front-end with compact neural networks for efficient feature extraction and classification. Gabor filters, inherently bandpass, extract early-stage features with spatially shifted filters covering the frequency plane to balance spatial and spectral localization. We introduce separate channels capturing low- and high-frequency components to enhance feature representation while maintaining efficiency. The approach reduces trainable parameters and training time while preserving accuracy, making it suitable for resource-constrained environments. Compared to MobileNetV2 and EfficientNetB0, our model trains approximately 4–6 × faster on average while using fewer parameters and FLOPs. We compare it to pretrained networks used as feature extractors, lightweight fine-tuned models, and classical descriptors (HOG, LBP). It achieves competitive results with faster training and reduced computation. The hybrid model uses only around 0.60 GFLOPs and 0.34 M parameters, and we apply statistical significance testing (ANOVA, paired t-tests) to validate performance gains. Inference takes 0.01–0.02 s per image, up to 15 × faster than EfficientNetB0 and 8 × faster than MobileNetV2. Grad-CAM visualizations confirm localized attention on relevant regions. This work highlights integrating traditional features with deep learning to improve efficiency for resource-limited applications. Future work will address color fusion, robustness to noise, and automated filter optimization.
Published: 2026-01-05T16:47:16+00:00
Venue: International Journal of Computer Vision
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rayyan Ahmed; Hamza Baali; Abdesselam Bouzerdoum&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02658-2"&gt;10.1007/s11263-025-02658-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Abstract Deep learning has revolutionized image analysis, but its applications are limited by the need for large datasets and high computational resources. Hybrid approaches that combine domain-specific, universal feature extractor with learnable neural networks offer a promising balance of efficiency and accuracy. This paper presents a hybrid model integrating a Gabor filter bank front-end with compact neural networks for efficient feature extraction and classification. Gabor filters, inherently bandpass, extract early-stage features with spatially shifted filters covering the frequency plane to balance spatial and spectral localization. We introduce separate channels capturing low- and high-frequency components to enhance feature representation while maintaining efficiency. The approach reduces trainable parameters and training time while preserving accuracy, making it suitable for resource-constrained environments. Compared to MobileNetV2 and EfficientNetB0, our model trains approximately 4–6 × faster on average while using fewer parameters and FLOPs. We compare it to pretrained networks used as feature extractors, lightweight fine-tuned models, and classical descriptors (HOG, LBP). It achieves competitive results with faster training and reduced computation. The hybrid model uses only around 0.60 GFLOPs and 0.34 M parameters, and we apply statistical significance testing (ANOVA, paired t-tests) to validate performance gains. Inference takes 0.01–0.02 s per image, up to 15 × faster than EfficientNetB0 and 8 × faster than MobileNetV2. Grad-CAM visualizations confirm localized attention on relevant regions. This work highlights integrating traditional features with deep learning to improve efficiency for resource-limited applications. Future work will address color fusion, robustness to noise, and automated filter optimization.&lt;/p&gt;</content:encoded></item></channel></rss>