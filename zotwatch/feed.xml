<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 26 Jan 2026 02:53:17 +0000</lastBuildDate><item><title>Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images</title><link>https://doi.org/10.1016/j.patcog.2026.113120</link><guid>10.1016/j.patcog.2026.113120</guid><pubDate>Sat, 24 Jan 2026 00:15:28 +0000</pubDate><dc:creator>Da Zhang</dc:creator><dc:creator>Mingmin Zeng</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113120</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.
Published: 2026-01-24T00:15:28+00:00
Venue: Pattern Recognition
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Zhang; Mingmin Zeng; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113120"&gt;10.1016/j.patcog.2026.113120&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.&lt;/p&gt;</content:encoded></item><item><title>HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</title><link>https://arxiv.org/abs/2601.16155v1</link><guid>http://arxiv.org/abs/2601.16155v1</guid><pubDate>Thu, 22 Jan 2026 17:57:42 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Sihang Cai</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.
Published: 2026-01-22T17:57:42+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Xin Liu; Boyun Zhang; Yuxiao Lin; Sihang Cai; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &amp;quot;blind&amp;quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Summarization via Coarse-and-Fine Granularity Synergy and Region Counterfactual Reasoning Filter</title><link>https://doi.org/10.1016/j.knosys.2026.115356</link><guid>10.1016/j.knosys.2026.115356</guid><pubDate>Sat, 24 Jan 2026 16:15:00 +0000</pubDate><dc:creator>Rulong Liu</dc:creator><dc:creator>Qing He</dc:creator><dc:creator>Yuji Wang</dc:creator><dc:creator>Nisuo Du</dc:creator><dc:creator>Zhihao Yang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115356</prism:doi><description>Multimodal Summarization (MS) generates high-quality summaries by integrating textual and visual information. However, existing MS research faces several challenges, including (1) ignoring fine-grained key information between visual and textual modalities and interaction with coarse-grained information, (2) cross-modal semantic inconsistency, which hinders alignment and fusion of visual and textual feature spaces, and (3) ignoring inherent heterogeneity of an image when filtering visual information, which causes excessive filtering or excessive retention. To address these issues, we propose Coarse-and-Fine Granularity Synergy and Region Counterfactual Reasoning Filter (CFCR) for MS. Specifically, we design Coarse-and-Fine Granularity Synergy (CFS) to capture both global (coarse-grained) and important detailed (fine-grained) information in text and image modalities. Based on this, we design Dual-granularity Contrastive Learning (DCL) for mapping coarse-grained and fine-grained visual features into the text semantic space, thereby reducing semantic inconsistency caused by modality differences at dual granularity levels, and facilitating cross-modal alignment. To address the issue of excessive filtering or excessive retention in visual information filtering, we design a Region Counterfactual Reasoning Filter (RCF) that employs Counterfactual Reasoning to determine the validity of image regions and generate category labels. These labels are then used to train Image Region Selector to select regions beneficial for summarization. Extensive experiments on the representative MMSS and MSMO dataset show that CFCR outperforms multiple strong baselines, particularly in terms of selecting and focusing on critical details, demonstrating its effectiveness in MS.
Published: 2026-01-24T16:15:00+00:00
Venue: Knowledge-Based Systems
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rulong Liu; Qing He; Yuji Wang; Nisuo Du; Zhihao Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115356"&gt;10.1016/j.knosys.2026.115356&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Summarization (MS) generates high-quality summaries by integrating textual and visual information. However, existing MS research faces several challenges, including (1) ignoring fine-grained key information between visual and textual modalities and interaction with coarse-grained information, (2) cross-modal semantic inconsistency, which hinders alignment and fusion of visual and textual feature spaces, and (3) ignoring inherent heterogeneity of an image when filtering visual information, which causes excessive filtering or excessive retention. To address these issues, we propose Coarse-and-Fine Granularity Synergy and Region Counterfactual Reasoning Filter (CFCR) for MS. Specifically, we design Coarse-and-Fine Granularity Synergy (CFS) to capture both global (coarse-grained) and important detailed (fine-grained) information in text and image modalities. Based on this, we design Dual-granularity Contrastive Learning (DCL) for mapping coarse-grained and fine-grained visual features into the text semantic space, thereby reducing semantic inconsistency caused by modality differences at dual granularity levels, and facilitating cross-modal alignment. To address the issue of excessive filtering or excessive retention in visual information filtering, we design a Region Counterfactual Reasoning Filter (RCF) that employs Counterfactual Reasoning to determine the validity of image regions and generate category labels. These labels are then used to train Image Region Selector to select regions beneficial for summarization. Extensive experiments on the representative MMSS and MSMO dataset show that CFCR outperforms multiple strong baselines, particularly in terms of selecting and focusing on critical details, demonstrating its effectiveness in MS.&lt;/p&gt;</content:encoded></item><item><title>X-Aligner: Composed Visual Retrieval without the Bells and Whistles</title><link>https://arxiv.org/abs/2601.16582v1</link><guid>http://arxiv.org/abs/2601.16582v1</guid><pubDate>Fri, 23 Jan 2026 09:33:38 +0000</pubDate><dc:creator>Yuqian Zheng</dc:creator><dc:creator>Mariana-Iuliana Georgescu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.
Published: 2026-01-23T09:33:38+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqian Zheng; Mariana-Iuliana Georgescu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.&lt;/p&gt;</content:encoded></item><item><title>Natural Language-Driven Global Mapping of Martian Landforms</title><link>https://arxiv.org/abs/2601.15949v1</link><guid>http://arxiv.org/abs/2601.15949v1</guid><pubDate>Thu, 22 Jan 2026 13:38:13 +0000</pubDate><dc:creator>Yiran Wang</dc:creator><dc:creator>Shuoyuan Wang</dc:creator><dc:creator>Zhaoran Wei</dc:creator><dc:creator>Jiannan Zhao</dc:creator><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Zejian Xie</dc:creator><dc:creator>Songxin Zhang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Bingyi Jing</dc:creator><dc:creator>Hongxin Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.
Published: 2026-01-22T13:38:13+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiran Wang; Shuoyuan Wang; Zhaoran Wei; Jiannan Zhao; Zhonghua Yao; Zejian Xie; Songxin Zhang; Jun Huang; Bingyi Jing; Hongxin Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.&lt;/p&gt;</content:encoded></item><item><title>Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</title><link>https://arxiv.org/abs/2601.15780v1</link><guid>http://arxiv.org/abs/2601.15780v1</guid><pubDate>Thu, 22 Jan 2026 09:14:11 +0000</pubDate><dc:creator>Pascal Benschop</dc:creator><dc:creator>Justin Dauwels</dc:creator><dc:creator>Jan van Gemert</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.
Published: 2026-01-22T09:14:11+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pascal Benschop; Justin Dauwels; Jan van Gemert&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.&lt;/p&gt;</content:encoded></item><item><title>ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search</title><link>https://arxiv.org/abs/2601.15931v1</link><guid>http://arxiv.org/abs/2601.15931v1</guid><pubDate>Thu, 22 Jan 2026 13:09:22 +0000</pubDate><dc:creator>Xiangyu Wang</dc:creator><dc:creator>Zhixin Lv</dc:creator><dc:creator>Yongjiao Sun</dc:creator><dc:creator>Anrui Han</dc:creator><dc:creator>Ye Yuan</dc:creator><dc:creator>Hangxu Ji</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.
Published: 2026-01-22T13:09:22+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Wang; Zhixin Lv; Yongjiao Sun; Anrui Han; Ye Yuan; Hangxu Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &amp;quot;Passive Observation&amp;quot; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.&lt;/p&gt;</content:encoded></item><item><title>OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding</title><link>https://arxiv.org/abs/2601.16538v1</link><guid>http://arxiv.org/abs/2601.16538v1</guid><pubDate>Fri, 23 Jan 2026 08:17:57 +0000</pubDate><dc:creator>Zixian Liu</dc:creator><dc:creator>Zhaoxi Chen</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.
Published: 2026-01-23T08:17:57+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixian Liu; Zhaoxi Chen; Liang Pan; Ziwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.&lt;/p&gt;</content:encoded></item><item><title>Keyframe-Based Feed-Forward Visual Odometry</title><link>https://arxiv.org/abs/2601.16020v1</link><guid>http://arxiv.org/abs/2601.16020v1</guid><pubDate>Thu, 22 Jan 2026 14:45:42 +0000</pubDate><dc:creator>Weichen Dai</dc:creator><dc:creator>Wenhan Su</dc:creator><dc:creator>Da Kong</dc:creator><dc:creator>Yuhang Ming</dc:creator><dc:creator>Wanzeng Kong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.
Published: 2026-01-22T14:45:42+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weichen Dai; Wenhan Su; Da Kong; Yuhang Ming; Wanzeng Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.&lt;/p&gt;</content:encoded></item><item><title>Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models</title><link>https://arxiv.org/abs/2601.16378v1</link><guid>http://arxiv.org/abs/2601.16378v1</guid><pubDate>Fri, 23 Jan 2026 00:21:27 +0000</pubDate><dc:creator>Bridget Leonard</dc:creator><dc:creator>Scott O. Murray</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.
Published: 2026-01-23T00:21:27+00:00
Venue: arXiv
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bridget Leonard; Scott O. Murray&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&amp;#x27;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.&lt;/p&gt;</content:encoded></item><item><title>Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion</title><link>https://arxiv.org/abs/2601.15829v1</link><guid>http://arxiv.org/abs/2601.15829v1</guid><pubDate>Thu, 22 Jan 2026 10:30:32 +0000</pubDate><dc:creator>Yonghao Xu</dc:creator><dc:creator>Pedram Ghamisi</dc:creator><dc:creator>Qihao Weng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).
Published: 2026-01-22T10:30:32+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonghao Xu; Pedram Ghamisi; Qihao Weng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).&lt;/p&gt;</content:encoded></item><item><title>REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion</title><link>https://arxiv.org/abs/2601.16788v1</link><guid>http://arxiv.org/abs/2601.16788v1</guid><pubDate>Fri, 23 Jan 2026 14:33:49 +0000</pubDate><dc:creator>Xuewei Li</dc:creator><dc:creator>Xinghan Bao</dc:creator><dc:creator>Zhimin Chen</dc:creator><dc:creator>Xi Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.
Published: 2026-01-23T14:33:49+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuewei Li; Xinghan Bao; Zhimin Chen; Xi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.&lt;/p&gt;</content:encoded></item><item><title>VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents</title><link>https://arxiv.org/abs/2601.16973v1</link><guid>http://arxiv.org/abs/2601.16973v1</guid><pubDate>Fri, 23 Jan 2026 18:43:34 +0000</pubDate><dc:creator>Zirui Wang</dc:creator><dc:creator>Junyi Zhang</dc:creator><dc:creator>Jiaxin Ge</dc:creator><dc:creator>Long Lian</dc:creator><dc:creator>Letian Fu</dc:creator><dc:creator>Lisa Dunlap</dc:creator><dc:creator>Ken Goldberg</dc:creator><dc:creator>XuDong Wang</dc:creator><dc:creator>Ion Stoica</dc:creator><dc:creator>David M. Chan</dc:creator><dc:creator>Sewon Min</dc:creator><dc:creator>Joseph E. Gonzalez</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.
Published: 2026-01-23T18:43:34+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zirui Wang; Junyi Zhang; Jiaxin Ge; Long Lian; Letian Fu; Lisa Dunlap; Ken Goldberg; XuDong Wang; Ion Stoica; David M. Chan; Sewon Min; Joseph E. Gonzalez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.&lt;/p&gt;</content:encoded></item><item><title>VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning</title><link>https://arxiv.org/abs/2601.15724v1</link><guid>http://arxiv.org/abs/2601.15724v1</guid><pubDate>Thu, 22 Jan 2026 07:47:29 +0000</pubDate><dc:creator>Chenglin Li</dc:creator><dc:creator>Qianglong Chen</dc:creator><dc:creator>Feng Han</dc:creator><dc:creator>Yikun Wang</dc:creator><dc:creator>Xingxi Yin</dc:creator><dc:creator>Yan Gong</dc:creator><dc:creator>Ruilin Li</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.
Published: 2026-01-22T07:47:29+00:00
Venue: arXiv
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenglin Li; Qianglong Chen; Feng Han; Yikun Wang; Xingxi Yin; Yan Gong; Ruilin Li; Yin Zhang; Jiaqi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.&lt;/p&gt;</content:encoded></item><item><title>Learning Gated Experts for Segment Anything in the Wild</title><link>https://doi.org/10.1016/j.patcog.2026.113064</link><guid>10.1016/j.patcog.2026.113064</guid><pubDate>Sat, 24 Jan 2026 07:41:53 +0000</pubDate><dc:creator>Yizhen Guo</dc:creator><dc:creator>Hang Guo</dc:creator><dc:creator>Tao Dai</dc:creator><dc:creator>Zhi Wang</dc:creator><dc:creator>Bin Chen</dc:creator><dc:creator>Shu-Tao Xia</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113064</prism:doi><description>Segment anything model (SAM) and its variants have recently shown promising performance as foundation models. However, existing SAM-based models can only handle scenarios seen during training, and usually suffer unstable performance when transferring to real-world unseen data, such as low-light, rainy, or blurred images, which is crucial for applications such as autopilot. Therefore, adapting SAM-based models for real-world degradation while not impairing their original ability remains an open challenge. In this work, we propose a novel gated Mixture-of-Experts (MoE) structure, called RouGE, to improve the robustness of SAM-based models. Specifically, RouGE uses multiple lightweight probability gates to decompose complex real-world image conditions and judge whether the feature needs to be adjusted, as well as to what extent the adjustment needs to be done, then handles them differently with a set of low-rank experts. During the inference stage, RouGE processes input images in a completely blind manner, thus improving the models performance in real-world scenarios. Extensive experiments demonstrate that RouGE consistently achieves state-of-the-art results on both degraded and clean images compared with other methods while tuning only 1.5% of parameters. Our source code is publicly accessible via: https://github.com/Guo-Yizhen/RouGE .
Published: 2026-01-24T07:41:53+00:00
Venue: Pattern Recognition
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yizhen Guo; Hang Guo; Tao Dai; Zhi Wang; Bin Chen; Shu-Tao Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113064"&gt;10.1016/j.patcog.2026.113064&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;Segment anything model (SAM) and its variants have recently shown promising performance as foundation models. However, existing SAM-based models can only handle scenarios seen during training, and usually suffer unstable performance when transferring to real-world unseen data, such as low-light, rainy, or blurred images, which is crucial for applications such as autopilot. Therefore, adapting SAM-based models for real-world degradation while not impairing their original ability remains an open challenge. In this work, we propose a novel gated Mixture-of-Experts (MoE) structure, called RouGE, to improve the robustness of SAM-based models. Specifically, RouGE uses multiple lightweight probability gates to decompose complex real-world image conditions and judge whether the feature needs to be adjusted, as well as to what extent the adjustment needs to be done, then handles them differently with a set of low-rank experts. During the inference stage, RouGE processes input images in a completely blind manner, thus improving the models performance in real-world scenarios. Extensive experiments demonstrate that RouGE consistently achieves state-of-the-art results on both degraded and clean images compared with other methods while tuning only 1.5% of parameters. Our source code is publicly accessible via: https://github.com/Guo-Yizhen/RouGE .&lt;/p&gt;</content:encoded></item><item><title>Adversarial perturbation for RGB-T tracking via intra-modal excavation and cross-modal collusion</title><link>https://doi.org/10.1016/j.inffus.2026.104183</link><guid>10.1016/j.inffus.2026.104183</guid><pubDate>Sat, 24 Jan 2026 00:25:58 +0000</pubDate><dc:creator>Xinyu Xiang</dc:creator><dc:creator>Xuying Wu</dc:creator><dc:creator>Shengxiang Li</dc:creator><dc:creator>Qinglong Yan</dc:creator><dc:creator>Tong Zou</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104183</prism:doi><description>Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the trackers brightest semantic response region, concentrating the perturbation to interfere with the trackers target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.
Published: 2026-01-24T00:25:58+00:00
Venue: Information Fusion
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Xiang; Xuying Wu; Shengxiang Li; Qinglong Yan; Tong Zou; Hao Zhang; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104183"&gt;10.1016/j.inffus.2026.104183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the trackers brightest semantic response region, concentrating the perturbation to interfere with the trackers target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.&lt;/p&gt;</content:encoded></item><item><title>VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection</title><link>https://arxiv.org/abs/2601.16381v1</link><guid>http://arxiv.org/abs/2601.16381v1</guid><pubDate>Fri, 23 Jan 2026 00:30:24 +0000</pubDate><dc:creator>Yuxin Jiang</dc:creator><dc:creator>Yunkang Cao</dc:creator><dc:creator>Yuqi Cheng</dc:creator><dc:creator>Yiheng Zhang</dc:creator><dc:creator>Weiming Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.
Published: 2026-01-23T00:30:24+00:00
Venue: arXiv
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Jiang; Yunkang Cao; Yuqi Cheng; Yiheng Zhang; Weiming Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.&lt;/p&gt;</content:encoded></item><item><title>GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss</title><link>https://arxiv.org/abs/2601.16885v1</link><guid>http://arxiv.org/abs/2601.16885v1</guid><pubDate>Fri, 23 Jan 2026 16:46:59 +0000</pubDate><dc:creator>Yangfan Xu</dc:creator><dc:creator>Lilian Zhang</dc:creator><dc:creator>Xiaofeng He</dc:creator><dc:creator>Pengdong Wu</dc:creator><dc:creator>Wenqi Wu</dc:creator><dc:creator>Jun Mao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.
Published: 2026-01-23T16:46:59+00:00
Venue: arXiv
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Xu; Lilian Zhang; Xiaofeng He; Pengdong Wu; Wenqi Wu; Jun Mao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.&lt;/p&gt;</content:encoded></item><item><title>MRHead: Online Reconstruct Vectorized HD Map via Manifold Structure Feature</title><link>https://doi.org/10.1016/j.eswa.2026.131307</link><guid>10.1016/j.eswa.2026.131307</guid><pubDate>Sat, 24 Jan 2026 16:15:38 +0000</pubDate><dc:creator>Taohong Zhu</dc:creator><dc:creator>Chunchit Siu</dc:creator><dc:creator>Yunhe Wu</dc:creator><dc:creator>Ke Song</dc:creator><dc:creator>Huiyuan Xiong</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131307</prism:doi><description>High-definition (HD) maps are crucial for autonomous driving. Traffic markings have a distinct manifold structure, but the elements expressed through vertex sequences often lose manifold featuresspecifically the inherent geometric continuity and smooth topological relationships embedded in road elements. The existing map constructor also overlook the effect of the implicit manifold structure on the vertex sequence. We introduce the manifold reconstruction head (MRHead) aimed at extracting the manifold features inherent in vertex sequence and reconstructing the HD map elements. Our approach suggests utilizing manifold distance rather than Euclidean distance to formulate an adjacency matrix for linear manifolds, while extracting manifold features grounded in object proximity and direction. Additionally, we employ graph networks for extracting patch manifold features. ultimately, the reconstruction model is built on extracted features to rectify the errors on geometric and semantic elements of HD map. Experiments conducted on the widely employed nuScenes dataset revealed improvements in mAP compared to MapTR, MapTRv2, and StreamMapNet by 4.4%, 0.9%, and 3.1%, respectively. Regarding computational cost, while the full model operates at 17.0 FPS, our proposed modular design allows for a training-only deployment strategy, achieving these gains with zero additional inference latency (operating at 19.6 FPS). The results suggest that our method can be seamlessly integrated into existing point-based regression frameworks (e.g., MapTR) as a plug-and-play auxiliary supervision module, enhancing performance without altering the backbone architecture.
Published: 2026-01-24T16:15:38+00:00
Venue: Expert Systems with Applications
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Taohong Zhu; Chunchit Siu; Yunhe Wu; Ke Song; Huiyuan Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131307"&gt;10.1016/j.eswa.2026.131307&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;High-definition (HD) maps are crucial for autonomous driving. Traffic markings have a distinct manifold structure, but the elements expressed through vertex sequences often lose manifold featuresspecifically the inherent geometric continuity and smooth topological relationships embedded in road elements. The existing map constructor also overlook the effect of the implicit manifold structure on the vertex sequence. We introduce the manifold reconstruction head (MRHead) aimed at extracting the manifold features inherent in vertex sequence and reconstructing the HD map elements. Our approach suggests utilizing manifold distance rather than Euclidean distance to formulate an adjacency matrix for linear manifolds, while extracting manifold features grounded in object proximity and direction. Additionally, we employ graph networks for extracting patch manifold features. ultimately, the reconstruction model is built on extracted features to rectify the errors on geometric and semantic elements of HD map. Experiments conducted on the widely employed nuScenes dataset revealed improvements in mAP compared to MapTR, MapTRv2, and StreamMapNet by 4.4%, 0.9%, and 3.1%, respectively. Regarding computational cost, while the full model operates at 17.0 FPS, our proposed modular design allows for a training-only deployment strategy, achieving these gains with zero additional inference latency (operating at 19.6 FPS). The results suggest that our method can be seamlessly integrated into existing point-based regression frameworks (e.g., MapTR) as a plug-and-play auxiliary supervision module, enhancing performance without altering the backbone architecture.&lt;/p&gt;</content:encoded></item><item><title>Multi-Agent Role-Playing by LLMs and LMMs: An Explainable Open-World Multi-Modal Crisis Tweet Classification Method</title><link>https://doi.org/10.1016/j.eswa.2026.131325</link><guid>10.1016/j.eswa.2026.131325</guid><pubDate>Sat, 24 Jan 2026 00:22:48 +0000</pubDate><dc:creator>Tong Bie</dc:creator><dc:creator>Yongli Hu</dc:creator><dc:creator>Yu Fu</dc:creator><dc:creator>Linjia Hao</dc:creator><dc:creator>Tengfei Liu</dc:creator><dc:creator>Huajie Jiang</dc:creator><dc:creator>Junbin Gao</dc:creator><dc:creator>Yanfeng Sun</dc:creator><dc:creator>Baocai Yin</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131325</prism:doi><description>How to better leverage Large Language Models (LLMs) and Large Multi-modal Models (LMMs) in downstream tasks has become a prominent research topic. In the field of crisis tweet classification, existing approaches often fail to simultaneously handle open-world scenarios, missing modalities, and explainable classification, while still relying on supervised training data. To address these challenges, we propose a Multi-Agent Role-Playing framework (MARP) in which multiple LLMs and LMMs collaborate through specialized role assignments. MARP includes a LMM-based image analysis expert agent and four LLM-based agents: ordinary social media user, humanitarian organization staff member, content verification expert and long-text summarization expert. The social media user and humanitarian staff member are assigned distinct tasks, gathering tweet information from different perspectives by consulting the image analysis expert. The verification expert analyzes interactions to identify potential issues, while the summarization expert consolidates chat logs into summaries. These summaries are processed by the classification expert to generate predictions while also serving as explanatory rationales. We also propose a Query-aware Dynamic Masking (QDM) that selectively filters irrelevant image regions based on cross-modal similarity, enhancing LMMs focus on question-relevant visual content. Experiments on the CrisisMMD dataset under both open-world and missing-modality settings demonstrate that MARP achieves zero-shot accuracy improvements of 6.44% and 2.47% over state-of-the-art baselines, respectively. MARP exhibits strong performance in a training-free manner, while also providing explanatory rationales.
Published: 2026-01-24T00:22:48+00:00
Venue: Expert Systems with Applications
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Bie; Yongli Hu; Yu Fu; Linjia Hao; Tengfei Liu; Huajie Jiang; Junbin Gao; Yanfeng Sun; Baocai Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131325"&gt;10.1016/j.eswa.2026.131325&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;How to better leverage Large Language Models (LLMs) and Large Multi-modal Models (LMMs) in downstream tasks has become a prominent research topic. In the field of crisis tweet classification, existing approaches often fail to simultaneously handle open-world scenarios, missing modalities, and explainable classification, while still relying on supervised training data. To address these challenges, we propose a Multi-Agent Role-Playing framework (MARP) in which multiple LLMs and LMMs collaborate through specialized role assignments. MARP includes a LMM-based image analysis expert agent and four LLM-based agents: ordinary social media user, humanitarian organization staff member, content verification expert and long-text summarization expert. The social media user and humanitarian staff member are assigned distinct tasks, gathering tweet information from different perspectives by consulting the image analysis expert. The verification expert analyzes interactions to identify potential issues, while the summarization expert consolidates chat logs into summaries. These summaries are processed by the classification expert to generate predictions while also serving as explanatory rationales. We also propose a Query-aware Dynamic Masking (QDM) that selectively filters irrelevant image regions based on cross-modal similarity, enhancing LMMs focus on question-relevant visual content. Experiments on the CrisisMMD dataset under both open-world and missing-modality settings demonstrate that MARP achieves zero-shot accuracy improvements of 6.44% and 2.47% over state-of-the-art baselines, respectively. MARP exhibits strong performance in a training-free manner, while also providing explanatory rationales.&lt;/p&gt;</content:encoded></item><item><title>Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs</title><link>https://arxiv.org/abs/2601.15698v1</link><guid>http://arxiv.org/abs/2601.15698v1</guid><pubDate>Thu, 22 Jan 2026 06:56:27 +0000</pubDate><dc:creator>Mingyu Yu</dc:creator><dc:creator>Lana Liu</dc:creator><dc:creator>Zhehao Zhao</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Sujuan Qin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.
Published: 2026-01-22T06:56:27+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyu Yu; Lana Liu; Zhehao Zhao; Wei Wang; Sujuan Qin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a &amp;quot;reconstruction-then-generation&amp;quot; strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources</title><link>https://arxiv.org/abs/2601.16108v1</link><guid>http://arxiv.org/abs/2601.16108v1</guid><pubDate>Thu, 22 Jan 2026 16:55:48 +0000</pubDate><dc:creator>Marzieh Adeli Shamsabad</dc:creator><dc:creator>Hamed Ghodrati</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.
Published: 2026-01-22T16:55:48+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marzieh Adeli Shamsabad; Hamed Ghodrati&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.&lt;/p&gt;</content:encoded></item><item><title>PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</title><link>https://arxiv.org/abs/2601.16210v1</link><guid>http://arxiv.org/abs/2601.16210v1</guid><pubDate>Thu, 22 Jan 2026 18:58:55 +0000</pubDate><dc:creator>Onkar Susladkar</dc:creator><dc:creator>Tushar Prakash</dc:creator><dc:creator>Adheesh Juvekar</dc:creator><dc:creator>Kiet A. Nguyen</dc:creator><dc:creator>Dong-Hwan Jang</dc:creator><dc:creator>Inderjit S Dhillon</dc:creator><dc:creator>Ismini Lourentzou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.
Published: 2026-01-22T18:58:55+00:00
Venue: arXiv
Score: 0.498 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Onkar Susladkar; Tushar Prakash; Adheesh Juvekar; Kiet A. Nguyen; Dong-Hwan Jang; Inderjit S Dhillon; Ismini Lourentzou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.498 (consider)&lt;/p&gt;
&lt;p&gt;Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.&lt;/p&gt;</content:encoded></item><item><title>Evaluating Large Vision-language Models for Surgical Tool Detection</title><link>https://arxiv.org/abs/2601.16895v1</link><guid>http://arxiv.org/abs/2601.16895v1</guid><pubDate>Fri, 23 Jan 2026 17:00:46 +0000</pubDate><dc:creator>Nakul Poudel</dc:creator><dc:creator>Richard Simon</dc:creator><dc:creator>Cristian A. Linte</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.
Published: 2026-01-23T17:00:46+00:00
Venue: arXiv
Score: 0.498 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nakul Poudel; Richard Simon; Cristian A. Linte&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.498 (consider)&lt;/p&gt;
&lt;p&gt;Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.&lt;/p&gt;</content:encoded></item><item><title>TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</title><link>https://arxiv.org/abs/2601.16520v1</link><guid>http://arxiv.org/abs/2601.16520v1</guid><pubDate>Fri, 23 Jan 2026 07:35:05 +0000</pubDate><dc:creator>Daixian Liu</dc:creator><dc:creator>Jiayi Kuang</dc:creator><dc:creator>Yinghui Li</dc:creator><dc:creator>Yangning Li</dc:creator><dc:creator>Di Yin</dc:creator><dc:creator>Haoyu Cao</dc:creator><dc:creator>Xing Sun</dc:creator><dc:creator>Ying Shen</dc:creator><dc:creator>Hai-Tao Zheng</dc:creator><dc:creator>Liang Lin</dc:creator><dc:creator>Philip S. Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.
Published: 2026-01-23T07:35:05+00:00
Venue: arXiv
Score: 0.496 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daixian Liu; Jiayi Kuang; Yinghui Li; Yangning Li; Di Yin; Haoyu Cao; Xing Sun; Ying Shen; Hai-Tao Zheng; Liang Lin; Philip S. Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.496 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Dual Attention Graph Contrastive Learning for Recommendation</title><link>https://doi.org/10.1016/j.knosys.2026.115404</link><guid>10.1016/j.knosys.2026.115404</guid><pubDate>Sat, 24 Jan 2026 00:22:15 +0000</pubDate><dc:creator>Shouxing Ma</dc:creator><dc:creator>Shiqing Wu</dc:creator><dc:creator>Yawen Zeng</dc:creator><dc:creator>Kaize Shi</dc:creator><dc:creator>Guandong Xu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115404</prism:doi><description>Multi-modal recommender systems, incorporating rich content information (e.g., images and texts) into user behavior modeling, have attracted significant attention recently. Current work has successfully combined graph neural networks (GNNs) and contrastive learning to improve recommendation accuracy and mitigate the inherent sparse data problem. Yet, view augmentation strategies borrowed from other domainssuch as edge or node dropouttend to distort the original graph structure, leading to unintended semantic drift and suboptimal representation learning. Moreover, prior work has predominantly focused on optimizing inter-modal weights while overlooking user-specific modality preferences and adaptation of modal features generated by generic models. To tackle the above issues, we propose a novel multi-m O dal d U al a T tention G raph c O ntrastive learning framework (OUTGO). Specifically, we first encode user and item representations by utilizing user and item homogeneous GNNs. Then, we employ designed intra- and inter-attention mechanisms, sequentially and adaptively, tuning each modal feature value based on the principal loss and considering fusing them with different modal perspectives. Additionally, semantic and structural contrastive learning tasks are introduced to alleviate the sparse data without destroying the original data structure. Extensive experiments on real-world datasets demonstrate the superiority of OUTGO compared to state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/OUTGO .
Published: 2026-01-24T00:22:15+00:00
Venue: Knowledge-Based Systems
Score: 0.495 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shouxing Ma; Shiqing Wu; Yawen Zeng; Kaize Shi; Guandong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115404"&gt;10.1016/j.knosys.2026.115404&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.495 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal recommender systems, incorporating rich content information (e.g., images and texts) into user behavior modeling, have attracted significant attention recently. Current work has successfully combined graph neural networks (GNNs) and contrastive learning to improve recommendation accuracy and mitigate the inherent sparse data problem. Yet, view augmentation strategies borrowed from other domainssuch as edge or node dropouttend to distort the original graph structure, leading to unintended semantic drift and suboptimal representation learning. Moreover, prior work has predominantly focused on optimizing inter-modal weights while overlooking user-specific modality preferences and adaptation of modal features generated by generic models. To tackle the above issues, we propose a novel multi-m O dal d U al a T tention G raph c O ntrastive learning framework (OUTGO). Specifically, we first encode user and item representations by utilizing user and item homogeneous GNNs. Then, we employ designed intra- and inter-attention mechanisms, sequentially and adaptively, tuning each modal feature value based on the principal loss and considering fusing them with different modal perspectives. Additionally, semantic and structural contrastive learning tasks are introduced to alleviate the sparse data without destroying the original data structure. Extensive experiments on real-world datasets demonstrate the superiority of OUTGO compared to state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/OUTGO .&lt;/p&gt;</content:encoded></item><item><title>CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback</title><link>https://arxiv.org/abs/2601.16214v1</link><guid>http://arxiv.org/abs/2601.16214v1</guid><pubDate>Thu, 22 Jan 2026 18:59:56 +0000</pubDate><dc:creator>Wenhang Ge</dc:creator><dc:creator>Guibao Shen</dc:creator><dc:creator>Jiawei Feng</dc:creator><dc:creator>Luozhou Wang</dc:creator><dc:creator>Hao Lu</dc:creator><dc:creator>Xingye Tian</dc:creator><dc:creator>Xin Tao</dc:creator><dc:creator>Ying-Cong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.
Published: 2026-01-22T18:59:56+00:00
Venue: arXiv
Score: 0.492 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhang Ge; Guibao Shen; Jiawei Feng; Luozhou Wang; Hao Lu; Xingye Tian; Xin Tao; Ying-Cong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.492 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</title><link>https://arxiv.org/abs/2601.16965v1</link><guid>http://arxiv.org/abs/2601.16965v1</guid><pubDate>Fri, 23 Jan 2026 18:33:45 +0000</pubDate><dc:creator>Riyang Bao</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Dazhou Yu</dc:creator><dc:creator>Zhexiang Tang</dc:creator><dc:creator>Gengchen Mai</dc:creator><dc:creator>Liang Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.
Published: 2026-01-23T18:33:45+00:00
Venue: arXiv
Score: 0.491 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Riyang Bao; Cheng Yang; Dazhou Yu; Zhexiang Tang; Gengchen Mai; Liang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.491 (consider)&lt;/p&gt;
&lt;p&gt;Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.&lt;/p&gt;</content:encoded></item><item><title>Generative Model-Based Mixed-Semantic Enhancement for Transductive Zero-Shot Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113124</link><guid>10.1016/j.patcog.2026.113124</guid><pubDate>Sat, 24 Jan 2026 23:17:12 +0000</pubDate><dc:creator>Huaizhou Qi</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Jungong Han</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113124</prism:doi><description>Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generators training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.
Published: 2026-01-24T23:17:12+00:00
Venue: Pattern Recognition
Score: 0.489 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huaizhou Qi; Yang Liu; Jungong Han; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113124"&gt;10.1016/j.patcog.2026.113124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.489 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generators training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.&lt;/p&gt;</content:encoded></item><item><title>Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition</title><link>https://arxiv.org/abs/2601.16211v1</link><guid>http://arxiv.org/abs/2601.16211v1</guid><pubDate>Thu, 22 Jan 2026 18:59:13 +0000</pubDate><dc:creator>Geo Ahn</dc:creator><dc:creator>Inwoong Lee</dc:creator><dc:creator>Taeoh Kim</dc:creator><dc:creator>Minho Shim</dc:creator><dc:creator>Dongyoon Wee</dc:creator><dc:creator>Jinwoo Choi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.
Published: 2026-01-22T18:59:13+00:00
Venue: arXiv
Score: 0.488 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Geo Ahn; Inwoong Lee; Taeoh Kim; Minho Shim; Dongyoon Wee; Jinwoo Choi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.488 (consider)&lt;/p&gt;
&lt;p&gt;We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.&lt;/p&gt;</content:encoded></item></channel></rss>