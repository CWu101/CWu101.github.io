<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 22 Jan 2026 03:04:51 +0000</lastBuildDate><item><title>Semantic-Spatial Subgraph-Based Relational Reasoning for Visual Question Answering</title><link>https://doi.org/10.1109/tmm.2026.3655504</link><guid>10.1109/tmm.2026.3655504</guid><pubDate>Tue, 20 Jan 2026 20:40:43 +0000</pubDate><dc:creator>Yike Wu</dc:creator><dc:creator>Jiahao Xia</dc:creator><dc:creator>Jingcheng Ke</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><dc:creator>Jian Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3655504</prism:doi><description>Visual question answering requires a comprehensive understanding of object relationships and attributes within an image. Utilizing Graph Neural Networks (GNNs) or attention mechanisms to explore these relationships provides an intuitive method that integrates all node features, presenting a global view for predicting the answer to the question. These graph-based methods treat each object as an independent region and build connections between nodes. However, full connections in the graph are often redundant, as question-conditioned connections are more important for accurate answer prediction, introducing irrelevant regions as noise in the reasoning process. To this end, this paper explores subgraph-based relational reasoning for visual question answering, which captures object interactions based on semantic-spatial relationships between regions and learns discriminative subgraph-level representations. Specifically, we first propose a novel subgraph construction principle that considers both spatial and semantic relationships of each candidate region. These subgraphs provide a basis to distinguish similar objects while maintaining discriminative power. Subsequently, to further eliminate the noise from irrelevant nodes and propagate vital messages for accurate answer prediction, an inter-subgraph relation learning (IRL) module is proposed to adaptively prune inter-subgraph connections according to the constructed subgraphs. Finally, to explicitly employ significant representations for reasoning, an intra-subgraph message aggregation (IMA) module is proposed to merge the region representations to infer the answer. Extensive experiments conducted on three benchmarks validate the effectiveness of our proposed method, and visualization results further demonstrate its interpretability.
Published: 2026-01-20T20:40:43+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.635 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yike Wu; Jiahao Xia; Jingcheng Ke; Chia-Wen Lin; Jian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3655504"&gt;10.1109/tmm.2026.3655504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.635 (must_read)&lt;/p&gt;
&lt;p&gt;Visual question answering requires a comprehensive understanding of object relationships and attributes within an image. Utilizing Graph Neural Networks (GNNs) or attention mechanisms to explore these relationships provides an intuitive method that integrates all node features, presenting a global view for predicting the answer to the question. These graph-based methods treat each object as an independent region and build connections between nodes. However, full connections in the graph are often redundant, as question-conditioned connections are more important for accurate answer prediction, introducing irrelevant regions as noise in the reasoning process. To this end, this paper explores subgraph-based relational reasoning for visual question answering, which captures object interactions based on semantic-spatial relationships between regions and learns discriminative subgraph-level representations. Specifically, we first propose a novel subgraph construction principle that considers both spatial and semantic relationships of each candidate region. These subgraphs provide a basis to distinguish similar objects while maintaining discriminative power. Subsequently, to further eliminate the noise from irrelevant nodes and propagate vital messages for accurate answer prediction, an inter-subgraph relation learning (IRL) module is proposed to adaptively prune inter-subgraph connections according to the constructed subgraphs. Finally, to explicitly employ significant representations for reasoning, an intra-subgraph message aggregation (IMA) module is proposed to merge the region representations to infer the answer. Extensive experiments conducted on three benchmarks validate the effectiveness of our proposed method, and visualization results further demonstrate its interpretability.&lt;/p&gt;</content:encoded></item><item><title>Momentor++: Advancing Video Large Language Models With Fine-Grained Long Video Reasoning</title><link>https://doi.org/10.1109/tpami.2026.3656169</link><guid>10.1109/tpami.2026.3656169</guid><pubDate>Tue, 20 Jan 2026 20:40:09 +0000</pubDate><dc:creator>Juncheng Li</dc:creator><dc:creator>Minghe Gao</dc:creator><dc:creator>Xiangnan He</dc:creator><dc:creator>Siliang Tang</dc:creator><dc:creator>Weishi Zheng</dc:creator><dc:creator>Jun Xiao</dc:creator><dc:creator>Meng Wang</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><dc:creator>Yueting Zhuang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3656169</prism:doi><description>Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.
Published: 2026-01-20T20:40:09+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.581 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juncheng Li; Minghe Gao; Xiangnan He; Siliang Tang; Weishi Zheng; Jun Xiao; Meng Wang; Tat-Seng Chua; Yueting Zhuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3656169"&gt;10.1109/tpami.2026.3656169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.581 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Spatio-Temporal Fusion: A Generalizable GCN-LSTM with Attention Framework for Urban Application</title><link>https://doi.org/10.1016/j.inffus.2026.104164</link><guid>10.1016/j.inffus.2026.104164</guid><pubDate>Tue, 20 Jan 2026 16:25:41 +0000</pubDate><dc:creator>Yunfei Guo</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104164</prism:doi><description>The proliferation of urban big data presents unprecedented opportunities for understanding cities, yet the analytical methods to harness this data are often fragmented and domain-specific. Existing predictive models in urban computing are typically highly specialized, creating analytical silos that inhibit knowledge transfer and are difficult to adapt across domains such as public safety, housing and transport. This paper confronts this critical gap by developing a generalizable, multimodal spatio-temporal deep learning framework engineered for both high predictive performance and interpretability, which is capable of mastering diverse urban prediction tasks without architectural modification. The hybrid architecture fuses a Multi-Head Graph Convolutional Network (GCN) for spatial diffusion, a Long Short-Term Memory (LSTM) network for temporal dynamics, and a learnable Gating Mechanism that weights the influence of spatial graph versus static external features. To validate this generalizability, the framework was tested on three distinct urban domains in London: crime forecasting, housing price estimation and transport network demand. The model outperformed traditional baselines (ARIMA, XGBoost) and state-of-the-art deep learning models (TabNet, TFT). Moreover, the framework moves beyond prediction to explanation by incorporating attention mechanisms and permutation feature importance analysis.
Published: 2026-01-20T16:25:41+00:00
Venue: Information Fusion
Score: 0.579 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfei Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104164"&gt;10.1016/j.inffus.2026.104164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.579 (consider)&lt;/p&gt;
&lt;p&gt;The proliferation of urban big data presents unprecedented opportunities for understanding cities, yet the analytical methods to harness this data are often fragmented and domain-specific. Existing predictive models in urban computing are typically highly specialized, creating analytical silos that inhibit knowledge transfer and are difficult to adapt across domains such as public safety, housing and transport. This paper confronts this critical gap by developing a generalizable, multimodal spatio-temporal deep learning framework engineered for both high predictive performance and interpretability, which is capable of mastering diverse urban prediction tasks without architectural modification. The hybrid architecture fuses a Multi-Head Graph Convolutional Network (GCN) for spatial diffusion, a Long Short-Term Memory (LSTM) network for temporal dynamics, and a learnable Gating Mechanism that weights the influence of spatial graph versus static external features. To validate this generalizability, the framework was tested on three distinct urban domains in London: crime forecasting, housing price estimation and transport network demand. The model outperformed traditional baselines (ARIMA, XGBoost) and state-of-the-art deep learning models (TabNet, TFT). Moreover, the framework moves beyond prediction to explanation by incorporating attention mechanisms and permutation feature importance analysis.&lt;/p&gt;</content:encoded></item><item><title>AGPL-KEM: Attribute-Guided Prompt Learning with Knowledge Experts Mixture for Few-Shot Remote Sensing Image Classification</title><link>https://doi.org/10.1016/j.knosys.2026.115375</link><guid>10.1016/j.knosys.2026.115375</guid><pubDate>Tue, 20 Jan 2026 07:43:24 +0000</pubDate><dc:creator>Chunlei Wu</dc:creator><dc:creator>Congzheng Zhu</dc:creator><dc:creator>Qinfu Xu</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Yongzhen Zhang</dc:creator><dc:creator>Leiquan Wang</dc:creator><dc:creator>Jie Wu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115375</prism:doi><description>Large-scale vision-language models (VLMs) have shown significant success in various computer vision tasks. However, adapting VLMs to remote sensing (RS) tasks remains challenging due to the distinct characteristics of RS imagery, such as spectral heterogeneity, fine-grained textures, and complex structural layouts. Existing methods attempt to encode diverse RS attributes into a unified latent space, but this implicit encoding strategy often leads to attribute conflation, undermining generalization under domain shifts. To address these limitations, we propose Attribute-Guided Prompt Learning with Knowledge Experts Mixture (AGPL-KEM), a prompt learning framework that explicitly disentangles RS semantics through structured domain knowledge. Specifically, AGPL-KEM introduces a Knowledge Experts Mixture module to partition the latent space into attribute-specific subspaces, thereby enhancing the model’s ability to capture and separate key RS attributes. To promote attribute-specific learning and reduce inter-expert redundancy, we design an Attribute-Guided Dual-Loss mechanism comprising an Attribute-Guided Semantic Alignment Loss for expert-attribute consistency and an Expert Semantic Orthogonality Loss that reduces semantic redundancy among experts through orthogonality constraints. Comprehensive experiments conducted on four remote sensing benchmark datasets (PatternNet, RSICD, RESISC45, and MLRSNet) demonstrate that AGPL-KEM achieves state-of-the-art performance, validating its effectiveness and robustness. Codes are available at https://github.com/4wlb/AGPL-KEM .
Published: 2026-01-20T07:43:24+00:00
Venue: Knowledge-Based Systems
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunlei Wu; Congzheng Zhu; Qinfu Xu; Xu Liu; Yongzhen Zhang; Leiquan Wang; Jie Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115375"&gt;10.1016/j.knosys.2026.115375&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;Large-scale vision-language models (VLMs) have shown significant success in various computer vision tasks. However, adapting VLMs to remote sensing (RS) tasks remains challenging due to the distinct characteristics of RS imagery, such as spectral heterogeneity, fine-grained textures, and complex structural layouts. Existing methods attempt to encode diverse RS attributes into a unified latent space, but this implicit encoding strategy often leads to attribute conflation, undermining generalization under domain shifts. To address these limitations, we propose Attribute-Guided Prompt Learning with Knowledge Experts Mixture (AGPL-KEM), a prompt learning framework that explicitly disentangles RS semantics through structured domain knowledge. Specifically, AGPL-KEM introduces a Knowledge Experts Mixture module to partition the latent space into attribute-specific subspaces, thereby enhancing the model’s ability to capture and separate key RS attributes. To promote attribute-specific learning and reduce inter-expert redundancy, we design an Attribute-Guided Dual-Loss mechanism comprising an Attribute-Guided Semantic Alignment Loss for expert-attribute consistency and an Expert Semantic Orthogonality Loss that reduces semantic redundancy among experts through orthogonality constraints. Comprehensive experiments conducted on four remote sensing benchmark datasets (PatternNet, RSICD, RESISC45, and MLRSNet) demonstrate that AGPL-KEM achieves state-of-the-art performance, validating its effectiveness and robustness. Codes are available at https://github.com/4wlb/AGPL-KEM .&lt;/p&gt;</content:encoded></item><item><title>Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</title><link>https://arxiv.org/abs/2601.13401v1</link><guid>http://arxiv.org/abs/2601.13401v1</guid><pubDate>Mon, 19 Jan 2026 21:14:34 +0000</pubDate><dc:creator>Peter A. Massih</dc:creator><dc:creator>Eric Cosatto</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.
Published: 2026-01-19T21:14:34+00:00
Venue: arXiv
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peter A. Massih; Eric Cosatto&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.&lt;/p&gt;</content:encoded></item><item><title>Graph Recognition via Subgraph Prediction</title><link>https://arxiv.org/abs/2601.15133v1</link><guid>http://arxiv.org/abs/2601.15133v1</guid><pubDate>Wed, 21 Jan 2026 16:07:17 +0000</pubDate><dc:creator>André Eberhard</dc:creator><dc:creator>Gerhard Neumann</dc:creator><dc:creator>Pascal Friederich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.
Published: 2026-01-21T16:07:17+00:00
Venue: arXiv
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; André Eberhard; Gerhard Neumann; Pascal Friederich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.&lt;/p&gt;</content:encoded></item><item><title>UrbanMFM: Spatial Graph-Based Multiscale Foundation Models for Learning Generalized Urban Representation</title><link>https://doi.org/10.1109/tkde.2026.3656202</link><guid>10.1109/tkde.2026.3656202</guid><pubDate>Tue, 20 Jan 2026 20:41:02 +0000</pubDate><dc:creator>Zhaoqi Zhang</dc:creator><dc:creator>Miao Xie</dc:creator><dc:creator>Pasquale Balsebre</dc:creator><dc:creator>Weiming Huang</dc:creator><dc:creator>Siqiang Luo</dc:creator><dc:creator>Gao Cong</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2026.3656202</prism:doi><description>As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.
Published: 2026-01-20T20:41:02+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoqi Zhang; Miao Xie; Pasquale Balsebre; Weiming Huang; Siqiang Luo; Gao Cong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2026.3656202"&gt;10.1109/tkde.2026.3656202&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration</title><link>https://arxiv.org/abs/2601.14060v1</link><guid>http://arxiv.org/abs/2601.14060v1</guid><pubDate>Tue, 20 Jan 2026 15:17:14 +0000</pubDate><dc:creator>Yongcong Ye</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Yanghai Zhang</dc:creator><dc:creator>Enhong Chen</dc:creator><dc:creator>Longfei Li</dc:creator><dc:creator>Jun Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.
Published: 2026-01-20T15:17:14+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongcong Ye; Kai Zhang; Yanghai Zhang; Enhong Chen; Longfei Li; Jun Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.&lt;/p&gt;</content:encoded></item><item><title>VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension</title><link>https://arxiv.org/abs/2601.12781v1</link><guid>http://arxiv.org/abs/2601.12781v1</guid><pubDate>Mon, 19 Jan 2026 07:21:19 +0000</pubDate><dc:creator>Hyejin Park</dc:creator><dc:creator>Junhyuk Kwon</dc:creator><dc:creator>Suha Kwak</dc:creator><dc:creator>Jungseul Ok</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.
Published: 2026-01-19T07:21:19+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyejin Park; Junhyuk Kwon; Suha Kwak; Jungseul Ok&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Feature Alignment Networks for Multi-label Image Classification</title><link>https://doi.org/10.1016/j.neunet.2026.108629</link><guid>10.1016/j.neunet.2026.108629</guid><pubDate>Tue, 20 Jan 2026 17:23:27 +0000</pubDate><dc:creator>Wenlan Kuang</dc:creator><dc:creator>Zhixin Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108629</prism:doi><description>Multi-label image classification is a classification task that assigns labels to multiple objects in an input image. Recent research ideas mainly focus on solving the semantic consistency of visual features and label features. However, since images contain complex scene content, the features captured by visual feature extraction networks based on grid or sequence representation may introduce redundant information or lack continuity when identifying irregular objects. In order to fully mine the visual information of complex objects in images and enhance the inter-modal interaction of images and labels, we introduce a flexible graph structure to explore the internal information of objects and design a multi-modal feature alignment (MMFA) network for multi-label image classification. To enhance the context awareness and semantic association of different patch regions, we propose a semantic-augmented interaction module that combines two kinds of visual semantic information with label embeddings for interactive learning. Finally, we refine the dependence between local intrinsic information and overall semantics by redefining semantic queries through semantically enhanced visual spatial features and graph aggregation features. Experiments on three large-scale public datasets: Microsoft COCO, Pascal VOC 2007 and NUS-WIDE demonstrate the effectiveness of our proposed MMFA and achieve state-of-the-art performance.
Published: 2026-01-20T17:23:27+00:00
Venue: Neural Networks
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenlan Kuang; Zhixin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108629"&gt;10.1016/j.neunet.2026.108629&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Multi-label image classification is a classification task that assigns labels to multiple objects in an input image. Recent research ideas mainly focus on solving the semantic consistency of visual features and label features. However, since images contain complex scene content, the features captured by visual feature extraction networks based on grid or sequence representation may introduce redundant information or lack continuity when identifying irregular objects. In order to fully mine the visual information of complex objects in images and enhance the inter-modal interaction of images and labels, we introduce a flexible graph structure to explore the internal information of objects and design a multi-modal feature alignment (MMFA) network for multi-label image classification. To enhance the context awareness and semantic association of different patch regions, we propose a semantic-augmented interaction module that combines two kinds of visual semantic information with label embeddings for interactive learning. Finally, we refine the dependence between local intrinsic information and overall semantics by redefining semantic queries through semantically enhanced visual spatial features and graph aggregation features. Experiments on three large-scale public datasets: Microsoft COCO, Pascal VOC 2007 and NUS-WIDE demonstrate the effectiveness of our proposed MMFA and achieve state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval</title><link>https://arxiv.org/abs/2601.13797v1</link><guid>http://arxiv.org/abs/2601.13797v1</guid><pubDate>Tue, 20 Jan 2026 09:57:04 +0000</pubDate><dc:creator>Gabriele Serussi</dc:creator><dc:creator>David Vainshtein</dc:creator><dc:creator>Jonathan Kouchly</dc:creator><dc:creator>Dotan Di Castro</dc:creator><dc:creator>Chaim Baskin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.
Published: 2026-01-20T09:57:04+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gabriele Serussi; David Vainshtein; Jonathan Kouchly; Dotan Di Castro; Chaim Baskin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.&lt;/p&gt;</content:encoded></item><item><title>SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild</title><link>https://doi.org/10.1109/tcsvt.2026.3656228</link><guid>10.1109/tcsvt.2026.3656228</guid><pubDate>Tue, 20 Jan 2026 20:41:28 +0000</pubDate><dc:creator>Jiawei Liu</dc:creator><dc:creator>Yuanzhi Zhu</dc:creator><dc:creator>Feiyu Gao</dc:creator><dc:creator>Zhibo Yang</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Junyang Lin</dc:creator><dc:creator>Xinggang Wang</dc:creator><dc:creator>Wenyu Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656228</prism:doi><description>Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, and cartoons), existing methods for natural scene visual text generation still have significant deficiencies: methods based on rendering engines rely on manually crafted rules, which struggle to adapt to diverse backgrounds and leave obvious artificial traces, while their text layouts may be placed in unreasonable areas (e.g., sky or ground) and text content is semantically disconnected from the scene; diffusion model-based methods, on the other hand, face difficulties in generating small characters, depend on manually designed prompts to ensure reasonable layout and content, fail to generate text at precise locations, and cannot effectively control text attributes (e.g., font and color). In this paper, we propose a two-stage method named SceneVTG++ to address these issues. SceneVTG++ comprises two core components: a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former leverages the world knowledge and visual reasoning capabilities of multimodal large language models to identify reasonable text areas and recommend scene-relevant text content based on natural scene background images; the latter generates controllable multilingual text using a diffusion model, ensuring alignment with the outputs of TLCG. Through extensive experiments, we verified the effectiveness of both TLCG and CLTD, and demonstrated that SceneVTG++ achieves state-of-the-art performance in natural scene visual text generation. Additionally, the images generated by SceneVTG++ exhibit superior utility for training natural scene optical character recognition (OCR) tasks, including text detection and text recognition. Codes and datasets will be made publicly available.
Published: 2026-01-20T20:41:28+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Liu; Yuanzhi Zhu; Feiyu Gao; Zhibo Yang; Peng Wang; Junyang Lin; Xinggang Wang; Wenyu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656228"&gt;10.1109/tcsvt.2026.3656228&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, and cartoons), existing methods for natural scene visual text generation still have significant deficiencies: methods based on rendering engines rely on manually crafted rules, which struggle to adapt to diverse backgrounds and leave obvious artificial traces, while their text layouts may be placed in unreasonable areas (e.g., sky or ground) and text content is semantically disconnected from the scene; diffusion model-based methods, on the other hand, face difficulties in generating small characters, depend on manually designed prompts to ensure reasonable layout and content, fail to generate text at precise locations, and cannot effectively control text attributes (e.g., font and color). In this paper, we propose a two-stage method named SceneVTG++ to address these issues. SceneVTG++ comprises two core components: a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former leverages the world knowledge and visual reasoning capabilities of multimodal large language models to identify reasonable text areas and recommend scene-relevant text content based on natural scene background images; the latter generates controllable multilingual text using a diffusion model, ensuring alignment with the outputs of TLCG. Through extensive experiments, we verified the effectiveness of both TLCG and CLTD, and demonstrated that SceneVTG++ achieves state-of-the-art performance in natural scene visual text generation. Additionally, the images generated by SceneVTG++ exhibit superior utility for training natural scene optical character recognition (OCR) tasks, including text detection and text recognition. Codes and datasets will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>Open Vocabulary Panoptic Segmentation With Retrieval Augmentation</title><link>https://arxiv.org/abs/2601.12779v1</link><guid>http://arxiv.org/abs/2601.12779v1</guid><pubDate>Mon, 19 Jan 2026 07:16:45 +0000</pubDate><dc:creator>Nafis Sadeq</dc:creator><dc:creator>Qingfeng Liu</dc:creator><dc:creator>Mostafa El-Khamy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.
Published: 2026-01-19T07:16:45+00:00
Venue: arXiv
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nafis Sadeq; Qingfeng Liu; Mostafa El-Khamy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.&lt;/p&gt;</content:encoded></item><item><title>Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</title><link>https://arxiv.org/abs/2601.13942v1</link><guid>http://arxiv.org/abs/2601.13942v1</guid><pubDate>Tue, 20 Jan 2026 13:18:18 +0000</pubDate><dc:creator>Hongbo Bai</dc:creator><dc:creator>Yujin Zhou</dc:creator><dc:creator>Yile Wu</dc:creator><dc:creator>Chi-Min Chan</dc:creator><dc:creator>Pengcheng Wen</dc:creator><dc:creator>Kunhao Pan</dc:creator><dc:creator>Sirui Han</dc:creator><dc:creator>Yike Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.
Published: 2026-01-20T13:18:18+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbo Bai; Yujin Zhou; Yile Wu; Chi-Min Chan; Pengcheng Wen; Kunhao Pan; Sirui Han; Yike Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&amp;#x27;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.&lt;/p&gt;</content:encoded></item><item><title>SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</title><link>https://arxiv.org/abs/2601.14895v1</link><guid>http://arxiv.org/abs/2601.14895v1</guid><pubDate>Wed, 21 Jan 2026 11:32:24 +0000</pubDate><dc:creator>Xinyi Zheng</dc:creator><dc:creator>Yunze Liu</dc:creator><dc:creator>Chi-Hao Wu</dc:creator><dc:creator>Fan Zhang</dc:creator><dc:creator>Hao Zheng</dc:creator><dc:creator>Wenqi Zhou</dc:creator><dc:creator>Walterio W. Mayol-Cuevas</dc:creator><dc:creator>Junxiao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.
Published: 2026-01-21T11:32:24+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyi Zheng; Yunze Liu; Chi-Hao Wu; Fan Zhang; Hao Zheng; Wenqi Zhou; Walterio W. Mayol-Cuevas; Junxiao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.&lt;/p&gt;</content:encoded></item><item><title>Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation</title><link>https://arxiv.org/abs/2601.14438v1</link><guid>http://arxiv.org/abs/2601.14438v1</guid><pubDate>Tue, 20 Jan 2026 19:50:42 +0000</pubDate><dc:creator>Danial Sadrian Zadeh</dc:creator><dc:creator>Otman A. Basir</dc:creator><dc:creator>Behzad Moshiri</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.
Published: 2026-01-20T19:50:42+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Danial Sadrian Zadeh; Otman A. Basir; Behzad Moshiri&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.&lt;/p&gt;</content:encoded></item><item><title>DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition</title><link>https://arxiv.org/abs/2601.12729v1</link><guid>http://arxiv.org/abs/2601.12729v1</guid><pubDate>Mon, 19 Jan 2026 05:19:56 +0000</pubDate><dc:creator>Hanyu Zhu</dc:creator><dc:creator>Zhihao Zhan</dc:creator><dc:creator>Yuhang Ming</dc:creator><dc:creator>Liang Li</dc:creator><dc:creator>Dibo Hou</dc:creator><dc:creator>Javier Civera</dc:creator><dc:creator>Wanzeng Kong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.
Published: 2026-01-19T05:19:56+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanyu Zhu; Zhihao Zhan; Yuhang Ming; Liang Li; Dibo Hou; Javier Civera; Wanzeng Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.&lt;/p&gt;</content:encoded></item><item><title>Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval</title><link>https://arxiv.org/abs/2601.12768v1</link><guid>http://arxiv.org/abs/2601.12768v1</guid><pubDate>Mon, 19 Jan 2026 06:55:33 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.
Published: 2026-01-19T06:55:33+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Boyun Zhang; Yuxiao Lin; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&amp;#x27;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.&lt;/p&gt;</content:encoded></item><item><title>Less is More: Label-Guided Summarization of Procedural and Instructional Videos</title><link>https://arxiv.org/abs/2601.12243v1</link><guid>http://arxiv.org/abs/2601.12243v1</guid><pubDate>Sun, 18 Jan 2026 03:41:48 +0000</pubDate><dc:creator>Shreya Rajpal</dc:creator><dc:creator>Michal Golovanesky</dc:creator><dc:creator>Carsten Eickhoff</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.
Published: 2026-01-18T03:41:48+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shreya Rajpal; Michal Golovanesky; Carsten Eickhoff&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what&amp;#x27;s happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.&lt;/p&gt;</content:encoded></item><item><title>DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities</title><link>https://arxiv.org/abs/2601.13502v1</link><guid>http://arxiv.org/abs/2601.13502v1</guid><pubDate>Tue, 20 Jan 2026 01:33:54 +0000</pubDate><dc:creator>Nhi Kieu</dc:creator><dc:creator>Kien Nguyen</dc:creator><dc:creator>Arnold Wiliem</dc:creator><dc:creator>Clinton Fookes</dc:creator><dc:creator>Sridha Sridharan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.
Published: 2026-01-20T01:33:54+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nhi Kieu; Kien Nguyen; Arnold Wiliem; Clinton Fookes; Sridha Sridharan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Visual Position Prompt for MLLM Based Visual Grounding</title><link>https://doi.org/10.1109/tmm.2026.3654372</link><guid>10.1109/tmm.2026.3654372</guid><pubDate>Tue, 20 Jan 2026 20:40:43 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Qinying Gu</dc:creator><dc:creator>Zechao Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654372</prism:doi><description>Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.
Published: 2026-01-20T20:40:43+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Yanpeng Sun; Qinying Gu; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654372"&gt;10.1109/tmm.2026.3654372&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.&lt;/p&gt;</content:encoded></item><item><title>ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</title><link>https://arxiv.org/abs/2601.14757v1</link><guid>http://arxiv.org/abs/2601.14757v1</guid><pubDate>Wed, 21 Jan 2026 08:21:35 +0000</pubDate><dc:creator>Kangcheng Zhou</dc:creator><dc:creator>Jun Jiang</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Shuang Zheng</dc:creator><dc:creator>Qingli Li</dc:creator><dc:creator>Shugong Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.
Published: 2026-01-21T08:21:35+00:00
Venue: arXiv
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kangcheng Zhou; Jun Jiang; Qing Zhang; Shuang Zheng; Qingli Li; Shugong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.&lt;/p&gt;</content:encoded></item><item><title>Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders</title><link>https://arxiv.org/abs/2601.13798v1</link><guid>http://arxiv.org/abs/2601.13798v1</guid><pubDate>Tue, 20 Jan 2026 09:57:26 +0000</pubDate><dc:creator>Kai Wittenmayer</dc:creator><dc:creator>Sukrut Rao</dc:creator><dc:creator>Amin Parchami-Araghi</dc:creator><dc:creator>Bernt Schiele</dc:creator><dc:creator>Jonas Fischer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.
Published: 2026-01-20T09:57:26+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Wittenmayer; Sukrut Rao; Amin Parchami-Araghi; Bernt Schiele; Jonas Fischer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.&lt;/p&gt;</content:encoded></item><item><title>SuperMapNet for long-range and high-accuracy vectorized HD map construction</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.023</link><guid>10.1016/j.isprsjprs.2026.01.023</guid><pubDate>Tue, 20 Jan 2026 16:22:22 +0000</pubDate><dc:creator>Ruqin Zhou</dc:creator><dc:creator>Chenguang Dai</dc:creator><dc:creator>Wanshou Jiang</dc:creator><dc:creator>Yongsheng Zhang</dc:creator><dc:creator>Zhenchao Zhang</dc:creator><dc:creator>San Jiang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.023</prism:doi><description>Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m in the X-axis and 60 m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .
Published: 2026-01-20T16:22:22+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruqin Zhou; Chenguang Dai; Wanshou Jiang; Yongsheng Zhang; Zhenchao Zhang; San Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023"&gt;10.1016/j.isprsjprs.2026.01.023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m in the X-axis and 60 m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .&lt;/p&gt;</content:encoded></item><item><title>Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation</title><link>https://arxiv.org/abs/2601.12964v1</link><guid>http://arxiv.org/abs/2601.12964v1</guid><pubDate>Mon, 19 Jan 2026 11:21:19 +0000</pubDate><dc:creator>John Waithaka</dc:creator><dc:creator>Gustave Bwirayesu</dc:creator><dc:creator>Moise Busogi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.
Published: 2026-01-19T11:21:19+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; John Waithaka; Gustave Bwirayesu; Moise Busogi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.&lt;/p&gt;</content:encoded></item><item><title>SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence</title><link>https://arxiv.org/abs/2601.12357v1</link><guid>http://arxiv.org/abs/2601.12357v1</guid><pubDate>Sun, 18 Jan 2026 11:31:46 +0000</pubDate><dc:creator>Hailing Jin</dc:creator><dc:creator>Huiying Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.
Published: 2026-01-18T11:31:46+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hailing Jin; Huiying Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.&lt;/p&gt;</content:encoded></item><item><title>CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.13622v1</link><guid>http://arxiv.org/abs/2601.13622v1</guid><pubDate>Tue, 20 Jan 2026 05:44:33 +0000</pubDate><dc:creator>Donghee Lee</dc:creator><dc:creator>Rui Cai</dc:creator><dc:creator>Zhe Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.
Published: 2026-01-20T05:44:33+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Donghee Lee; Rui Cai; Zhe Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&amp;#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.&lt;/p&gt;</content:encoded></item><item><title>AMPUNet: Hierarchical Attention Map Pyramid for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2026.3656191</link><guid>10.1109/jstars.2026.3656191</guid><pubDate>Tue, 20 Jan 2026 20:40:26 +0000</pubDate><dc:creator>Yang Yang</dc:creator><dc:creator>Wei Ao</dc:creator><dc:creator>Shunyi Zheng</dc:creator><dc:creator>Zhao Liu</dc:creator><dc:creator>Yunni Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3656191</prism:doi><description>Accurate semantic segmentation of high-resolution remote sensing imagery is essential for applications ranging from urban planning to environmental monitoring. However, this task remains fundamentally challenging due to the complex spatial patterns, extreme scale variations, and fine-grained details inherent in geographical scenes. While attention mechanisms, particularly global and sparse attention, have shown promise in capturing long-range dependencies, existing approaches often suffer from three interconnected limitations: prohibitive computational complexity, misalignment when integrating multi-scale representations, and loss of semantic information during the decoder's upsampling stages. This paper introduces AMPUNet, a novel framework designed to overcome these limitations through the construction of a hierarchical, coarse-to-fine attention map pyramid. Our core innovation lies in explicitly propagating and refining attention maps across network layers rather than operating solely on feature maps. Specifically, we design: (1) a hybrid sparse attention framework combining a Block Attention Module and a Column Attention Module to model global context efficiently; (2) a Dimension Correspondence Module to achieve tensor-level granularity alignment for multi-scale attention maps; and (3) an Attention Map Merging Module with a Cross-layer Attention Weighting strategy, which directly transfers high-level semantic information from deep to shallow layers, mitigating information degradation. Extensive experiments on the ISPRS Vaihingen, Potsdam, and LoveDA benchmarks demonstrate that AMPUNet achieves superior performance, with mIoU scores of 75.43% on Vaihingen, 78.03% on Potsdam, and 50.94% on LoveDA, while maintaining competitive inference efficiency. Our findings confirm that structuring attention into a learnable pyramid is a highly effective paradigm for remote sensing semantic segmentation, successfully balancing precise detail preservation with robust global und...
Published: 2026-01-20T20:40:26+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Yang; Wei Ao; Shunyi Zheng; Zhao Liu; Yunni Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3656191"&gt;10.1109/jstars.2026.3656191&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Accurate semantic segmentation of high-resolution remote sensing imagery is essential for applications ranging from urban planning to environmental monitoring. However, this task remains fundamentally challenging due to the complex spatial patterns, extreme scale variations, and fine-grained details inherent in geographical scenes. While attention mechanisms, particularly global and sparse attention, have shown promise in capturing long-range dependencies, existing approaches often suffer from three interconnected limitations: prohibitive computational complexity, misalignment when integrating multi-scale representations, and loss of semantic information during the decoder&amp;#x27;s upsampling stages. This paper introduces AMPUNet, a novel framework designed to overcome these limitations through the construction of a hierarchical, coarse-to-fine attention map pyramid. Our core innovation lies in explicitly propagating and refining attention maps across network layers rather than operating solely on feature maps. Specifically, we design: (1) a hybrid sparse attention framework combining a Block Attention Module and a Column Attention Module to model global context efficiently; (2) a Dimension Correspondence Module to achieve tensor-level granularity alignment for multi-scale attention maps; and (3) an Attention Map Merging Module with a Cross-layer Attention Weighting strategy, which directly transfers high-level semantic information from deep to shallow layers, mitigating information degradation. Extensive experiments on the ISPRS Vaihingen, Potsdam, and LoveDA benchmarks demonstrate that AMPUNet achieves superior performance, with mIoU scores of 75.43% on Vaihingen, 78.03% on Potsdam, and 50.94% on LoveDA, while maintaining competitive inference efficiency. Our findings confirm that structuring attention into a learnable pyramid is a highly effective paradigm for remote sensing semantic segmentation, successfully balancing precise detail preservation with robust global und...&lt;/p&gt;</content:encoded></item><item><title>M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention</title><link>https://arxiv.org/abs/2601.14776v1</link><guid>http://arxiv.org/abs/2601.14776v1</guid><pubDate>Wed, 21 Jan 2026 08:55:07 +0000</pubDate><dc:creator>Xiaofan Yang</dc:creator><dc:creator>Yubin Liu</dc:creator><dc:creator>Wei Pan</dc:creator><dc:creator>Guoqing Chu</dc:creator><dc:creator>Junming Zhang</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Zhuoqi Man</dc:creator><dc:creator>Xuanming Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.
Published: 2026-01-21T08:55:07+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaofan Yang; Yubin Liu; Wei Pan; Guoqing Chu; Junming Zhang; Jie Zhao; Zhuoqi Man; Xuanming Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Knowing Where to Focus: Attention-Guided Alignment for Text-based Person Search</title><link>https://doi.org/10.1007/s11263-025-02717-8</link><guid>10.1007/s11263-025-02717-8</guid><pubDate>Tue, 20 Jan 2026 04:36:46 +0000</pubDate><dc:creator>Lei Tan</dc:creator><dc:creator>Weihao Li</dc:creator><dc:creator>Pingyang Dai</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Liujuan Cao</dc:creator><dc:creator>Rongrong Ji</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02717-8</prism:doi><description>In the realm of Text-Based Person Search (TBPS), mainstream methods aim to explore more efficient interaction frameworks between text descriptions and visual data. However, recent approaches encounter two principal challenges. Firstly, the widely used random-based Masked Language Modeling (MLM) considers all the words in the text equally during training. However, massive semantically vacuous words (‘with’, ‘the’, etc.) be masked fail to contribute to efficient interaction in the cross-modal MLM and hampers the representation alignment. Secondly, manual descriptions in TBPS datasets are tedious and inevitably contain several inaccuracies. To address these issues, we introduce an Attention-Guided Alignment (AGA) framework featuring two innovative components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module (TEM). AGM dynamically masks semantically meaningful words by aggregating the attention weight derived from the text encoding process, thereby cross-modal MLM can capture information related to the masked word from text context and images and align their representations. Meanwhile, TEM alleviates low-quality representations caused by repetitive and erroneous text descriptions by replacing those semantically meaningful words with MLM’s prediction. It not only enriches text descriptions but also prevents overfitting. Extensive experiments across three challenging benchmarks demonstrate the effectiveness of our AGA, achieving new state-of-the-art results with Rank-1 accuracy reaching $$78.36\%$$ , $$67.31\%$$ , and $$67.4\%$$ on CUHK-PEDES, ICFG-PEDES, and RSTPReid, respectively.
Published: 2026-01-20T04:36:46+00:00
Venue: International Journal of Computer Vision
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lei Tan; Weihao Li; Pingyang Dai; Jie Chen; Liujuan Cao; Rongrong Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02717-8"&gt;10.1007/s11263-025-02717-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;In the realm of Text-Based Person Search (TBPS), mainstream methods aim to explore more efficient interaction frameworks between text descriptions and visual data. However, recent approaches encounter two principal challenges. Firstly, the widely used random-based Masked Language Modeling (MLM) considers all the words in the text equally during training. However, massive semantically vacuous words (‘with’, ‘the’, etc.) be masked fail to contribute to efficient interaction in the cross-modal MLM and hampers the representation alignment. Secondly, manual descriptions in TBPS datasets are tedious and inevitably contain several inaccuracies. To address these issues, we introduce an Attention-Guided Alignment (AGA) framework featuring two innovative components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module (TEM). AGM dynamically masks semantically meaningful words by aggregating the attention weight derived from the text encoding process, thereby cross-modal MLM can capture information related to the masked word from text context and images and align their representations. Meanwhile, TEM alleviates low-quality representations caused by repetitive and erroneous text descriptions by replacing those semantically meaningful words with MLM’s prediction. It not only enriches text descriptions but also prevents overfitting. Extensive experiments across three challenging benchmarks demonstrate the effectiveness of our AGA, achieving new state-of-the-art results with Rank-1 accuracy reaching $$78.36\%$$ , $$67.31\%$$ , and $$67.4\%$$ on CUHK-PEDES, ICFG-PEDES, and RSTPReid, respectively.&lt;/p&gt;</content:encoded></item></channel></rss>