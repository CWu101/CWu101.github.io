<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 13 Jan 2026 02:39:39 +0000</lastBuildDate><item><title>High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition</title><link>https://doi.org/10.1016/j.knosys.2026.115285</link><guid>10.1016/j.knosys.2026.115285</guid><pubDate>Sun, 11 Jan 2026 15:11:40 +0000</pubDate><dc:creator>Longhao Wang</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Beibei Wu</dc:creator><dc:creator>Fushan Yao</dc:creator><dc:creator>Zijun Wei</dc:creator><dc:creator>Tian Gao</dc:creator><dc:creator>Hanyang Yu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115285</prism:doi><description>Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .
Published: 2026-01-11T15:11:40+00:00
Venue: Knowledge-Based Systems
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longhao Wang; Chaozhen Lan; Beibei Wu; Fushan Yao; Zijun Wei; Tian Gao; Hanyang Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115285"&gt;10.1016/j.knosys.2026.115285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .&lt;/p&gt;</content:encoded></item><item><title>SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving</title><link>https://arxiv.org/abs/2601.05640v1</link><guid>http://arxiv.org/abs/2601.05640v1</guid><pubDate>Fri, 09 Jan 2026 08:55:42 +0000</pubDate><dc:creator>Jingyu Li</dc:creator><dc:creator>Junjie Wu</dc:creator><dc:creator>Dongnan Hu</dc:creator><dc:creator>Xiangkai Huang</dc:creator><dc:creator>Bin Sun</dc:creator><dc:creator>Zhihui Hao</dc:creator><dc:creator>Xianpeng Lang</dc:creator><dc:creator>Xiatian Zhu</dc:creator><dc:creator>Li Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.
Published: 2026-01-09T08:55:42+00:00
Venue: arXiv
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingyu Li; Junjie Wu; Dongnan Hu; Xiangkai Huang; Bin Sun; Zhihui Hao; Xianpeng Lang; Xiatian Zhu; Li Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM&amp;#x27;s representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module</title><link>https://doi.org/10.1016/j.inffus.2026.104142</link><guid>10.1016/j.inffus.2026.104142</guid><pubDate>Sun, 11 Jan 2026 15:13:21 +0000</pubDate><dc:creator>He Li</dc:creator><dc:creator>Taiyu Liao</dc:creator><dc:creator>Weihang Kong</dc:creator><dc:creator>Xingchen Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104142</prism:doi><description>Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.
Published: 2026-01-11T15:13:21+00:00
Venue: Information Fusion
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; He Li; Taiyu Liao; Weihang Kong; Xingchen Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104142"&gt;10.1016/j.inffus.2026.104142&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Exploring Difference Semantic Prior Guidance for Remote Sensing Image Change Captioning</title><link>https://doi.org/10.3390/rs18020232</link><guid>10.3390/rs18020232</guid><pubDate>Mon, 12 Jan 2026 07:30:34 +0000</pubDate><dc:creator>Yunpeng Li</dc:creator><dc:creator>Xiangrong Zhang</dc:creator><dc:creator>Guanchun Wang</dc:creator><dc:creator>Tianyang Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020232</prism:doi><description>Understanding complex change scenes is a crucial challenge in remote sensing field. Remote sensing image change captioning (RSICC) task has emerged as a promising approach to translate appeared changes between bi-temporal remote sensing images into textual descriptions, enabling users to make accurate decisions. Current RSICC methods frequently encounter difficulties in consistency for contextual awareness and semantic prior guidance. Therefore, this study explores difference semantic prior guidance network to reason context-rich sentence for capturing appeared vision changes. Specifically, the context-aware difference module is introduced to guarantee the consistency of unchanged/changed context features, strengthening multi-level changed information to improve the ability of semantic change feature representation. Moreover, to effectively mine higher-level cognition ability to reason salient/weak changes, we employ difference comprehending with shallow change information to realize semantic change knowledge learning. In addition, the designed parallel cross refined attention in Transformer decoder can balance vision difference and semantic knowledge for implicit knowledge distilling, enabling fine-grained perception changes of semantic details and reducing pseudochanges. Compared with advanced algorithms on the LEVIR-CC and Dubai-CC datasets, experimental results validate the outstanding performance of the designed model in RSICC tasks. Notably, on the LEVIR-CC dataset, it reaches a CIDEr score of 143.34%, representing a 3.11% improvement over the most competitive SAT-cap.
Published: 2026-01-12T07:30:34+00:00
Venue: Remote Sensing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunpeng Li; Xiangrong Zhang; Guanchun Wang; Tianyang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020232"&gt;10.3390/rs18020232&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Understanding complex change scenes is a crucial challenge in remote sensing field. Remote sensing image change captioning (RSICC) task has emerged as a promising approach to translate appeared changes between bi-temporal remote sensing images into textual descriptions, enabling users to make accurate decisions. Current RSICC methods frequently encounter difficulties in consistency for contextual awareness and semantic prior guidance. Therefore, this study explores difference semantic prior guidance network to reason context-rich sentence for capturing appeared vision changes. Specifically, the context-aware difference module is introduced to guarantee the consistency of unchanged/changed context features, strengthening multi-level changed information to improve the ability of semantic change feature representation. Moreover, to effectively mine higher-level cognition ability to reason salient/weak changes, we employ difference comprehending with shallow change information to realize semantic change knowledge learning. In addition, the designed parallel cross refined attention in Transformer decoder can balance vision difference and semantic knowledge for implicit knowledge distilling, enabling fine-grained perception changes of semantic details and reducing pseudochanges. Compared with advanced algorithms on the LEVIR-CC and Dubai-CC datasets, experimental results validate the outstanding performance of the designed model in RSICC tasks. Notably, on the LEVIR-CC dataset, it reaches a CIDEr score of 143.34%, representing a 3.11% improvement over the most competitive SAT-cap.&lt;/p&gt;</content:encoded></item><item><title>MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</title><link>https://arxiv.org/abs/2601.06874v1</link><guid>http://arxiv.org/abs/2601.06874v1</guid><pubDate>Sun, 11 Jan 2026 11:44:07 +0000</pubDate><dc:creator>Changli Wu</dc:creator><dc:creator>Haodong Wang</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Yutian Yao</dc:creator><dc:creator>Chunsai Du</dc:creator><dc:creator>Jihua Kang</dc:creator><dc:creator>Yanwei Fu</dc:creator><dc:creator>Liujuan Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.
Published: 2026-01-11T11:44:07+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changli Wu; Haodong Wang; Jiayi Ji; Yutian Yao; Chunsai Du; Jihua Kang; Yanwei Fu; Liujuan Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.&lt;/p&gt;</content:encoded></item><item><title>SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation</title><link>https://arxiv.org/abs/2601.06806v1</link><guid>http://arxiv.org/abs/2601.06806v1</guid><pubDate>Sun, 11 Jan 2026 08:39:19 +0000</pubDate><dc:creator>Jiwen Zhang</dc:creator><dc:creator>Zejun Li</dc:creator><dc:creator>Siyuan Wang</dc:creator><dc:creator>Xiangyu Shi</dc:creator><dc:creator>Zhongyu Wei</dc:creator><dc:creator>Qi Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.
Published: 2026-01-11T08:39:19+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiwen Zhang; Zejun Li; Siyuan Wang; Xiangyu Shi; Zhongyu Wei; Qi Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.&lt;/p&gt;</content:encoded></item><item><title>MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding</title><link>https://arxiv.org/abs/2601.05495v1</link><guid>http://arxiv.org/abs/2601.05495v1</guid><pubDate>Fri, 09 Jan 2026 02:59:05 +0000</pubDate><dc:creator>Zizhong Li</dc:creator><dc:creator>Haopeng Zhang</dc:creator><dc:creator>Jiawei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.
Published: 2026-01-09T02:59:05+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zizhong Li; Haopeng Zhang; Jiawei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.&lt;/p&gt;</content:encoded></item><item><title>Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</title><link>https://arxiv.org/abs/2601.05927v1</link><guid>http://arxiv.org/abs/2601.05927v1</guid><pubDate>Fri, 09 Jan 2026 16:41:08 +0000</pubDate><dc:creator>Yohann Perron</dc:creator><dc:creator>Vladyslav Sydorov</dc:creator><dc:creator>Christophe Pottier</dc:creator><dc:creator>Loic Landrieu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .
Published: 2026-01-09T16:41:08+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yohann Perron; Vladyslav Sydorov; Christophe Pottier; Loic Landrieu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .&lt;/p&gt;</content:encoded></item><item><title>QCaption: Video Captioning and Q&amp;A through Fusion of Large Multimodal Models</title><link>https://arxiv.org/abs/2601.06566v1</link><guid>http://arxiv.org/abs/2601.06566v1</guid><pubDate>Sat, 10 Jan 2026 13:28:43 +0000</pubDate><dc:creator>Jiale Wang</dc:creator><dc:creator>Gee Wah Ng</dc:creator><dc:creator>Lee Onn Mak</dc:creator><dc:creator>Randall Cher</dc:creator><dc:creator>Ng Ding Hei Ryan</dc:creator><dc:creator>Davis Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.23919/FUSION59988.2024.10706514</prism:doi><description>This paper introduces QCaption, a novel video captioning and Q&amp;A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&amp;A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&amp;A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.
Published: 2026-01-10T13:28:43+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiale Wang; Gee Wah Ng; Lee Onn Mak; Randall Cher; Ng Ding Hei Ryan; Davis Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.23919/FUSION59988.2024.10706514"&gt;10.23919/FUSION59988.2024.10706514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;This paper introduces QCaption, a novel video captioning and Q&amp;amp;A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&amp;amp;A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&amp;amp;A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.&lt;/p&gt;</content:encoded></item><item><title>Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression</title><link>https://arxiv.org/abs/2601.07092v1</link><guid>http://arxiv.org/abs/2601.07092v1</guid><pubDate>Sun, 11 Jan 2026 23:25:49 +0000</pubDate><dc:creator>Yuliang Cai</dc:creator><dc:creator>Dongqiangzi Ye</dc:creator><dc:creator>Zitian Chen</dc:creator><dc:creator>Chongruo Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.
Published: 2026-01-11T23:25:49+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Cai; Dongqiangzi Ye; Zitian Chen; Chongruo Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.&lt;/p&gt;</content:encoded></item><item><title>ArrowGEV: Grounding Events in Video via Learning the Arrow of Time</title><link>https://arxiv.org/abs/2601.06559v1</link><guid>http://arxiv.org/abs/2601.06559v1</guid><pubDate>Sat, 10 Jan 2026 13:05:23 +0000</pubDate><dc:creator>Fangxu Yu</dc:creator><dc:creator>Ziyao Lu</dc:creator><dc:creator>Liqiang Niu</dc:creator><dc:creator>Fandong Meng</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.
Published: 2026-01-10T13:05:23+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fangxu Yu; Ziyao Lu; Liqiang Niu; Fandong Meng; Jie Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.&lt;/p&gt;</content:encoded></item><item><title>LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</title><link>https://arxiv.org/abs/2601.06550v1</link><guid>http://arxiv.org/abs/2601.06550v1</guid><pubDate>Sat, 10 Jan 2026 12:18:12 +0000</pubDate><dc:creator>Pan Liao</dc:creator><dc:creator>Feng Yang</dc:creator><dc:creator>Di Wu</dc:creator><dc:creator>Jinwen Yu</dc:creator><dc:creator>Yuhua Zhu</dc:creator><dc:creator>Wenhui Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.
Published: 2026-01-10T12:18:12+00:00
Venue: arXiv
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pan Liao; Feng Yang; Di Wu; Jinwen Yu; Yuhua Zhu; Wenhui Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.&lt;/p&gt;</content:encoded></item><item><title>PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering</title><link>https://arxiv.org/abs/2601.05465v1</link><guid>http://arxiv.org/abs/2601.05465v1</guid><pubDate>Fri, 09 Jan 2026 01:38:38 +0000</pubDate><dc:creator>Yu Liu</dc:creator><dc:creator>Wenxiao Zhang</dc:creator><dc:creator>Cong Cao</dc:creator><dc:creator>Wenxuan Lu</dc:creator><dc:creator>Fangfang Yuan</dc:creator><dc:creator>Diandian Guo</dc:creator><dc:creator>Kun Peng</dc:creator><dc:creator>Qiang Sun</dc:creator><dc:creator>Kaiyan Zhang</dc:creator><dc:creator>Yanbing Liu</dc:creator><dc:creator>Jin B. Hong</dc:creator><dc:creator>Bowen Zhou</dc:creator><dc:creator>Zhiyuan Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.
Published: 2026-01-09T01:38:38+00:00
Venue: arXiv
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Liu; Wenxiao Zhang; Cong Cao; Wenxuan Lu; Fangfang Yuan; Diandian Guo; Kun Peng; Qiang Sun; Kaiyan Zhang; Yanbing Liu; Jin B. Hong; Bowen Zhou; Zhiyuan Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA&amp;#x27;s strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner&amp;#x27;s decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector&amp;#x27;s ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>SCVI: A semi-coupled visible-infrared small object detection method based on multimodal proposal-level probability fusion strategy</title><link>https://doi.org/10.1016/j.neucom.2026.132688</link><guid>10.1016/j.neucom.2026.132688</guid><pubDate>Mon, 12 Jan 2026 17:00:27 +0000</pubDate><dc:creator>Haozhi Xu</dc:creator><dc:creator>Xiaofang Yuan</dc:creator><dc:creator>Jinlei Wang</dc:creator><dc:creator>Yaonan Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132688</prism:doi><description>Visible-Infrared (VI) fusion is widely adopted to improve robustness for all-weather object detection. However, VI small object detection remains challenging: small objects exhibit weaker feature than larger ones, while cross-modal misalignment and modality-specific degradation make feature-level fusion prone to suppressing or corrupting these fragile cues. Once such object’s feature is lost during fusion, later decoding stages can hardly recover it, leading to systematic small object omissions. To mitigate this issue, a semi-coupled VI detection framework, tailored for small objects is proposed, called SCVI. It first generates modality-specific candidate proposals independently from two branches. Then, a multimodal proposal-level probabilistic fusion strategy selectively matches, filters, and fuses candidates to form a consolidated set of high-quality queries, with improved tolerance to uncertainty and a preference for small objects. Finally, these queries interact with modality-specific features via modality-selective deformable attention, enabling controlled cross-modal collaboration without coupled feature fusion. Experiments on established VI small object detection benchmarks demonstrate that SCVI achieves competitive accuracy and robustness. The implementation code will be made publicly available at https://github.com/XUhaozhi88/SC-VI-SOD.git .
Published: 2026-01-12T17:00:27+00:00
Venue: Neurocomputing
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haozhi Xu; Xiaofang Yuan; Jinlei Wang; Yaonan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132688"&gt;10.1016/j.neucom.2026.132688&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Visible-Infrared (VI) fusion is widely adopted to improve robustness for all-weather object detection. However, VI small object detection remains challenging: small objects exhibit weaker feature than larger ones, while cross-modal misalignment and modality-specific degradation make feature-level fusion prone to suppressing or corrupting these fragile cues. Once such object’s feature is lost during fusion, later decoding stages can hardly recover it, leading to systematic small object omissions. To mitigate this issue, a semi-coupled VI detection framework, tailored for small objects is proposed, called SCVI. It first generates modality-specific candidate proposals independently from two branches. Then, a multimodal proposal-level probabilistic fusion strategy selectively matches, filters, and fuses candidates to form a consolidated set of high-quality queries, with improved tolerance to uncertainty and a preference for small objects. Finally, these queries interact with modality-specific features via modality-selective deformable attention, enabling controlled cross-modal collaboration without coupled feature fusion. Experiments on established VI small object detection benchmarks demonstrate that SCVI achieves competitive accuracy and robustness. The implementation code will be made publicly available at https://github.com/XUhaozhi88/SC-VI-SOD.git .&lt;/p&gt;</content:encoded></item><item><title>A dual-path network for semantic scene completion of single-frame LiDAR point clouds</title><link>https://doi.org/10.1016/j.jag.2025.105020</link><guid>10.1016/j.jag.2025.105020</guid><pubDate>Mon, 12 Jan 2026 10:56:45 +0000</pubDate><dc:creator>Wei Liu</dc:creator><dc:creator>Ziwen Kang</dc:creator><dc:creator>Yongtao Yu</dc:creator><dc:creator>Zheng Gong</dc:creator><dc:creator>Yuchao Zheng</dc:creator><dc:creator>Xiaohui Huang</dc:creator><dc:creator>Haiyan Guan</dc:creator><dc:creator>Lingfei Ma</dc:creator><dc:creator>Dedong Zhang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105020</prism:doi><description>Semantic Scene Completion (SSC) is a fundamental yet challenging task in 3D environment perception, as the sparsity and noise of LiDAR point clouds make it difficult to accurately recover both geometry and semantics. To address these challenges, we propose DPS2CNet, a novel Dual-Path SSC Network that integrates voxel-based and bird’s-eye view (BEV) representations to exploit their complementary strengths. Specifically, DPS2CNet employs a Cylinder3D-enhanced voxel branch to capture fine-grained 3D geometry and a UNet-based BEV branch to model large-scale contextual information. To further boost performance, we incorporate CARAFE for efficient feature upsampling and design a tailored loss function optimized for SSC. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 demonstrate that DPS2CNet achieves state-of-the-art results. In particular, it ranks first on the SemanticKITTI test set with an IoU of 62.6% among all open-source submissions 1 , highlighting its effectiveness in complex real-world driving scenarios.
Published: 2026-01-12T10:56:45+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Liu; Ziwen Kang; Yongtao Yu; Zheng Gong; Yuchao Zheng; Xiaohui Huang; Haiyan Guan; Lingfei Ma; Dedong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105020"&gt;10.1016/j.jag.2025.105020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Semantic Scene Completion (SSC) is a fundamental yet challenging task in 3D environment perception, as the sparsity and noise of LiDAR point clouds make it difficult to accurately recover both geometry and semantics. To address these challenges, we propose DPS2CNet, a novel Dual-Path SSC Network that integrates voxel-based and bird’s-eye view (BEV) representations to exploit their complementary strengths. Specifically, DPS2CNet employs a Cylinder3D-enhanced voxel branch to capture fine-grained 3D geometry and a UNet-based BEV branch to model large-scale contextual information. To further boost performance, we incorporate CARAFE for efficient feature upsampling and design a tailored loss function optimized for SSC. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 demonstrate that DPS2CNet achieves state-of-the-art results. In particular, it ranks first on the SemanticKITTI test set with an IoU of 62.6% among all open-source submissions 1 , highlighting its effectiveness in complex real-world driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</title><link>https://arxiv.org/abs/2601.06496v1</link><guid>http://arxiv.org/abs/2601.06496v1</guid><pubDate>Sat, 10 Jan 2026 09:13:10 +0000</pubDate><dc:creator>Hao Tang</dc:creator><dc:creator>Ting Huang</dc:creator><dc:creator>Zeyu Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.
Published: 2026-01-10T09:13:10+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Tang; Ting Huang; Zeyu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.&lt;/p&gt;</content:encoded></item><item><title>MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning</title><link>https://arxiv.org/abs/2601.07107v1</link><guid>http://arxiv.org/abs/2601.07107v1</guid><pubDate>Mon, 12 Jan 2026 00:11:10 +0000</pubDate><dc:creator>Meng Lu</dc:creator><dc:creator>Yuxing Lu</dc:creator><dc:creator>Yuchen Zhuang</dc:creator><dc:creator>Megan Mullins</dc:creator><dc:creator>Yang Xie</dc:creator><dc:creator>Guanghua Xiao</dc:creator><dc:creator>Charles Fleming</dc:creator><dc:creator>Wenqi Shi</dc:creator><dc:creator>Xuan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.
Published: 2026-01-12T00:11:10+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Lu; Yuxing Lu; Yuchen Zhuang; Megan Mullins; Yang Xie; Guanghua Xiao; Charles Fleming; Wenqi Shi; Xuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.&lt;/p&gt;</content:encoded></item><item><title>Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?</title><link>https://arxiv.org/abs/2601.06993v1</link><guid>http://arxiv.org/abs/2601.06993v1</guid><pubDate>Sun, 11 Jan 2026 17:07:47 +0000</pubDate><dc:creator>Jie Zhu</dc:creator><dc:creator>Yiyang Su</dc:creator><dc:creator>Xiaoming Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.
Published: 2026-01-11T17:07:47+00:00
Venue: arXiv
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Zhu; Yiyang Su; Xiaoming Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking&amp;#x27;&amp;#x27;. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.&lt;/p&gt;</content:encoded></item><item><title>Grouped Query Attention Supported with Graph-based Query Clustering</title><link>https://doi.org/10.1016/j.knosys.2026.115311</link><guid>10.1016/j.knosys.2026.115311</guid><pubDate>Mon, 12 Jan 2026 16:25:43 +0000</pubDate><dc:creator>Ling Zheng</dc:creator><dc:creator>Yujia Zhang</dc:creator><dc:creator>Liang Shen</dc:creator><dc:creator>Chong Miao</dc:creator><dc:creator>Qiang Shen</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115311</prism:doi><description>Grouped Query Attention (GQA), is a generalized form of multi-query attention, crafted to minimize the size of models by enabling shared key-value pairs for given queries. This approach facilitates faster decoder inference. Nonetheless, disparities between the queries may introduce uncertainty in the key-value pairs, leading to a decline in the message propagation effectiveness of the attention models. A potential solution to this problem is to cluster queries based on their proximity into groups that share a key-value pair. This paper proposes a novel framework to enhance GQA, exploiting a graph method to cluster akin queries. It adaptively guides the fusion of key-value pairs to reduce model parameters while preserving inference performance. Three different clustering methods, namely k -means, Hungarian Algorithm, and Blossom Algorithm, have each been employed to instantiate the clustering-based GQA (CGQA). This approach is validated through applications to two real-world problems, addressing tasks of Natural Language Understanding and Natural Language Generation, respectively. The comparative experimental analysis carried out demonstrates that the proposed approach outperforms the existing GQA methods, reinforcing the effectiveness of the underlying attention models.
Published: 2026-01-12T16:25:43+00:00
Venue: Knowledge-Based Systems
Score: 0.508 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ling Zheng; Yujia Zhang; Liang Shen; Chong Miao; Qiang Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115311"&gt;10.1016/j.knosys.2026.115311&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.508 (consider)&lt;/p&gt;
&lt;p&gt;Grouped Query Attention (GQA), is a generalized form of multi-query attention, crafted to minimize the size of models by enabling shared key-value pairs for given queries. This approach facilitates faster decoder inference. Nonetheless, disparities between the queries may introduce uncertainty in the key-value pairs, leading to a decline in the message propagation effectiveness of the attention models. A potential solution to this problem is to cluster queries based on their proximity into groups that share a key-value pair. This paper proposes a novel framework to enhance GQA, exploiting a graph method to cluster akin queries. It adaptively guides the fusion of key-value pairs to reduce model parameters while preserving inference performance. Three different clustering methods, namely k -means, Hungarian Algorithm, and Blossom Algorithm, have each been employed to instantiate the clustering-based GQA (CGQA). This approach is validated through applications to two real-world problems, addressing tasks of Natural Language Understanding and Natural Language Generation, respectively. The comparative experimental analysis carried out demonstrates that the proposed approach outperforms the existing GQA methods, reinforcing the effectiveness of the underlying attention models.&lt;/p&gt;</content:encoded></item><item><title>SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning</title><link>https://arxiv.org/abs/2601.06474v1</link><guid>http://arxiv.org/abs/2601.06474v1</guid><pubDate>Sat, 10 Jan 2026 07:54:20 +0000</pubDate><dc:creator>Chenxu Dang</dc:creator><dc:creator>Jie Wang</dc:creator><dc:creator>Guang Li</dc:creator><dc:creator>Zhiwen Hou</dc:creator><dc:creator>Zihan You</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Jie Ma</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Yan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.
Published: 2026-01-10T07:54:20+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenxu Dang; Jie Wang; Guang Li; Zhiwen Hou; Zihan You; Hangjun Ye; Jie Ma; Long Chen; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.&lt;/p&gt;</content:encoded></item><item><title>ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction</title><link>https://arxiv.org/abs/2601.05470v1</link><guid>http://arxiv.org/abs/2601.05470v1</guid><pubDate>Fri, 09 Jan 2026 02:02:37 +0000</pubDate><dc:creator>Tingwei Xie</dc:creator><dc:creator>Jinxin He</dc:creator><dc:creator>Yonghong Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.
Published: 2026-01-09T02:02:37+00:00
Venue: arXiv
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tingwei Xie; Jinxin He; Yonghong Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot change detection in optical and SAR remote sensing images for disaster response</title><link>https://doi.org/10.1016/j.jag.2026.105100</link><guid>10.1016/j.jag.2026.105100</guid><pubDate>Mon, 12 Jan 2026 19:23:17 +0000</pubDate><dc:creator>Di Wang</dc:creator><dc:creator>Guorui Ma</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Ronghao Yang</dc:creator><dc:creator>Yongxian Zhang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105100</prism:doi><description>Few-shot change detection in optical and Synthetic Aperture Radar images is a critical task for disaster monitoring. offering significant application value in complex scenarios with extremely limited labeled samples. However, the randomness of disasters causes a notable data distribution shift between public datasets and real disaster scenarios. With only a few annotated image pairs, existing methods struggle to effectively fuse features from heterogeneous images, leading to severe performance degradation. To address this challenge, we propose a Dual-Stage Training framework for Change Detection (DSTCD), specifically designed for few-shot scenarios involving fewer than 20 labeled image pairs. DSTCD first undergoes source task pre-training on a heterogeneous image registration dataset. Subsequently, in the target task stage, it leverages task guided feature transfer module to transfer the structural and semantic features of image registration to the change detection task. This mechanism significantly enriches the feature representations under few-shot conditions, enabling accurate identification of affected areas. To validate its performance, we conducted comparative and ablation studies against eleven state-of-the-art methods on four public datasets covering both urban expansion and water expansion scenarios. Experimental results demonstrate that DSTCD achieves a significant performance lead. Its average F1-score surpasses the second-best method by 6.98% in urban expansion scenarios and by 13.09% in water expansion scenarios, proving its superior performance and strong multi-scenario adaptability. Furthermore, robustness analysis of varying training sample sizes and real-world disaster application validation further confirm the method’s practicality and robustness for data-scarce disaster monitoring tasks. The code of the proposed method will be made available at https://github.com/Lucky-DW/DSTCD .
Published: 2026-01-12T19:23:17+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Di Wang; Guorui Ma; Xiao Wang; Ronghao Yang; Yongxian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105100"&gt;10.1016/j.jag.2026.105100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot change detection in optical and Synthetic Aperture Radar images is a critical task for disaster monitoring. offering significant application value in complex scenarios with extremely limited labeled samples. However, the randomness of disasters causes a notable data distribution shift between public datasets and real disaster scenarios. With only a few annotated image pairs, existing methods struggle to effectively fuse features from heterogeneous images, leading to severe performance degradation. To address this challenge, we propose a Dual-Stage Training framework for Change Detection (DSTCD), specifically designed for few-shot scenarios involving fewer than 20 labeled image pairs. DSTCD first undergoes source task pre-training on a heterogeneous image registration dataset. Subsequently, in the target task stage, it leverages task guided feature transfer module to transfer the structural and semantic features of image registration to the change detection task. This mechanism significantly enriches the feature representations under few-shot conditions, enabling accurate identification of affected areas. To validate its performance, we conducted comparative and ablation studies against eleven state-of-the-art methods on four public datasets covering both urban expansion and water expansion scenarios. Experimental results demonstrate that DSTCD achieves a significant performance lead. Its average F1-score surpasses the second-best method by 6.98% in urban expansion scenarios and by 13.09% in water expansion scenarios, proving its superior performance and strong multi-scenario adaptability. Furthermore, robustness analysis of varying training sample sizes and real-world disaster application validation further confirm the method’s practicality and robustness for data-scarce disaster monitoring tasks. The code of the proposed method will be made available at https://github.com/Lucky-DW/DSTCD .&lt;/p&gt;</content:encoded></item><item><title>Bio-Inspired Temporal Difference and Spatial-Frequency Gaming Network for Video Camouflaged Object Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115306</link><guid>10.1016/j.knosys.2026.115306</guid><pubDate>Sun, 11 Jan 2026 22:46:32 +0000</pubDate><dc:creator>Guangyu Zhao</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Meiling Gao</dc:creator><dc:creator>Jin Duan</dc:creator><dc:creator>Mingxin Zhao</dc:creator><dc:creator>Jilong Tang</dc:creator><dc:creator>Xiaojiao Jiang</dc:creator><dc:creator>Peng Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115306</prism:doi><description>Video camouflaged object detection (VCOD) poses formidable challenges arising from temporal dynamics, multi-scale spatial heterogeneities, and pronounced feature indistinguishability between targets and backgrounds. Drawing inspiration from human visual cognition, which employs iterative multi-scale information gaming to unmask concealed entities, we present TSGNet, a biologically motivated temporal difference and spatial-frequency gaming network tailored for VCOD. The architecture incorporates a dual-branch encoder to extract semantic and textural features across disparate resolutions, augmented by a channel contrast enhancement submodule that amplifies subtle discriminative cues. A hierarchical spatio-frequency deformable aggregator synergistically merges adaptive spatial deformable convolutions with bidirectional frequency-domain attention, facilitating robust multi-scale feature fusion. Complementarily, a temporal difference modeling unit exploits multi-shift temporal scales to capture intricate dynamic evolutions, while a multi-scale consistency comparison mechanism iteratively refines predictions through cross-resolution alignment. Rigorous evaluations on established VCOD benchmarks, including MoCA-Mask and CAD, substantiate our preeminence, and cross-domain generalizations on video salient object detection, video object segmentation and medical polyp segmentation datasets underscore its adaptability and broader applicability in computer vision paradigms. The code and results will be released at https://github.com/Hello-GYu/TSGNet .
Published: 2026-01-11T22:46:32+00:00
Venue: Knowledge-Based Systems
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangyu Zhao; Yang Yang; Meiling Gao; Jin Duan; Mingxin Zhao; Jilong Tang; Xiaojiao Jiang; Peng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115306"&gt;10.1016/j.knosys.2026.115306&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;Video camouflaged object detection (VCOD) poses formidable challenges arising from temporal dynamics, multi-scale spatial heterogeneities, and pronounced feature indistinguishability between targets and backgrounds. Drawing inspiration from human visual cognition, which employs iterative multi-scale information gaming to unmask concealed entities, we present TSGNet, a biologically motivated temporal difference and spatial-frequency gaming network tailored for VCOD. The architecture incorporates a dual-branch encoder to extract semantic and textural features across disparate resolutions, augmented by a channel contrast enhancement submodule that amplifies subtle discriminative cues. A hierarchical spatio-frequency deformable aggregator synergistically merges adaptive spatial deformable convolutions with bidirectional frequency-domain attention, facilitating robust multi-scale feature fusion. Complementarily, a temporal difference modeling unit exploits multi-shift temporal scales to capture intricate dynamic evolutions, while a multi-scale consistency comparison mechanism iteratively refines predictions through cross-resolution alignment. Rigorous evaluations on established VCOD benchmarks, including MoCA-Mask and CAD, substantiate our preeminence, and cross-domain generalizations on video salient object detection, video object segmentation and medical polyp segmentation datasets underscore its adaptability and broader applicability in computer vision paradigms. The code and results will be released at https://github.com/Hello-GYu/TSGNet .&lt;/p&gt;</content:encoded></item><item><title>Multi-Head Attention Residual Unfolded Network for Model-Based Pansharpening</title><link>https://doi.org/10.1007/s11263-025-02651-9</link><guid>10.1007/s11263-025-02651-9</guid><pubDate>Mon, 12 Jan 2026 04:59:18 +0000</pubDate><dc:creator>Ivan Pereira-Sánchez</dc:creator><dc:creator>Eloi Sans</dc:creator><dc:creator>Julia Navarro</dc:creator><dc:creator>Joan Duran</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02651-9</prism:doi><description>The objective of pansharpening and hypersharpening is to accurately fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods integrate the powerful representation capabilities of deep learning with the robustness of model-based approaches. These techniques usually involve unrolling the steps of the optimization scheme derived from the minimization of a variational energy into a deep learning framework, resulting in efficient and highly interpretable architectures. In this paper, we present a model-based deep unfolded method for satellite image fusion. Our approach relies on a variational formulation that incorporates the classic observation model for MS/HS data, a high-frequency injection constraint, and a general prior. For the unfolding stage, we design upsampling and downsampling layers that leverage geometric information encoded in the PAN image through residual networks. The core of our method is a Multi-Head Attention Residual Network (MARNet), which combines multiple head attentions with residual learning to capture image self-similarities using nonlocal patch-based operators. Additionally, we include a post-processing module based on the MARNet architecture to further enhance the quality of the fused images. Experimental results on PRISMA, QuickBird, and WorldView2 datasets demonstrate the superior performance of our method, both at reduced and full-scale resolutions, along with its ability to generalize across different sensor configurations and varying spatial and spectral resolutions. The source code will be available at https://github.com/TAMI-UIB/MARNet .
Published: 2026-01-12T04:59:18+00:00
Venue: International Journal of Computer Vision
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Pereira-Sánchez; Eloi Sans; Julia Navarro; Joan Duran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02651-9"&gt;10.1007/s11263-025-02651-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;The objective of pansharpening and hypersharpening is to accurately fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods integrate the powerful representation capabilities of deep learning with the robustness of model-based approaches. These techniques usually involve unrolling the steps of the optimization scheme derived from the minimization of a variational energy into a deep learning framework, resulting in efficient and highly interpretable architectures. In this paper, we present a model-based deep unfolded method for satellite image fusion. Our approach relies on a variational formulation that incorporates the classic observation model for MS/HS data, a high-frequency injection constraint, and a general prior. For the unfolding stage, we design upsampling and downsampling layers that leverage geometric information encoded in the PAN image through residual networks. The core of our method is a Multi-Head Attention Residual Network (MARNet), which combines multiple head attentions with residual learning to capture image self-similarities using nonlocal patch-based operators. Additionally, we include a post-processing module based on the MARNet architecture to further enhance the quality of the fused images. Experimental results on PRISMA, QuickBird, and WorldView2 datasets demonstrate the superior performance of our method, both at reduced and full-scale resolutions, along with its ability to generalize across different sensor configurations and varying spatial and spectral resolutions. The source code will be available at https://github.com/TAMI-UIB/MARNet .&lt;/p&gt;</content:encoded></item><item><title>Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</title><link>https://arxiv.org/abs/2601.06943v1</link><guid>http://arxiv.org/abs/2601.06943v1</guid><pubDate>Sun, 11 Jan 2026 15:07:37 +0000</pubDate><dc:creator>Chengwen Liu</dc:creator><dc:creator>Xiaomin Yu</dc:creator><dc:creator>Zhuoyue Chang</dc:creator><dc:creator>Zhe Huang</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Heng Lian</dc:creator><dc:creator>Kunyi Wang</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Sen Hu</dc:creator><dc:creator>Jianheng Hou</dc:creator><dc:creator>Hao Peng</dc:creator><dc:creator>Chengwei Qin</dc:creator><dc:creator>Xiaobin Hu</dc:creator><dc:creator>Hong Peng</dc:creator><dc:creator>Ronghao Chen</dc:creator><dc:creator>Huacan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.
Published: 2026-01-11T15:07:37+00:00
Venue: arXiv
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chengwen Liu; Xiaomin Yu; Zhuoyue Chang; Zhe Huang; Shuo Zhang; Heng Lian; Kunyi Wang; Rui Xu; Sen Hu; Jianheng Hou; Hao Peng; Chengwei Qin; Xiaobin Hu; Hong Peng; Ronghao Chen; Huacan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model&amp;#x27;s ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.&lt;/p&gt;</content:encoded></item><item><title>FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time</title><link>https://arxiv.org/abs/2601.05738v1</link><guid>http://arxiv.org/abs/2601.05738v1</guid><pubDate>Fri, 09 Jan 2026 11:40:16 +0000</pubDate><dc:creator>Christopher Thirgood</dc:creator><dc:creator>Oscar Mendez</dc:creator><dc:creator>Erin Ling</dc:creator><dc:creator>Jon Storey</dc:creator><dc:creator>Simon Hadfield</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.
Published: 2026-01-09T11:40:16+00:00
Venue: arXiv
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Christopher Thirgood; Oscar Mendez; Erin Ling; Jon Storey; Simon Hadfield&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.&lt;/p&gt;</content:encoded></item><item><title>ConSensus: Multi-Agent Collaboration for Multimodal Sensing</title><link>https://arxiv.org/abs/2601.06453v1</link><guid>http://arxiv.org/abs/2601.06453v1</guid><pubDate>Sat, 10 Jan 2026 06:41:01 +0000</pubDate><dc:creator>Hyungjun Yoon</dc:creator><dc:creator>Mohammad Malekzadeh</dc:creator><dc:creator>Sung-Ju Lee</dc:creator><dc:creator>Fahim Kawsar</dc:creator><dc:creator>Lorena Qendro</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.
Published: 2026-01-10T06:41:01+00:00
Venue: arXiv
Score: 0.503 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyungjun Yoon; Mohammad Malekzadeh; Sung-Ju Lee; Fahim Kawsar; Lorena Qendro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.503 (consider)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.&lt;/p&gt;</content:encoded></item><item><title>DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion</title><link>https://arxiv.org/abs/2601.05538v1</link><guid>http://arxiv.org/abs/2601.05538v1</guid><pubDate>Fri, 09 Jan 2026 05:26:54 +0000</pubDate><dc:creator>Yiming Sun</dc:creator><dc:creator>Zifan Ye</dc:creator><dc:creator>Qinghua Hu</dc:creator><dc:creator>Pengfei Zhu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.
Published: 2026-01-09T05:26:54+00:00
Venue: arXiv
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Sun; Zifan Ye; Qinghua Hu; Pengfei Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.&lt;/p&gt;</content:encoded></item><item><title>Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion</title><link>https://arxiv.org/abs/2601.05629v1</link><guid>http://arxiv.org/abs/2601.05629v1</guid><pubDate>Fri, 09 Jan 2026 08:34:05 +0000</pubDate><dc:creator>Jiapu Wang</dc:creator><dc:creator>Xinghe Cheng</dc:creator><dc:creator>Zezheng Wu</dc:creator><dc:creator>Ruiqi Ma</dc:creator><dc:creator>Rui Wang</dc:creator><dc:creator>Zhichao Yan</dc:creator><dc:creator>Haoran Luo</dc:creator><dc:creator>Yuhao Jiang</dc:creator><dc:creator>Kai Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.
Published: 2026-01-09T08:34:05+00:00
Venue: arXiv
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiapu Wang; Xinghe Cheng; Zezheng Wu; Ruiqi Ma; Rui Wang; Zhichao Yan; Haoran Luo; Yuhao Jiang; Kai Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration</title><link>https://arxiv.org/abs/2601.06605v1</link><guid>http://arxiv.org/abs/2601.06605v1</guid><pubDate>Sat, 10 Jan 2026 16:01:14 +0000</pubDate><dc:creator>Yingying Deng</dc:creator><dc:creator>Xiangyu He</dc:creator><dc:creator>Fan Tang</dc:creator><dc:creator>Weiming Dong</dc:creator><dc:creator>Xucheng Yin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-guided image generation has advanced rapidly with large-scale diffusion models, yet achieving precise stylization with visual exemplars remains difficult. Existing approaches often depend on task-specific retraining or expensive inversion procedures, which can compromise content integrity, reduce style fidelity, and lead to an unsatisfactory trade-off between semantic prompt adherence and style alignment. In this work, we introduce a training-free framework that reformulates style-guided synthesis as an in-context learning task. Guided by textual semantic prompts, our method concatenates a reference style image with a masked target image, leveraging a pretrained ReFlow-based inpainting model to seamlessly integrate semantic content with the desired style through multimodal attention fusion. We further analyze the imbalance and noise sensitivity inherent in multimodal attention fusion and propose a Dynamic Semantic-Style Integration (DSSI) mechanism that reweights attention between textual semantic and style visual tokens, effectively resolving guidance conflicts and enhancing output coherence. Experiments show that our approach achieves high-fidelity stylization with superior semantic-style balance and visual quality, offering a simple yet powerful alternative to complex, artifact-prone prior methods.
Published: 2026-01-10T16:01:14+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingying Deng; Xiangyu He; Fan Tang; Weiming Dong; Xucheng Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Text-guided image generation has advanced rapidly with large-scale diffusion models, yet achieving precise stylization with visual exemplars remains difficult. Existing approaches often depend on task-specific retraining or expensive inversion procedures, which can compromise content integrity, reduce style fidelity, and lead to an unsatisfactory trade-off between semantic prompt adherence and style alignment. In this work, we introduce a training-free framework that reformulates style-guided synthesis as an in-context learning task. Guided by textual semantic prompts, our method concatenates a reference style image with a masked target image, leveraging a pretrained ReFlow-based inpainting model to seamlessly integrate semantic content with the desired style through multimodal attention fusion. We further analyze the imbalance and noise sensitivity inherent in multimodal attention fusion and propose a Dynamic Semantic-Style Integration (DSSI) mechanism that reweights attention between textual semantic and style visual tokens, effectively resolving guidance conflicts and enhancing output coherence. Experiments show that our approach achieves high-fidelity stylization with superior semantic-style balance and visual quality, offering a simple yet powerful alternative to complex, artifact-prone prior methods.&lt;/p&gt;</content:encoded></item></channel></rss>