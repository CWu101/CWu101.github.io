<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 25 Jan 2026 02:53:13 +0000</lastBuildDate><item><title>DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation</title><link>https://doi.org/10.1016/j.jag.2026.105113</link><guid>10.1016/j.jag.2026.105113</guid><pubDate>Fri, 23 Jan 2026 20:33:45 +0000</pubDate><dc:creator>Boyi Li</dc:creator><dc:creator>Ce Zhang</dc:creator><dc:creator>Richard M. Timmerman</dc:creator><dc:creator>Wenxuan Bao</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105113</prism:doi><description>The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.
Published: 2026-01-23T20:33:45+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.600 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyi Li; Ce Zhang; Richard M. Timmerman; Wenxuan Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105113"&gt;10.1016/j.jag.2026.105113&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.600 (must_read)&lt;/p&gt;
&lt;p&gt;The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.&lt;/p&gt;</content:encoded></item><item><title>RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment</title><link>https://doi.org/10.1109/tip.2026.3655117</link><guid>10.1109/tip.2026.3655117</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Pengfei Chen</dc:creator><dc:creator>Jiebin Yan</dc:creator><dc:creator>Rajiv Soundararajan</dc:creator><dc:creator>Giuseppe Valenzise</dc:creator><dc:creator>Cai Li</dc:creator><dc:creator>Leida Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3655117</prism:doi><description>Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.583 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Chen; Jiebin Yan; Rajiv Soundararajan; Giuseppe Valenzise; Cai Li; Leida Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3655117"&gt;10.1109/tip.2026.3655117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.583 (consider)&lt;/p&gt;
&lt;p&gt;Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.&lt;/p&gt;</content:encoded></item><item><title>GEOMR: Integrating Image Geographic Features and Human Reasoning Knowledge for Image Geolocalization</title><link>https://doi.org/10.1016/j.knosys.2026.115391</link><guid>10.1016/j.knosys.2026.115391</guid><pubDate>Fri, 23 Jan 2026 16:51:03 +0000</pubDate><dc:creator>Jian Fang</dc:creator><dc:creator>Siyi Qian</dc:creator><dc:creator>Shaohui Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115391</prism:doi><description>Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.
Published: 2026-01-23T16:51:03+00:00
Venue: Knowledge-Based Systems
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Fang; Siyi Qian; Shaohui Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115391"&gt;10.1016/j.knosys.2026.115391&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Graph Recognition via Subgraph Prediction</title><link>https://arxiv.org/abs/2601.15133v1</link><guid>http://arxiv.org/abs/2601.15133v1</guid><pubDate>Wed, 21 Jan 2026 16:07:17 +0000</pubDate><dc:creator>André Eberhard</dc:creator><dc:creator>Gerhard Neumann</dc:creator><dc:creator>Pascal Friederich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.
Published: 2026-01-21T16:07:17+00:00
Venue: arXiv
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; André Eberhard; Gerhard Neumann; Pascal Friederich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.&lt;/p&gt;</content:encoded></item><item><title>All-weather Multi-Modality Image Fusion: Unified Framework and 100k Benchmark</title><link>https://doi.org/10.1016/j.inffus.2026.104130</link><guid>10.1016/j.inffus.2026.104130</guid><pubDate>Fri, 23 Jan 2026 16:54:15 +0000</pubDate><dc:creator>Xilai Li</dc:creator><dc:creator>Wuyang Liu</dc:creator><dc:creator>Xiaosong Li</dc:creator><dc:creator>Fuqiang Zhou</dc:creator><dc:creator>Huafeng Li</dc:creator><dc:creator>Feiping Nie</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104130</prism:doi><description>Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .
Published: 2026-01-23T16:54:15+00:00
Venue: Information Fusion
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xilai Li; Wuyang Liu; Xiaosong Li; Fuqiang Zhou; Huafeng Li; Feiping Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104130"&gt;10.1016/j.inffus.2026.104130&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .&lt;/p&gt;</content:encoded></item><item><title>Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.024</link><guid>10.1016/j.isprsjprs.2026.01.024</guid><pubDate>Fri, 23 Jan 2026 14:36:28 +0000</pubDate><dc:creator>Liuqian Wang</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Guangming Mi</dc:creator><dc:creator>Li Zhuo</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.024</prism:doi><description>Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .
Published: 2026-01-23T14:36:28+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.558 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liuqian Wang; Jing Zhang; Guangming Mi; Li Zhuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024"&gt;10.1016/j.isprsjprs.2026.01.024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.558 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .&lt;/p&gt;</content:encoded></item><item><title>Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images</title><link>https://doi.org/10.1016/j.patcog.2026.113120</link><guid>10.1016/j.patcog.2026.113120</guid><pubDate>Sat, 24 Jan 2026 00:15:28 +0000</pubDate><dc:creator>Da Zhang</dc:creator><dc:creator>Mingmin Zeng</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113120</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.
Published: 2026-01-24T00:15:28+00:00
Venue: Pattern Recognition
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Zhang; Mingmin Zeng; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113120"&gt;10.1016/j.patcog.2026.113120&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Domain Characteristics to Refine Deep-Learning-Based Semantic Segmentation of Outdoor Point Clouds</title><link>https://doi.org/10.1109/tgrs.2026.3657418</link><guid>10.1109/tgrs.2026.3657418</guid><pubDate>Fri, 23 Jan 2026 20:58:37 +0000</pubDate><dc:creator>Kevin Qiu</dc:creator><dc:creator>Qipeng Mei</dc:creator><dc:creator>Dimitri Bulatov</dc:creator><dc:creator>Dorota Iwaszczuk</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657418</prism:doi><description>High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.
Published: 2026-01-23T20:58:37+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.555 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kevin Qiu; Qipeng Mei; Dimitri Bulatov; Dorota Iwaszczuk&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657418"&gt;10.1109/tgrs.2026.3657418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.555 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.&lt;/p&gt;</content:encoded></item><item><title>PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models</title><link>https://doi.org/10.1016/j.inffus.2026.104186</link><guid>10.1016/j.inffus.2026.104186</guid><pubDate>Fri, 23 Jan 2026 16:17:45 +0000</pubDate><dc:creator>Yongcai Chen</dc:creator><dc:creator>Qinghua Zhang</dc:creator><dc:creator>Xinfa Shi</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104186</prism:doi><description>Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.
Published: 2026-01-23T16:17:45+00:00
Venue: Information Fusion
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongcai Chen; Qinghua Zhang; Xinfa Shi; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104186"&gt;10.1016/j.inffus.2026.104186&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.&lt;/p&gt;</content:encoded></item><item><title>SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</title><link>https://arxiv.org/abs/2601.14895v1</link><guid>http://arxiv.org/abs/2601.14895v1</guid><pubDate>Wed, 21 Jan 2026 11:32:24 +0000</pubDate><dc:creator>Xinyi Zheng</dc:creator><dc:creator>Yunze Liu</dc:creator><dc:creator>Chi-Hao Wu</dc:creator><dc:creator>Fan Zhang</dc:creator><dc:creator>Hao Zheng</dc:creator><dc:creator>Wenqi Zhou</dc:creator><dc:creator>Walterio W. Mayol-Cuevas</dc:creator><dc:creator>Junxiao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.
Published: 2026-01-21T11:32:24+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyi Zheng; Yunze Liu; Chi-Hao Wu; Fan Zhang; Hao Zheng; Wenqi Zhou; Walterio W. Mayol-Cuevas; Junxiao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.&lt;/p&gt;</content:encoded></item><item><title>HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</title><link>https://arxiv.org/abs/2601.16155v1</link><guid>http://arxiv.org/abs/2601.16155v1</guid><pubDate>Thu, 22 Jan 2026 17:57:42 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Sihang Cai</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.
Published: 2026-01-22T17:57:42+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Xin Liu; Boyun Zhang; Yuxiao Lin; Sihang Cai; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &amp;quot;blind&amp;quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics</title><link>https://doi.org/10.1109/tip.2026.3654770</link><guid>10.1109/tip.2026.3654770</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Can Peng</dc:creator><dc:creator>Manxin Chao</dc:creator><dc:creator>Ruoyu Li</dc:creator><dc:creator>Zaiqing Chen</dc:creator><dc:creator>Lijun Yun</dc:creator><dc:creator>Yuelong Xia</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654770</prism:doi><description>Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Can Peng; Manxin Chao; Ruoyu Li; Zaiqing Chen; Lijun Yun; Yuelong Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654770"&gt;10.1109/tip.2026.3654770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...&lt;/p&gt;</content:encoded></item><item><title>Natural Language-Driven Global Mapping of Martian Landforms</title><link>https://arxiv.org/abs/2601.15949v1</link><guid>http://arxiv.org/abs/2601.15949v1</guid><pubDate>Thu, 22 Jan 2026 13:38:13 +0000</pubDate><dc:creator>Yiran Wang</dc:creator><dc:creator>Shuoyuan Wang</dc:creator><dc:creator>Zhaoran Wei</dc:creator><dc:creator>Jiannan Zhao</dc:creator><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Zejian Xie</dc:creator><dc:creator>Songxin Zhang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Bingyi Jing</dc:creator><dc:creator>Hongxin Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.
Published: 2026-01-22T13:38:13+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiran Wang; Shuoyuan Wang; Zhaoran Wei; Jiannan Zhao; Zhonghua Yao; Zejian Xie; Songxin Zhang; Jun Huang; Bingyi Jing; Hongxin Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.&lt;/p&gt;</content:encoded></item><item><title>ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</title><link>https://arxiv.org/abs/2601.14757v1</link><guid>http://arxiv.org/abs/2601.14757v1</guid><pubDate>Wed, 21 Jan 2026 08:21:35 +0000</pubDate><dc:creator>Kangcheng Zhou</dc:creator><dc:creator>Jun Jiang</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Shuang Zheng</dc:creator><dc:creator>Qingli Li</dc:creator><dc:creator>Shugong Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.
Published: 2026-01-21T08:21:35+00:00
Venue: arXiv
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kangcheng Zhou; Jun Jiang; Qing Zhang; Shuang Zheng; Qingli Li; Shugong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.&lt;/p&gt;</content:encoded></item><item><title>DEM super-resolution guided by high-resolution remote sensing images using multitask learning</title><link>https://doi.org/10.1016/j.jag.2026.105099</link><guid>10.1016/j.jag.2026.105099</guid><pubDate>Fri, 23 Jan 2026 12:54:20 +0000</pubDate><dc:creator>Wei Liu</dc:creator><dc:creator>Yuhang Zhong</dc:creator><dc:creator>Shida Zhao</dc:creator><dc:creator>Songling Luo</dc:creator><dc:creator>Yongtao Yu</dc:creator><dc:creator>Xiaomei Zhong</dc:creator><dc:creator>Weikai Tan</dc:creator><dc:creator>Haiyan Guan</dc:creator><dc:creator>Hongjie He</dc:creator><dc:creator>Jonathan Li</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105099</prism:doi><description>High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.
Published: 2026-01-23T12:54:20+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Liu; Yuhang Zhong; Shida Zhao; Songling Luo; Yongtao Yu; Xiaomei Zhong; Weikai Tan; Haiyan Guan; Hongjie He; Jonathan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105099"&gt;10.1016/j.jag.2026.105099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.&lt;/p&gt;</content:encoded></item><item><title>DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</title><link>https://doi.org/10.1109/tcsvt.2026.3657415</link><guid>10.1109/tcsvt.2026.3657415</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Li Yu</dc:creator><dc:creator>Situo Wang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Moncef Gabbouj</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657415</prism:doi><description>Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Yu; Situo Wang; Wei Zhou; Moncef Gabbouj&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657415"&gt;10.1109/tcsvt.2026.3657415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.&lt;/p&gt;</content:encoded></item><item><title>Grading-Inspired Complementary Enhancing for Multimodal Sentiment Analysis</title><link>https://doi.org/10.1016/j.inffus.2026.104174</link><guid>10.1016/j.inffus.2026.104174</guid><pubDate>Fri, 23 Jan 2026 08:08:53 +0000</pubDate><dc:creator>Zhijing Huang</dc:creator><dc:creator>Wen-Jue He</dc:creator><dc:creator>Baotian Hu</dc:creator><dc:creator>Zheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104174</prism:doi><description>Due to its strong capacity for integrating heterogeneous multi-source information, multimodal sentiment analysis (MSA) has achieved remarkable progress in affective computing. However, existing methods typically adopt symmetric fusion strategies that treat all modalities equally, overlooking their inherent performance disparities that some modalities excel at discriminative representation, while others carry underutilized supportive cues. This limitation leads to insufficiency in cross-modal complementary correlation exploration. To address this issue, we propose a novel Grading-Inspired Complementary Enhancing (GCE) framework for MSA, which is one of the first attempts to conduct dynamic assessment for knowledge transfer in progressive multimodal fusion and cooperation. Specifically, based on cross-modal interaction, a task-aware grading mechanism categorizes modality-pair associations into dominant (high-performing) and supplementary (low-performing) branches according to their task performance. Accordingly, a relation filtering module selectively identifies the trustworthy information from the dominant branch to enhance consistency exploration in supplementary modality pairs with minimized redundancy. Afterwards, a weight adaptation module is adopted to dynamically adjust the guiding weight of individual samples for adaptability and generalization. Extensive experiments conducted on three benchmark datasets evidence that our proposed GCE approach can outperform the state-of-the-art MSA methods. Our code is available at https://github.com/hka-7/GCEforMSA .
Published: 2026-01-23T08:08:53+00:00
Venue: Information Fusion
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijing Huang; Wen-Jue He; Baotian Hu; Zheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104174"&gt;10.1016/j.inffus.2026.104174&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Due to its strong capacity for integrating heterogeneous multi-source information, multimodal sentiment analysis (MSA) has achieved remarkable progress in affective computing. However, existing methods typically adopt symmetric fusion strategies that treat all modalities equally, overlooking their inherent performance disparities that some modalities excel at discriminative representation, while others carry underutilized supportive cues. This limitation leads to insufficiency in cross-modal complementary correlation exploration. To address this issue, we propose a novel Grading-Inspired Complementary Enhancing (GCE) framework for MSA, which is one of the first attempts to conduct dynamic assessment for knowledge transfer in progressive multimodal fusion and cooperation. Specifically, based on cross-modal interaction, a task-aware grading mechanism categorizes modality-pair associations into dominant (high-performing) and supplementary (low-performing) branches according to their task performance. Accordingly, a relation filtering module selectively identifies the trustworthy information from the dominant branch to enhance consistency exploration in supplementary modality pairs with minimized redundancy. Afterwards, a weight adaptation module is adopted to dynamically adjust the guiding weight of individual samples for adaptability and generalization. Extensive experiments conducted on three benchmark datasets evidence that our proposed GCE approach can outperform the state-of-the-art MSA methods. Our code is available at https://github.com/hka-7/GCEforMSA .&lt;/p&gt;</content:encoded></item><item><title>Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</title><link>https://arxiv.org/abs/2601.15780v1</link><guid>http://arxiv.org/abs/2601.15780v1</guid><pubDate>Thu, 22 Jan 2026 09:14:11 +0000</pubDate><dc:creator>Pascal Benschop</dc:creator><dc:creator>Justin Dauwels</dc:creator><dc:creator>Jan van Gemert</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.
Published: 2026-01-22T09:14:11+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pascal Benschop; Justin Dauwels; Jan van Gemert&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.&lt;/p&gt;</content:encoded></item><item><title>Channel-hierarchical graph convolutional network with semantic alignment for long-tailed multi-label image recognition</title><link>https://doi.org/10.1016/j.neucom.2026.132832</link><guid>10.1016/j.neucom.2026.132832</guid><pubDate>Fri, 23 Jan 2026 00:23:49 +0000</pubDate><dc:creator>Liuyi Fan</dc:creator><dc:creator>Xinbo Ai</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132832</prism:doi><description>Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.
Published: 2026-01-23T00:23:49+00:00
Venue: Neurocomputing
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liuyi Fan; Xinbo Ai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132832"&gt;10.1016/j.neucom.2026.132832&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.&lt;/p&gt;</content:encoded></item><item><title>M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention</title><link>https://arxiv.org/abs/2601.14776v1</link><guid>http://arxiv.org/abs/2601.14776v1</guid><pubDate>Wed, 21 Jan 2026 08:55:07 +0000</pubDate><dc:creator>Xiaofan Yang</dc:creator><dc:creator>Yubin Liu</dc:creator><dc:creator>Wei Pan</dc:creator><dc:creator>Guoqing Chu</dc:creator><dc:creator>Junming Zhang</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Zhuoqi Man</dc:creator><dc:creator>Xuanming Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.
Published: 2026-01-21T08:55:07+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaofan Yang; Yubin Liu; Wei Pan; Guoqing Chu; Junming Zhang; Jie Zhao; Zhuoqi Man; Xuanming Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search</title><link>https://arxiv.org/abs/2601.15931v1</link><guid>http://arxiv.org/abs/2601.15931v1</guid><pubDate>Thu, 22 Jan 2026 13:09:22 +0000</pubDate><dc:creator>Xiangyu Wang</dc:creator><dc:creator>Zhixin Lv</dc:creator><dc:creator>Yongjiao Sun</dc:creator><dc:creator>Anrui Han</dc:creator><dc:creator>Ye Yuan</dc:creator><dc:creator>Hangxu Ji</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.
Published: 2026-01-22T13:09:22+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Wang; Zhixin Lv; Yongjiao Sun; Anrui Han; Ye Yuan; Hangxu Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &amp;quot;Passive Observation&amp;quot; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.&lt;/p&gt;</content:encoded></item><item><title>AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.14702v1</link><guid>http://arxiv.org/abs/2601.14702v1</guid><pubDate>Wed, 21 Jan 2026 06:29:09 +0000</pubDate><dc:creator>Zecong Tang</dc:creator><dc:creator>Zixu Wang</dc:creator><dc:creator>Yifei Wang</dc:creator><dc:creator>Weitong Lian</dc:creator><dc:creator>Tianjian Gao</dc:creator><dc:creator>Haoran Li</dc:creator><dc:creator>Tengju Ru</dc:creator><dc:creator>Lingyi Meng</dc:creator><dc:creator>Zhejun Cui</dc:creator><dc:creator>Yichen Zhu</dc:creator><dc:creator>Qi Kang</dc:creator><dc:creator>Kaixuan Wang</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.
Published: 2026-01-21T06:29:09+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zecong Tang; Zixu Wang; Yifei Wang; Weitong Lian; Tianjian Gao; Haoran Li; Tengju Ru; Lingyi Meng; Zhejun Cui; Yichen Zhu; Qi Kang; Kaixuan Wang; Yu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&amp;#x27; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>U-RWKV: Accurate and Efficient Volumetric Medical Image Segmentation via RWKV</title><link>https://doi.org/10.1109/tip.2026.3654389</link><guid>10.1109/tip.2026.3654389</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Hongyu Cai</dc:creator><dc:creator>Yifan Wang</dc:creator><dc:creator>Liu Wang</dc:creator><dc:creator>Jian Zhao</dc:creator><dc:creator>Zhejun Kuang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654389</prism:doi><description>Accurate and efficient volumetric medical image segmentation is vital for clinical diagnosis, pre-operative planning, and disease-progression monitoring. Conventional convolutional neural networks (CNNs) struggle to capture long-range contextual information, whereas Transformer-based methods suffer from quadratic computational complexity, making it challenging to couple global modeling with high efficiency. To address these limitations, we explore an effective yet accurate segmentation model for volumetric data. Specifically, we introduce a novel linear-complexity sequence modeling technique, RWKV, and leverage it to design a Tri-directional Spatial Enhancement RWKV (TSE-R) block; this module performs global modeling via RWKV and incorporates two optimizations tailored to three-dimensional data: (1) a spatial-shift strategy that enlarges the local receptive field and facilitates inter-block interaction, thereby alleviating the structural information loss caused by sequence serialization; and (2) a tri-directional scanning mechanism that constructs sequences along three distinct directions, applies global modeling via WKV, and fuses them with learnable weights to preserve the inherent 3D spatial structure. Building upon the TSE-R block, we develop an end-to-end 3D segmentation network, termed U-RWKV, and extensive experiments on three public 3D medical segmentation benchmarks demonstrate that U-RWKV outperforms state-of-the-art CNN-, Transformer-, and Mamba-based counterparts, achieving a Dice score of 87.21% on the Synapse multi-organ abdominal dataset while reducing parameter count by a factor of 16.08 compared with leading methods.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongyu Cai; Yifan Wang; Liu Wang; Jian Zhao; Zhejun Kuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654389"&gt;10.1109/tip.2026.3654389&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Accurate and efficient volumetric medical image segmentation is vital for clinical diagnosis, pre-operative planning, and disease-progression monitoring. Conventional convolutional neural networks (CNNs) struggle to capture long-range contextual information, whereas Transformer-based methods suffer from quadratic computational complexity, making it challenging to couple global modeling with high efficiency. To address these limitations, we explore an effective yet accurate segmentation model for volumetric data. Specifically, we introduce a novel linear-complexity sequence modeling technique, RWKV, and leverage it to design a Tri-directional Spatial Enhancement RWKV (TSE-R) block; this module performs global modeling via RWKV and incorporates two optimizations tailored to three-dimensional data: (1) a spatial-shift strategy that enlarges the local receptive field and facilitates inter-block interaction, thereby alleviating the structural information loss caused by sequence serialization; and (2) a tri-directional scanning mechanism that constructs sequences along three distinct directions, applies global modeling via WKV, and fuses them with learnable weights to preserve the inherent 3D spatial structure. Building upon the TSE-R block, we develop an end-to-end 3D segmentation network, termed U-RWKV, and extensive experiments on three public 3D medical segmentation benchmarks demonstrate that U-RWKV outperforms state-of-the-art CNN-, Transformer-, and Mamba-based counterparts, achieving a Dice score of 87.21% on the Synapse multi-organ abdominal dataset while reducing parameter count by a factor of 16.08 compared with leading methods.&lt;/p&gt;</content:encoded></item><item><title>A weakly supervised approach for large-scale agricultural parcel extraction from VHR imagery via foundation models and adaptive noise correction</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.030</link><guid>10.1016/j.isprsjprs.2026.01.030</guid><pubDate>Fri, 23 Jan 2026 05:27:30 +0000</pubDate><dc:creator>Wenpeng Zhao</dc:creator><dc:creator>Shanchuan Guo</dc:creator><dc:creator>Xueliang Zhang</dc:creator><dc:creator>Pengfei Tang</dc:creator><dc:creator>Xiaoquan Pan</dc:creator><dc:creator>Haowei Mu</dc:creator><dc:creator>Chenghan Yang</dc:creator><dc:creator>Zilong Xia</dc:creator><dc:creator>Zheng Wang</dc:creator><dc:creator>Jun Du</dc:creator><dc:creator>Peijun Du</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.030</prism:doi><description>Large-scale and fine-grained extraction of agricultural parcels from very-high-resolution (VHR) imagery is essential for precision agriculture. However, traditional parcel segmentation methods and fully supervised deep learning approaches typically face scalability constraints due to costly manual annotations, while extraction accuracy is generally limited by the inadequate capacity of segmentation architectures to represent complex agricultural scenes. To address these challenges, this study proposes a Weakly Supervised approach for agricultural Parcel Extraction (WSPE), which leverages publicly available 10 m resolution images and labels to guide the delineation of 0.5 m agricultural parcels. The WSPE framework integrates the tabular (Tabular Prior-data Fitted Network, TabPFN) and the vision foundation model (Segment Anything Model 2, SAM2) to initially generate pseudo-labels with high geometric precision. These pseudo-labels are further refined for semantic accuracy through an adaptive noisy label correction module based on curriculum learning. The refined knowledge is distilled into the proposed Triple-branch Kolmogorov-Arnold enhanced Boundary-aware Network (TKBNet), a prompt-free end-to-end architecture enabling rapid inference and scalable deployment, with outputs vectorized through post-processing. The effectiveness of WSPE was evaluated on a self-constructed dataset from nine agricultural zones in China, the public AI4Boundaries and FGFD datasets, and three large-scale regions: Zhoukou, Hengshui, and Fengcheng. Results demonstrate that WSPE and its integrated TKBNet achieve robust performance across datasets with diverse agricultural scenes, validated by extensive comparative and ablation experiments. The weakly supervised approach achieves 97.7 % of fully supervised performance, and large-scale deployment verifies its scalability and generalization, offering a practical solution for fine-grained, large-scale agricultural parcel mapping. Code is available at https://github.com/zhaowenpeng/WSPE .
Published: 2026-01-23T05:27:30+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenpeng Zhao; Shanchuan Guo; Xueliang Zhang; Pengfei Tang; Xiaoquan Pan; Haowei Mu; Chenghan Yang; Zilong Xia; Zheng Wang; Jun Du; Peijun Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.030"&gt;10.1016/j.isprsjprs.2026.01.030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Large-scale and fine-grained extraction of agricultural parcels from very-high-resolution (VHR) imagery is essential for precision agriculture. However, traditional parcel segmentation methods and fully supervised deep learning approaches typically face scalability constraints due to costly manual annotations, while extraction accuracy is generally limited by the inadequate capacity of segmentation architectures to represent complex agricultural scenes. To address these challenges, this study proposes a Weakly Supervised approach for agricultural Parcel Extraction (WSPE), which leverages publicly available 10 m resolution images and labels to guide the delineation of 0.5 m agricultural parcels. The WSPE framework integrates the tabular (Tabular Prior-data Fitted Network, TabPFN) and the vision foundation model (Segment Anything Model 2, SAM2) to initially generate pseudo-labels with high geometric precision. These pseudo-labels are further refined for semantic accuracy through an adaptive noisy label correction module based on curriculum learning. The refined knowledge is distilled into the proposed Triple-branch Kolmogorov-Arnold enhanced Boundary-aware Network (TKBNet), a prompt-free end-to-end architecture enabling rapid inference and scalable deployment, with outputs vectorized through post-processing. The effectiveness of WSPE was evaluated on a self-constructed dataset from nine agricultural zones in China, the public AI4Boundaries and FGFD datasets, and three large-scale regions: Zhoukou, Hengshui, and Fengcheng. Results demonstrate that WSPE and its integrated TKBNet achieve robust performance across datasets with diverse agricultural scenes, validated by extensive comparative and ablation experiments. The weakly supervised approach achieves 97.7 % of fully supervised performance, and large-scale deployment verifies its scalability and generalization, offering a practical solution for fine-grained, large-scale agricultural parcel mapping. Code is available at https://github.com/zhaowenpeng/WSPE .&lt;/p&gt;</content:encoded></item><item><title>OACI: Object-Aware Contextual Integration for Image Captioning</title><link>https://doi.org/10.1016/j.knosys.2026.115374</link><guid>10.1016/j.knosys.2026.115374</guid><pubDate>Fri, 23 Jan 2026 00:24:26 +0000</pubDate><dc:creator>Shuhan Xu</dc:creator><dc:creator>Mengya Han</dc:creator><dc:creator>Wei Yu</dc:creator><dc:creator>Zheng He</dc:creator><dc:creator>Xin Zhou</dc:creator><dc:creator>Yong Luo</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115374</prism:doi><description>Image captioning is a fundamental task in visual understanding, aiming to generate textual descriptions for given images. Current image captioning methods are gradually shifting towards a fully end-to-end paradigm, which leverages pre-trained vision models to process images directly and generate captions, eliminating the need for separating object detectors. These methods typically rely on global features, neglecting the precise perception of local ones. The lack of fine-grained focus on the object may result in suboptimal prototype features contaminated by surrounding noise, and thus negatively affect the generation of object-related captions. To address this issue, we propose a novel method termed object-aware context integration (OACI), which captures the salient prototypes of individual objects and understands their relationships by leveraging the global context of the entire scene. Specifically, we propose an object-aware prototype learning (OAPL) module that focuses on regions containing objects to enhance object perception and selects the most confident regions for learning object prototypes. Moreover, a class affinity constraint (CAC) is designed to facilitate the learning of these prototypes. To understand the relationships between objects, we further propose an object-context integration (OCI) module that integrates global context with local object prototypes, enhancing the understanding of image content and improving the generated image captions. We conduct extensive experiments on the popular MSCOCO, Flickr8k and Flickr30k datasets, and the results demonstrate that integrating global context with local object details significantly improves the quality of generated captions, validating the effectiveness of the proposed OACI method.
Published: 2026-01-23T00:24:26+00:00
Venue: Knowledge-Based Systems
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuhan Xu; Mengya Han; Wei Yu; Zheng He; Xin Zhou; Yong Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115374"&gt;10.1016/j.knosys.2026.115374&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Image captioning is a fundamental task in visual understanding, aiming to generate textual descriptions for given images. Current image captioning methods are gradually shifting towards a fully end-to-end paradigm, which leverages pre-trained vision models to process images directly and generate captions, eliminating the need for separating object detectors. These methods typically rely on global features, neglecting the precise perception of local ones. The lack of fine-grained focus on the object may result in suboptimal prototype features contaminated by surrounding noise, and thus negatively affect the generation of object-related captions. To address this issue, we propose a novel method termed object-aware context integration (OACI), which captures the salient prototypes of individual objects and understands their relationships by leveraging the global context of the entire scene. Specifically, we propose an object-aware prototype learning (OAPL) module that focuses on regions containing objects to enhance object perception and selects the most confident regions for learning object prototypes. Moreover, a class affinity constraint (CAC) is designed to facilitate the learning of these prototypes. To understand the relationships between objects, we further propose an object-context integration (OCI) module that integrates global context with local object prototypes, enhancing the understanding of image content and improving the generated image captions. We conduct extensive experiments on the popular MSCOCO, Flickr8k and Flickr30k datasets, and the results demonstrate that integrating global context with local object details significantly improves the quality of generated captions, validating the effectiveness of the proposed OACI method.&lt;/p&gt;</content:encoded></item><item><title>3D Space as a Scratchpad for Editable Text-to-Image Generation</title><link>https://arxiv.org/abs/2601.14602v1</link><guid>http://arxiv.org/abs/2601.14602v1</guid><pubDate>Wed, 21 Jan 2026 02:40:19 +0000</pubDate><dc:creator>Oindrila Saha</dc:creator><dc:creator>Vojtech Krs</dc:creator><dc:creator>Radomir Mech</dc:creator><dc:creator>Subhransu Maji</dc:creator><dc:creator>Matheus Gadelha</dc:creator><dc:creator>Kevin Blackburn-Matzen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/
Published: 2026-01-21T02:40:19+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Oindrila Saha; Vojtech Krs; Radomir Mech; Subhransu Maji; Matheus Gadelha; Kevin Blackburn-Matzen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/&lt;/p&gt;</content:encoded></item><item><title>Keyframe-Based Feed-Forward Visual Odometry</title><link>https://arxiv.org/abs/2601.16020v1</link><guid>http://arxiv.org/abs/2601.16020v1</guid><pubDate>Thu, 22 Jan 2026 14:45:42 +0000</pubDate><dc:creator>Weichen Dai</dc:creator><dc:creator>Wenhan Su</dc:creator><dc:creator>Da Kong</dc:creator><dc:creator>Yuhang Ming</dc:creator><dc:creator>Wanzeng Kong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.
Published: 2026-01-22T14:45:42+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weichen Dai; Wenhan Su; Da Kong; Yuhang Ming; Wanzeng Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.&lt;/p&gt;</content:encoded></item><item><title>Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering</title><link>https://doi.org/10.1007/s11263-026-02740-3</link><guid>10.1007/s11263-026-02740-3</guid><pubDate>Fri, 23 Jan 2026 04:10:44 +0000</pubDate><dc:creator>Yurui Chen</dc:creator><dc:creator>Chun Gu</dc:creator><dc:creator>Junzhe Jiang</dc:creator><dc:creator>Xiatian Zhu</dc:creator><dc:creator>Li Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02740-3</prism:doi><description>Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent and large scene representation learning with sparse training data, we introduce a novel temporal smoothing mechanism and a position-aware adaptive control strategy respectively. Extensive experiments on Waymo Open Dataset&amp;nbsp;(Sun et al., 2020) and KITTI benchmarks&amp;nbsp;(Geiger et al., 2012) demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 900-fold acceleration in rendering over the best alternative. The code is available at https://github.com/fudan-zvg/PVG .
Published: 2026-01-23T04:10:44+00:00
Venue: International Journal of Computer Vision
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yurui Chen; Chun Gu; Junzhe Jiang; Xiatian Zhu; Li Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02740-3"&gt;10.1007/s11263-026-02740-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent and large scene representation learning with sparse training data, we introduce a novel temporal smoothing mechanism and a position-aware adaptive control strategy respectively. Extensive experiments on Waymo Open Dataset&amp;amp;nbsp;(Sun et al., 2020) and KITTI benchmarks&amp;amp;nbsp;(Geiger et al., 2012) demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 900-fold acceleration in rendering over the best alternative. The code is available at https://github.com/fudan-zvg/PVG .&lt;/p&gt;</content:encoded></item><item><title>Rethinking Multi-Focus Image Fusion: An Input Space Optimisation View</title><link>https://doi.org/10.1109/tip.2026.3654370</link><guid>10.1109/tip.2026.3654370</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Zeyu Wang</dc:creator><dc:creator>Shuang Yu</dc:creator><dc:creator>Haoran Duan</dc:creator><dc:creator>Shidong Wang</dc:creator><dc:creator>Yang Long</dc:creator><dc:creator>Ling Shao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654370</prism:doi><description>Multi-focus image fusion (MFIF) addresses the challenge of partial focus by integrating multiple source images taken at different focal depths. Unlike most existing methods that rely on complex loss functions or large-scale synthetic datasets, this study approaches MFIF from a novel perspective: optimizing the input space. The core idea is to construct a high-quality MFIF input space in a cost-effective manner by using intermediate features from well-trained, non-MFIF networks. To this end, we propose a cascaded framework comprising two feature extractors, a Feature Distillation and Fusion Module (FDFM), and a focus segmentation network YUNet. Based on our observation that discrepancy and edge features are essential for MFIF, we select a image deblurring network and a salient object detection network as feature extractors. To transform these extracted features into an MFIF-suitable input space, we propose FDFM as a training-free feature adapter. To make FDFM compatible with high-dimensional feature maps, we extend the manifold theory from the edge-preserving field and design a novel isometric domain transformation. Extensive experiments on six benchmark datasets show that (i) our model consistently outperforms 13 state-of-the-art methods in both qualitative and quantitative evaluations, and (ii) the constructed input space can directly enhance the performance of many MFIF models without additional requirements.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Wang; Shuang Yu; Haoran Duan; Shidong Wang; Yang Long; Ling Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654370"&gt;10.1109/tip.2026.3654370&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Multi-focus image fusion (MFIF) addresses the challenge of partial focus by integrating multiple source images taken at different focal depths. Unlike most existing methods that rely on complex loss functions or large-scale synthetic datasets, this study approaches MFIF from a novel perspective: optimizing the input space. The core idea is to construct a high-quality MFIF input space in a cost-effective manner by using intermediate features from well-trained, non-MFIF networks. To this end, we propose a cascaded framework comprising two feature extractors, a Feature Distillation and Fusion Module (FDFM), and a focus segmentation network YUNet. Based on our observation that discrepancy and edge features are essential for MFIF, we select a image deblurring network and a salient object detection network as feature extractors. To transform these extracted features into an MFIF-suitable input space, we propose FDFM as a training-free feature adapter. To make FDFM compatible with high-dimensional feature maps, we extend the manifold theory from the edge-preserving field and design a novel isometric domain transformation. Extensive experiments on six benchmark datasets show that (i) our model consistently outperforms 13 state-of-the-art methods in both qualitative and quantitative evaluations, and (ii) the constructed input space can directly enhance the performance of many MFIF models without additional requirements.&lt;/p&gt;</content:encoded></item><item><title>Cross-Cycle Structured Graph Autoencoder for Unsupervised Cross-Sensor Image Change Detection</title><link>https://doi.org/10.1109/taes.2026.3657315</link><guid>10.1109/taes.2026.3657315</guid><pubDate>Fri, 23 Jan 2026 21:01:34 +0000</pubDate><dc:creator>Yankun Huang</dc:creator><dc:creator>Yun Zhang</dc:creator><dc:creator>Yaohua Li</dc:creator><dc:creator>Haoxuan Yuan</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2026.3657315</prism:doi><description>Unsupervised cross-sensor change detection (CSCD) is a significant yet challenging task in remote sensing, primarily due to substantial domain shifts across heterogeneous images and the difficulty of accurately modeling semantic changes. Existing methods typically rely on image translation or invariant feature extraction, where changes are indirectly inferred as reconstruction residuals. These approaches often suffer from fragile domain adaptation mappings and the entanglement of transformation errors with true changes, thereby degrading final detection performance. To address these issues, this article proposes a cross-cycle structured graph autoencoder (CC-SGAE) framework. Our model utilizes a bidirectional, cycle-consistent graph autoencoder architecture to explicitly disentangle the CSCD task from the complex domain transformation. Crucially, it introduces learnable structural difference graphs designed to directly represent semantic changes in the latent space, independent of modality-specific characteristics. The entire framework is guided by a comprehensive multi-component loss function that enforces critical priors, including cycle-consistency, structural regularization, change sparsity, bilateral change alignment, and spatial smoothness. This explicit modeling strategy suppresses the interference of heterogeneous discrepancies and maintains the structural integrity of the detection results. Extensive experiments on five benchmark cross-sensor datasets demonstrate that our proposed CC-SGAE outperforms state-of-the-art methods, confirming its effectiveness and high potential for practical applications in unsupervised CSCD. The code is available at https://github.com/CodeofHuang/CC-SGAE.
Published: 2026-01-23T21:01:34+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yankun Huang; Yun Zhang; Yaohua Li; Haoxuan Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2026.3657315"&gt;10.1109/taes.2026.3657315&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Unsupervised cross-sensor change detection (CSCD) is a significant yet challenging task in remote sensing, primarily due to substantial domain shifts across heterogeneous images and the difficulty of accurately modeling semantic changes. Existing methods typically rely on image translation or invariant feature extraction, where changes are indirectly inferred as reconstruction residuals. These approaches often suffer from fragile domain adaptation mappings and the entanglement of transformation errors with true changes, thereby degrading final detection performance. To address these issues, this article proposes a cross-cycle structured graph autoencoder (CC-SGAE) framework. Our model utilizes a bidirectional, cycle-consistent graph autoencoder architecture to explicitly disentangle the CSCD task from the complex domain transformation. Crucially, it introduces learnable structural difference graphs designed to directly represent semantic changes in the latent space, independent of modality-specific characteristics. The entire framework is guided by a comprehensive multi-component loss function that enforces critical priors, including cycle-consistency, structural regularization, change sparsity, bilateral change alignment, and spatial smoothness. This explicit modeling strategy suppresses the interference of heterogeneous discrepancies and maintains the structural integrity of the detection results. Extensive experiments on five benchmark cross-sensor datasets demonstrate that our proposed CC-SGAE outperforms state-of-the-art methods, confirming its effectiveness and high potential for practical applications in unsupervised CSCD. The code is available at https://github.com/CodeofHuang/CC-SGAE.&lt;/p&gt;</content:encoded></item></channel></rss>