<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 14 Jan 2026 03:22:16 +0000</lastBuildDate><item><title>Parse, Align and Aggregate: Graph-driven Compositional Reasoning for Video Question Answering</title><link>https://doi.org/10.1109/tpami.2026.3650864</link><guid>10.1109/tpami.2026.3650864</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jiangtong Li</dc:creator><dc:creator>Zhaohe Liao</dc:creator><dc:creator>Fengshun Xiao</dc:creator><dc:creator>Tianjiao Li</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Haohua Zhao</dc:creator><dc:creator>Li Niu</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Liqing Zhang</dc:creator><dc:creator>Changjun Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650864</prism:doi><description>Video Question-Answering (VideoQA) enables machines to interpret and respond to complex video content, advanc ing human-computer interaction. However, existing multimodal large language models (MLLMs) often provide incomplete or opaque explanations and existing benchmarks mainly focus on the correction of final answers, limiting insight into their reasoning processes and hindering both transparency and verifiability. To address this gap, we propose the Question Parsing, Video Alignment and Answer Aggregation framework (QPVA3), which leverages a compositional graph to drive visual and logical reasoning in VideoQA. Specifically, QPVA3 consists of three core components, the planner, executor, and reasoner to generate the compositional graph and conduct graph-driven reasoning. For the original question, the planner parses it into the compositional graph, capturing the underlying reasoning logic and structuring it into a series of interconnected questions. For each question in compositional graph, the executor aligns the video by selecting relevant video clips and generates answers, ensuring accurate, context-specific responses. For each question with its first-order descents, the reasoner aggregates answers by integrating rea soning logic with visual evidence, resolving conflicts to produce a coherent and accurate response. Moreover, to assess the performance of existing MLLMs in the reasoning processes of VideoQA, we introduce novel compositional consistency metrics and construct a VideoQA benchmark (QPVA3Bench) with 3,492 question-video tuples, each annotated with detailed composi tional graphs and fine-grained answers. We evaluate the QPVA3 framework on QPVA3Bench and 5 other VideoQA benchmarks. Experimental results demonstrate that our framework improves both consistency and accuracy compared to baselines, leading to a more transparent and verifiable VideoQA system. This approach has the potential to advance the field, as supported by our comprehensive evaluation an...
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.619 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiangtong Li; Zhaohe Liao; Fengshun Xiao; Tianjiao Li; Qiang Zhang; Haohua Zhao; Li Niu; Guang Chen; Liqing Zhang; Changjun Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650864"&gt;10.1109/tpami.2026.3650864&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.619 (must_read)&lt;/p&gt;
&lt;p&gt;Video Question-Answering (VideoQA) enables machines to interpret and respond to complex video content, advanc ing human-computer interaction. However, existing multimodal large language models (MLLMs) often provide incomplete or opaque explanations and existing benchmarks mainly focus on the correction of final answers, limiting insight into their reasoning processes and hindering both transparency and verifiability. To address this gap, we propose the Question Parsing, Video Alignment and Answer Aggregation framework (QPVA3), which leverages a compositional graph to drive visual and logical reasoning in VideoQA. Specifically, QPVA3 consists of three core components, the planner, executor, and reasoner to generate the compositional graph and conduct graph-driven reasoning. For the original question, the planner parses it into the compositional graph, capturing the underlying reasoning logic and structuring it into a series of interconnected questions. For each question in compositional graph, the executor aligns the video by selecting relevant video clips and generates answers, ensuring accurate, context-specific responses. For each question with its first-order descents, the reasoner aggregates answers by integrating rea soning logic with visual evidence, resolving conflicts to produce a coherent and accurate response. Moreover, to assess the performance of existing MLLMs in the reasoning processes of VideoQA, we introduce novel compositional consistency metrics and construct a VideoQA benchmark (QPVA3Bench) with 3,492 question-video tuples, each annotated with detailed composi tional graphs and fine-grained answers. We evaluate the QPVA3 framework on QPVA3Bench and 5 other VideoQA benchmarks. Experimental results demonstrate that our framework improves both consistency and accuracy compared to baselines, leading to a more transparent and verifiable VideoQA system. This approach has the potential to advance the field, as supported by our comprehensive evaluation an...&lt;/p&gt;</content:encoded></item><item><title>RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction With Spatio-Temporal Aggregation</title><link>https://doi.org/10.1109/tro.2026.3651674</link><guid>10.1109/tro.2026.3651674</guid><pubDate>Mon, 12 Jan 2026 22:05:09 +0000</pubDate><dc:creator>Naman Patel</dc:creator><dc:creator>Prashanth Krishnamurthy</dc:creator><dc:creator>Farshad Khorrami</dc:creator><prism:publicationName>IEEE Transactions on Robotics</prism:publicationName><prism:doi>10.1109/tro.2026.3651674</prism:doi><description>Mapping and understanding complex 3D environments is fundamental to how autonomous systems perceive and interact with the physical world, requiring both precise geometric reconstruction and rich semantic comprehension. While existing 3D semantic mapping systems excel at reconstructing and identifying predefined object instances, they lack the flexibility to efficiently build semantic maps with open-vocabulary during online operation. Although recent vision-language models have enabled open-vocabulary object recognition in 2D images, they haven't yet bridged the gap to 3D spatial understanding. The critical challenge lies in developing a training-free unified system that can simultaneously construct accurate 3D maps while maintaining semantic consistency and supporting natural language interactions in real time. In this paper, we develop a zero-shot framework that seamlessly integrates GPU-accelerated geometric reconstruction with open-vocabulary vision-language models through online instance-level semantic embedding fusion, guided by hierarchical object association with spatial indexing. Our training-free system achieves superior performance through incremental processing and unified geometric-semantic updates, while robustly handling 2D segmentation inconsistencies. The proposed general-purpose 3D scene understanding framework can be used for various tasks including zero-shot 3D instance retrieval, segmentation, and object detection to reason about previously unseen objects and interpret natural language queries.
Published: 2026-01-12T22:05:09+00:00
Venue: IEEE Transactions on Robotics
Score: 0.589 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naman Patel; Prashanth Krishnamurthy; Farshad Khorrami&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Robotics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tro.2026.3651674"&gt;10.1109/tro.2026.3651674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.589 (consider)&lt;/p&gt;
&lt;p&gt;Mapping and understanding complex 3D environments is fundamental to how autonomous systems perceive and interact with the physical world, requiring both precise geometric reconstruction and rich semantic comprehension. While existing 3D semantic mapping systems excel at reconstructing and identifying predefined object instances, they lack the flexibility to efficiently build semantic maps with open-vocabulary during online operation. Although recent vision-language models have enabled open-vocabulary object recognition in 2D images, they haven&amp;#x27;t yet bridged the gap to 3D spatial understanding. The critical challenge lies in developing a training-free unified system that can simultaneously construct accurate 3D maps while maintaining semantic consistency and supporting natural language interactions in real time. In this paper, we develop a zero-shot framework that seamlessly integrates GPU-accelerated geometric reconstruction with open-vocabulary vision-language models through online instance-level semantic embedding fusion, guided by hierarchical object association with spatial indexing. Our training-free system achieves superior performance through incremental processing and unified geometric-semantic updates, while robustly handling 2D segmentation inconsistencies. The proposed general-purpose 3D scene understanding framework can be used for various tasks including zero-shot 3D instance retrieval, segmentation, and object detection to reason about previously unseen objects and interpret natural language queries.&lt;/p&gt;</content:encoded></item><item><title>Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation</title><link>https://arxiv.org/abs/2601.08728v1</link><guid>http://arxiv.org/abs/2601.08728v1</guid><pubDate>Tue, 13 Jan 2026 16:57:09 +0000</pubDate><dc:creator>Runfeng Qu</dc:creator><dc:creator>Ole Hall</dc:creator><dc:creator>Pia K Bideau</dc:creator><dc:creator>Julie Ouerfelli-Ethier</dc:creator><dc:creator>Martin Rolfs</dc:creator><dc:creator>Klaus Obermayer</dc:creator><dc:creator>Olaf Hellwich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision
Published: 2026-01-13T16:57:09+00:00
Venue: arXiv
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runfeng Qu; Ole Hall; Pia K Bideau; Julie Ouerfelli-Ethier; Martin Rolfs; Klaus Obermayer; Olaf Hellwich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision&lt;/p&gt;</content:encoded></item><item><title>A Multiscale Vision-Text Collaborative Dual-Encoder for Referring RS Image Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3651598</link><guid>10.1109/tgrs.2026.3651598</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Jingwen Zhang</dc:creator><dc:creator>Lingling Li</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Wenping Ma</dc:creator><dc:creator>Shuyuan Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651598</prism:doi><description>Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing images using natural language descriptions. Existing methods employing a single backbone and sequential fusion struggle to capture fine-grained semantics in remote sensing data due to their complex and multi-scale nature. Moreover, vision models pretrained on natural images often fail to generalize to remote sensing images due to domain-specific spatial and semantic discrepancies. To address these issues, we propose an innovative Multiscale Vision-Text Collaborative Dual-Encoder network, named MCD-Net. We first introduce a frozen SAM encoder as a structure-aware auxiliary branch to inject general-purpose spatial priors, enhancing the Swin Transformer’s ability to model fine-grained geometry and object-level information. To improve semantic consistency across scales and better align with referring expressions, we propose a Multi-scale Frequency-aware Alignment module that decomposes visual features into frequency components and modulates them via cross-scale textual attention. An Adaptive Deep Fusion module bridges the representation gap between dual visual branches while preserving spatial-semantic coherence. Extensive experiments on public RRSIS benchmarks demonstrate that our method outperforms state-of-the-art approaches, particularly in scenes with dense objects and ambiguous expressions. Our code will be released upon publication.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.577 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingwen Zhang; Lingling Li; Licheng Jiao; Xu Liu; Fang Liu; Wenping Ma; Shuyuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651598"&gt;10.1109/tgrs.2026.3651598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.577 (consider)&lt;/p&gt;
&lt;p&gt;Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing images using natural language descriptions. Existing methods employing a single backbone and sequential fusion struggle to capture fine-grained semantics in remote sensing data due to their complex and multi-scale nature. Moreover, vision models pretrained on natural images often fail to generalize to remote sensing images due to domain-specific spatial and semantic discrepancies. To address these issues, we propose an innovative Multiscale Vision-Text Collaborative Dual-Encoder network, named MCD-Net. We first introduce a frozen SAM encoder as a structure-aware auxiliary branch to inject general-purpose spatial priors, enhancing the Swin Transformer’s ability to model fine-grained geometry and object-level information. To improve semantic consistency across scales and better align with referring expressions, we propose a Multi-scale Frequency-aware Alignment module that decomposes visual features into frequency components and modulates them via cross-scale textual attention. An Adaptive Deep Fusion module bridges the representation gap between dual visual branches while preserving spatial-semantic coherence. Extensive experiments on public RRSIS benchmarks demonstrate that our method outperforms state-of-the-art approaches, particularly in scenes with dense objects and ambiguous expressions. Our code will be released upon publication.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Power of Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation</title><link>https://doi.org/10.1109/tpami.2026.3651728</link><guid>10.1109/tpami.2026.3651728</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Duo Peng</dc:creator><dc:creator>Zhengbo Zhang</dc:creator><dc:creator>Ping Hu</dc:creator><dc:creator>Qiuhong Ke</dc:creator><dc:creator>De Wen Soh</dc:creator><dc:creator>Mohammed Bennamoun</dc:creator><dc:creator>Jun Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651728</prism:doi><description>Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.571 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Duo Peng; Zhengbo Zhang; Ping Hu; Qiuhong Ke; De Wen Soh; Mohammed Bennamoun; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651728"&gt;10.1109/tpami.2026.3651728&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.571 (consider)&lt;/p&gt;
&lt;p&gt;Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Fine-grained Image Analysis by Semantic-Part Alignment</title><link>https://doi.org/10.1109/tip.2025.3649364</link><guid>10.1109/tip.2025.3649364</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Qi Bi</dc:creator><dc:creator>Jingjun Yi</dc:creator><dc:creator>Haolan Zhan</dc:creator><dc:creator>Wei Ji</dc:creator><dc:creator>Gui-Song Xia</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649364</prism:doi><description>Fine-grained image analysis is widely recognized as highly challenging, since distinguishing individual differences within a certain category, species, or type often depends on tiny, subtle patterns. However, learning fine-grained semantic categories from these subtle part patterns is inherently fragile, as they can easily be overwhelmed by the dominant patterns resting in the coarse-category information. Therefore, how to enhance the relation between the fine-grained semantics and these subtle patterns is the key. To push this frontier, a novel semantic-part alignment (SPA) learning scheme is proposed in this paper. Its general idea is to firstly measure the relevance of each part to the fine-grained semantics, and then regularize the fine-grained visual representation learning. Specifically, it consists of three key components, namely, joint semantic-part modeling, semantic-part set modeling, and optimal semantic-part transport. The joint semantic-part modeling associates each part in an image with the fine-grained semantics in a latent space. Then, the optimal semantic-part transport component is devised to enhance the relation between fine-grained semantic embeddings and the discriminative part embeddings. Notably, the proposed SPA is plug-in-and-play, easy-to-implement, and insensitive to the latent embedding dimension and loss weight. Experiments show the proposed method can substantially boost performance on multiple fine-grained image analysis tasks across various baselines.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.569 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qi Bi; Jingjun Yi; Haolan Zhan; Wei Ji; Gui-Song Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649364"&gt;10.1109/tip.2025.3649364&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.569 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained image analysis is widely recognized as highly challenging, since distinguishing individual differences within a certain category, species, or type often depends on tiny, subtle patterns. However, learning fine-grained semantic categories from these subtle part patterns is inherently fragile, as they can easily be overwhelmed by the dominant patterns resting in the coarse-category information. Therefore, how to enhance the relation between the fine-grained semantics and these subtle patterns is the key. To push this frontier, a novel semantic-part alignment (SPA) learning scheme is proposed in this paper. Its general idea is to firstly measure the relevance of each part to the fine-grained semantics, and then regularize the fine-grained visual representation learning. Specifically, it consists of three key components, namely, joint semantic-part modeling, semantic-part set modeling, and optimal semantic-part transport. The joint semantic-part modeling associates each part in an image with the fine-grained semantics in a latent space. Then, the optimal semantic-part transport component is devised to enhance the relation between fine-grained semantic embeddings and the discriminative part embeddings. Notably, the proposed SPA is plug-in-and-play, easy-to-implement, and insensitive to the latent embedding dimension and loss weight. Experiments show the proposed method can substantially boost performance on multiple fine-grained image analysis tasks across various baselines.&lt;/p&gt;</content:encoded></item><item><title>Completing Missing Entities: Exploring Consistency Reasoning for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/tip.2025.3648164</link><guid>10.1109/tip.2025.3648164</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Peng Sun</dc:creator><dc:creator>Yongbin Zheng</dc:creator><dc:creator>Wanying Xu</dc:creator><dc:creator>Jian Li</dc:creator><dc:creator>Jiansong Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648164</prism:doi><description>Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peng Sun; Yongbin Zheng; Wanying Xu; Jian Li; Jiansong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648164"&gt;10.1109/tip.2025.3648164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.&lt;/p&gt;</content:encoded></item><item><title>&amp;amp;Segloc: Dual-Decoder Based Semantic-Enhanced Remote Sensing Localization Method</title><link>https://doi.org/10.1109/tgrs.2026.3652613</link><guid>10.1109/tgrs.2026.3652613</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Yuzhuo Ma</dc:creator><dc:creator>Tao Liu</dc:creator><dc:creator>Kan Ren</dc:creator><dc:creator>Qian Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3652613</prism:doi><description>Cross-view remote sensing geolocation is among the most widely adopted solutions in GNSS-denied environments. The primary challenge lies in learning highly discriminative feature representations to bridge the domain gap between heterogeneous image sources. While existing single-encoder-single-decoder methods have made progress, they still have significant limitations: first, excessive reliance on deep network features leads to the loss of geometric information; second, the single decoding task limits the feature representation capacity. To address these issues and better leverage semantic information, this paper proposes a novel plug-and-play single-encoder dual-decoder collaborative learning framework. By introducing an auxiliary semantic decoder and a new semantic matching strategy, the proposed approach overcomes the performance bottleneck of traditional classification networks. Additionally, to address the scarcity of semantic annotations in cross-view remote sensing data, we have constructed a new semantically labeled dataset. To our knowledge, this is among the first studies in cross-view geolocation to unify decoupled feature learning with multi-task collaboration and to introduce multi-label semantic segmentation into the cross-view matching framework, providing a new research paradigm for cross-platform geolocation. Experimental results on public benchmarks demonstrate that the proposed method achieves substantial performance improvements and attains excellent performance in UAV-view target geolocation tasks, validating its effectiveness and robustness. Moreover, the method exhibits strong adaptability to challenging conditions, including variations in viewpoint, illumination, scale, and rotation.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuzhuo Ma; Tao Liu; Kan Ren; Qian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3652613"&gt;10.1109/tgrs.2026.3652613&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view remote sensing geolocation is among the most widely adopted solutions in GNSS-denied environments. The primary challenge lies in learning highly discriminative feature representations to bridge the domain gap between heterogeneous image sources. While existing single-encoder-single-decoder methods have made progress, they still have significant limitations: first, excessive reliance on deep network features leads to the loss of geometric information; second, the single decoding task limits the feature representation capacity. To address these issues and better leverage semantic information, this paper proposes a novel plug-and-play single-encoder dual-decoder collaborative learning framework. By introducing an auxiliary semantic decoder and a new semantic matching strategy, the proposed approach overcomes the performance bottleneck of traditional classification networks. Additionally, to address the scarcity of semantic annotations in cross-view remote sensing data, we have constructed a new semantically labeled dataset. To our knowledge, this is among the first studies in cross-view geolocation to unify decoupled feature learning with multi-task collaboration and to introduce multi-label semantic segmentation into the cross-view matching framework, providing a new research paradigm for cross-platform geolocation. Experimental results on public benchmarks demonstrate that the proposed method achieves substantial performance improvements and attains excellent performance in UAV-view target geolocation tasks, validating its effectiveness and robustness. Moreover, the method exhibits strong adaptability to challenging conditions, including variations in viewpoint, illumination, scale, and rotation.&lt;/p&gt;</content:encoded></item><item><title>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3651774</link><guid>10.1109/tcsvt.2026.3651774</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Jintao Rong</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Linlin Ou</dc:creator><dc:creator>Tianxiao Chen</dc:creator><dc:creator>Xinyi Yu</dc:creator><dc:creator>Yifan Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651774</prism:doi><description>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jintao Rong; Hao Chen; Linlin Ou; Tianxiao Chen; Xinyi Yu; Yifan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651774"&gt;10.1109/tcsvt.2026.3651774&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.&lt;/p&gt;</content:encoded></item><item><title>G
                    &lt;sup&gt;2&lt;/sup&gt;
                    HFNet: GeoGran-Aware Hierarchical Feature Fusion Network for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tcsvt.2026.3653188</link><guid>10.1109/tcsvt.2026.3653188</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Bin Wan</dc:creator><dc:creator>Runmin Cong</dc:creator><dc:creator>Xiaofei Zhou</dc:creator><dc:creator>Hao Fang</dc:creator><dc:creator>Chengtao Lv</dc:creator><dc:creator>Sam Kwong</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3653188</prism:doi><description>Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Wan; Runmin Cong; Xiaofei Zhou; Hao Fang; Chengtao Lv; Sam Kwong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3653188"&gt;10.1109/tcsvt.2026.3653188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.&lt;/p&gt;</content:encoded></item><item><title>How to Break It Down for Building It Up? Theory-Guided Graph Decomposition Learning for Spatiotemporal Traffic Prediction</title><link>https://doi.org/10.1109/tpami.2026.3651246</link><guid>10.1109/tpami.2026.3651246</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jiahao Ji</dc:creator><dc:creator>Jingyuan Wang</dc:creator><dc:creator>Yu Mou</dc:creator><dc:creator>Cheng Long</dc:creator><dc:creator>Junjie Wu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651246</prism:doi><description>Traffic state prediction based on spatiotemporal data has become a prominent focus in data-driven AI research. While significant progress has been made, most mainstream approaches assume uniform spatial and temporal correlations across conditions and use shared parameters for all scenarios. This simplification overlooks the complexity and heterogeneity inherent in human mobility patterns, often leading to suboptimal predictions. Recently, methods adopting the “decompose, then predict” (DTP) paradigm have gained traction. These methods break down data into smaller, manageable subcomponents, each predicted using dedicated parameters. Although effective in practice, DTP methods face unresolved theoretical questions: What type of decomposition truly makes subcomponents more manageable than the original data? To address this, we present an information theory-based analysis that derives sufficient conditions for a decomposition algorithm to reduce data-induced prediction errors. These conditions suggest that an effective algorithm should ensure decomposed components are as independent as possible, a principle we term the Component Independence Principle. Guided by this principle, we introduce the Theory-guided Graph Decomposition Learning (TGDL) framework, which decomposes graph-based multivariate time series data into approximately independent subgraph components that are easier to predict than the original data. Moreover, TGDL is a portable framework that can be integrated into any graph-based traffic prediction model to improve its predictive performance. Extensive experiments on four public datasets demonstrate the effectiveness of our approach. With a solid theoretical foundation, our TGDL enhances the performance of diverse traffic prediction models, yielding an average improvement of 19.37% across experiments. Our code is available at https://github.com/bigscity/TGDL.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Ji; Jingyuan Wang; Yu Mou; Cheng Long; Junjie Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651246"&gt;10.1109/tpami.2026.3651246&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Traffic state prediction based on spatiotemporal data has become a prominent focus in data-driven AI research. While significant progress has been made, most mainstream approaches assume uniform spatial and temporal correlations across conditions and use shared parameters for all scenarios. This simplification overlooks the complexity and heterogeneity inherent in human mobility patterns, often leading to suboptimal predictions. Recently, methods adopting the “decompose, then predict” (DTP) paradigm have gained traction. These methods break down data into smaller, manageable subcomponents, each predicted using dedicated parameters. Although effective in practice, DTP methods face unresolved theoretical questions: What type of decomposition truly makes subcomponents more manageable than the original data? To address this, we present an information theory-based analysis that derives sufficient conditions for a decomposition algorithm to reduce data-induced prediction errors. These conditions suggest that an effective algorithm should ensure decomposed components are as independent as possible, a principle we term the Component Independence Principle. Guided by this principle, we introduce the Theory-guided Graph Decomposition Learning (TGDL) framework, which decomposes graph-based multivariate time series data into approximately independent subgraph components that are easier to predict than the original data. Moreover, TGDL is a portable framework that can be integrated into any graph-based traffic prediction model to improve its predictive performance. Extensive experiments on four public datasets demonstrate the effectiveness of our approach. With a solid theoretical foundation, our TGDL enhances the performance of diverse traffic prediction models, yielding an average improvement of 19.37% across experiments. Our code is available at https://github.com/bigscity/TGDL.&lt;/p&gt;</content:encoded></item><item><title>Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy</title><link>https://arxiv.org/abs/2601.06801v1</link><guid>http://arxiv.org/abs/2601.06801v1</guid><pubDate>Sun, 11 Jan 2026 08:25:34 +0000</pubDate><dc:creator>Shujian Gao</dc:creator><dc:creator>Yuan Wang</dc:creator><dc:creator>Jiangtao Yan</dc:creator><dc:creator>Zuxuan Wu</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \textbf{Thinking with Deltas}, a framework driven by a \textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \textit{visual robustness}). By aligning reasoning variations strictly with the \textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.
Published: 2026-01-11T08:25:34+00:00
Venue: arXiv
Score: 0.558 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shujian Gao; Yuan Wang; Jiangtao Yan; Zuxuan Wu; Yu-Gang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.558 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \textbf{Thinking with Deltas}, a framework driven by a \textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \textit{visual robustness}). By aligning reasoning variations strictly with the \textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Pretrained Diffusion Model for Semantic 3D Reconstruction from Monocular Remote Sensing Image</title><link>https://doi.org/10.1109/tgrs.2026.3653117</link><guid>10.1109/tgrs.2026.3653117</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Xin Xu</dc:creator><dc:creator>Ruizhe Deng</dc:creator><dc:creator>Qinglong Cao</dc:creator><dc:creator>Zhiling Guo</dc:creator><dc:creator>Yuntian Chen</dc:creator><dc:creator>Jinyue Yan</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3653117</prism:doi><description>Semantic 3D reconstruction from monocular imagery serves as a cost-effective tool for many urban applications, such as energy system modeling, resilience analysis, and urban planning. However, the generalization of task-specific models for semantic 3D reconstruction remains limited by the available data scale and diversity. In contrast, visual foundation models (VFMs) are trained on large-scale, diverse datasets, enabling stronger adaptability and richer visual knowledge across different tasks. Unlike most VFMs that focus on discrimination or feature extraction, pretrained diffusion models (PDMs) are generative, combining high-level semantic understanding with the ability to produce high-fidelity details and textures. Building upon these advantages, this study proposes a novel task-adaptive framework that harnesses PDMs for semantic 3D reconstruction from monocular remote sensing images. Our framework employs low-rank adaptation to efficiently fine-tune the denoising network, effectively modeling the high-dimensional features required for semantic 3D reconstruction while only training a minimal fraction of parameters. We further design a lightweight, task-specific decoder to map these features into target elevation and semantic maps. In addition, we introduce an evidential height regression method, which incorporates uncertainty awareness into height estimation without introducing additional computational overhead. Experiments on the public US3D JAX and Open Data DC datasets demonstrate that our framework significantly outperforms other existing methods in both subtasks of height estimation and semantic segmentation, achieving high-fidelity semantic 3D reconstruction of remote sensing scenes. This technology holds significant potential for advancing urban modeling, enabling more accurate and efficient large-scale geographic analysis.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Xu; Ruizhe Deng; Qinglong Cao; Zhiling Guo; Yuntian Chen; Jinyue Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3653117"&gt;10.1109/tgrs.2026.3653117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Semantic 3D reconstruction from monocular imagery serves as a cost-effective tool for many urban applications, such as energy system modeling, resilience analysis, and urban planning. However, the generalization of task-specific models for semantic 3D reconstruction remains limited by the available data scale and diversity. In contrast, visual foundation models (VFMs) are trained on large-scale, diverse datasets, enabling stronger adaptability and richer visual knowledge across different tasks. Unlike most VFMs that focus on discrimination or feature extraction, pretrained diffusion models (PDMs) are generative, combining high-level semantic understanding with the ability to produce high-fidelity details and textures. Building upon these advantages, this study proposes a novel task-adaptive framework that harnesses PDMs for semantic 3D reconstruction from monocular remote sensing images. Our framework employs low-rank adaptation to efficiently fine-tune the denoising network, effectively modeling the high-dimensional features required for semantic 3D reconstruction while only training a minimal fraction of parameters. We further design a lightweight, task-specific decoder to map these features into target elevation and semantic maps. In addition, we introduce an evidential height regression method, which incorporates uncertainty awareness into height estimation without introducing additional computational overhead. Experiments on the public US3D JAX and Open Data DC datasets demonstrate that our framework significantly outperforms other existing methods in both subtasks of height estimation and semantic segmentation, achieving high-fidelity semantic 3D reconstruction of remote sensing scenes. This technology holds significant potential for advancing urban modeling, enabling more accurate and efficient large-scale geographic analysis.&lt;/p&gt;</content:encoded></item><item><title>Prototype-based Meta-Prompt Tuning: Toward Rehearsal-free Few-Shot Class-Incremental Learning for Multimodal Remote Sensing Image</title><link>https://doi.org/10.1109/tip.2025.3650395</link><guid>10.1109/tip.2025.3650395</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Yuanbo Yang</dc:creator><dc:creator>Jiahui Qu</dc:creator><dc:creator>Wenqian Dong</dc:creator><dc:creator>Ling Huang</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3650395</prism:doi><description>Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanbo Yang; Jiahui Qu; Wenqian Dong; Ling Huang; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3650395"&gt;10.1109/tip.2025.3650395&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.&lt;/p&gt;</content:encoded></item><item><title>Semantic Boosting via Knowledge Sharing and Feedback for Video Anomaly Detection</title><link>https://doi.org/10.1109/tcsvt.2026.3652591</link><guid>10.1109/tcsvt.2026.3652591</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Xiaojie Cai</dc:creator><dc:creator>Yucheng Qian</dc:creator><dc:creator>Chong Wang</dc:creator><dc:creator>Xiaohao Peng</dc:creator><dc:creator>Yuanbin Qian</dc:creator><dc:creator>Jiafei Wu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3652591</prism:doi><description>Vision-language models have the potential to enrich purely visual tasks by utilizing the combined representation of images/videos and corresponding textual descriptions. Recent advances in video anomaly detection have also integrated textual information to enhance the understanding of abnormal events. However, existing approaches often merge visual and textual modalities in a straightforward, bottom-up manner, failing to fully explore their interconnections. Moreover, textual captions themselves do not inherently convey “abnormal” attributes. Consequently, these joint representations tend to highlight all salient input features without adequately focusing on high-level tasks such as video anomaly detection. To direct the model’s attention towards anomalies more effectively, we propose incorporating a top-down mechanism into weakly supervised video anomaly detection tasks. A new Knowledge Sharing and Feedback (KSF) framework is designed to unify the representation of anomalies across both video and text. Specifically, we develop a category pattern sharing module that performs knowledge matching, acting as an alignment bridge between abnormal events and their corresponding descriptions. This ensures consistent representations for identical anomalies while maintaining distinct representations for different ones. Following this alignment process, matched high-level semantic priors are fed back into the forward path to enhance differentiation between abnormal and normal patterns. Comprehensive experiments on three benchmark datasets demonstrate the superiority of our proposed method in learning the implicit definition of anomaly patterns. The code is available at https://github.com/XJ-Cai/KSF.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaojie Cai; Yucheng Qian; Chong Wang; Xiaohao Peng; Yuanbin Qian; Jiafei Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3652591"&gt;10.1109/tcsvt.2026.3652591&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models have the potential to enrich purely visual tasks by utilizing the combined representation of images/videos and corresponding textual descriptions. Recent advances in video anomaly detection have also integrated textual information to enhance the understanding of abnormal events. However, existing approaches often merge visual and textual modalities in a straightforward, bottom-up manner, failing to fully explore their interconnections. Moreover, textual captions themselves do not inherently convey “abnormal” attributes. Consequently, these joint representations tend to highlight all salient input features without adequately focusing on high-level tasks such as video anomaly detection. To direct the model’s attention towards anomalies more effectively, we propose incorporating a top-down mechanism into weakly supervised video anomaly detection tasks. A new Knowledge Sharing and Feedback (KSF) framework is designed to unify the representation of anomalies across both video and text. Specifically, we develop a category pattern sharing module that performs knowledge matching, acting as an alignment bridge between abnormal events and their corresponding descriptions. This ensures consistent representations for identical anomalies while maintaining distinct representations for different ones. Following this alignment process, matched high-level semantic priors are fed back into the forward path to enhance differentiation between abnormal and normal patterns. Comprehensive experiments on three benchmark datasets demonstrate the superiority of our proposed method in learning the implicit definition of anomaly patterns. The code is available at https://github.com/XJ-Cai/KSF.&lt;/p&gt;</content:encoded></item><item><title>Seeing through Satellite Images at Street Views</title><link>https://doi.org/10.1109/tpami.2026.3652860</link><guid>10.1109/tpami.2026.3652860</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Ming Qian</dc:creator><dc:creator>Bin Tan</dc:creator><dc:creator>Qiuyu Wang</dc:creator><dc:creator>Xianwei Zheng</dc:creator><dc:creator>Hanjiang Xiong</dc:creator><dc:creator>Gui-Song Xia</dc:creator><dc:creator>Yujun Shen</dc:creator><dc:creator>Nan Xue</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652860</prism:doi><description>This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Qian; Bin Tan; Qiuyu Wang; Xianwei Zheng; Hanjiang Xiong; Gui-Song Xia; Yujun Shen; Nan Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652860"&gt;10.1109/tpami.2026.3652860&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.&lt;/p&gt;</content:encoded></item><item><title>A Hierarchical Prior Mining Approach for Non-local Multi-view Stereo</title><link>https://doi.org/10.1109/tpami.2026.3652616</link><guid>10.1109/tpami.2026.3652616</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jiaqi Yang</dc:creator><dc:creator>Yanan He</dc:creator><dc:creator>Chunlin Ren</dc:creator><dc:creator>Qingshan Xu</dc:creator><dc:creator>Siwen Quan</dc:creator><dc:creator>Xiyu Zhang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652616</prism:doi><description>As a fundamental problem in computer vision, multi-view stereo (MVS) aims at recovering the 3D geometry of the target from a set of 2D images. However, the reconstructed quality is significantly impacted by the presence of low-textured areas. In this paper, we propose a Hierarchical Prior Mining (HPM) framework for non-local multi-view stereo. Different from most existing works dedicated to focusing on local information and only using a single prior, HPM captures non-local structural cues and leverages multi-source priors for geometry recovery. Based on the framework, we first propose HPM-MVS, which obtains precise initial hypotheses through non-local operations, simultaneously constructing a better planar prior model in an HPM framework to further facilitate hypothesis generation. In addition, we futher propose HPM-MVS++, which excavates the structured region information of images and spatial geometric relationships of hypotheses as prior knowledge. Then, it incorporates them into probabilistic graphical models, ultimately deducing two novel multi-view matching costs. This significantly enhances the robustness to challenging situations and improves the completeness of the reconstruction. Experimental results on the ETH3D and Tanks &amp; Temples have verified the superior performance and strong generalization capability of our approach. The code is available at https://github.com/CLinvx.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Yang; Yanan He; Chunlin Ren; Qingshan Xu; Siwen Quan; Xiyu Zhang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652616"&gt;10.1109/tpami.2026.3652616&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;As a fundamental problem in computer vision, multi-view stereo (MVS) aims at recovering the 3D geometry of the target from a set of 2D images. However, the reconstructed quality is significantly impacted by the presence of low-textured areas. In this paper, we propose a Hierarchical Prior Mining (HPM) framework for non-local multi-view stereo. Different from most existing works dedicated to focusing on local information and only using a single prior, HPM captures non-local structural cues and leverages multi-source priors for geometry recovery. Based on the framework, we first propose HPM-MVS, which obtains precise initial hypotheses through non-local operations, simultaneously constructing a better planar prior model in an HPM framework to further facilitate hypothesis generation. In addition, we futher propose HPM-MVS++, which excavates the structured region information of images and spatial geometric relationships of hypotheses as prior knowledge. Then, it incorporates them into probabilistic graphical models, ultimately deducing two novel multi-view matching costs. This significantly enhances the robustness to challenging situations and improves the completeness of the reconstruction. Experimental results on the ETH3D and Tanks &amp;amp; Temples have verified the superior performance and strong generalization capability of our approach. The code is available at https://github.com/CLinvx.&lt;/p&gt;</content:encoded></item><item><title>CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval</title><link>https://arxiv.org/abs/2601.08175v1</link><guid>http://arxiv.org/abs/2601.08175v1</guid><pubDate>Tue, 13 Jan 2026 03:09:35 +0000</pubDate><dc:creator>Feiran Wang</dc:creator><dc:creator>Junyi Wu</dc:creator><dc:creator>Dawen Cai</dc:creator><dc:creator>Yuan Hong</dc:creator><dc:creator>Yan Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.
Published: 2026-01-13T03:09:35+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feiran Wang; Junyi Wu; Dawen Cai; Yuan Hong; Yan Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.&lt;/p&gt;</content:encoded></item><item><title>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</title><link>https://doi.org/10.1109/tip.2025.3644175</link><guid>10.1109/tip.2025.3644175</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Ziyu Liu</dc:creator><dc:creator>Zeyi Sun</dc:creator><dc:creator>Yuhang Zang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Pan Zhang</dc:creator><dc:creator>Xiaoyi Dong</dc:creator><dc:creator>Yuanjun Xiong</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Jiaqi Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644175</prism:doi><description>CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyu Liu; Zeyi Sun; Yuhang Zang; Wei Li; Pan Zhang; Xiaoyi Dong; Yuanjun Xiong; Dahua Lin; Jiaqi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644175"&gt;10.1109/tip.2025.3644175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.&lt;/p&gt;</content:encoded></item><item><title>Semantic Misalignment in Vision-Language Models under Perceptual Degradation</title><link>https://arxiv.org/abs/2601.08355v1</link><guid>http://arxiv.org/abs/2601.08355v1</guid><pubDate>Tue, 13 Jan 2026 09:13:05 +0000</pubDate><dc:creator>Guo Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.
Published: 2026-01-13T09:13:05+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guo Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.&lt;/p&gt;</content:encoded></item><item><title>Group-Relative Visual Discrimination Enhancement for Unlocking Intrinsic Capability of MLLMs</title><link>https://doi.org/10.1109/tcsvt.2026.3652189</link><guid>10.1109/tcsvt.2026.3652189</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Fang Peng</dc:creator><dc:creator>Xiaoshan Yang</dc:creator><dc:creator>Yaowei Wang</dc:creator><dc:creator>Changsheng Xu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3652189</prism:doi><description>Although Multimodal Large Language Models (MLLMs) have shown remarkable generalization across diverse vision-language tasks, recent studies reveal their limitations in visual discrimination. These challenges arise not from insufficient model capacity, but from existing training paradigms that favor linguistic priors over detailed visual analysis. While existing approaches address this limitation through external interventions such as feature integration or knowledge augmentation, we propose a Group-Relative Visual Discrimination Enhancement framework to unlock intrinsic capability of MLLMs and requires no external resources. Our method introduces a Group-Relative Reinforcement Learning paradigm equipped with a lightweight Visual Patch Selection Plugin to dynamically select discriminative visual tokens. The framework establishes a self-feedback loop between visual encoder and language decoder, leveraging the dual reward-penalty signals derived from the model’s internal language feedback to optimize the visual focus, thereby enhancing the model’s visual discrimination capabilities. Extensive experimental results across six visual recognition benchmarks and two VQA benchmarks demonstrate the effectiveness of our method. Code is available at https://github.com/FannierPeng/GROVE.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fang Peng; Xiaoshan Yang; Yaowei Wang; Changsheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3652189"&gt;10.1109/tcsvt.2026.3652189&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Although Multimodal Large Language Models (MLLMs) have shown remarkable generalization across diverse vision-language tasks, recent studies reveal their limitations in visual discrimination. These challenges arise not from insufficient model capacity, but from existing training paradigms that favor linguistic priors over detailed visual analysis. While existing approaches address this limitation through external interventions such as feature integration or knowledge augmentation, we propose a Group-Relative Visual Discrimination Enhancement framework to unlock intrinsic capability of MLLMs and requires no external resources. Our method introduces a Group-Relative Reinforcement Learning paradigm equipped with a lightweight Visual Patch Selection Plugin to dynamically select discriminative visual tokens. The framework establishes a self-feedback loop between visual encoder and language decoder, leveraging the dual reward-penalty signals derived from the model’s internal language feedback to optimize the visual focus, thereby enhancing the model’s visual discrimination capabilities. Extensive experimental results across six visual recognition benchmarks and two VQA benchmarks demonstrate the effectiveness of our method. Code is available at https://github.com/FannierPeng/GROVE.&lt;/p&gt;</content:encoded></item><item><title>Robust Distributed Cooperative Classification with Learned Compressed-Feature Diffusion</title><link>https://doi.org/10.1109/tpami.2026.3652297</link><guid>10.1109/tpami.2026.3652297</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Xiling Yao</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Jingdong Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652297</prism:doi><description>Cooperative inference in distributed sensor networks is challenged by limited communication bandwidth and the risk of node failures. This paper introduces Compressed Feature Diffusion for Decentralized Classification (CFD-DC), a novel framework that addresses these challenges. Each node performs local inference using its own features and compressed feature representations received from other nodes. Our approach relies on two key components: first, a trainable feature compressor at each node that learns compact representations, reducing communication while preserving critical discriminative information; second, an adaptive node weighting mechanism that dynamically adjusts the influence of local and remote features, providing robustness to unreliable or failed nodes. Experiments on multi-view image classification and a simulated multi-node underwater acoustic target classification task demonstrate the effectiveness of the framework. The results show competitive performance compared to centralized and state-of-the-art multi-view methods, reduced communication costs, and superior robustness in scenarios with node failures.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiling Yao; Jie Chen; Jingdong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652297"&gt;10.1109/tpami.2026.3652297&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Cooperative inference in distributed sensor networks is challenged by limited communication bandwidth and the risk of node failures. This paper introduces Compressed Feature Diffusion for Decentralized Classification (CFD-DC), a novel framework that addresses these challenges. Each node performs local inference using its own features and compressed feature representations received from other nodes. Our approach relies on two key components: first, a trainable feature compressor at each node that learns compact representations, reducing communication while preserving critical discriminative information; second, an adaptive node weighting mechanism that dynamically adjusts the influence of local and remote features, providing robustness to unreliable or failed nodes. Experiments on multi-view image classification and a simulated multi-node underwater acoustic target classification task demonstrate the effectiveness of the framework. The results show competitive performance compared to centralized and state-of-the-art multi-view methods, reduced communication costs, and superior robustness in scenarios with node failures.&lt;/p&gt;</content:encoded></item><item><title>Remote Sensing Image Semantic Segmentation Utilizing Geographical Element Association Features</title><link>https://doi.org/10.1109/tgrs.2026.3653687</link><guid>10.1109/tgrs.2026.3653687</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Ruiqi Yang</dc:creator><dc:creator>Haoyu Fu</dc:creator><dc:creator>Nan Chen</dc:creator><dc:creator>Shilin Tao</dc:creator><dc:creator>Liang Hong</dc:creator><dc:creator>Leiguang Wang</dc:creator><dc:creator>Chen Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3653687</prism:doi><description>Semantic segmentation of high-resolution remote sensing images remains challenging due to complex spatial structures, fine-grained category variations, and hierarchical semantic dependencies. Existing deep learning models often treat semantic categories as independent, flat labels, which leads to inconsistent predictions across hierarchical levels (e.g., a pixel predicted as Tree but not as Vegetation) and weak generalization in heterogeneous landscapes. To address these issues, we propose HAG Net (Hierarchical Attention Gate Multi-Residual UNet), a novel framework that explicitly models hierarchical semantics and enforces cross-level consistency. First, a multi-residual encoder–decoder backbone with hierarchical attention gates (HAGs) enhances multi-scale representation while filtering irrelevant background noise. Second, a Mixture-of-Head (MoH) attention module enables bidirectional semantic interaction between coarse- and fine-level features, mitigating error propagation caused by unidirectional designs. Third, a Hierarchical Interaction (HI) Loss introduces a dynamic category interaction matrix to adaptively constrain predictions, ensuring consistency across levels. Extensive experiments on two large-scale datasets, GID (5- and 15-class) and Ascend Cup 2020 (8- and 17-class), demonstrate that HAG Net consistently outperforms state-of-the-art methods, including DeepLabv3+, CGGLNet, and MAE-BG. Specifically, HAG Net achieves up to 4.5 % improvement in FWIoU and significantly enhances per-class accuracy for small and structurally complex objects. These results confirm the effectiveness of incorporating hierarchical semantics into segmentation networks and highlight the potential of HAG Net for large-scale land cover mapping, ecological monitoring, and urban planning applications.The source code is available at: https://github.com/yangruiqi-kiki/HAG.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Yang; Haoyu Fu; Nan Chen; Shilin Tao; Liang Hong; Leiguang Wang; Chen Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3653687"&gt;10.1109/tgrs.2026.3653687&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of high-resolution remote sensing images remains challenging due to complex spatial structures, fine-grained category variations, and hierarchical semantic dependencies. Existing deep learning models often treat semantic categories as independent, flat labels, which leads to inconsistent predictions across hierarchical levels (e.g., a pixel predicted as Tree but not as Vegetation) and weak generalization in heterogeneous landscapes. To address these issues, we propose HAG Net (Hierarchical Attention Gate Multi-Residual UNet), a novel framework that explicitly models hierarchical semantics and enforces cross-level consistency. First, a multi-residual encoder–decoder backbone with hierarchical attention gates (HAGs) enhances multi-scale representation while filtering irrelevant background noise. Second, a Mixture-of-Head (MoH) attention module enables bidirectional semantic interaction between coarse- and fine-level features, mitigating error propagation caused by unidirectional designs. Third, a Hierarchical Interaction (HI) Loss introduces a dynamic category interaction matrix to adaptively constrain predictions, ensuring consistency across levels. Extensive experiments on two large-scale datasets, GID (5- and 15-class) and Ascend Cup 2020 (8- and 17-class), demonstrate that HAG Net consistently outperforms state-of-the-art methods, including DeepLabv3+, CGGLNet, and MAE-BG. Specifically, HAG Net achieves up to 4.5 % improvement in FWIoU and significantly enhances per-class accuracy for small and structurally complex objects. These results confirm the effectiveness of incorporating hierarchical semantics into segmentation networks and highlight the potential of HAG Net for large-scale land cover mapping, ecological monitoring, and urban planning applications.The source code is available at: https://github.com/yangruiqi-kiki/HAG.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Semantic-Visual Fusion of Visible and Near-Infrared Images for Long-Range Haze Removal</title><link>https://doi.org/10.1109/tmm.2026.3651088</link><guid>10.1109/tmm.2026.3651088</guid><pubDate>Mon, 12 Jan 2026 22:01:52 +0000</pubDate><dc:creator>Yi Li</dc:creator><dc:creator>Xiaoxiong Wang</dc:creator><dc:creator>Jiawei Wang</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Kai Cao</dc:creator><dc:creator>Luxin Yan</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651088</prism:doi><description>While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near- infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near- infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near- infrared by fusing complementary cues from both visible and near- infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible- infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.
Published: 2026-01-12T22:01:52+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Li; Xiaoxiong Wang; Jiawei Wang; Yi Chang; Kai Cao; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651088"&gt;10.1109/tmm.2026.3651088&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near- infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near- infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near- infrared by fusing complementary cues from both visible and near- infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible- infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.&lt;/p&gt;</content:encoded></item><item><title>HCL-Net: Heterogeneous Collaborative Learning for Lightweight Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3652161</link><guid>10.1109/tgrs.2026.3652161</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Jin Xie</dc:creator><dc:creator>Wujie Zhou</dc:creator><dc:creator>Caie Xu</dc:creator><dc:creator>Yuanyuan Liu</dc:creator><dc:creator>Fangfang Qiang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3652161</prism:doi><description>Semantic segmentation of remote sensing images remains challenging due to large intra-class variations, high inter-class similarity, and the demand for lightweight deployment. Conventional single-architecture models and homogeneous collaborative frameworks struggle to balance local detail extraction with global context modeling. To address these limitations, we propose HCL-Net, a heterogeneous collaborative learning framework that integrates convolutional and Transformer architectures. HCL-Net consists of two complementary student networks: the Frequency-domain Local Detail Network (FLDNet), based on ResNet18 with a wavelet phase–amplitude fusion block to capture multi-frequency information, and the Spatial-domain Global Structure Network (SGSNet), built on a DFormer-T backbone with a dynamic texture–edge perception module for robust global context modeling. A dual collaborative strategy enhances knowledge transfer between networks through (1) bidirectional feature reconstruction, which aligns high-order statistics using Gram matrix alignment and enforces feature-space consistency via variational information distillation, and (2) regional pixel-level contrastive learning, which improves intra-class compactness while reducing inter-class confusion. Experiments on the Vaihingen dataset demonstrate that collaborative training yields substantial gains over independent training, with FLDNet achieving mAcc 89.92% / mIoU 82.12% and SGSNet achieving mAcc 89.95% / mIoU 82.16%, improving accuracy by 2.26%/2.08% and IoU by 2.53%/2.19%, respectively. With only 24.25M and 12.36M parameters and computational costs of 6.13G and 6.35G, FLDNet and SGSNet outperform 19 state-of-the-art methods while remaining efficient for resource-constrained environments. Code and experimental results are available at https://github.com/110-011/HCL-Net.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jin Xie; Wujie Zhou; Caie Xu; Yuanyuan Liu; Fangfang Qiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3652161"&gt;10.1109/tgrs.2026.3652161&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of remote sensing images remains challenging due to large intra-class variations, high inter-class similarity, and the demand for lightweight deployment. Conventional single-architecture models and homogeneous collaborative frameworks struggle to balance local detail extraction with global context modeling. To address these limitations, we propose HCL-Net, a heterogeneous collaborative learning framework that integrates convolutional and Transformer architectures. HCL-Net consists of two complementary student networks: the Frequency-domain Local Detail Network (FLDNet), based on ResNet18 with a wavelet phase–amplitude fusion block to capture multi-frequency information, and the Spatial-domain Global Structure Network (SGSNet), built on a DFormer-T backbone with a dynamic texture–edge perception module for robust global context modeling. A dual collaborative strategy enhances knowledge transfer between networks through (1) bidirectional feature reconstruction, which aligns high-order statistics using Gram matrix alignment and enforces feature-space consistency via variational information distillation, and (2) regional pixel-level contrastive learning, which improves intra-class compactness while reducing inter-class confusion. Experiments on the Vaihingen dataset demonstrate that collaborative training yields substantial gains over independent training, with FLDNet achieving mAcc 89.92% / mIoU 82.12% and SGSNet achieving mAcc 89.95% / mIoU 82.16%, improving accuracy by 2.26%/2.08% and IoU by 2.53%/2.19%, respectively. With only 24.25M and 12.36M parameters and computational costs of 6.13G and 6.35G, FLDNet and SGSNet outperform 19 state-of-the-art methods while remaining efficient for resource-constrained environments. Code and experimental results are available at https://github.com/110-011/HCL-Net.&lt;/p&gt;</content:encoded></item><item><title>VENUS: Visual Editing with Noise Inversion Using Scene Graphs</title><link>https://arxiv.org/abs/2601.07219v1</link><guid>http://arxiv.org/abs/2601.07219v1</guid><pubDate>Mon, 12 Jan 2026 05:24:58 +0000</pubDate><dc:creator>Thanh-Nhan Vo</dc:creator><dc:creator>Trong-Thuan Nguyen</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.
Published: 2026-01-12T05:24:58+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.&lt;/p&gt;</content:encoded></item><item><title>OIF-PCR++: Point Cloud Registration via Progressive Distillation of Conditional Positional Encoding</title><link>https://doi.org/10.1109/tpami.2026.3652316</link><guid>10.1109/tpami.2026.3652316</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Fan Yang</dc:creator><dc:creator>Zhi Chen</dc:creator><dc:creator>Nanjun Yuan</dc:creator><dc:creator>Lin Guo</dc:creator><dc:creator>Wenbing Tao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652316</prism:doi><description>Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Yang; Zhi Chen; Nanjun Yuan; Lin Guo; Wenbing Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652316"&gt;10.1109/tpami.2026.3652316&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...&lt;/p&gt;</content:encoded></item><item><title>Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models</title><link>https://doi.org/10.1109/tpami.2026.3651319</link><guid>10.1109/tpami.2026.3651319</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Hao Dong</dc:creator><dc:creator>Moru Liu</dc:creator><dc:creator>Kaiyang Zhou</dc:creator><dc:creator>Eleni Chatzi</dc:creator><dc:creator>Juho Kannala</dc:creator><dc:creator>Cyrill Stachniss</dc:creator><dc:creator>Olga Fink</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651319</prism:doi><description>Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Dong; Moru Liu; Kaiyang Zhou; Eleni Chatzi; Juho Kannala; Cyrill Stachniss; Olga Fink&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651319"&gt;10.1109/tpami.2026.3651319&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.&lt;/p&gt;</content:encoded></item><item><title>CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey</title><link>https://doi.org/10.1109/tpami.2026.3651700</link><guid>10.1109/tpami.2026.3651700</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jindong Li</dc:creator><dc:creator>Yongguang Li</dc:creator><dc:creator>Yali Fu</dc:creator><dc:creator>Jiahong Liu</dc:creator><dc:creator>Yixin Liu</dc:creator><dc:creator>Menglin Yang</dc:creator><dc:creator>Irwin King</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651700</prism:doi><description>As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP's growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jindong Li; Yongguang Li; Yali Fu; Jiahong Liu; Yixin Liu; Menglin Yang; Irwin King&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651700"&gt;10.1109/tpami.2026.3651700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP&amp;#x27;s growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...&lt;/p&gt;</content:encoded></item><item><title>HGNNv2: Stable Hypergraph Neural Networks</title><link>https://doi.org/10.1109/tpami.2026.3652225</link><guid>10.1109/tpami.2026.3652225</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Yue Gao</dc:creator><dc:creator>Jielong Yan</dc:creator><dc:creator>Yifan Feng</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Shihui Ying</dc:creator><dc:creator>Zongze Wu</dc:creator><dc:creator>Han Hu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652225</prism:doi><description>Hypergraph neural networks (HGNNs) are widely used models for analyzing higher-order relational data. HGNNs suffer from the rapid performance degradation with increasing layers. Hypergraph dynamic system (HDS) is a potential way to deal with this challenge. However, hypergraph dynamic system is confined to a time-continuous isotropic model, lacking positional information in the structural space of the hypergraph. In contrast, anisotropic diffusion can capture structural space differences among vertices, providing a more precise representation of the information propagation process in hypergraph structures than isotropic diffusion. In this paper, we introduce HGNNv2, a stable hypergraph neural network, which is built as a hypergraph dynamic system with partial differential equation (PDE). This model incorporates a position-aware anisotropic diffusion term and an external control term. We further present the vertex-rooted subtree method to determine anisotropic diffusion intensity. HGNNv2 has properties that vertices occupying equivalent positions in the structural space share equivalent structural labels and positional features. Experiments on 6 hypergraph datasets and 3 graph datasets reveal that HGNNv2 outperforms all 12 compared methods. HGNNv2 is capable of achieving stable final representations and task accuracy even under noisy conditions. HGNNv2 achieves stable performance with fewer layers than hypergraph dynamic systems employing isotropic diffusion. We provide feature visualizations to illustrate the evolution of representations.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Gao; Jielong Yan; Yifan Feng; Xiangmin Han; Shihui Ying; Zongze Wu; Han Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652225"&gt;10.1109/tpami.2026.3652225&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Hypergraph neural networks (HGNNs) are widely used models for analyzing higher-order relational data. HGNNs suffer from the rapid performance degradation with increasing layers. Hypergraph dynamic system (HDS) is a potential way to deal with this challenge. However, hypergraph dynamic system is confined to a time-continuous isotropic model, lacking positional information in the structural space of the hypergraph. In contrast, anisotropic diffusion can capture structural space differences among vertices, providing a more precise representation of the information propagation process in hypergraph structures than isotropic diffusion. In this paper, we introduce HGNNv2, a stable hypergraph neural network, which is built as a hypergraph dynamic system with partial differential equation (PDE). This model incorporates a position-aware anisotropic diffusion term and an external control term. We further present the vertex-rooted subtree method to determine anisotropic diffusion intensity. HGNNv2 has properties that vertices occupying equivalent positions in the structural space share equivalent structural labels and positional features. Experiments on 6 hypergraph datasets and 3 graph datasets reveal that HGNNv2 outperforms all 12 compared methods. HGNNv2 is capable of achieving stable final representations and task accuracy even under noisy conditions. HGNNv2 achieves stable performance with fewer layers than hypergraph dynamic systems employing isotropic diffusion. We provide feature visualizations to illustrate the evolution of representations.&lt;/p&gt;</content:encoded></item></channel></rss>