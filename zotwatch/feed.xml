<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 29 Jan 2026 03:20:54 +0000</lastBuildDate><item><title>DFormer++: Improving RGBD Representation Learning for Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2026.3658114</link><guid>10.1109/tpami.2026.3658114</guid><pubDate>Tue, 27 Jan 2026 20:29:16 +0000</pubDate><dc:creator>Bo-Wen Yin</dc:creator><dc:creator>Jiao-Long Cao</dc:creator><dc:creator>Dan Xu</dc:creator><dc:creator>Ming-Ming Cheng</dc:creator><dc:creator>Qibin Hou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658114</prism:doi><description>We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.
Published: 2026-01-27T20:29:16+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.588 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo-Wen Yin; Jiao-Long Cao; Dan Xu; Ming-Ming Cheng; Qibin Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658114"&gt;10.1109/tpami.2026.3658114&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.588 (consider)&lt;/p&gt;
&lt;p&gt;We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.&lt;/p&gt;</content:encoded></item><item><title>TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document</title><link>https://doi.org/10.1109/tpami.2026.3653415</link><guid>10.1109/tpami.2026.3653415</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Yuliang Liu</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Qiang Liu</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Zhiyin Ma</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Xiang Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653415</prism:doi><description>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.573 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Liu; Biao Yang; Qiang Liu; Zhang Li; Zhiyin Ma; Shuo Zhang; Xiang Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653415"&gt;10.1109/tpami.2026.3653415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.573 (consider)&lt;/p&gt;
&lt;p&gt;We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&amp;#x27;s performance. Moreover, by expanding our model&amp;#x27;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.&lt;/p&gt;</content:encoded></item><item><title>Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models</title><link>https://arxiv.org/abs/2601.19060v1</link><guid>http://arxiv.org/abs/2601.19060v1</guid><pubDate>Tue, 27 Jan 2026 00:46:08 +0000</pubDate><dc:creator>Jeonghwan Kim</dc:creator><dc:creator>Renjie Tao</dc:creator><dc:creator>Sanat Sharma</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Kai Sun</dc:creator><dc:creator>Zhaojiang Lin</dc:creator><dc:creator>Seungwhan Moon</dc:creator><dc:creator>Lambert Mathias</dc:creator><dc:creator>Anuj Kumar</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Xin Luna Dong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &lt;search&gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.
Published: 2026-01-27T00:46:08+00:00
Venue: arXiv
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeonghwan Kim; Renjie Tao; Sanat Sharma; Jiaqi Wang; Kai Sun; Zhaojiang Lin; Seungwhan Moon; Lambert Mathias; Anuj Kumar; Heng Ji; Xin Luna Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &amp;lt;search&amp;gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.&lt;/p&gt;</content:encoded></item><item><title>QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding</title><link>https://arxiv.org/abs/2601.18195v1</link><guid>http://arxiv.org/abs/2601.18195v1</guid><pubDate>Mon, 26 Jan 2026 06:27:03 +0000</pubDate><dc:creator>Linhan Cao</dc:creator><dc:creator>Wei Sun</dc:creator><dc:creator>Weixia Zhang</dc:creator><dc:creator>Xiangyang Zhu</dc:creator><dc:creator>Kaiwei Zhang</dc:creator><dc:creator>Jun Jia</dc:creator><dc:creator>Dandan Zhu</dc:creator><dc:creator>Guangtao Zhai</dc:creator><dc:creator>Xiongkuo Min</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.
Published: 2026-01-26T06:27:03+00:00
Venue: arXiv
Score: 0.568 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linhan Cao; Wei Sun; Weixia Zhang; Xiangyang Zhu; Kaiwei Zhang; Jun Jia; Dandan Zhu; Guangtao Zhai; Xiongkuo Min&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.568 (consider)&lt;/p&gt;
&lt;p&gt;Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.&lt;/p&gt;</content:encoded></item><item><title>bi-modal textual prompt learning for vision-language models in remote sensing</title><link>https://arxiv.org/abs/2601.20675v1</link><guid>http://arxiv.org/abs/2601.20675v1</guid><pubDate>Wed, 28 Jan 2026 14:58:14 +0000</pubDate><dc:creator>Pankhi Kashyap</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.
Published: 2026-01-28T14:58:14+00:00
Venue: arXiv
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pankhi Kashyap; Mainak Singha; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.&lt;/p&gt;</content:encoded></item><item><title>Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing</title><link>https://arxiv.org/abs/2601.17673v1</link><guid>http://arxiv.org/abs/2601.17673v1</guid><pubDate>Sun, 25 Jan 2026 03:22:26 +0000</pubDate><dc:creator>Weiyu Zhang</dc:creator><dc:creator>Yuan Hu</dc:creator><dc:creator>Yong Li</dc:creator><dc:creator>Yu Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.
Published: 2026-01-25T03:22:26+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiyu Zhang; Yuan Hu; Yong Li; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.&lt;/p&gt;</content:encoded></item><item><title>Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</title><link>https://arxiv.org/abs/2601.18190v1</link><guid>http://arxiv.org/abs/2601.18190v1</guid><pubDate>Mon, 26 Jan 2026 06:16:53 +0000</pubDate><dc:creator>Yifan Li</dc:creator><dc:creator>Shiying Wang</dc:creator><dc:creator>Jianqiang Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.
Published: 2026-01-26T06:16:53+00:00
Venue: arXiv
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Li; Shiying Wang; Jianqiang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Conditioned Reasoning in Long-Egocentric Videos</title><link>https://arxiv.org/abs/2601.18100v1</link><guid>http://arxiv.org/abs/2601.18100v1</guid><pubDate>Mon, 26 Jan 2026 03:21:35 +0000</pubDate><dc:creator>James Tribble</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Si-En Hong</dc:creator><dc:creator>Chaoyi Zhou</dc:creator><dc:creator>Ashish Bastola</dc:creator><dc:creator>Siyu Huang</dc:creator><dc:creator>Abolfazl Razi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.
Published: 2026-01-26T03:21:35+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James Tribble; Hao Wang; Si-En Hong; Chaoyi Zhou; Ashish Bastola; Siyu Huang; Abolfazl Razi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.&lt;/p&gt;</content:encoded></item><item><title>DeepSeek-OCR 2: Visual Causal Flow</title><link>https://arxiv.org/abs/2601.20552v1</link><guid>http://arxiv.org/abs/2601.20552v1</guid><pubDate>Wed, 28 Jan 2026 12:46:07 +0000</pubDate><dc:creator>Haoran Wei</dc:creator><dc:creator>Yaofeng Sun</dc:creator><dc:creator>Yukun Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.
Published: 2026-01-28T12:46:07+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Wei; Yaofeng Sun; Yukun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.&lt;/p&gt;</content:encoded></item><item><title>m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</title><link>https://arxiv.org/abs/2601.19099v1</link><guid>http://arxiv.org/abs/2601.19099v1</guid><pubDate>Tue, 27 Jan 2026 02:01:56 +0000</pubDate><dc:creator>Yosub Shin</dc:creator><dc:creator>Michael Buriek</dc:creator><dc:creator>Igor Molybog</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.
Published: 2026-01-27T02:01:56+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yosub Shin; Michael Buriek; Igor Molybog&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Interpretable Image Recognition Network via Language-Guided Global-Local Collaboratively Alignment</title><link>https://doi.org/10.1016/j.knosys.2026.115422</link><guid>10.1016/j.knosys.2026.115422</guid><pubDate>Wed, 28 Jan 2026 07:55:11 +0000</pubDate><dc:creator>Sulan Zhang</dc:creator><dc:creator>Peijun Zhang</dc:creator><dc:creator>Lihua Hu</dc:creator><dc:creator>Xin Wen</dc:creator><dc:creator>Jifu Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115422</prism:doi><description>Interpretability is crucial for establishing user trust in image recognition models in high-risk domains such as medical diagnosis and autonomous driving. Recent studies have enhanced model interpretability through visual-language alignment.However, existing methods mostly rely on coarse-grained alignment between overall image representations and semantic concepts, making it difficult to achieve fine-grained interactions between local visual regions and semantic concepts. This limitation restricts further improvements in practical interpretability and recognition performance. To address this issue, we propose a Language-Guided Global-Local Collaboratively Aligned Multimodal Interpretable Image Recognition Network (LGLCA-Net), which uses text concepts generated by large language models (LLMs) to guide the collaborative alignment of images and text in terms of global and local semantics.The network first designs a concept de-redundancy and visual recognizability verification strategy, driving the large language model to generate high-quality and visually relevant semantic concepts as the foundation for language guidance. Subsequently, we utilize the multimodal space provided by the CLIP model to construct a dual-branch alignment structure for global semantics and local visual semantics.In the local visual semantics branch, we introduce a visual prompting mechanism that extracts discriminative local regions to achieve fine-grained alignment with semantic concepts. Finally, we design a learnable dynamic weighting mechanism to adaptively fuse the alignment information from both branches, achieving collaborative alignment of global-local semantics and semantic concepts. Extensive experiments show that our method not only provides finer-grained and more trustworthy visual explanations but also improves recognition performance.
Published: 2026-01-28T07:55:11+00:00
Venue: Knowledge-Based Systems
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sulan Zhang; Peijun Zhang; Lihua Hu; Xin Wen; Jifu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115422"&gt;10.1016/j.knosys.2026.115422&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is crucial for establishing user trust in image recognition models in high-risk domains such as medical diagnosis and autonomous driving. Recent studies have enhanced model interpretability through visual-language alignment.However, existing methods mostly rely on coarse-grained alignment between overall image representations and semantic concepts, making it difficult to achieve fine-grained interactions between local visual regions and semantic concepts. This limitation restricts further improvements in practical interpretability and recognition performance. To address this issue, we propose a Language-Guided Global-Local Collaboratively Aligned Multimodal Interpretable Image Recognition Network (LGLCA-Net), which uses text concepts generated by large language models (LLMs) to guide the collaborative alignment of images and text in terms of global and local semantics.The network first designs a concept de-redundancy and visual recognizability verification strategy, driving the large language model to generate high-quality and visually relevant semantic concepts as the foundation for language guidance. Subsequently, we utilize the multimodal space provided by the CLIP model to construct a dual-branch alignment structure for global semantics and local visual semantics.In the local visual semantics branch, we introduce a visual prompting mechanism that extracts discriminative local regions to achieve fine-grained alignment with semantic concepts. Finally, we design a learnable dynamic weighting mechanism to adaptively fuse the alignment information from both branches, achieving collaborative alignment of global-local semantics and semantic concepts. Extensive experiments show that our method not only provides finer-grained and more trustworthy visual explanations but also improves recognition performance.&lt;/p&gt;</content:encoded></item><item><title>DuoNet: Joint Optimization of Representation Learning and Prototype Classifier for Unbiased Scene Graph Generation</title><link>https://doi.org/10.1016/j.patcog.2026.113152</link><guid>10.1016/j.patcog.2026.113152</guid><pubDate>Tue, 27 Jan 2026 16:59:22 +0000</pubDate><dc:creator>Zhaodi Wang</dc:creator><dc:creator>Biao Leng</dc:creator><dc:creator>Shuo Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113152</prism:doi><description>Unbiased Scene Graph Generation (SGG) aims to parse visual scenes into highly informative graphs under the long-tail challenge. While prototype-based methods have shown promise in unbiased SGG, they highlight the importance of learning discriminative features that are intra-class compact and inter-class separable. In this paper, we revisit prototype-based methods and analyze critical roles of representation learning and prototype classifier in driving unbiased SGG, and accordingly propose a novel framework DuoNet. To enhance intra-class compactness, we introduce a Bi-Directional Representation Refinement (BiDR 2 ) module that captures relation-sensitive visual variability and within-relation visual consistency of entities. This module adopts relation-to-entity-to-relation refinement by integrating dual-level relation pattern modeling with a relation-specific entity constraint. Furthermore, a Knowledge-Guided Prototype Learning (KGPL) module is devised to strengthen inter-class separability by constructing an equidistributed prototypical classifier with maximum inter-class margins. The equidistributed prototype classifier is frozen during SGG training to mitigate long-tail bias, thus a knowledge-driven triplet loss is developed to strengthen the learning of BiDR 2 , enhancing relation-prototype matching. Extensive experiments demonstrate the effectiveness of our method, which sets new state-of-the-art performance on Visual Genome, GQA and Open Images datasets.
Published: 2026-01-27T16:59:22+00:00
Venue: Pattern Recognition
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaodi Wang; Biao Leng; Shuo Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113152"&gt;10.1016/j.patcog.2026.113152&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Unbiased Scene Graph Generation (SGG) aims to parse visual scenes into highly informative graphs under the long-tail challenge. While prototype-based methods have shown promise in unbiased SGG, they highlight the importance of learning discriminative features that are intra-class compact and inter-class separable. In this paper, we revisit prototype-based methods and analyze critical roles of representation learning and prototype classifier in driving unbiased SGG, and accordingly propose a novel framework DuoNet. To enhance intra-class compactness, we introduce a Bi-Directional Representation Refinement (BiDR 2 ) module that captures relation-sensitive visual variability and within-relation visual consistency of entities. This module adopts relation-to-entity-to-relation refinement by integrating dual-level relation pattern modeling with a relation-specific entity constraint. Furthermore, a Knowledge-Guided Prototype Learning (KGPL) module is devised to strengthen inter-class separability by constructing an equidistributed prototypical classifier with maximum inter-class margins. The equidistributed prototype classifier is frozen during SGG training to mitigate long-tail bias, thus a knowledge-driven triplet loss is developed to strengthen the learning of BiDR 2 , enhancing relation-prototype matching. Extensive experiments demonstrate the effectiveness of our method, which sets new state-of-the-art performance on Visual Genome, GQA and Open Images datasets.&lt;/p&gt;</content:encoded></item><item><title>RMNet: Dual-Dimensional Difference Recalibration-Guided CNN-VMamba Synergistic Network for Remote Sensing Image Change Captioning</title><link>https://doi.org/10.1109/tgrs.2026.3658213</link><guid>10.1109/tgrs.2026.3658213</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Xintong Cao</dc:creator><dc:creator>Wenqian Dong</dc:creator><dc:creator>Jiahui Qu</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658213</prism:doi><description>Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xintong Cao; Wenqian Dong; Jiahui Qu; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658213"&gt;10.1109/tgrs.2026.3658213&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.&lt;/p&gt;</content:encoded></item><item><title>Agentic Very Long Video Understanding</title><link>https://arxiv.org/abs/2601.18157v1</link><guid>http://arxiv.org/abs/2601.18157v1</guid><pubDate>Mon, 26 Jan 2026 05:20:47 +0000</pubDate><dc:creator>Aniket Rege</dc:creator><dc:creator>Arka Sadhu</dc:creator><dc:creator>Yuliang Li</dc:creator><dc:creator>Kejie Li</dc:creator><dc:creator>Ramya Korlakai Vinayak</dc:creator><dc:creator>Yuning Chai</dc:creator><dc:creator>Yong Jae Lee</dc:creator><dc:creator>Hyo Jin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.
Published: 2026-01-26T05:20:47+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aniket Rege; Arka Sadhu; Yuliang Li; Kejie Li; Ramya Korlakai Vinayak; Yuning Chai; Yong Jae Lee; Hyo Jin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.&lt;/p&gt;</content:encoded></item><item><title>LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge</title><link>https://arxiv.org/abs/2601.19155v1</link><guid>http://arxiv.org/abs/2601.19155v1</guid><pubDate>Tue, 27 Jan 2026 03:40:03 +0000</pubDate><dc:creator>Qiujun Li</dc:creator><dc:creator>Zijin Xiao</dc:creator><dc:creator>Xulin Wang</dc:creator><dc:creator>Zhidan Ma</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Haifeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.
Published: 2026-01-27T03:40:03+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiujun Li; Zijin Xiao; Xulin Wang; Zhidan Ma; Cheng Yang; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.&lt;/p&gt;</content:encoded></item><item><title>See then tell: Enhancing key information extraction with vision grounding</title><link>https://doi.org/10.1016/j.neucom.2026.132858</link><guid>10.1016/j.neucom.2026.132858</guid><pubDate>Tue, 27 Jan 2026 07:44:13 +0000</pubDate><dc:creator>Shuhang Liu</dc:creator><dc:creator>Zhenrong Zhang</dc:creator><dc:creator>Pengfei Hu</dc:creator><dc:creator>Jiefeng Ma</dc:creator><dc:creator>Jun Du</dc:creator><dc:creator>Qing Wang</dc:creator><dc:creator>Jianshu Zhang</dc:creator><dc:creator>Chenyu Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132858</prism:doi><description>In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( " role="presentation"&gt; ee then " role="presentation"&gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel " role="presentation"&gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, " role="presentation"&gt; directs the model to first " role="presentation"&gt;  attending to image regions relevant to the question  and then " role="presentation"&gt; , emitting the textual answer. To enhance the models " role="presentation"&gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( " role="presentation"&gt; ableQA with " role="presentation"&gt; ision " role="presentation"&gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.
Published: 2026-01-27T07:44:13+00:00
Venue: Neurocomputing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuhang Liu; Zhenrong Zhang; Pengfei Hu; Jiefeng Ma; Jun Du; Qing Wang; Jianshu Zhang; Chenyu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132858"&gt;10.1016/j.neucom.2026.132858&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ee then &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; directs the model to first &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt;  attending to image regions relevant to the question  and then &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; , emitting the textual answer. To enhance the models &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ableQA with &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ision &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>Learning From Videos Through Graph-to-Graphs Generative Modeling for Robotic Manipulation</title><link>https://doi.org/10.1109/tro.2026.3658211</link><guid>10.1109/tro.2026.3658211</guid><pubDate>Tue, 27 Jan 2026 20:31:39 +0000</pubDate><dc:creator>Guangyan Chen</dc:creator><dc:creator>Meiling Wang</dc:creator><dc:creator>Te Cui</dc:creator><dc:creator>Chengcai Yang</dc:creator><dc:creator>Mengxiao Hu</dc:creator><dc:creator>Haoyang Lu</dc:creator><dc:creator>Zicai Peng</dc:creator><dc:creator>Tianxing Zhou</dc:creator><dc:creator>Xinran Jiang</dc:creator><dc:creator>Yi Yang</dc:creator><dc:creator>Yufeng Yue</dc:creator><prism:publicationName>IEEE Transactions on Robotics</prism:publicationName><prism:doi>10.1109/tro.2026.3658211</prism:doi><description>Learning from demonstration is a powerful method for robotic skill acquisition. Nevertheless, a critical limitation lies in the substantial costs associated with gathering demonstration datasets, typically action-labeled robot data, which creates a fundamental constraint in the field. Video data offer a compelling solution as an alternative rich data source, containing diverse behavioral and physical knowledge. This study introduces G3M, an innovative framework that exploits video data via Graph-to-Graphs Generative Modeling, which pre-trains models to generate future graphs conditioned on the graph within a video frame. The proposed G3M abstracts video frame into graph representations by identifying object and visual action vertices for capturing state information. It then effectively models internal structures and spatial relationships present in these graph constructions, with the objective of predicting forthcoming graphs. The generated graphs function as conditional inputs that guide the control policy in determining robotic behaviors. This concise method effectively encodes critical spatial relationships while facilitating accurate prediction of subsequent graph sequences, thus allowing the development of resilient control policy despite constraints in action-annotated training samples. Furthermore, these transferable graph representations enable the effective extraction of manipulation knowledge through human videos as well as recordings from robots with different embodiments. The experimental results demonstrate that G3M attains superior performance using merely 20% action-labeled data relative to comparable approaches. Moreover, our method outperforms the state-of-the-art method, showing performance gains exceeding 19% in simulated environments and 23% in real-world experiments, while delivering improvements of over 35% in cross-embodiment transfer experiments and exhibiting strong performance on long-horizon tasks. Our project page is available at https://...
Published: 2026-01-27T20:31:39+00:00
Venue: IEEE Transactions on Robotics
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangyan Chen; Meiling Wang; Te Cui; Chengcai Yang; Mengxiao Hu; Haoyang Lu; Zicai Peng; Tianxing Zhou; Xinran Jiang; Yi Yang; Yufeng Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Robotics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tro.2026.3658211"&gt;10.1109/tro.2026.3658211&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Learning from demonstration is a powerful method for robotic skill acquisition. Nevertheless, a critical limitation lies in the substantial costs associated with gathering demonstration datasets, typically action-labeled robot data, which creates a fundamental constraint in the field. Video data offer a compelling solution as an alternative rich data source, containing diverse behavioral and physical knowledge. This study introduces G3M, an innovative framework that exploits video data via Graph-to-Graphs Generative Modeling, which pre-trains models to generate future graphs conditioned on the graph within a video frame. The proposed G3M abstracts video frame into graph representations by identifying object and visual action vertices for capturing state information. It then effectively models internal structures and spatial relationships present in these graph constructions, with the objective of predicting forthcoming graphs. The generated graphs function as conditional inputs that guide the control policy in determining robotic behaviors. This concise method effectively encodes critical spatial relationships while facilitating accurate prediction of subsequent graph sequences, thus allowing the development of resilient control policy despite constraints in action-annotated training samples. Furthermore, these transferable graph representations enable the effective extraction of manipulation knowledge through human videos as well as recordings from robots with different embodiments. The experimental results demonstrate that G3M attains superior performance using merely 20% action-labeled data relative to comparable approaches. Moreover, our method outperforms the state-of-the-art method, showing performance gains exceeding 19% in simulated environments and 23% in real-world experiments, while delivering improvements of over 35% in cross-embodiment transfer experiments and exhibiting strong performance on long-horizon tasks. Our project page is available at https://...&lt;/p&gt;</content:encoded></item><item><title>DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.20064v1</link><guid>http://arxiv.org/abs/2601.20064v1</guid><pubDate>Tue, 27 Jan 2026 21:15:10 +0000</pubDate><dc:creator>Zhen Yao</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Taotao Jing</dc:creator><dc:creator>Shuai Zhang</dc:creator><dc:creator>Mooi Choo Chuah</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.
Published: 2026-01-27T21:15:10+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Yao; Xin Li; Taotao Jing; Shuai Zhang; Mooi Choo Chuah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</title><link>https://arxiv.org/abs/2601.19433v1</link><guid>http://arxiv.org/abs/2601.19433v1</guid><pubDate>Tue, 27 Jan 2026 10:10:55 +0000</pubDate><dc:creator>Jisheng Chu</dc:creator><dc:creator>Wenrui Li</dc:creator><dc:creator>Rui Zhao</dc:creator><dc:creator>Wangmeng Zuo</dc:creator><dc:creator>Shifeng Chen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.
Published: 2026-01-27T10:10:55+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Chu; Wenrui Li; Rui Zhao; Wangmeng Zuo; Shifeng Chen; Xiaopeng Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.&lt;/p&gt;</content:encoded></item><item><title>ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning</title><link>https://arxiv.org/abs/2601.17818v1</link><guid>http://arxiv.org/abs/2601.17818v1</guid><pubDate>Sun, 25 Jan 2026 12:47:30 +0000</pubDate><dc:creator>Wen Luo</dc:creator><dc:creator>Peng Chen</dc:creator><dc:creator>Xiaotao Huang</dc:creator><dc:creator>LiQun Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.
Published: 2026-01-25T12:47:30+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wen Luo; Peng Chen; Xiaotao Huang; LiQun Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.&lt;/p&gt;</content:encoded></item><item><title>VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</title><link>https://arxiv.org/abs/2601.19887v1</link><guid>http://arxiv.org/abs/2601.19887v1</guid><pubDate>Tue, 27 Jan 2026 18:54:29 +0000</pubDate><dc:creator>Dominic Maggio</dc:creator><dc:creator>Luca Carlone</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.
Published: 2026-01-27T18:54:29+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dominic Maggio; Luca Carlone&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.&lt;/p&gt;</content:encoded></item><item><title>Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning</title><link>https://arxiv.org/abs/2601.18356v1</link><guid>http://arxiv.org/abs/2601.18356v1</guid><pubDate>Mon, 26 Jan 2026 11:03:00 +0000</pubDate><dc:creator>Weiqin Yang</dc:creator><dc:creator>Haowen Xue</dc:creator><dc:creator>Qingyi Peng</dc:creator><dc:creator>Hexuan Hu</dc:creator><dc:creator>Qian Huang</dc:creator><dc:creator>Tingbo Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.
Published: 2026-01-26T11:03:00+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiqin Yang; Haowen Xue; Qingyi Peng; Hexuan Hu; Qian Huang; Tingbo Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.&lt;/p&gt;</content:encoded></item><item><title>Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing</title><link>https://arxiv.org/abs/2601.20107v1</link><guid>http://arxiv.org/abs/2601.20107v1</guid><pubDate>Tue, 27 Jan 2026 22:50:11 +0000</pubDate><dc:creator>Zhuchenyang Liu</dc:creator><dc:creator>Ziyu Hu</dc:creator><dc:creator>Yao Zhang</dc:creator><dc:creator>Yu Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.
Published: 2026-01-27T22:50:11+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuchenyang Liu; Ziyu Hu; Yao Zhang; Yu Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&amp;gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.&lt;/p&gt;</content:encoded></item><item><title>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</title><link>https://arxiv.org/abs/2601.17866v1</link><guid>http://arxiv.org/abs/2601.17866v1</guid><pubDate>Sun, 25 Jan 2026 15:00:37 +0000</pubDate><dc:creator>Yoonwoo Jeong</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.
Published: 2026-01-25T15:00:37+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yoonwoo Jeong; Cheng Sun; Yu-Chiang Frank Wang; Minsu Cho; Jaesung Choe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization</title><link>https://arxiv.org/abs/2601.20355v1</link><guid>http://arxiv.org/abs/2601.20355v1</guid><pubDate>Wed, 28 Jan 2026 08:15:56 +0000</pubDate><dc:creator>Yue Liang</dc:creator><dc:creator>Jiatong Du</dc:creator><dc:creator>Ziyi Yang</dc:creator><dc:creator>Yanjun Huang</dc:creator><dc:creator>Hong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.
Published: 2026-01-28T08:15:56+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Liang; Jiatong Du; Ziyi Yang; Yanjun Huang; Hong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.&lt;/p&gt;</content:encoded></item><item><title>Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework</title><link>https://arxiv.org/abs/2601.19640v1</link><guid>http://arxiv.org/abs/2601.19640v1</guid><pubDate>Tue, 27 Jan 2026 14:17:04 +0000</pubDate><dc:creator>Hao Chang</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Lingxiang Wu</dc:creator><dc:creator>Peijin Wang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Jinqiao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.
Published: 2026-01-27T14:17:04+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Chang; Zhihui Wang; Lingxiang Wu; Peijin Wang; Wenhui Diao; Jinqiao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.&lt;/p&gt;</content:encoded></item><item><title>Towards Pixel-Level VLM Perception via Simple Points Prediction</title><link>https://arxiv.org/abs/2601.19228v1</link><guid>http://arxiv.org/abs/2601.19228v1</guid><pubDate>Tue, 27 Jan 2026 05:50:40 +0000</pubDate><dc:creator>Tianhui Song</dc:creator><dc:creator>Haoyu Lu</dc:creator><dc:creator>Hao Yang</dc:creator><dc:creator>Lin Sui</dc:creator><dc:creator>Haoning Wu</dc:creator><dc:creator>Zaida Zhou</dc:creator><dc:creator>Zhiqi Huang</dc:creator><dc:creator>Yiping Bao</dc:creator><dc:creator>Y. Charles</dc:creator><dc:creator>Xinyu Zhou</dc:creator><dc:creator>Limin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/
Published: 2026-01-27T05:50:40+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianhui Song; Haoyu Lu; Hao Yang; Lin Sui; Haoning Wu; Zaida Zhou; Zhiqi Huang; Yiping Bao; Y. Charles; Xinyu Zhou; Limin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/&lt;/p&gt;</content:encoded></item><item><title>DBFNM: Dual-Branch Fusion Network with Mamba Decoder for Indoor Depth Completion</title><link>https://doi.org/10.1109/tcsvt.2026.3657718</link><guid>10.1109/tcsvt.2026.3657718</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Yujie Diao</dc:creator><dc:creator>Zhisheng Wang</dc:creator><dc:creator>Jiayu Fan</dc:creator><dc:creator>Yuhua Cong</dc:creator><dc:creator>Quan Ouyang</dc:creator><dc:creator>Xin Tang</dc:creator><dc:creator>Rufei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657718</prism:doi><description>Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujie Diao; Zhisheng Wang; Jiayu Fan; Yuhua Cong; Quan Ouyang; Xin Tang; Rufei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657718"&gt;10.1109/tcsvt.2026.3657718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.&lt;/p&gt;</content:encoded></item><item><title>MC-MVSNet: When Multi-View Stereo meets Monocular Cues</title><link>https://doi.org/10.1016/j.patcog.2026.113166</link><guid>10.1016/j.patcog.2026.113166</guid><pubDate>Wed, 28 Jan 2026 00:23:45 +0000</pubDate><dc:creator>Xincheng Tang</dc:creator><dc:creator>Mengqi Rong</dc:creator><dc:creator>Bin Fan</dc:creator><dc:creator>Hongmin Liu</dc:creator><dc:creator>Shuhan Shen</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113166</prism:doi><description>Learning-based Multi-View Stereo (MVS) has become a key technique for reconstructing dense 3D point clouds from multiple calibrated images. However, real-world challenges such as occlusions and textureless regions often hinder accurate depth estimation. Recent advances in monocular Vision Foundation Models (VFMs) have demonstrated strong generalization capabilities in scene understanding, offering new opportunities to enhance the robustness of MVS. In this paper, we present MC-MVSNet, a novel MVS framework that integrates diverse monocular cues to improve depth estimation under challenging conditions. During feature extraction, we fuse conventional CNN features with VFM-derived representations through a hybrid feature fusion module, effectively combining local details and global context for more discriminative feature matching. We also propose a cost volume filtering module that enforces cross-view geometric consistency on monocular depth predictions, pruning redundant depth hypotheses to reduce the depth search space and mitigate matching ambiguity. Additionally, we leverage monocular surface normals to construct a curved patch cost aggregation module that aggregates costs over geometry-aligned curved patches, which improves depth estimation accuracy in curved and textureless regions. Extensive experiments on the DTU, Tanks and Temples, and ETH3D benchmarks demonstrate that MC-MVSNet achieves state-of-the-art performance and exhibits strong generalization capabilities, validating the effectiveness and robustness of the proposed method.
Published: 2026-01-28T00:23:45+00:00
Venue: Pattern Recognition
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xincheng Tang; Mengqi Rong; Bin Fan; Hongmin Liu; Shuhan Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113166"&gt;10.1016/j.patcog.2026.113166&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Learning-based Multi-View Stereo (MVS) has become a key technique for reconstructing dense 3D point clouds from multiple calibrated images. However, real-world challenges such as occlusions and textureless regions often hinder accurate depth estimation. Recent advances in monocular Vision Foundation Models (VFMs) have demonstrated strong generalization capabilities in scene understanding, offering new opportunities to enhance the robustness of MVS. In this paper, we present MC-MVSNet, a novel MVS framework that integrates diverse monocular cues to improve depth estimation under challenging conditions. During feature extraction, we fuse conventional CNN features with VFM-derived representations through a hybrid feature fusion module, effectively combining local details and global context for more discriminative feature matching. We also propose a cost volume filtering module that enforces cross-view geometric consistency on monocular depth predictions, pruning redundant depth hypotheses to reduce the depth search space and mitigate matching ambiguity. Additionally, we leverage monocular surface normals to construct a curved patch cost aggregation module that aggregates costs over geometry-aligned curved patches, which improves depth estimation accuracy in curved and textureless regions. Extensive experiments on the DTU, Tanks and Temples, and ETH3D benchmarks demonstrate that MC-MVSNet achieves state-of-the-art performance and exhibits strong generalization capabilities, validating the effectiveness and robustness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Video-KTR: Reinforcing Video Reasoning via Key Token Attribution</title><link>https://arxiv.org/abs/2601.19686v1</link><guid>http://arxiv.org/abs/2601.19686v1</guid><pubDate>Tue, 27 Jan 2026 15:02:23 +0000</pubDate><dc:creator>Ziyue Wang</dc:creator><dc:creator>Sheng Jin</dc:creator><dc:creator>Zhongrong Zuo</dc:creator><dc:creator>Jiawei Wu</dc:creator><dc:creator>Han Qiu</dc:creator><dc:creator>Qi She</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Xudong Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.
Published: 2026-01-27T15:02:23+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyue Wang; Sheng Jin; Zhongrong Zuo; Jiawei Wu; Han Qiu; Qi She; Hao Zhang; Xudong Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.&lt;/p&gt;</content:encoded></item></channel></rss>