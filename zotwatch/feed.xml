<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 04 Feb 2026 03:37:27 +0000</lastBuildDate><item><title>Bidirectional Cross-Modal Collaborative Alignment via Semantic-Guided Visual Embeddings for Partially Relevant Video Retrieval</title><link>https://doi.org/10.1109/tip.2026.3658218</link><guid>10.1109/tip.2026.3658218</guid><pubDate>Mon, 02 Feb 2026 20:46:59 +0000</pubDate><dc:creator>Huafeng Li</dc:creator><dc:creator>Jialong Zhao</dc:creator><dc:creator>Yafei Zhang</dc:creator><dc:creator>Jie Wen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3658218</prism:doi><description>Partially Relevant Video Retrieval (PRVR) aims to retrieve videos that match a given textual query only partially. This task is inherently challenging due to the modality gap between text and video, which is further exacerbated by the partial semantic correspondence between linguistic descriptions and visual content. To address these challenges, we propose a bidirectional cross-modal alignment mechanism that collaboratively optimizes both visual and textual modalities. In the visual modality, a major difficulty lies in the absence of visual cues that directly correspond to textual semantics, limiting the models ability to align visual representations with textual meanings under unsupervised conditions. To overcome this issue, we construct a semantic-visual association library, which stores paired visual and textual features with semantic annotations. During training, the model dynamically retrieves the most semantically similar visual samples from this library based on the current visual feature vector. These retrieved samples, preliminarily associated with semantics via cross-modal matching, are used to form dynamic anchors that guide visual representation learning. By leveraging these enriched visual features, the model progressively refines the visual representations to achieve better alignment with the corresponding textual inputs, thereby enhancing cross-modal consistency. In the textual modality, we enhance textual representations by integrating semantically aligned visual features selected from the same association library, further narrowing the modality gap. Extensive experiments on benchmark datasets under partial semantic correspondence scenarios demonstrate that our method achieves state-of-the-art performance. The source code of the paper is available at https://github.com/cyanlll/BOA.
Published: 2026-02-02T20:46:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.600 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huafeng Li; Jialong Zhao; Yafei Zhang; Jie Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3658218"&gt;10.1109/tip.2026.3658218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.600 (must_read)&lt;/p&gt;
&lt;p&gt;Partially Relevant Video Retrieval (PRVR) aims to retrieve videos that match a given textual query only partially. This task is inherently challenging due to the modality gap between text and video, which is further exacerbated by the partial semantic correspondence between linguistic descriptions and visual content. To address these challenges, we propose a bidirectional cross-modal alignment mechanism that collaboratively optimizes both visual and textual modalities. In the visual modality, a major difficulty lies in the absence of visual cues that directly correspond to textual semantics, limiting the models ability to align visual representations with textual meanings under unsupervised conditions. To overcome this issue, we construct a semantic-visual association library, which stores paired visual and textual features with semantic annotations. During training, the model dynamically retrieves the most semantically similar visual samples from this library based on the current visual feature vector. These retrieved samples, preliminarily associated with semantics via cross-modal matching, are used to form dynamic anchors that guide visual representation learning. By leveraging these enriched visual features, the model progressively refines the visual representations to achieve better alignment with the corresponding textual inputs, thereby enhancing cross-modal consistency. In the textual modality, we enhance textual representations by integrating semantically aligned visual features selected from the same association library, further narrowing the modality gap. Extensive experiments on benchmark datasets under partial semantic correspondence scenarios demonstrate that our method achieves state-of-the-art performance. The source code of the paper is available at https://github.com/cyanlll/BOA.&lt;/p&gt;</content:encoded></item><item><title>Multi-Modal Refined Prompting for Advancing Knowledge-Based Visual Question Answering</title><link>https://doi.org/10.1109/tmm.2026.3660119</link><guid>10.1109/tmm.2026.3660119</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Lei Zhu</dc:creator><dc:creator>Mengxi Ying</dc:creator><dc:creator>Chengyuan Zhang</dc:creator><dc:creator>Deyin Liu</dc:creator><dc:creator>Lin Wu Shichao Zhang</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660119</prism:doi><description>Knowledge-based Visual Question Answering (KB-VQA) has surfaced as a critical task in advancing AI capabilities. Despite significant progress enabled by large language models (LLMs), there are still three major challenges: (1) flawed image captions cause unreliable reasoning; (2) noisy explicit knowledge can disrupts answering; and (3) massive LLMs scale is irreplaceable to robustness. To overcome these challenges, we develop a novel approach, Multi-Modal Refined Prompting (MMRP), which generates high-quality prompts tailored for LLMs. To tackle the first challenge, a multi-faceted image captioning strategy is employed to generate detailed, contextually relevant visual descriptions. In addition, we introduce a complementary knowledge retrieval and refinement strategy to deliver concise, contextually relevant knowledge, effectively overcoming the second challenge. These enhanced image captions and explicit knowledge are then integrated into a knowledge-infused in-context prompt, effectively activating the reasoning capabilities of LLMs. Importantly, MMRP eliminates reliance on massive LLMs and avoids the need for model fine-tuning, while achieving significant improvements in answer accuracy. Extensive evaluations on the widely-used OK-VQA benchmark against 22 baselines prove the superiority of MMRP, establishing a new state-of-the-art in KB-VQA.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.586 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lei Zhu; Mengxi Ying; Chengyuan Zhang; Deyin Liu; Lin Wu Shichao Zhang; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660119"&gt;10.1109/tmm.2026.3660119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.586 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge-based Visual Question Answering (KB-VQA) has surfaced as a critical task in advancing AI capabilities. Despite significant progress enabled by large language models (LLMs), there are still three major challenges: (1) flawed image captions cause unreliable reasoning; (2) noisy explicit knowledge can disrupts answering; and (3) massive LLMs scale is irreplaceable to robustness. To overcome these challenges, we develop a novel approach, Multi-Modal Refined Prompting (MMRP), which generates high-quality prompts tailored for LLMs. To tackle the first challenge, a multi-faceted image captioning strategy is employed to generate detailed, contextually relevant visual descriptions. In addition, we introduce a complementary knowledge retrieval and refinement strategy to deliver concise, contextually relevant knowledge, effectively overcoming the second challenge. These enhanced image captions and explicit knowledge are then integrated into a knowledge-infused in-context prompt, effectively activating the reasoning capabilities of LLMs. Importantly, MMRP eliminates reliance on massive LLMs and avoids the need for model fine-tuning, while achieving significant improvements in answer accuracy. Extensive evaluations on the widely-used OK-VQA benchmark against 22 baselines prove the superiority of MMRP, establishing a new state-of-the-art in KB-VQA.&lt;/p&gt;</content:encoded></item><item><title>LaVPR: Benchmarking Language and Vision for Place Recognition</title><link>https://arxiv.org/abs/2602.03253v1</link><guid>http://arxiv.org/abs/2602.03253v1</guid><pubDate>Tue, 03 Feb 2026 08:38:38 +0000</pubDate><dc:creator>Ofer Idan</dc:creator><dc:creator>Dan Badur</dc:creator><dc:creator>Yosi Keller</dc:creator><dc:creator>Yoli Shavit</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.
Published: 2026-02-03T08:38:38+00:00
Venue: arXiv
Score: 0.575 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ofer Idan; Dan Badur; Yosi Keller; Yoli Shavit&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.575 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &amp;quot;blind&amp;quot; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.&lt;/p&gt;</content:encoded></item><item><title>VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning</title><link>https://arxiv.org/abs/2602.00637v1</link><guid>http://arxiv.org/abs/2602.00637v1</guid><pubDate>Sat, 31 Jan 2026 10:11:27 +0000</pubDate><dc:creator>Vivek Madhavaram</dc:creator><dc:creator>Vartika Sengar</dc:creator><dc:creator>Arkadipta De</dc:creator><dc:creator>Charu Sharma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.
Published: 2026-01-31T10:11:27+00:00
Venue: arXiv
Score: 0.573 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vivek Madhavaram; Vartika Sengar; Arkadipta De; Charu Sharma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.573 (consider)&lt;/p&gt;
&lt;p&gt;Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like &amp;quot;left/right&amp;quot;, which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object&amp;#x27;s front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>RegionReasoner: Region-Grounded Multi-Round Visual Reasoning</title><link>https://arxiv.org/abs/2602.03733v1</link><guid>http://arxiv.org/abs/2602.03733v1</guid><pubDate>Tue, 03 Feb 2026 16:52:16 +0000</pubDate><dc:creator>Wenfang Sun</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Yingjun Du</dc:creator><dc:creator>Yefeng Zheng</dc:creator><dc:creator>Cees G. M. Snoek</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.
Published: 2026-02-03T16:52:16+00:00
Venue: arXiv
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenfang Sun; Hao Chen; Yingjun Du; Yefeng Zheng; Cees G. M. Snoek&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.&lt;/p&gt;</content:encoded></item><item><title>Dual-Branch Collaborative Implicit-Explicit Mutual Learning for Weakly Supervised Video-Sentence Retrieval and Grounding</title><link>https://doi.org/10.1109/tmm.2026.3660172</link><guid>10.1109/tmm.2026.3660172</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Xiaoyan Yu</dc:creator><dc:creator>Mengzhao Wang</dc:creator><dc:creator>Huafeng Li</dc:creator><dc:creator>Yafei Zhang</dc:creator><dc:creator>Dapeng Tao</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660172</prism:doi><description>Video-Sentence Retrieval and Grounding (VSRG) task aims to retrieve the corresponding video from a video corpus based on a single sentence query and accurately localize the temporal boundary of the sentence within the video. However, employing an end-to-end joint optimization strategy faces a critical challenge in the VSRG task: a task conflict exists between the retrieval and grounding tasks. Specifically, the retrieval task focuses on global video-text matching, while the grounding task requires fine-grained alignment between a local video segment and the sentence query. To address this issue, we propose a Dual-Branch Collaborative Implicit-Explicit Mutual Learning (DCIML) framework. The framework adopts a dual-branch structure, where the retrieval branch is responsible for cross-modal video-text retrieval, and the grounding branch achieves precise temporal grounding based on the sentence query. Furthermore, we employ an iterative optimization strategy to train each task. Meanwhile, to promote collaborative optimization of the dual tasks, we design a dual-branch mutual learning module that facilitates cross-task knowledge transfer through bidirectional explicit and implicit pathways. The DCIML framework not only effectively resolves the task conflict between the retrieval and grounding tasks but also achieves collaborative optimization of the dual tasks. Experimental results demonstrate that DCIML performs excellently on the ActivityNet-Captions and Charades-STA datasets, validating its effectiveness. The code is available at https://github.com/X7J92/DCIML.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyan Yu; Mengzhao Wang; Huafeng Li; Yafei Zhang; Dapeng Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660172"&gt;10.1109/tmm.2026.3660172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Video-Sentence Retrieval and Grounding (VSRG) task aims to retrieve the corresponding video from a video corpus based on a single sentence query and accurately localize the temporal boundary of the sentence within the video. However, employing an end-to-end joint optimization strategy faces a critical challenge in the VSRG task: a task conflict exists between the retrieval and grounding tasks. Specifically, the retrieval task focuses on global video-text matching, while the grounding task requires fine-grained alignment between a local video segment and the sentence query. To address this issue, we propose a Dual-Branch Collaborative Implicit-Explicit Mutual Learning (DCIML) framework. The framework adopts a dual-branch structure, where the retrieval branch is responsible for cross-modal video-text retrieval, and the grounding branch achieves precise temporal grounding based on the sentence query. Furthermore, we employ an iterative optimization strategy to train each task. Meanwhile, to promote collaborative optimization of the dual tasks, we design a dual-branch mutual learning module that facilitates cross-task knowledge transfer through bidirectional explicit and implicit pathways. The DCIML framework not only effectively resolves the task conflict between the retrieval and grounding tasks but also achieves collaborative optimization of the dual tasks. Experimental results demonstrate that DCIML performs excellently on the ActivityNet-Captions and Charades-STA datasets, validating its effectiveness. The code is available at https://github.com/X7J92/DCIML.&lt;/p&gt;</content:encoded></item><item><title>Ranking Vision-Language Models in Fully Unlabeled Tasks</title><link>https://doi.org/10.1109/tmm.2026.3660133</link><guid>10.1109/tmm.2026.3660133</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Yuhe Ding</dc:creator><dc:creator>Bo Jiang</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Qin Xu</dc:creator><dc:creator>Jian Liang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660133</prism:doi><description>Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on supervised auxiliary datasets and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of unsupervised vision-language model selection, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs' performance on unlabeled downstream tasks.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhe Ding; Bo Jiang; Aihua Zheng; Qin Xu; Jian Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660133"&gt;10.1109/tmm.2026.3660133&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on supervised auxiliary datasets and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of unsupervised vision-language model selection, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs&amp;#x27; performance on unlabeled downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Visible-guided Multigranularity Prompt Learning for Visible-Infrared Person Re-identification</title><link>https://doi.org/10.1016/j.eswa.2026.131464</link><guid>10.1016/j.eswa.2026.131464</guid><pubDate>Tue, 03 Feb 2026 00:32:33 +0000</pubDate><dc:creator>Yangyan Luo</dc:creator><dc:creator>Ying Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131464</prism:doi><description>Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.
Published: 2026-02-03T00:32:33+00:00
Venue: Expert Systems with Applications
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangyan Luo; Ying Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131464"&gt;10.1016/j.eswa.2026.131464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.&lt;/p&gt;</content:encoded></item><item><title>Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images</title><link>https://arxiv.org/abs/2602.01954v1</link><guid>http://arxiv.org/abs/2602.01954v1</guid><pubDate>Mon, 02 Feb 2026 11:03:01 +0000</pubDate><dc:creator>Shuai Yang</dc:creator><dc:creator>Ziyue Huang</dc:creator><dc:creator>Jiaxin Chen</dc:creator><dc:creator>Qingjie Liu</dc:creator><dc:creator>Yunhong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.
Published: 2026-02-02T11:03:01+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yang; Ziyue Huang; Jiaxin Chen; Qingjie Liu; Yunhong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.&lt;/p&gt;</content:encoded></item><item><title>SCALAR: Spatial-Concept Alignment for Robust Vision in Harsh Open World</title><link>https://doi.org/10.1016/j.patcog.2026.113203</link><guid>10.1016/j.patcog.2026.113203</guid><pubDate>Tue, 03 Feb 2026 00:28:30 +0000</pubDate><dc:creator>Xiaoyu Yang</dc:creator><dc:creator>Lijian Xu</dc:creator><dc:creator>Xingyu Zeng</dc:creator><dc:creator>Xiaosong Wang</dc:creator><dc:creator>Hongsheng Li</dc:creator><dc:creator>Shaoting Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113203</prism:doi><description>Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .
Published: 2026-02-03T00:28:30+00:00
Venue: Pattern Recognition
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyu Yang; Lijian Xu; Xingyu Zeng; Xiaosong Wang; Hongsheng Li; Shaoting Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113203"&gt;10.1016/j.patcog.2026.113203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .&lt;/p&gt;</content:encoded></item><item><title>Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation</title><link>https://arxiv.org/abs/2602.03595v1</link><guid>http://arxiv.org/abs/2602.03595v1</guid><pubDate>Tue, 03 Feb 2026 14:48:12 +0000</pubDate><dc:creator>Haichao Jiang</dc:creator><dc:creator>Tianming Liang</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Jian-Fang Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.
Published: 2026-02-03T14:48:12+00:00
Venue: arXiv
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haichao Jiang; Tianming Liang; Wei-Shi Zheng; Jian-Fang Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&amp;#x27;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences</title><link>https://arxiv.org/abs/2602.02974v1</link><guid>http://arxiv.org/abs/2602.02974v1</guid><pubDate>Tue, 03 Feb 2026 01:22:07 +0000</pubDate><dc:creator>Seok-Young Kim</dc:creator><dc:creator>Dooyoung Kim</dc:creator><dc:creator>Woojin Cho</dc:creator><dc:creator>Hail Song</dc:creator><dc:creator>Suji Kang</dc:creator><dc:creator>Woontack Woo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.
Published: 2026-02-03T01:22:07+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seok-Young Kim; Dooyoung Kim; Woojin Cho; Hail Song; Suji Kang; Woontack Woo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user&amp;#x27;s space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.&lt;/p&gt;</content:encoded></item><item><title>IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</title><link>https://arxiv.org/abs/2602.03060v1</link><guid>http://arxiv.org/abs/2602.03060v1</guid><pubDate>Tue, 03 Feb 2026 03:39:31 +0000</pubDate><dc:creator>Zhichao Sun</dc:creator><dc:creator>Yidong Ma</dc:creator><dc:creator>Gang Liu</dc:creator><dc:creator>Yibo Chen</dc:creator><dc:creator>Xu Tang</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Yongchao Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.
Published: 2026-02-03T03:39:31+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhichao Sun; Yidong Ma; Gang Liu; Yibo Chen; Xu Tang; Yao Hu; Yongchao Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.&lt;/p&gt;</content:encoded></item><item><title>Explicitly Learning Semantic Relevance for Salient Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1016/j.eswa.2026.131387</link><guid>10.1016/j.eswa.2026.131387</guid><pubDate>Mon, 02 Feb 2026 15:51:31 +0000</pubDate><dc:creator>Tao Gao</dc:creator><dc:creator>Weiguang Zhao</dc:creator><dc:creator>Mengkun Liu</dc:creator><dc:creator>Ting Chen</dc:creator><dc:creator>Ziqi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131387</prism:doi><description>Salient object detection in remote sensing images (RSI-SOD) is crucial for computer vision in both high-altitude and low-altitude scenarios. Most existing methods primarily focus on multiscale feature integration, yet they encounter difficulties in achieving precise segmentation, particularly when confronted with complex object topologies and cluttered backgrounds. To address this, we propose a novel framework, ELSRNet, tailored to capturing the intrinsic semantic differences among features with diverse attributes, thereby facilitating pixel-wise separation of salient regions. This approach incorporates the deployment of a Foreground-Background Semantic Perception module (FBSP), which explicitly scrutinizes the semantic interactions through a more comprehensive Attention Guided Loss, ultimately strengthening the capacity to learn objects with complex structural characteristics. Going further, considering that the coupling between noise norms and convolutional kernels in cluttered backgrounds may amplify irrelevant responses and lead to false saliency predictions, the Non-Matching Feature Enhancement block (NMFE) is introduced to suppress such interference based on matching scores, and further refine the features through a gating mechanism. Concluding the process, the Global Perceptual Feature Aggregation module (GPFA) is designed to decouple features into semantic and structural information. It achieves saliency region localization while preserving fine-grained boundaries, producing high-quality saliency detection results. Experimental results and theoretical analysis reveal that the proposed network outperforms existing methods in enhancing detection capabilities across three benchmark datasets.
Published: 2026-02-02T15:51:31+00:00
Venue: Expert Systems with Applications
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Gao; Weiguang Zhao; Mengkun Liu; Ting Chen; Ziqi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131387"&gt;10.1016/j.eswa.2026.131387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Salient object detection in remote sensing images (RSI-SOD) is crucial for computer vision in both high-altitude and low-altitude scenarios. Most existing methods primarily focus on multiscale feature integration, yet they encounter difficulties in achieving precise segmentation, particularly when confronted with complex object topologies and cluttered backgrounds. To address this, we propose a novel framework, ELSRNet, tailored to capturing the intrinsic semantic differences among features with diverse attributes, thereby facilitating pixel-wise separation of salient regions. This approach incorporates the deployment of a Foreground-Background Semantic Perception module (FBSP), which explicitly scrutinizes the semantic interactions through a more comprehensive Attention Guided Loss, ultimately strengthening the capacity to learn objects with complex structural characteristics. Going further, considering that the coupling between noise norms and convolutional kernels in cluttered backgrounds may amplify irrelevant responses and lead to false saliency predictions, the Non-Matching Feature Enhancement block (NMFE) is introduced to suppress such interference based on matching scores, and further refine the features through a gating mechanism. Concluding the process, the Global Perceptual Feature Aggregation module (GPFA) is designed to decouple features into semantic and structural information. It achieves saliency region localization while preserving fine-grained boundaries, producing high-quality saliency detection results. Experimental results and theoretical analysis reveal that the proposed network outperforms existing methods in enhancing detection capabilities across three benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>CompoVis: Is Cross-modal Semantic Alignment of CLIP Optimal? A Visual Analysis Attempt</title><link>https://doi.org/10.1109/tmm.2026.3660158</link><guid>10.1109/tmm.2026.3660158</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Tong Li</dc:creator><dc:creator>Guodao Sun</dc:creator><dc:creator>Xueqian Zheng</dc:creator><dc:creator>Qi Jiang</dc:creator><dc:creator>Wang Xia</dc:creator><dc:creator>Xu Tan</dc:creator><dc:creator>Haidong Gao</dc:creator><dc:creator>Haixia Wang</dc:creator><dc:creator>Ronghua Liang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660158</prism:doi><description>Vision-language pre-trained models (VLMs) have shown impressive cross-modal understanding, yet their “compositional understanding” ability remains under investigation. We introduce CompoVis, a framework for visually probing cross-modal gaps in VLMs. CompoVis optimizes the grid layout to highlight alignment clusters and boundaries, visually interprets multi-head attention and semantic drift, and enables interactive fine-tuning unconstrained by closed datasets or offline models. Quantitative experiments and case studies explore key insights: VLMs rely on entity shortcuts rather than comprehension-driven; stubborn global modality isolation and suboptimal fine-grained alignment remain; fine-tuning with negative samples does not fundamentally alleviate the gaps. Approximately 89% of participants ( n = 27 n=27 ) found that, compared to methods relying solely on data metrics, CompoVis offers a more innovative and effective approach for investigating modality gaps in VLMs.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Li; Guodao Sun; Xueqian Zheng; Qi Jiang; Wang Xia; Xu Tan; Haidong Gao; Haixia Wang; Ronghua Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660158"&gt;10.1109/tmm.2026.3660158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language pre-trained models (VLMs) have shown impressive cross-modal understanding, yet their “compositional understanding” ability remains under investigation. We introduce CompoVis, a framework for visually probing cross-modal gaps in VLMs. CompoVis optimizes the grid layout to highlight alignment clusters and boundaries, visually interprets multi-head attention and semantic drift, and enables interactive fine-tuning unconstrained by closed datasets or offline models. Quantitative experiments and case studies explore key insights: VLMs rely on entity shortcuts rather than comprehension-driven; stubborn global modality isolation and suboptimal fine-grained alignment remain; fine-tuning with negative samples does not fundamentally alleviate the gaps. Approximately 89% of participants ( n = 27 n=27 ) found that, compared to methods relying solely on data metrics, CompoVis offers a more innovative and effective approach for investigating modality gaps in VLMs.&lt;/p&gt;</content:encoded></item><item><title>MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement</title><link>https://arxiv.org/abs/2602.01760v1</link><guid>http://arxiv.org/abs/2602.01760v1</guid><pubDate>Mon, 02 Feb 2026 07:43:29 +0000</pubDate><dc:creator>Hao Zhang</dc:creator><dc:creator>Yanping Zha</dc:creator><dc:creator>Zizhuo Li</dc:creator><dc:creator>Meiqi Gong</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.
Published: 2026-02-02T07:43:29+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhang; Yanping Zha; Zizhuo Li; Meiqi Gong; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.&lt;/p&gt;</content:encoded></item><item><title>DGFFA: Joint Multimodal Entity-Relation Extraction via Dual-Channel Graph Fusion and Fine-Grained Alignment</title><link>https://doi.org/10.1016/j.knosys.2026.115470</link><guid>10.1016/j.knosys.2026.115470</guid><pubDate>Tue, 03 Feb 2026 00:09:05 +0000</pubDate><dc:creator>Wenjie Liu</dc:creator><dc:creator>Xingwen Li</dc:creator><dc:creator>Zhijie Ren</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115470</prism:doi><description>Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.
Published: 2026-02-03T00:09:05+00:00
Venue: Knowledge-Based Systems
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Liu; Xingwen Li; Zhijie Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115470"&gt;10.1016/j.knosys.2026.115470&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.&lt;/p&gt;</content:encoded></item><item><title>Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning</title><link>https://arxiv.org/abs/2602.02951v1</link><guid>http://arxiv.org/abs/2602.02951v1</guid><pubDate>Tue, 03 Feb 2026 00:51:03 +0000</pubDate><dc:creator>Yihong Huang</dc:creator><dc:creator>Fei Ma</dc:creator><dc:creator>Yihua Shao</dc:creator><dc:creator>Jingcai Guo</dc:creator><dc:creator>Zitong Yu</dc:creator><dc:creator>Laizhong Cui</dc:creator><dc:creator>Qi Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).
Published: 2026-02-03T00:51:03+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihong Huang; Fei Ma; Yihua Shao; Jingcai Guo; Zitong Yu; Laizhong Cui; Qi Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&amp;#x27;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&amp;#x27; positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).&lt;/p&gt;</content:encoded></item><item><title>Preserving Localized Patch Semantics in VLMs</title><link>https://arxiv.org/abs/2602.01530v1</link><guid>http://arxiv.org/abs/2602.01530v1</guid><pubDate>Mon, 02 Feb 2026 01:48:11 +0000</pubDate><dc:creator>Parsa Esmaeilkhani</dc:creator><dc:creator>Longin Jan Latecki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.
Published: 2026-02-02T01:48:11+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Parsa Esmaeilkhani; Longin Jan Latecki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word &amp;quot;cat&amp;quot;), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.&lt;/p&gt;</content:encoded></item><item><title>MDA-MAA: A Collaborative Augmentation Approach for Generalizing Cross-Domain Retrieval</title><link>https://doi.org/10.1109/tip.2026.3658223</link><guid>10.1109/tip.2026.3658223</guid><pubDate>Mon, 02 Feb 2026 20:46:59 +0000</pubDate><dc:creator>Ming Jin</dc:creator><dc:creator>Richang Hong</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3658223</prism:doi><description>In video-text cross-domain retrieval tasks, the generalization ability of the retrieval models is key to improving their performance and is crucial for enhancing their practical applicability. However, existing retrieval models exhibit significant deficiencies in cross-domain generalization. On one hand, models tend to overfit specific training domain data, resulting in poor cross-domain matching and significantly reduced retrieval accuracy when dealing with data from different, new, or mixed domains. On the other hand, although data augmentation is a vital strategy for enhancing model generalization, most existing methods focus on unimodal augmentation and fail to fully exploit the multimodal correlations between video and text. As a result, the augmented data lack semantic diversity, which further limits the model’s ability to understand and perform in complex cross-domain scenarios. To address these challenges, this paper proposes an innovative collaborative augmentation approach named MDA-MAA, which includes two core modules: the Masked Attention Augmentation (MAA) module and the Multimodal Diffusion Augmentation (MDA) module. The MAA module applies masking to the original video frame features and uses an attention mechanism to predict the masked features, effectively reducing overfitting to training data and enhancing model generalization. The MDA module generates subtitles from video frames and uses the LLaMA model to infer comprehensive video captions. These captions, combined with the original video frames, are integrated into a diffusion model for joint learning, ultimately generating semantically enriched augmented video frames. This process leverages the multimodal relationship between video and text to increase the diversity of the training data distribution. Experimental results demonstrate that this collaborative augmentation method significantly improves the performance of video-text cross-domain retrieval models, validating its effectiveness in enhan...
Published: 2026-02-02T20:46:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Jin; Richang Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3658223"&gt;10.1109/tip.2026.3658223&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;In video-text cross-domain retrieval tasks, the generalization ability of the retrieval models is key to improving their performance and is crucial for enhancing their practical applicability. However, existing retrieval models exhibit significant deficiencies in cross-domain generalization. On one hand, models tend to overfit specific training domain data, resulting in poor cross-domain matching and significantly reduced retrieval accuracy when dealing with data from different, new, or mixed domains. On the other hand, although data augmentation is a vital strategy for enhancing model generalization, most existing methods focus on unimodal augmentation and fail to fully exploit the multimodal correlations between video and text. As a result, the augmented data lack semantic diversity, which further limits the model’s ability to understand and perform in complex cross-domain scenarios. To address these challenges, this paper proposes an innovative collaborative augmentation approach named MDA-MAA, which includes two core modules: the Masked Attention Augmentation (MAA) module and the Multimodal Diffusion Augmentation (MDA) module. The MAA module applies masking to the original video frame features and uses an attention mechanism to predict the masked features, effectively reducing overfitting to training data and enhancing model generalization. The MDA module generates subtitles from video frames and uses the LLaMA model to infer comprehensive video captions. These captions, combined with the original video frames, are integrated into a diffusion model for joint learning, ultimately generating semantically enriched augmented video frames. This process leverages the multimodal relationship between video and text to increase the diversity of the training data distribution. Experimental results demonstrate that this collaborative augmentation method significantly improves the performance of video-text cross-domain retrieval models, validating its effectiveness in enhan...&lt;/p&gt;</content:encoded></item><item><title>Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition</title><link>https://arxiv.org/abs/2602.00841v1</link><guid>http://arxiv.org/abs/2602.00841v1</guid><pubDate>Sat, 31 Jan 2026 18:12:29 +0000</pubDate><dc:creator>Jintao Cheng</dc:creator><dc:creator>Weibin Li</dc:creator><dc:creator>Zhijian He</dc:creator><dc:creator>Jin Wu</dc:creator><dc:creator>Chi Man Vong</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.
Published: 2026-01-31T18:12:29+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jintao Cheng; Weibin Li; Zhijian He; Jin Wu; Chi Man Vong; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.&lt;/p&gt;</content:encoded></item><item><title>ReasonEdit: Editing Vision-Language Models using Human Reasoning</title><link>https://arxiv.org/abs/2602.02408v2</link><guid>http://arxiv.org/abs/2602.02408v2</guid><pubDate>Mon, 02 Feb 2026 18:06:14 +0000</pubDate><dc:creator>Jiaxing Qiu</dc:creator><dc:creator>Kaihua Hou</dc:creator><dc:creator>Roxana Daneshjou</dc:creator><dc:creator>Ahmed Alaa</dc:creator><dc:creator>Thomas Hartvigsen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.
Published: 2026-02-02T18:06:14+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxing Qiu; Kaihua Hou; Roxana Daneshjou; Ahmed Alaa; Thomas Hartvigsen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.&lt;/p&gt;</content:encoded></item><item><title>Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles</title><link>https://arxiv.org/abs/2602.01452v1</link><guid>http://arxiv.org/abs/2602.01452v1</guid><pubDate>Sun, 01 Feb 2026 21:43:02 +0000</pubDate><dc:creator>Penghao Deng</dc:creator><dc:creator>Jidong J. Yang</dc:creator><dc:creator>Jiachen Bian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.
Published: 2026-02-01T21:43:02+00:00
Venue: arXiv
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Penghao Deng; Jidong J. Yang; Jiachen Bian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle&amp;#x27;s front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a &amp;quot;part-versus-whole&amp;quot; semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.&lt;/p&gt;</content:encoded></item><item><title>ObjEmbed: Towards Universal Multimodal Object Embeddings</title><link>https://arxiv.org/abs/2602.01753v2</link><guid>http://arxiv.org/abs/2602.01753v2</guid><pubDate>Mon, 02 Feb 2026 07:38:45 +0000</pubDate><dc:creator>Shenghao Fu</dc:creator><dc:creator>Yukun Su</dc:creator><dc:creator>Fengyun Rao</dc:creator><dc:creator>Jing Lyu</dc:creator><dc:creator>Xiaohua Xie</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.
Published: 2026-02-02T07:38:45+00:00
Venue: arXiv
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shenghao Fu; Yukun Su; Fengyun Rao; Jing Lyu; Xiaohua Xie; Wei-Shi Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.&lt;/p&gt;</content:encoded></item><item><title>AnomalyLVM:Vision-Language Models for Zero-Shot Anomaly Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131392</link><guid>10.1016/j.eswa.2026.131392</guid><pubDate>Mon, 02 Feb 2026 16:34:29 +0000</pubDate><dc:creator>Yuqing Zhao</dc:creator><dc:creator>Min Meng</dc:creator><dc:creator>Jigang Wu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131392</prism:doi><description>Zero-Shot Anomaly Detection (ZSAD) has emerged as a promising approach for identifying unseen defects without requiring annotated training samples, but existing methods typically focus only on image-level detection and overlook fine-grained pixel-level localization. To address this gap, we propose AnomalyLVM, a unified vision-language framework designed to simultaneously handle image-level classification and pixel-level segmentation in zero-shot settings. AnomalyLVM leverages frozen SAM2 and DINO-X as dual visual encoders to extract complementary spatial and semantic features, which are fused and decoded via a lightweight decoder to generate localization maps. Meanwhile, a frozen CLIP text encoder guides image-level detection through semantic similarity matching. To enhance the accuracy of pixel-wise supervision, we introduce a Feature Enhancement Module (FEM) that dynamically refines static LayerCAM maps by integrating attention cues from both visual encoders and decoder affinity signals, resulting in more consistent and context-aware pseudo labels. Additionally, we adopt a prompt-free, object-agnostic strategy that replaces handcrafted templates with learnable, generic prompts, enabling AnomalyLVM to generalize across diverse categories and defect types without relying on domain-specific knowledge. Extensive experiments conducted across 17 real-world anomaly detection datasets from industrial and medical domains indicate that AnomalyLVM outperforms other ZSAD methods and can generalize better to different categories and even domains. Code will be made available at https://github.com/hanli6688/AnomalyLVM
Published: 2026-02-02T16:34:29+00:00
Venue: Expert Systems with Applications
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqing Zhao; Min Meng; Jigang Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131392"&gt;10.1016/j.eswa.2026.131392&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Zero-Shot Anomaly Detection (ZSAD) has emerged as a promising approach for identifying unseen defects without requiring annotated training samples, but existing methods typically focus only on image-level detection and overlook fine-grained pixel-level localization. To address this gap, we propose AnomalyLVM, a unified vision-language framework designed to simultaneously handle image-level classification and pixel-level segmentation in zero-shot settings. AnomalyLVM leverages frozen SAM2 and DINO-X as dual visual encoders to extract complementary spatial and semantic features, which are fused and decoded via a lightweight decoder to generate localization maps. Meanwhile, a frozen CLIP text encoder guides image-level detection through semantic similarity matching. To enhance the accuracy of pixel-wise supervision, we introduce a Feature Enhancement Module (FEM) that dynamically refines static LayerCAM maps by integrating attention cues from both visual encoders and decoder affinity signals, resulting in more consistent and context-aware pseudo labels. Additionally, we adopt a prompt-free, object-agnostic strategy that replaces handcrafted templates with learnable, generic prompts, enabling AnomalyLVM to generalize across diverse categories and defect types without relying on domain-specific knowledge. Extensive experiments conducted across 17 real-world anomaly detection datasets from industrial and medical domains indicate that AnomalyLVM outperforms other ZSAD methods and can generalize better to different categories and even domains. Code will be made available at https://github.com/hanli6688/AnomalyLVM&lt;/p&gt;</content:encoded></item><item><title>ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</title><link>https://arxiv.org/abs/2602.02873v1</link><guid>http://arxiv.org/abs/2602.02873v1</guid><pubDate>Mon, 02 Feb 2026 22:29:57 +0000</pubDate><dc:creator>Weihang You</dc:creator><dc:creator>Qingchan Zhu</dc:creator><dc:creator>David Liu</dc:creator><dc:creator>Yi Pan</dc:creator><dc:creator>Geng Yuan</dc:creator><dc:creator>Hanqi Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.
Published: 2026-02-02T22:29:57+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weihang You; Qingchan Zhu; David Liu; Yi Pan; Geng Yuan; Hanqi Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.&lt;/p&gt;</content:encoded></item><item><title>MG-TVMF: Multi-grained Text-Video Matching and Fusing for Weakly Supervised Video Anomaly Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113201</link><guid>10.1016/j.patcog.2026.113201</guid><pubDate>Tue, 03 Feb 2026 00:28:41 +0000</pubDate><dc:creator>Ping He</dc:creator><dc:creator>Xiaonan Gao</dc:creator><dc:creator>Huibin Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113201</prism:doi><description>Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.
Published: 2026-02-03T00:28:41+00:00
Venue: Pattern Recognition
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ping He; Xiaonan Gao; Huibin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113201"&gt;10.1016/j.patcog.2026.113201&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.&lt;/p&gt;</content:encoded></item><item><title>Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings</title><link>https://arxiv.org/abs/2602.00574v1</link><guid>http://arxiv.org/abs/2602.00574v1</guid><pubDate>Sat, 31 Jan 2026 07:36:38 +0000</pubDate><dc:creator>Yifei Shao</dc:creator><dc:creator>Kun Zhou</dc:creator><dc:creator>Ziming Xu</dc:creator><dc:creator>Mohammad Atif Quamar</dc:creator><dc:creator>Shibo Hao</dc:creator><dc:creator>Zhen Wang</dc:creator><dc:creator>Zhiting Hu</dc:creator><dc:creator>Biwei Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.
Published: 2026-01-31T07:36:38+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Shao; Kun Zhou; Ziming Xu; Mohammad Atif Quamar; Shibo Hao; Zhen Wang; Zhiting Hu; Biwei Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.&lt;/p&gt;</content:encoded></item><item><title>LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs</title><link>https://arxiv.org/abs/2602.00462v1</link><guid>http://arxiv.org/abs/2602.00462v1</guid><pubDate>Sat, 31 Jan 2026 02:33:07 +0000</pubDate><dc:creator>Benno Krojer</dc:creator><dc:creator>Shravan Nayak</dc:creator><dc:creator>Oscar Mañas</dc:creator><dc:creator>Vaibhav Adlakha</dc:creator><dc:creator>Desmond Elliott</dc:creator><dc:creator>Siva Reddy</dc:creator><dc:creator>Marius Mosbach</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.
Published: 2026-01-31T02:33:07+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Benno Krojer; Shravan Nayak; Oscar Mañas; Vaibhav Adlakha; Desmond Elliott; Siva Reddy; Marius Mosbach&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.&lt;/p&gt;</content:encoded></item><item><title>Multi-Domain Incremental Learning for Semantic Segmentation via Visual Domain Prompt in Remote Sensing Data</title><link>https://doi.org/10.3390/rs18030464</link><guid>10.3390/rs18030464</guid><pubDate>Mon, 02 Feb 2026 14:12:46 +0000</pubDate><dc:creator>Junxi Li</dc:creator><dc:creator>Zhiyuan Yan</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Yidan Zhang</dc:creator><dc:creator>Zicong Zhu</dc:creator><dc:creator>Yichen Tian</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030464</prism:doi><description>Domain incremental learning for semantic segmentation has gained lots of attention due to its importance for many fields including urban planning and autonomous driving. The catastrophic forgetting problem caused by domain shift has been alleviated by structure expansion of the model or data rehearsal. However, these methods ignore similar contextual knowledge between the new and the old data domain and assume that new knowledge and old knowledge are completely mutually exclusive, which cause the model to be trained in a suboptimal direction. Motivated by the prompt learning, we proposed a new domain incremental learning framework named RS-VDP. The key innovation of RS-VDP is to utilize a visual domain prompt to change the optimization direction from input data space and feature space. First, we designed a domain prompt based on a dynamic location module, which applied a visual domain prompt according to a local entropy map to update the distribution of the input images. Second, in order to filter the feature vectors with high confidence, a representation feature alignment based on an entropy map module is proposed. This module ensures the accuracy and stability of the feature vectors involved in the regularization loss, alleviating the problem of semantic drift. Finally, we introduced a new evaluation metric to measure the overall performance of the incremental learning models, solving the problem that the traditional evaluation metric is affected by the single-task accuracy. Comprehensive experiments demonstrated the effectiveness of the proposed method by significantly reducing the degree of catastrophic forgetting.
Published: 2026-02-02T14:12:46+00:00
Venue: Remote Sensing
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junxi Li; Zhiyuan Yan; Wenhui Diao; Yidan Zhang; Zicong Zhu; Yichen Tian; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030464"&gt;10.3390/rs18030464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Domain incremental learning for semantic segmentation has gained lots of attention due to its importance for many fields including urban planning and autonomous driving. The catastrophic forgetting problem caused by domain shift has been alleviated by structure expansion of the model or data rehearsal. However, these methods ignore similar contextual knowledge between the new and the old data domain and assume that new knowledge and old knowledge are completely mutually exclusive, which cause the model to be trained in a suboptimal direction. Motivated by the prompt learning, we proposed a new domain incremental learning framework named RS-VDP. The key innovation of RS-VDP is to utilize a visual domain prompt to change the optimization direction from input data space and feature space. First, we designed a domain prompt based on a dynamic location module, which applied a visual domain prompt according to a local entropy map to update the distribution of the input images. Second, in order to filter the feature vectors with high confidence, a representation feature alignment based on an entropy map module is proposed. This module ensures the accuracy and stability of the feature vectors involved in the regularization loss, alleviating the problem of semantic drift. Finally, we introduced a new evaluation metric to measure the overall performance of the incremental learning models, solving the problem that the traditional evaluation metric is affected by the single-task accuracy. Comprehensive experiments demonstrated the effectiveness of the proposed method by significantly reducing the degree of catastrophic forgetting.&lt;/p&gt;</content:encoded></item></channel></rss>