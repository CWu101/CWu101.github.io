<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 10 Jan 2026 02:35:31 +0000</lastBuildDate><item><title>Modular and adaptive implementation of Semantic Segmentation Models for Satellite Images and Open Source tools suitable for complex geographical contexts</title><link>https://doi.org/10.1016/j.jag.2025.105069</link><guid>10.1016/j.jag.2025.105069</guid><pubDate>Thu, 08 Jan 2026 21:51:41 +0000</pubDate><dc:creator>Adrien Le Guillou</dc:creator><dc:creator>Simona Niculescu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105069</prism:doi><description>Semantic segmentation, the process of assigning a semantic label to each pixel in an image, is a critical computer vision task for extracting detailed information from remote sensing data. However, its application to complex geographical contexts, such as coastal wetlands, is often constrained by the need for highly specialized implementations, class imbalance, and limited accessibility for non-specialists. This paper introduces a novel, modular, and adaptive open-source framework for semantic segmentation tailored to satellite imagery. Designed for maximum flexibility, the framework supports both binary and multi-class segmentation tasks and incorporates specific training strategies to handle severe class imbalances inherent in ecological detection, such as salt marsh mapping. The implementation provides a fully configurable pipeline that bridges the gap between Geographic Information Systems (GIS) and Deep Learning (DL). It integrates QGIS for intuitive spatial preprocessing and grid generation with a Python-based training and prediction workflow, thereby democratizing access to advanced segmentation techniques. The framework is architecture-agnostic, allowing the seamless deployment and benchmarking of various state-of-the-art encoder–decoder models, which are effective at combining multi-scale contextual information with high spatial resolution. A key contribution is the integration of a multifaceted training methodology that includes hybrid loss functions with dynamic class weighting and spectral-consistent data augmentation to ensure robust model generalization from limited and imbalanced datasets. We demonstrate the framework’s efficacy and scalability through two distinct case studies: a multi-class land cover classification on the Crozon Peninsula using Pléiades and a binary salt marsh detection in the Mont-Saint-Michel Bay Sentinel-2 imagery. The results show that accurate segmentation can be achieved with modest computational resources, promoting more sustainable and ethical AI applications in environmental monitoring. This work provides a critical tool for researchers and practitioners aiming to apply advanced DL segmentation to domain specific remote sensing challenges beyond conventional benchmarks.
Published: 2026-01-08T21:51:41+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Adrien Le Guillou; Simona Niculescu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105069"&gt;10.1016/j.jag.2025.105069&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation, the process of assigning a semantic label to each pixel in an image, is a critical computer vision task for extracting detailed information from remote sensing data. However, its application to complex geographical contexts, such as coastal wetlands, is often constrained by the need for highly specialized implementations, class imbalance, and limited accessibility for non-specialists. This paper introduces a novel, modular, and adaptive open-source framework for semantic segmentation tailored to satellite imagery. Designed for maximum flexibility, the framework supports both binary and multi-class segmentation tasks and incorporates specific training strategies to handle severe class imbalances inherent in ecological detection, such as salt marsh mapping. The implementation provides a fully configurable pipeline that bridges the gap between Geographic Information Systems (GIS) and Deep Learning (DL). It integrates QGIS for intuitive spatial preprocessing and grid generation with a Python-based training and prediction workflow, thereby democratizing access to advanced segmentation techniques. The framework is architecture-agnostic, allowing the seamless deployment and benchmarking of various state-of-the-art encoder–decoder models, which are effective at combining multi-scale contextual information with high spatial resolution. A key contribution is the integration of a multifaceted training methodology that includes hybrid loss functions with dynamic class weighting and spectral-consistent data augmentation to ensure robust model generalization from limited and imbalanced datasets. We demonstrate the framework’s efficacy and scalability through two distinct case studies: a multi-class land cover classification on the Crozon Peninsula using Pléiades and a binary salt marsh detection in the Mont-Saint-Michel Bay Sentinel-2 imagery. The results show that accurate segmentation can be achieved with modest computational resources, promoting more sustainable and ethical AI applications in environmental monitoring. This work provides a critical tool for researchers and practitioners aiming to apply advanced DL segmentation to domain specific remote sensing challenges beyond conventional benchmarks.&lt;/p&gt;</content:encoded></item><item><title>SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization</title><link>https://arxiv.org/abs/2601.03579v1</link><guid>http://arxiv.org/abs/2601.03579v1</guid><pubDate>Wed, 07 Jan 2026 04:50:39 +0000</pubDate><dc:creator>Tianyi Shang</dc:creator><dc:creator>Pengjie Xu</dc:creator><dc:creator>Zhaojun Deng</dc:creator><dc:creator>Zhenyu Li</dc:creator><dc:creator>Zhicong Chen</dc:creator><dc:creator>Lijun Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.
Published: 2026-01-07T04:50:39+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Shang; Pengjie Xu; Zhaojun Deng; Zhenyu Li; Zhicong Chen; Lijun Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.&lt;/p&gt;</content:encoded></item><item><title>AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs</title><link>https://arxiv.org/abs/2601.02771v1</link><guid>http://arxiv.org/abs/2601.02771v1</guid><pubDate>Tue, 06 Jan 2026 07:05:35 +0000</pubDate><dc:creator>Boyu Chang</dc:creator><dc:creator>Qi Wang</dc:creator><dc:creator>Xi Guo</dc:creator><dc:creator>Zhixiong Nan</dc:creator><dc:creator>Yazhou Yao</dc:creator><dc:creator>Tianfei Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.
Published: 2026-01-06T07:05:35+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyu Chang; Qi Wang; Xi Guo; Zhixiong Nan; Yazhou Yao; Tianfei Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER&amp;#x27;s output embeddings to &amp;quot;imagine&amp;quot; plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs&amp;#x27; contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.&lt;/p&gt;</content:encoded></item><item><title>GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.04777v1</link><guid>http://arxiv.org/abs/2601.04777v1</guid><pubDate>Thu, 08 Jan 2026 09:58:35 +0000</pubDate><dc:creator>Shurong Zheng</dc:creator><dc:creator>Yousong Zhu</dc:creator><dc:creator>Hongyin Zhao</dc:creator><dc:creator>Fan Yang</dc:creator><dc:creator>Yufei Zhan</dc:creator><dc:creator>Ming Tang</dc:creator><dc:creator>Jinqiao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.
Published: 2026-01-08T09:58:35+00:00
Venue: arXiv
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shurong Zheng; Yousong Zhu; Hongyin Zhao; Fan Yang; Yufei Zhan; Ming Tang; Jinqiao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model&amp;#x27;s overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Resolution Gap: Semantic-Aware Alignment for Cross-Resolution Change Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113053</link><guid>10.1016/j.patcog.2026.113053</guid><pubDate>Thu, 08 Jan 2026 16:11:21 +0000</pubDate><dc:creator>Wang Hao</dc:creator><dc:creator>Fengchao Xiong</dc:creator><dc:creator>Yijun Zhang</dc:creator><dc:creator>Jianfeng Lu</dc:creator><dc:creator>Jingzhou Chen</dc:creator><dc:creator>Yuntao Qian</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113053</prism:doi><description>Cross-resolution change detection aims to identify changes between bitemporal images acquired from heterogeneous platforms (e.g., drones and satellites) with varying spatial resolutions. It plays a vital role in applications such as rapid disaster assessment. However, semantic inconsistencies caused by resolution differences and spatial misalignment resulting from imperfect registration pose significant challenges to robust change detection. To address these issues, this paper proposes a Semantic-Aware Alignment Network for cross-resolution change detection. For spatial alignment, a semantic-aware Transformer is introduced to establish global semantic correspondences and estimate geometric transformation parameters between the two images. To enforce semantic consistency, the high-resolution image is used as a reference and degraded to simulate low-resolution observations. By enforcing feature-level consistency between the high-resolution and low-resolution representations, the two inputs are mapped into a unified, resolution-invariant feature space, thereby substantially reducing semantic discrepancies. Benefiting from the joint semantic consistency learning and spatial alignment strategies, the proposed method outperforms existing state-of-the-art approaches, achieving F1-scores of 77.99%, 90.37%, and 51.40% on the HTCD, MRCDD, and DECD datasets, respectively.
Published: 2026-01-08T16:11:21+00:00
Venue: Pattern Recognition
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wang Hao; Fengchao Xiong; Yijun Zhang; Jianfeng Lu; Jingzhou Chen; Yuntao Qian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113053"&gt;10.1016/j.patcog.2026.113053&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Cross-resolution change detection aims to identify changes between bitemporal images acquired from heterogeneous platforms (e.g., drones and satellites) with varying spatial resolutions. It plays a vital role in applications such as rapid disaster assessment. However, semantic inconsistencies caused by resolution differences and spatial misalignment resulting from imperfect registration pose significant challenges to robust change detection. To address these issues, this paper proposes a Semantic-Aware Alignment Network for cross-resolution change detection. For spatial alignment, a semantic-aware Transformer is introduced to establish global semantic correspondences and estimate geometric transformation parameters between the two images. To enforce semantic consistency, the high-resolution image is used as a reference and degraded to simulate low-resolution observations. By enforcing feature-level consistency between the high-resolution and low-resolution representations, the two inputs are mapped into a unified, resolution-invariant feature space, thereby substantially reducing semantic discrepancies. Benefiting from the joint semantic consistency learning and spatial alignment strategies, the proposed method outperforms existing state-of-the-art approaches, achieving F1-scores of 77.99%, 90.37%, and 51.40% on the HTCD, MRCDD, and DECD datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions</title><link>https://arxiv.org/abs/2601.03590v1</link><guid>http://arxiv.org/abs/2601.03590v1</guid><pubDate>Wed, 07 Jan 2026 05:13:52 +0000</pubDate><dc:creator>Zhongbin Guo</dc:creator><dc:creator>Zhen Yang</dc:creator><dc:creator>Yushan Li</dc:creator><dc:creator>Xinyue Zhang</dc:creator><dc:creator>Wenyu Gao</dc:creator><dc:creator>Jiacheng Wang</dc:creator><dc:creator>Chengzhi Li</dc:creator><dc:creator>Xiangrui Liu</dc:creator><dc:creator>Ping Jian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant "spatial gap" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .
Published: 2026-01-07T05:13:52+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongbin Guo; Zhen Yang; Yushan Li; Xinyue Zhang; Wenyu Gao; Jiacheng Wang; Chengzhi Li; Xiangrui Liu; Ping Jian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant &amp;quot;spatial gap&amp;quot; remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .&lt;/p&gt;</content:encoded></item><item><title>A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</title><link>https://arxiv.org/abs/2601.05143v1</link><guid>http://arxiv.org/abs/2601.05143v1</guid><pubDate>Thu, 08 Jan 2026 17:31:09 +0000</pubDate><dc:creator>Md. Zahid Hossain</dc:creator><dc:creator>Most. Sharmin Sultana Samu</dc:creator><dc:creator>Md. Rakibul Islam</dc:creator><dc:creator>Md. Siam Ansary</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.
Published: 2026-01-08T17:31:09+00:00
Venue: arXiv
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md. Zahid Hossain; Most. Sharmin Sultana Samu; Md. Rakibul Islam; Md. Siam Ansary&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.&lt;/p&gt;</content:encoded></item><item><title>GBGCN: Adaptive granular-ball graph representation and clarity-aware GCN for multi-focus image fusion</title><link>https://doi.org/10.1016/j.knosys.2026.115271</link><guid>10.1016/j.knosys.2026.115271</guid><pubDate>Thu, 08 Jan 2026 16:18:29 +0000</pubDate><dc:creator>Zhendong Xu</dc:creator><dc:creator>Hao Zhai</dc:creator><dc:creator>Zhi Zeng</dc:creator><dc:creator>Bo Lin</dc:creator><dc:creator>Minyu Deng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115271</prism:doi><description>Multi-focus image fusion technology aims to combine images taken at different focal lengths into a globally clear all-in-focus image. However, traditional methods and existing deep learning methods still face challenges in balancing global semantic modeling with natural boundary preservation. To address this, this paper proposes a novel method that integrates granular-ball computing with graph convolutional neural networks, constructing a dual-branch hybrid architecture. In the graph convolutional neural network branch, we introduce granular-ball computing theory to represent the image as a series of adaptively generated semantic units (i.e., granular-ball), and employ an iterative optimization strategy guided by a deep clarity map to naturally align the granular-ball distribution with the focused regions in the image. Meanwhile, a clarity-aware graph convolutional network is designed to accurately identify focused areas by integrating multidimensional clarity features with a gating mechanism. In the convolutional neural network branch, a lightweight network is responsible for extracting rich local detail features. The two branches achieve deep collaboration through a multi-level feature interaction mechanism. Experimental results on four public datasets demonstrate that, compared to current mainstream methods, the proposed method shows significant advantages in both qualitative and quantitative evaluations.
Published: 2026-01-08T16:18:29+00:00
Venue: Knowledge-Based Systems
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhendong Xu; Hao Zhai; Zhi Zeng; Bo Lin; Minyu Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115271"&gt;10.1016/j.knosys.2026.115271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Multi-focus image fusion technology aims to combine images taken at different focal lengths into a globally clear all-in-focus image. However, traditional methods and existing deep learning methods still face challenges in balancing global semantic modeling with natural boundary preservation. To address this, this paper proposes a novel method that integrates granular-ball computing with graph convolutional neural networks, constructing a dual-branch hybrid architecture. In the graph convolutional neural network branch, we introduce granular-ball computing theory to represent the image as a series of adaptively generated semantic units (i.e., granular-ball), and employ an iterative optimization strategy guided by a deep clarity map to naturally align the granular-ball distribution with the focused regions in the image. Meanwhile, a clarity-aware graph convolutional network is designed to accurately identify focused areas by integrating multidimensional clarity features with a gating mechanism. In the convolutional neural network branch, a lightweight network is responsible for extracting rich local detail features. The two branches achieve deep collaboration through a multi-level feature interaction mechanism. Experimental results on four public datasets demonstrate that, compared to current mainstream methods, the proposed method shows significant advantages in both qualitative and quantitative evaluations.&lt;/p&gt;</content:encoded></item><item><title>Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning</title><link>https://arxiv.org/abs/2601.03400v1</link><guid>http://arxiv.org/abs/2601.03400v1</guid><pubDate>Tue, 06 Jan 2026 20:27:29 +0000</pubDate><dc:creator>Ali Najar</dc:creator><dc:creator>Alireza Mirrokni</dc:creator><dc:creator>Arshia Izadyari</dc:creator><dc:creator>Sadegh Mohammadian</dc:creator><dc:creator>Amir Homayoon Sharifizade</dc:creator><dc:creator>Asal Meskin</dc:creator><dc:creator>Mobin Bagherian</dc:creator><dc:creator>Ehsaneddin Asgari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.
Published: 2026-01-06T20:27:29+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ali Najar; Alireza Mirrokni; Arshia Izadyari; Sadegh Mohammadian; Amir Homayoon Sharifizade; Asal Meskin; Mobin Bagherian; Ehsaneddin Asgari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models&amp;#x27; ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.&lt;/p&gt;</content:encoded></item><item><title>VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding</title><link>https://arxiv.org/abs/2601.05125v1</link><guid>http://arxiv.org/abs/2601.05125v1</guid><pubDate>Thu, 08 Jan 2026 17:15:15 +0000</pubDate><dc:creator>Ignacio de Rodrigo</dc:creator><dc:creator>Alvaro J. Lopez-Lopez</dc:creator><dc:creator>Jaime Boal</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.
Published: 2026-01-08T17:15:15+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ignacio de Rodrigo; Alvaro J. Lopez-Lopez; Jaime Boal&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.&lt;/p&gt;</content:encoded></item><item><title>LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.02757v1</link><guid>http://arxiv.org/abs/2601.02757v1</guid><pubDate>Tue, 06 Jan 2026 06:49:51 +0000</pubDate><dc:creator>Zixuan Xiao</dc:creator><dc:creator>Jun Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1016/j.autcon.2025.106341</prism:doi><description>Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.
Published: 2026-01-06T06:49:51+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Xiao; Jun Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.autcon.2025.106341"&gt;10.1016/j.autcon.2025.106341&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent&amp;#x27;s tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding</title><link>https://arxiv.org/abs/2601.02927v2</link><guid>http://arxiv.org/abs/2601.02927v2</guid><pubDate>Tue, 06 Jan 2026 11:11:06 +0000</pubDate><dc:creator>Iñaki Erregue</dc:creator><dc:creator>Kamal Nasrollahi</dc:creator><dc:creator>Sergio Escalera</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.
Published: 2026-01-06T11:11:06+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Iñaki Erregue; Kamal Nasrollahi; Sergio Escalera&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.&lt;/p&gt;</content:encoded></item><item><title>VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</title><link>https://arxiv.org/abs/2601.05175v1</link><guid>http://arxiv.org/abs/2601.05175v1</guid><pubDate>Thu, 08 Jan 2026 18:00:59 +0000</pubDate><dc:creator>Shuming Liu</dc:creator><dc:creator>Mingchen Zhuge</dc:creator><dc:creator>Changsheng Zhao</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Lemeng Wu</dc:creator><dc:creator>Zechun Liu</dc:creator><dc:creator>Chenchen Zhu</dc:creator><dc:creator>Zhipeng Cai</dc:creator><dc:creator>Chong Zhou</dc:creator><dc:creator>Haozhe Liu</dc:creator><dc:creator>Ernie Chang</dc:creator><dc:creator>Saksham Suri</dc:creator><dc:creator>Hongyu Xu</dc:creator><dc:creator>Qi Qian</dc:creator><dc:creator>Wei Wen</dc:creator><dc:creator>Balakrishnan Varadarajan</dc:creator><dc:creator>Zhuang Liu</dc:creator><dc:creator>Hu Xu</dc:creator><dc:creator>Florian Bordes</dc:creator><dc:creator>Raghuraman Krishnamoorthi</dc:creator><dc:creator>Bernard Ghanem</dc:creator><dc:creator>Vikas Chandra</dc:creator><dc:creator>Yunyang Xiong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.
Published: 2026-01-08T18:00:59+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuming Liu; Mingchen Zhuge; Changsheng Zhao; Jun Chen; Lemeng Wu; Zechun Liu; Chenchen Zhu; Zhipeng Cai; Chong Zhou; Haozhe Liu; Ernie Chang; Saksham Suri; Hongyu Xu; Qi Qian; Wei Wen; Balakrishnan Varadarajan; Zhuang Liu; Hu Xu; Florian Bordes; Raghuraman Krishnamoorthi; Bernard Ghanem; Vikas Chandra; Yunyang Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.&lt;/p&gt;</content:encoded></item><item><title>Training a Custom CNN on Five Heterogeneous Image Datasets</title><link>https://arxiv.org/abs/2601.04727v1</link><guid>http://arxiv.org/abs/2601.04727v1</guid><pubDate>Thu, 08 Jan 2026 08:44:17 +0000</pubDate><dc:creator>Anika Tabassum</dc:creator><dc:creator>Tasnuva Mahazabin Tuba</dc:creator><dc:creator>Nafisa Naznin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.
Published: 2026-01-08T08:44:17+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anika Tabassum; Tasnuva Mahazabin Tuba; Nafisa Naznin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.&lt;/p&gt;</content:encoded></item><item><title>A transformer based multi-task deep learning model for urban livability evaluation by fusing remote sensing and textual geospatial data</title><link>https://doi.org/10.1016/j.rse.2026.115232</link><guid>10.1016/j.rse.2026.115232</guid><pubDate>Thu, 08 Jan 2026 23:28:03 +0000</pubDate><dc:creator>Wen Zhou</dc:creator><dc:creator>Claudio Persello</dc:creator><dc:creator>Dongping Ming</dc:creator><dc:creator>Shaowen Wang</dc:creator><dc:creator>Alfred Stein</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115232</prism:doi><description>Livable cities enhance urban economic development, improve physical and mental health, foster well-being, and foster urban sustainability. Evaluating urban livability is therefore important for policymakers to develop urban planning and development strategies aimed at improving livability. Mainstream methods of evaluating urban livability assign different weights to diverse indicators extracted from survey data, statistical data, and geospatial data. To relieve such time-consuming and labor-intensive data collection, this study proposes a transformer-based multi-task multimodal regression (TMTMR) model for the simultaneous evaluation of urban livability focusing on five domain-specific scores. Pretrained state-of-the-art computer vision and natural language processing models serve as backbones to extract features from high spatial resolution remote sensing (RS) images, digital surface models (DSM), night light remote sensing (NLRS) images and point of interest (POI) data. An attention mechanism helps the TMTMR model to assign varying significance levels to features from different modalities, thus capturing both intrinsic information and interrelationships among modalities for livability evaluation. Focusing on 13 Dutch areas, our research demonstrates that the TMTMR model efficiently evaluates urban livability with correlation coefficients ranging from 0.605 to 0.779, and root mean square error values between 0.070 and 0.112 in four unseen test areas. Furthermore, we analyze the synergy between different modalities. We found that modalities of urban livability can be effectively evaluated by aligning, in a descending order, contributions from RS images, NLRS images, DSM, and POI data. We demonstrated that the proposed TMTMR model is capable of effectively evaluating urban livability directly from multimodal geospatial data.
Published: 2026-01-08T23:28:03+00:00
Venue: Remote Sensing of Environment
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wen Zhou; Claudio Persello; Dongping Ming; Shaowen Wang; Alfred Stein&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115232"&gt;10.1016/j.rse.2026.115232&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Livable cities enhance urban economic development, improve physical and mental health, foster well-being, and foster urban sustainability. Evaluating urban livability is therefore important for policymakers to develop urban planning and development strategies aimed at improving livability. Mainstream methods of evaluating urban livability assign different weights to diverse indicators extracted from survey data, statistical data, and geospatial data. To relieve such time-consuming and labor-intensive data collection, this study proposes a transformer-based multi-task multimodal regression (TMTMR) model for the simultaneous evaluation of urban livability focusing on five domain-specific scores. Pretrained state-of-the-art computer vision and natural language processing models serve as backbones to extract features from high spatial resolution remote sensing (RS) images, digital surface models (DSM), night light remote sensing (NLRS) images and point of interest (POI) data. An attention mechanism helps the TMTMR model to assign varying significance levels to features from different modalities, thus capturing both intrinsic information and interrelationships among modalities for livability evaluation. Focusing on 13 Dutch areas, our research demonstrates that the TMTMR model efficiently evaluates urban livability with correlation coefficients ranging from 0.605 to 0.779, and root mean square error values between 0.070 and 0.112 in four unseen test areas. Furthermore, we analyze the synergy between different modalities. We found that modalities of urban livability can be effectively evaluated by aligning, in a descending order, contributions from RS images, NLRS images, DSM, and POI data. We demonstrated that the proposed TMTMR model is capable of effectively evaluating urban livability directly from multimodal geospatial data.&lt;/p&gt;</content:encoded></item><item><title>MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents</title><link>https://arxiv.org/abs/2601.03236v1</link><guid>http://arxiv.org/abs/2601.03236v1</guid><pubDate>Tue, 06 Jan 2026 18:29:43 +0000</pubDate><dc:creator>Dongming Jiang</dc:creator><dc:creator>Yi Li</dc:creator><dc:creator>Guanpeng Li</dc:creator><dc:creator>Bingzhe Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.
Published: 2026-01-06T18:29:43+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongming Jiang; Yi Li; Guanpeng Li; Bingzhe Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.&lt;/p&gt;</content:encoded></item><item><title>A spectral index using generic global endmembers from Landsat multispectral data for mapping urban areas</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.025</link><guid>10.1016/j.isprsjprs.2025.12.025</guid><pubDate>Thu, 08 Jan 2026 08:43:49 +0000</pubDate><dc:creator>Ruiyi Zhao</dc:creator><dc:creator>Cai Cai</dc:creator><dc:creator>Xinfan Cai</dc:creator><dc:creator>Peijun Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.025</prism:doi><description>Quantifying urban land cover from space is crucial for studying and understanding its spatial distribution and changes, as well as for assessing the impact of these changes on environmental and socio-economic dynamics worldwide. Owing to the diversity and spectral heterogeneity of urban reflectance, spectrally mixed pixels predominate in moderate resolution multispectral images. The standardized linear spectral mixture model from Landsat multispectral images, which represents the radiance measurements of mixed pixels as linear mixtures of generic global Substrate (S), Vegetation (V), and Dark Surfaces (D) endmember radiances, offers an effective method for characterizing urban reflectance. Based on the analysis of SVD endmember fractions of urban land and other land cover types, this study proposes a spectral index using Landsat global SVD endmembers, termed the Urban Index using Global Endmembers (GEUI), to highlight and map urban land. GEUI is evaluated through comparisons with five established spectral indices: including Normalized Difference Built-up Index (NDBI), Index-based Built-up Index (IBI), Biophysical Composition Index (BCI), Built-up Land Features Extraction Index (BLFEI), and Urban Composition Index (UCI), all of which rely on pure spectral signatures of urban pixels. Additionally, GEUI is compared to two deep learning methods in urban area mapping, i.e., two-dimensional convolutional neural network (2D CNN) and one-dimensional CNN (1D CNN). The results demonstrate that the proposed GEUI outperforms these comparative indices in qualitative evaluation, separability analysis, and urban land mapping, and also showed superior performance in urban land mapping compared to CNN methods. GEUI achieved overall accuracies ranging from 84.36% to 93.02% and F-scores between 84.80% and 92.64%, obtaining the highest accuracy in half of the study urban areas. Since S, V, and D endmembers used in GEUI are globally available, the proposed GEUI has the advantage of being applicable across diverse locations and times. Furthermore, GEUI can be readily extended to other broadband multispectral data, such as Sentinel-2 and MODIS. Therefore, the proposed GEUI provides an effective variable for mapping urban land and holds the potential for diverse urban applications.
Published: 2026-01-08T08:43:49+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiyi Zhao; Cai Cai; Xinfan Cai; Peijun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.025"&gt;10.1016/j.isprsjprs.2025.12.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Quantifying urban land cover from space is crucial for studying and understanding its spatial distribution and changes, as well as for assessing the impact of these changes on environmental and socio-economic dynamics worldwide. Owing to the diversity and spectral heterogeneity of urban reflectance, spectrally mixed pixels predominate in moderate resolution multispectral images. The standardized linear spectral mixture model from Landsat multispectral images, which represents the radiance measurements of mixed pixels as linear mixtures of generic global Substrate (S), Vegetation (V), and Dark Surfaces (D) endmember radiances, offers an effective method for characterizing urban reflectance. Based on the analysis of SVD endmember fractions of urban land and other land cover types, this study proposes a spectral index using Landsat global SVD endmembers, termed the Urban Index using Global Endmembers (GEUI), to highlight and map urban land. GEUI is evaluated through comparisons with five established spectral indices: including Normalized Difference Built-up Index (NDBI), Index-based Built-up Index (IBI), Biophysical Composition Index (BCI), Built-up Land Features Extraction Index (BLFEI), and Urban Composition Index (UCI), all of which rely on pure spectral signatures of urban pixels. Additionally, GEUI is compared to two deep learning methods in urban area mapping, i.e., two-dimensional convolutional neural network (2D CNN) and one-dimensional CNN (1D CNN). The results demonstrate that the proposed GEUI outperforms these comparative indices in qualitative evaluation, separability analysis, and urban land mapping, and also showed superior performance in urban land mapping compared to CNN methods. GEUI achieved overall accuracies ranging from 84.36% to 93.02% and F-scores between 84.80% and 92.64%, obtaining the highest accuracy in half of the study urban areas. Since S, V, and D endmembers used in GEUI are globally available, the proposed GEUI has the advantage of being applicable across diverse locations and times. Furthermore, GEUI can be readily extended to other broadband multispectral data, such as Sentinel-2 and MODIS. Therefore, the proposed GEUI provides an effective variable for mapping urban land and holds the potential for diverse urban applications.&lt;/p&gt;</content:encoded></item><item><title>T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs</title><link>https://arxiv.org/abs/2601.04945v1</link><guid>http://arxiv.org/abs/2601.04945v1</guid><pubDate>Thu, 08 Jan 2026 13:49:12 +0000</pubDate><dc:creator>Chunyu Wei</dc:creator><dc:creator>Huaiyu Qin</dc:creator><dc:creator>Siyuan He</dc:creator><dc:creator>Yunhai Wang</dc:creator><dc:creator>Yueguo Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.
Published: 2026-01-08T13:49:12+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunyu Wei; Huaiyu Qin; Siyuan He; Yunhai Wang; Yueguo Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models&amp;#x27; ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph&amp;#x27;s natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.&lt;/p&gt;</content:encoded></item><item><title>Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</title><link>https://arxiv.org/abs/2601.02918v1</link><guid>http://arxiv.org/abs/2601.02918v1</guid><pubDate>Tue, 06 Jan 2026 11:00:17 +0000</pubDate><dc:creator>Guoqiang Liang</dc:creator><dc:creator>Jianyi Wang</dc:creator><dc:creator>Zhonghua Wu</dc:creator><dc:creator>Shangchen Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.
Published: 2026-01-06T11:00:17+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoqiang Liang; Jianyi Wang; Zhonghua Wu; Shangchen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.&lt;/p&gt;</content:encoded></item><item><title>DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection</title><link>https://arxiv.org/abs/2601.02831v1</link><guid>http://arxiv.org/abs/2601.02831v1</guid><pubDate>Tue, 06 Jan 2026 09:04:23 +0000</pubDate><dc:creator>Yuetong Li</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Yilin Zhao</dc:creator><dc:creator>Gongyang Li</dc:creator><dc:creator>Zeming Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.
Published: 2026-01-06T09:04:23+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuetong Li; Qing Zhang; Yilin Zhao; Gongyang Li; Zeming Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&amp;quot; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.&lt;/p&gt;</content:encoded></item><item><title>HG-RSOVSSeg: Hierarchical Guidance Open-Vocabulary Semantic Segmentation Framework of High-Resolution Remote Sensing Images</title><link>https://doi.org/10.3390/rs18020213</link><guid>10.3390/rs18020213</guid><pubDate>Fri, 09 Jan 2026 10:35:52 +0000</pubDate><dc:creator>Wubiao Huang</dc:creator><dc:creator>Fei Deng</dc:creator><dc:creator>Huchen Li</dc:creator><dc:creator>Jing Yang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020213</prism:doi><description>Remote sensing image semantic segmentation (RSISS) aims to assign a correct class label to each pixel in remote sensing images and has wide applications. With the development of artificial intelligence, RSISS based on deep learning has made significant progress. However, existing methods remain more focused on predefined semantic classes and require costly retraining when confronted with new classes. To address this limitation, we propose the hierarchical guidance open-vocabulary semantic segmentation framework for remote sensing images (named HG-RSOVSSeg), enabling flexible segmentation of arbitrary semantic classes without model retraining. Our framework leverages pretrained text-embedding models to provide class common knowledge and aligns multimodal features through a dual-stream architecture. Specifically, we propose a multimodal feature aggregation module for pixel-level alignment and a hierarchical visual feature decoder guided by text feature alignment, which progressively refines visual features using language priors, preserving semantic coherence during high-resolution decoding. Extensive experiments were conducted on six representative public datasets, and the results showed that our method has the highest mean mIoU value, establishing state-of-the-art performance in the field of open-vocabulary semantic segmentation of remote sensing images.
Published: 2026-01-09T10:35:52+00:00
Venue: Remote Sensing
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wubiao Huang; Fei Deng; Huchen Li; Jing Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020213"&gt;10.3390/rs18020213&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image semantic segmentation (RSISS) aims to assign a correct class label to each pixel in remote sensing images and has wide applications. With the development of artificial intelligence, RSISS based on deep learning has made significant progress. However, existing methods remain more focused on predefined semantic classes and require costly retraining when confronted with new classes. To address this limitation, we propose the hierarchical guidance open-vocabulary semantic segmentation framework for remote sensing images (named HG-RSOVSSeg), enabling flexible segmentation of arbitrary semantic classes without model retraining. Our framework leverages pretrained text-embedding models to provide class common knowledge and aligns multimodal features through a dual-stream architecture. Specifically, we propose a multimodal feature aggregation module for pixel-level alignment and a hierarchical visual feature decoder guided by text feature alignment, which progressively refines visual features using language priors, preserving semantic coherence during high-resolution decoding. Extensive experiments were conducted on six representative public datasets, and the results showed that our method has the highest mean mIoU value, establishing state-of-the-art performance in the field of open-vocabulary semantic segmentation of remote sensing images.&lt;/p&gt;</content:encoded></item><item><title>ImLoc: Revisiting Visual Localization with Image-based Representation</title><link>https://arxiv.org/abs/2601.04185v1</link><guid>http://arxiv.org/abs/2601.04185v1</guid><pubDate>Wed, 07 Jan 2026 18:51:51 +0000</pubDate><dc:creator>Xudong Jiang</dc:creator><dc:creator>Fangjinhua Wang</dc:creator><dc:creator>Silvano Galliani</dc:creator><dc:creator>Christoph Vogel</dc:creator><dc:creator>Marc Pollefeys</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.
Published: 2026-01-07T18:51:51+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xudong Jiang; Fangjinhua Wang; Silvano Galliani; Christoph Vogel; Marc Pollefeys&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.&lt;/p&gt;</content:encoded></item><item><title>Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning</title><link>https://doi.org/10.3390/rs18020222</link><guid>10.3390/rs18020222</guid><pubDate>Fri, 09 Jan 2026 16:03:16 +0000</pubDate><dc:creator>Qingyun Li</dc:creator><dc:creator>Shuran Ma</dc:creator><dc:creator>Junwei Luo</dc:creator><dc:creator>Yi Yu</dc:creator><dc:creator>Yue Zhou</dc:creator><dc:creator>Fengxiang Wang</dc:creator><dc:creator>Xudong Lu</dc:creator><dc:creator>Xiaoxing Wang</dc:creator><dc:creator>Xin He</dc:creator><dc:creator>Yushi Chen</dc:creator><dc:creator>Xue Yang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020222</prism:doi><description>With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.
Published: 2026-01-09T16:03:16+00:00
Venue: Remote Sensing
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyun Li; Shuran Ma; Junwei Luo; Yi Yu; Yue Zhou; Fengxiang Wang; Xudong Lu; Xiaoxing Wang; Xin He; Yushi Chen; Xue Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020222"&gt;10.3390/rs18020222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.&lt;/p&gt;</content:encoded></item><item><title>CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation</title><link>https://arxiv.org/abs/2601.03490v1</link><guid>http://arxiv.org/abs/2601.03490v1</guid><pubDate>Wed, 07 Jan 2026 01:02:39 +0000</pubDate><dc:creator>Yuzhe Sun</dc:creator><dc:creator>Zhe Dong</dc:creator><dc:creator>Haochen Jiang</dc:creator><dc:creator>Tianzhu Liu</dc:creator><dc:creator>Yanfeng Gu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.
Published: 2026-01-07T01:02:39+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuzhe Sun; Zhe Dong; Haochen Jiang; Tianzhu Liu; Yanfeng Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.&lt;/p&gt;</content:encoded></item><item><title>MLGO: Multi-Layer Graph Neural ODEs for Traffic Forecasting</title><link>https://doi.org/10.1016/j.neunet.2026.108540</link><guid>10.1016/j.neunet.2026.108540</guid><pubDate>Fri, 09 Jan 2026 16:43:56 +0000</pubDate><dc:creator>Mengzhou Gao</dc:creator><dc:creator>Huangqian Yu</dc:creator><dc:creator>Pengfei Jiao</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108540</prism:doi><description>Spatial-temporal graph neural networks have become increasingly prominent in traffic forecasting, driven by the inherently graph-structured nature of traffic networks. While temporal dependencies have been extensively studied, spatial correlations remain underutilized. Existing methods typically use predefined matrices that encode fixed spatial relations (e.g., spatial distances or traffic flow similarities), or adaptive ones that capture task-specific latent patterns. However, relying on a single type of graph structure limits the ability to comprehensively capture the diverse spatial dependencies in traffic networks. To address these issues, we propose a general and extensible framework named M ulti- L ayer G raph neural O rdinary differential equations (MLGO), which integrates multiple complementary graph structures to enhance spatial representation. Specifically, MLGO employs a multi-layer graph architecture that incorporates a time-varying graph, a predefined road network, and an adaptively learned graph into a unified representation, where different graphs may compensate for or constrain one another, enabling richer and more expressive modeling of spatial correlations. Neural ordinary differential equations are further used to enable both inter-layer and intra-layer spatial aggregation, while ensuring continuity in temporal dynamics. Experiments on five real-world traffic datasets demonstrate that MLGO outperforms most state-of-the-art baselines and offers improved interpretability through the use of explicit and complementary graph structures.
Published: 2026-01-09T16:43:56+00:00
Venue: Neural Networks
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengzhou Gao; Huangqian Yu; Pengfei Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108540"&gt;10.1016/j.neunet.2026.108540&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Spatial-temporal graph neural networks have become increasingly prominent in traffic forecasting, driven by the inherently graph-structured nature of traffic networks. While temporal dependencies have been extensively studied, spatial correlations remain underutilized. Existing methods typically use predefined matrices that encode fixed spatial relations (e.g., spatial distances or traffic flow similarities), or adaptive ones that capture task-specific latent patterns. However, relying on a single type of graph structure limits the ability to comprehensively capture the diverse spatial dependencies in traffic networks. To address these issues, we propose a general and extensible framework named M ulti- L ayer G raph neural O rdinary differential equations (MLGO), which integrates multiple complementary graph structures to enhance spatial representation. Specifically, MLGO employs a multi-layer graph architecture that incorporates a time-varying graph, a predefined road network, and an adaptively learned graph into a unified representation, where different graphs may compensate for or constrain one another, enabling richer and more expressive modeling of spatial correlations. Neural ordinary differential equations are further used to enable both inter-layer and intra-layer spatial aggregation, while ensuring continuity in temporal dynamics. Experiments on five real-world traffic datasets demonstrate that MLGO outperforms most state-of-the-art baselines and offers improved interpretability through the use of explicit and complementary graph structures.&lt;/p&gt;</content:encoded></item><item><title>X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval</title><link>https://doi.org/10.1016/j.eswa.2026.131169</link><guid>10.1016/j.eswa.2026.131169</guid><pubDate>Fri, 09 Jan 2026 08:03:00 +0000</pubDate><dc:creator>Aparna H</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:creator>Avik Hati</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131169</prism:doi><description>Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.
Published: 2026-01-09T08:03:00+00:00
Venue: Expert Systems with Applications
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aparna H; Biplab Banerjee; Avik Hati&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131169"&gt;10.1016/j.eswa.2026.131169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.&lt;/p&gt;</content:encoded></item><item><title>HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps</title><link>https://arxiv.org/abs/2601.02730v2</link><guid>http://arxiv.org/abs/2601.02730v2</guid><pubDate>Tue, 06 Jan 2026 05:48:47 +0000</pubDate><dc:creator>Xuchang Zhong</dc:creator><dc:creator>Xu Cao</dc:creator><dc:creator>Jinke Feng</dc:creator><dc:creator>Hao Fang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.
Published: 2026-01-06T05:48:47+00:00
Venue: arXiv
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuchang Zhong; Xu Cao; Jinke Feng; Hao Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.&lt;/p&gt;</content:encoded></item><item><title>RadDiff: Describing Differences in Radiology Image Sets with Natural Language</title><link>https://arxiv.org/abs/2601.03733v1</link><guid>http://arxiv.org/abs/2601.03733v1</guid><pubDate>Wed, 07 Jan 2026 09:25:04 +0000</pubDate><dc:creator>Xiaoxian Shen</dc:creator><dc:creator>Yuhui Zhang</dc:creator><dc:creator>Sahithi Ankireddy</dc:creator><dc:creator>Xiaohan Wang</dc:creator><dc:creator>Maya Varma</dc:creator><dc:creator>Henry Guo</dc:creator><dc:creator>Curtis Langlotz</dc:creator><dc:creator>Serena Yeung-Levy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.
Published: 2026-01-07T09:25:04+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxian Shen; Yuhui Zhang; Sahithi Ankireddy; Xiaohan Wang; Maya Varma; Henry Guo; Curtis Langlotz; Serena Yeung-Levy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff&amp;#x27;s versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.&lt;/p&gt;</content:encoded></item><item><title>CoV: Chain-of-View Prompting for Spatial Reasoning</title><link>https://arxiv.org/abs/2601.05172v1</link><guid>http://arxiv.org/abs/2601.05172v1</guid><pubDate>Thu, 08 Jan 2026 17:59:42 +0000</pubDate><dc:creator>Haoyu Zhao</dc:creator><dc:creator>Akide Liu</dc:creator><dc:creator>Zeyu Zhang</dc:creator><dc:creator>Weijie Wang</dc:creator><dc:creator>Feng Chen</dc:creator><dc:creator>Ruihan Zhu</dc:creator><dc:creator>Gholamreza Haffari</dc:creator><dc:creator>Bohan Zhuang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.
Published: 2026-01-08T17:59:42+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyu Zhao; Akide Liu; Zeyu Zhang; Weijie Wang; Feng Chen; Ruihan Zhu; Gholamreza Haffari; Bohan Zhuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.&lt;/p&gt;</content:encoded></item><item><title>Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs</title><link>https://arxiv.org/abs/2601.03100v1</link><guid>http://arxiv.org/abs/2601.03100v1</guid><pubDate>Tue, 06 Jan 2026 15:31:19 +0000</pubDate><dc:creator>Chenchen Lin</dc:creator><dc:creator>Sanbao Su</dc:creator><dc:creator>Rachel Luo</dc:creator><dc:creator>Yuxiao Chen</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Marco Pavone</dc:creator><dc:creator>Fei Miao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.
Published: 2026-01-06T15:31:19+00:00
Venue: arXiv
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenchen Lin; Sanbao Su; Rachel Luo; Yuxiao Chen; Yan Wang; Marco Pavone; Fei Miao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder&amp;#x27;s rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise &amp;quot;experts&amp;quot; and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.&lt;/p&gt;</content:encoded></item></channel></rss>