<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 05 Feb 2026 03:34:36 +0000</lastBuildDate><item><title>Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation</title><link>https://doi.org/10.1016/j.inffus.2026.104204</link><guid>10.1016/j.inffus.2026.104204</guid><pubDate>Wed, 04 Feb 2026 00:27:18 +0000</pubDate><dc:creator>Hongxing You</dc:creator><dc:creator>Yangtao Wang</dc:creator><dc:creator>Xiaocui Li</dc:creator><dc:creator>Yanzhao Xie</dc:creator><dc:creator>Da Chen</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104204</prism:doi><description>Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .
Published: 2026-02-04T00:27:18+00:00
Venue: Information Fusion
Score: 0.589 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongxing You; Yangtao Wang; Xiaocui Li; Yanzhao Xie; Da Chen; Xinyu Zhang; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104204"&gt;10.1016/j.inffus.2026.104204&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.589 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .&lt;/p&gt;</content:encoded></item><item><title>Full-Scope Vectorization of Geographical Elements from Large-Size Remote Sensing Imagery</title><link>https://doi.org/10.1109/tpami.2026.3660934</link><guid>10.1109/tpami.2026.3660934</guid><pubDate>Tue, 03 Feb 2026 20:54:47 +0000</pubDate><dc:creator>Yansheng Li</dc:creator><dc:creator>Wanchun Li</dc:creator><dc:creator>Bo Dang</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Wei Chen</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Bingnan Yang</dc:creator><dc:creator>Yongjun Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3660934</prism:doi><description>Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks' vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.
Published: 2026-02-03T20:54:47+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.578 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yansheng Li; Wanchun Li; Bo Dang; Yu Wang; Wei Chen; Lei Wang; Bingnan Yang; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3660934"&gt;10.1109/tpami.2026.3660934&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.578 (consider)&lt;/p&gt;
&lt;p&gt;Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks&amp;#x27; vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.&lt;/p&gt;</content:encoded></item><item><title>LaVPR: Benchmarking Language and Vision for Place Recognition</title><link>https://arxiv.org/abs/2602.03253v1</link><guid>http://arxiv.org/abs/2602.03253v1</guid><pubDate>Tue, 03 Feb 2026 08:38:38 +0000</pubDate><dc:creator>Ofer Idan</dc:creator><dc:creator>Dan Badur</dc:creator><dc:creator>Yosi Keller</dc:creator><dc:creator>Yoli Shavit</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.
Published: 2026-02-03T08:38:38+00:00
Venue: arXiv
Score: 0.575 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ofer Idan; Dan Badur; Yosi Keller; Yoli Shavit&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.575 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &amp;quot;blind&amp;quot; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.&lt;/p&gt;</content:encoded></item><item><title>ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3659897</link><guid>10.1109/tgrs.2026.3659897</guid><pubDate>Tue, 03 Feb 2026 20:54:52 +0000</pubDate><dc:creator>Hanbo Bi</dc:creator><dc:creator>Yulong Xu</dc:creator><dc:creator>Ya Li</dc:creator><dc:creator>Yongqiang Mao</dc:creator><dc:creator>Boyuan Tong</dc:creator><dc:creator>Chongyang Li</dc:creator><dc:creator>Chunbo Lang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Hongqi Wang</dc:creator><dc:creator>Yingchao Feng</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3659897</prism:doi><description>The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits strong generalization in generic segmentation tasks. However, applying SAM to remote sensing (RS) images still faces two major challenges. First, manually constructing precise prompts for each image (e.g., points or boxes) is labor-intensive and inefficient, especially in RS scenarios with dense small objects or spatially fragmented distributions. Second, SAM lacks domain adaptability, as it is pre-trained primarily on natural images and struggles to capture RS-specific semantics and spatial characteristics, especially when segmenting novel or unseen classes. To address these issues, inspired by few-shot learning, we propose ViRefSAM, a novel framework that guides SAM utilizing only a few annotated reference images that contain class-specific objects. Without requiring manual prompts, ViRefSAM enables automatic segmentation of class-consistent objects across RS images. Specifically, ViRefSAM introduces two key components while keeping SAM’s original architecture intact: (1) a Visual Contextual Prompt Encoder that extracts class-specific semantic clues from reference images and generates object-aware prompts via contextual interaction with target images; and (2) a Dynamic Target Alignment Adapter, integrated into SAM’s image encoder, which mitigates the domain gap by injecting class-specific semantics into target image features, enabling SAM to dynamically focus on task-relevant regions. Extensive experiments on three few-shot segmentation benchmarks, including iSAID-5i, LoveDA-2i, and COCO-20i, demonstrate that ViRefSAM enables accurate and automatic segmentation of unseen classes by leveraging only a few reference images, and consistently outperforms existing few-shot segmentation methods across diverse datasets.
Published: 2026-02-03T20:54:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanbo Bi; Yulong Xu; Ya Li; Yongqiang Mao; Boyuan Tong; Chongyang Li; Chunbo Lang; Wenhui Diao; Hongqi Wang; Yingchao Feng; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3659897"&gt;10.1109/tgrs.2026.3659897&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits strong generalization in generic segmentation tasks. However, applying SAM to remote sensing (RS) images still faces two major challenges. First, manually constructing precise prompts for each image (e.g., points or boxes) is labor-intensive and inefficient, especially in RS scenarios with dense small objects or spatially fragmented distributions. Second, SAM lacks domain adaptability, as it is pre-trained primarily on natural images and struggles to capture RS-specific semantics and spatial characteristics, especially when segmenting novel or unseen classes. To address these issues, inspired by few-shot learning, we propose ViRefSAM, a novel framework that guides SAM utilizing only a few annotated reference images that contain class-specific objects. Without requiring manual prompts, ViRefSAM enables automatic segmentation of class-consistent objects across RS images. Specifically, ViRefSAM introduces two key components while keeping SAM’s original architecture intact: (1) a Visual Contextual Prompt Encoder that extracts class-specific semantic clues from reference images and generates object-aware prompts via contextual interaction with target images; and (2) a Dynamic Target Alignment Adapter, integrated into SAM’s image encoder, which mitigates the domain gap by injecting class-specific semantics into target image features, enabling SAM to dynamically focus on task-relevant regions. Extensive experiments on three few-shot segmentation benchmarks, including iSAID-5i, LoveDA-2i, and COCO-20i, demonstrate that ViRefSAM enables accurate and automatic segmentation of unseen classes by leveraging only a few reference images, and consistently outperforms existing few-shot segmentation methods across diverse datasets.&lt;/p&gt;</content:encoded></item><item><title>RegionReasoner: Region-Grounded Multi-Round Visual Reasoning</title><link>https://arxiv.org/abs/2602.03733v1</link><guid>http://arxiv.org/abs/2602.03733v1</guid><pubDate>Tue, 03 Feb 2026 16:52:16 +0000</pubDate><dc:creator>Wenfang Sun</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Yingjun Du</dc:creator><dc:creator>Yefeng Zheng</dc:creator><dc:creator>Cees G. M. Snoek</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.
Published: 2026-02-03T16:52:16+00:00
Venue: arXiv
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenfang Sun; Hao Chen; Yingjun Du; Yefeng Zheng; Cees G. M. Snoek&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.&lt;/p&gt;</content:encoded></item><item><title>Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search</title><link>https://arxiv.org/abs/2602.04454v1</link><guid>http://arxiv.org/abs/2602.04454v1</guid><pubDate>Wed, 04 Feb 2026 11:33:16 +0000</pubDate><dc:creator>Tianming Liang</dc:creator><dc:creator>Qirui Du</dc:creator><dc:creator>Jian-Fang Hu</dc:creator><dc:creator>Haichao Jiang</dc:creator><dc:creator>Zicheng Lin</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.
Published: 2026-02-04T11:33:16+00:00
Venue: arXiv
Score: 0.563 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianming Liang; Qirui Du; Jian-Fang Hu; Haichao Jiang; Zicheng Lin; Wei-Shi Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.563 (consider)&lt;/p&gt;
&lt;p&gt;Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.&lt;/p&gt;</content:encoded></item><item><title>Visible-guided Multigranularity Prompt Learning for Visible-Infrared Person Re-identification</title><link>https://doi.org/10.1016/j.eswa.2026.131464</link><guid>10.1016/j.eswa.2026.131464</guid><pubDate>Tue, 03 Feb 2026 00:32:33 +0000</pubDate><dc:creator>Yangyan Luo</dc:creator><dc:creator>Ying Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131464</prism:doi><description>Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.
Published: 2026-02-03T00:32:33+00:00
Venue: Expert Systems with Applications
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangyan Luo; Ying Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131464"&gt;10.1016/j.eswa.2026.131464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.&lt;/p&gt;</content:encoded></item><item><title>Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images</title><link>https://arxiv.org/abs/2602.01954v1</link><guid>http://arxiv.org/abs/2602.01954v1</guid><pubDate>Mon, 02 Feb 2026 11:03:01 +0000</pubDate><dc:creator>Shuai Yang</dc:creator><dc:creator>Ziyue Huang</dc:creator><dc:creator>Jiaxin Chen</dc:creator><dc:creator>Qingjie Liu</dc:creator><dc:creator>Yunhong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.
Published: 2026-02-02T11:03:01+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yang; Ziyue Huang; Jiaxin Chen; Qingjie Liu; Yunhong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.&lt;/p&gt;</content:encoded></item><item><title>Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement</title><link>https://arxiv.org/abs/2602.04304v1</link><guid>http://arxiv.org/abs/2602.04304v1</guid><pubDate>Wed, 04 Feb 2026 08:13:01 +0000</pubDate><dc:creator>Zipeng Zhu</dc:creator><dc:creator>Zhanghao Hu</dc:creator><dc:creator>Qinglin Zhu</dc:creator><dc:creator>Yuxi Hong</dc:creator><dc:creator>Yijun Liu</dc:creator><dc:creator>Jingyong Su</dc:creator><dc:creator>Yulan He</dc:creator><dc:creator>Lin Gui</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.
Published: 2026-02-04T08:13:01+00:00
Venue: arXiv
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zipeng Zhu; Zhanghao Hu; Qinglin Zhu; Yuxi Hong; Yijun Liu; Jingyong Su; Yulan He; Lin Gui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &amp;quot;magic layer&amp;quot; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.&lt;/p&gt;</content:encoded></item><item><title>Deep Learning-Based Remote Sensing Image Super-Resolution: Recent Advances and Challenges</title><link>https://doi.org/10.1016/j.neucom.2026.132939</link><guid>10.1016/j.neucom.2026.132939</guid><pubDate>Tue, 03 Feb 2026 15:57:47 +0000</pubDate><dc:creator>Jiawei Yang</dc:creator><dc:creator>Hongliang Ren</dc:creator><dc:creator>Zhichao He</dc:creator><dc:creator>Mengjie Zeng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132939</prism:doi><description>As a crucial data source for Earth science research and spatial information applications, remote sensing images often face limitations in spatial resolution due to factors such as sensor performance, imaging conditions, and costs, making it challenging to meet the growing demand for fine-grained analysis. In recent years, deep learning-based remote sensing image super-resolution (RSISR) technology has demonstrated significant potential by reconstructing high-resolution (HR) details from low-resolution (LR) remote sensing images, quickly becoming a research hotspot. However, systematic reviews of RSISR methodologies, network architecture evolution, domain development characteristics, and future directions remain relatively scarce. To address this, this study comprehensively reviews the major advancements in the field since 2020 based on the development of deep learning-based RSISR frameworks. First, it defines the RSISR problem, provides a comprehensive statistical analysis of RSISR algorithms published from 2020 onward, and selects over 100 deep learning-related publications for in-depth study. Subsequently, existing research is systematically categorized according to methodological principles: supervised learning methods are divided into six categories based on convolutional neural networks, attention mechanisms, generative adversarial networks, Transformers, diffusion models, and Mamba, while unsupervised learning methods are grouped into four frameworks including self-supervised learning, contrastive learning, zero-shot learning, and generative methods. Additionally, commonly used datasets, loss functions, and evaluation metrics in RSISR tasks are reviewed, and existing performance assessment methods are discussed in detail. Finally, the study summarizes the current development trends, future directions, and key challenges in the field, aiming to provide theoretical reference and practical guidance for related research.
Published: 2026-02-03T15:57:47+00:00
Venue: Neurocomputing
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Yang; Hongliang Ren; Zhichao He; Mengjie Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132939"&gt;10.1016/j.neucom.2026.132939&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;As a crucial data source for Earth science research and spatial information applications, remote sensing images often face limitations in spatial resolution due to factors such as sensor performance, imaging conditions, and costs, making it challenging to meet the growing demand for fine-grained analysis. In recent years, deep learning-based remote sensing image super-resolution (RSISR) technology has demonstrated significant potential by reconstructing high-resolution (HR) details from low-resolution (LR) remote sensing images, quickly becoming a research hotspot. However, systematic reviews of RSISR methodologies, network architecture evolution, domain development characteristics, and future directions remain relatively scarce. To address this, this study comprehensively reviews the major advancements in the field since 2020 based on the development of deep learning-based RSISR frameworks. First, it defines the RSISR problem, provides a comprehensive statistical analysis of RSISR algorithms published from 2020 onward, and selects over 100 deep learning-related publications for in-depth study. Subsequently, existing research is systematically categorized according to methodological principles: supervised learning methods are divided into six categories based on convolutional neural networks, attention mechanisms, generative adversarial networks, Transformers, diffusion models, and Mamba, while unsupervised learning methods are grouped into four frameworks including self-supervised learning, contrastive learning, zero-shot learning, and generative methods. Additionally, commonly used datasets, loss functions, and evaluation metrics in RSISR tasks are reviewed, and existing performance assessment methods are discussed in detail. Finally, the study summarizes the current development trends, future directions, and key challenges in the field, aiming to provide theoretical reference and practical guidance for related research.&lt;/p&gt;</content:encoded></item><item><title>SCALAR: Spatial-Concept Alignment for Robust Vision in Harsh Open World</title><link>https://doi.org/10.1016/j.patcog.2026.113203</link><guid>10.1016/j.patcog.2026.113203</guid><pubDate>Tue, 03 Feb 2026 00:28:30 +0000</pubDate><dc:creator>Xiaoyu Yang</dc:creator><dc:creator>Lijian Xu</dc:creator><dc:creator>Xingyu Zeng</dc:creator><dc:creator>Xiaosong Wang</dc:creator><dc:creator>Hongsheng Li</dc:creator><dc:creator>Shaoting Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113203</prism:doi><description>Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .
Published: 2026-02-03T00:28:30+00:00
Venue: Pattern Recognition
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyu Yang; Lijian Xu; Xingyu Zeng; Xiaosong Wang; Hongsheng Li; Shaoting Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113203"&gt;10.1016/j.patcog.2026.113203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .&lt;/p&gt;</content:encoded></item><item><title>Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation</title><link>https://arxiv.org/abs/2602.03595v1</link><guid>http://arxiv.org/abs/2602.03595v1</guid><pubDate>Tue, 03 Feb 2026 14:48:12 +0000</pubDate><dc:creator>Haichao Jiang</dc:creator><dc:creator>Tianming Liang</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Jian-Fang Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.
Published: 2026-02-03T14:48:12+00:00
Venue: arXiv
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haichao Jiang; Tianming Liang; Wei-Shi Zheng; Jian-Fang Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&amp;#x27;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>DEEP: Decoupled Semantic Prompt Learning, Guiding and Embedding for Multi-Spectral Object Re-Identification</title><link>https://doi.org/10.1109/tmm.2026.3660160</link><guid>10.1109/tmm.2026.3660160</guid><pubDate>Tue, 03 Feb 2026 20:55:41 +0000</pubDate><dc:creator>Shihao Li</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660160</prism:doi><description>Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.
Published: 2026-02-03T20:55:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihao Li; Chenglong Li; Aihua Zheng; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660160"&gt;10.1109/tmm.2026.3660160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.&lt;/p&gt;</content:encoded></item><item><title>SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences</title><link>https://arxiv.org/abs/2602.02974v1</link><guid>http://arxiv.org/abs/2602.02974v1</guid><pubDate>Tue, 03 Feb 2026 01:22:07 +0000</pubDate><dc:creator>Seok-Young Kim</dc:creator><dc:creator>Dooyoung Kim</dc:creator><dc:creator>Woojin Cho</dc:creator><dc:creator>Hail Song</dc:creator><dc:creator>Suji Kang</dc:creator><dc:creator>Woontack Woo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.
Published: 2026-02-03T01:22:07+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seok-Young Kim; Dooyoung Kim; Woojin Cho; Hail Song; Suji Kang; Woontack Woo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user&amp;#x27;s space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.&lt;/p&gt;</content:encoded></item><item><title>Beyond Similarity: Mutual Information-Guided Retrieval for In-Context Learning in VQA</title><link>https://doi.org/10.1016/j.patcog.2026.113214</link><guid>10.1016/j.patcog.2026.113214</guid><pubDate>Tue, 03 Feb 2026 15:54:37 +0000</pubDate><dc:creator>Jun Zhang</dc:creator><dc:creator>Zezhong Lv</dc:creator><dc:creator>Jian Zhao</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Tianle Zhang</dc:creator><dc:creator>Yuchen Yuan</dc:creator><dc:creator>Yuchu Jiang</dc:creator><dc:creator>Chi Zhang</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113214</prism:doi><description>Visual Question Answering (VQA) is a challenging multi-modal task. In-context Learning (ICL) has shown promise in improving the generalization of pre-trained models on VQA by retrieving image-text pairs that are similar to the given query. However, existing approaches overlook two critical issues: i) The effectiveness of the In-context Demonstration (ICD) in prompting a pre-trained model is not strictly correlated with the feature similarity. ii) As a multi-modal task involving both vision and language, VQA requires a joint understanding of visual and textual modalities, which is difficult to achieve when retrieval is based on a single modality. To address these limitations, we propose a novel Mutual Information-Guided Retrieval (MIGR) model. Specifically, we annotate a small subset of data (5% of the dataset) with ICD quality scores based on VQA performance, and train our model to maximize the multi-modal mutual information between each query and its corresponding high-quality ICDs. This enables the model to capture more complex relationships beyond feature-level similarity, leading to improved generalization in ICL. Extensive experiments demonstrate that our mutual information-based retrieval strategy significantly outperforms conventional similarity-based retrieval methods in VQA tasks.
Published: 2026-02-03T15:54:37+00:00
Venue: Pattern Recognition
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun Zhang; Zezhong Lv; Jian Zhao; Yan Wang; Tianle Zhang; Yuchen Yuan; Yuchu Jiang; Chi Zhang; Wenqi Ren; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113214"&gt;10.1016/j.patcog.2026.113214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) is a challenging multi-modal task. In-context Learning (ICL) has shown promise in improving the generalization of pre-trained models on VQA by retrieving image-text pairs that are similar to the given query. However, existing approaches overlook two critical issues: i) The effectiveness of the In-context Demonstration (ICD) in prompting a pre-trained model is not strictly correlated with the feature similarity. ii) As a multi-modal task involving both vision and language, VQA requires a joint understanding of visual and textual modalities, which is difficult to achieve when retrieval is based on a single modality. To address these limitations, we propose a novel Mutual Information-Guided Retrieval (MIGR) model. Specifically, we annotate a small subset of data (5% of the dataset) with ICD quality scores based on VQA performance, and train our model to maximize the multi-modal mutual information between each query and its corresponding high-quality ICDs. This enables the model to capture more complex relationships beyond feature-level similarity, leading to improved generalization in ICL. Extensive experiments demonstrate that our mutual information-based retrieval strategy significantly outperforms conventional similarity-based retrieval methods in VQA tasks.&lt;/p&gt;</content:encoded></item><item><title>IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</title><link>https://arxiv.org/abs/2602.03060v1</link><guid>http://arxiv.org/abs/2602.03060v1</guid><pubDate>Tue, 03 Feb 2026 03:39:31 +0000</pubDate><dc:creator>Zhichao Sun</dc:creator><dc:creator>Yidong Ma</dc:creator><dc:creator>Gang Liu</dc:creator><dc:creator>Yibo Chen</dc:creator><dc:creator>Xu Tang</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Yongchao Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.
Published: 2026-02-03T03:39:31+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhichao Sun; Yidong Ma; Gang Liu; Yibo Chen; Xu Tang; Yao Hu; Yongchao Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.&lt;/p&gt;</content:encoded></item><item><title>MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement</title><link>https://arxiv.org/abs/2602.01760v1</link><guid>http://arxiv.org/abs/2602.01760v1</guid><pubDate>Mon, 02 Feb 2026 07:43:29 +0000</pubDate><dc:creator>Hao Zhang</dc:creator><dc:creator>Yanping Zha</dc:creator><dc:creator>Zizhuo Li</dc:creator><dc:creator>Meiqi Gong</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.
Published: 2026-02-02T07:43:29+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhang; Yanping Zha; Zizhuo Li; Meiqi Gong; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.&lt;/p&gt;</content:encoded></item><item><title>DGFFA: Joint Multimodal Entity-Relation Extraction via Dual-Channel Graph Fusion and Fine-Grained Alignment</title><link>https://doi.org/10.1016/j.knosys.2026.115470</link><guid>10.1016/j.knosys.2026.115470</guid><pubDate>Tue, 03 Feb 2026 00:09:05 +0000</pubDate><dc:creator>Wenjie Liu</dc:creator><dc:creator>Xingwen Li</dc:creator><dc:creator>Zhijie Ren</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115470</prism:doi><description>Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.
Published: 2026-02-03T00:09:05+00:00
Venue: Knowledge-Based Systems
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Liu; Xingwen Li; Zhijie Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115470"&gt;10.1016/j.knosys.2026.115470&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.&lt;/p&gt;</content:encoded></item><item><title>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</title><link>https://arxiv.org/abs/2602.04712v1</link><guid>http://arxiv.org/abs/2602.04712v1</guid><pubDate>Wed, 04 Feb 2026 16:23:16 +0000</pubDate><dc:creator>David F. Ramirez</dc:creator><dc:creator>Tim Overman</dc:creator><dc:creator>Kristen Jaskie</dc:creator><dc:creator>Joe Marvin</dc:creator><dc:creator>Andreas Spanias</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.
Published: 2026-02-04T16:23:16+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David F. Ramirez; Tim Overman; Kristen Jaskie; Joe Marvin; Andreas Spanias&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.&lt;/p&gt;</content:encoded></item><item><title>Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning</title><link>https://arxiv.org/abs/2602.02951v1</link><guid>http://arxiv.org/abs/2602.02951v1</guid><pubDate>Tue, 03 Feb 2026 00:51:03 +0000</pubDate><dc:creator>Yihong Huang</dc:creator><dc:creator>Fei Ma</dc:creator><dc:creator>Yihua Shao</dc:creator><dc:creator>Jingcai Guo</dc:creator><dc:creator>Zitong Yu</dc:creator><dc:creator>Laizhong Cui</dc:creator><dc:creator>Qi Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).
Published: 2026-02-03T00:51:03+00:00
Venue: arXiv
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihong Huang; Fei Ma; Yihua Shao; Jingcai Guo; Zitong Yu; Laizhong Cui; Qi Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&amp;#x27;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&amp;#x27; positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).&lt;/p&gt;</content:encoded></item><item><title>Preserving Localized Patch Semantics in VLMs</title><link>https://arxiv.org/abs/2602.01530v1</link><guid>http://arxiv.org/abs/2602.01530v1</guid><pubDate>Mon, 02 Feb 2026 01:48:11 +0000</pubDate><dc:creator>Parsa Esmaeilkhani</dc:creator><dc:creator>Longin Jan Latecki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.
Published: 2026-02-02T01:48:11+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Parsa Esmaeilkhani; Longin Jan Latecki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word &amp;quot;cat&amp;quot;), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.&lt;/p&gt;</content:encoded></item><item><title>Learning to Prompt with Refining Text Knowledge for Zero-shot Video Action Recognition</title><link>https://doi.org/10.1109/tmm.2026.3660143</link><guid>10.1109/tmm.2026.3660143</guid><pubDate>Tue, 03 Feb 2026 20:55:41 +0000</pubDate><dc:creator>Hao Wang</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Jiahao Wang</dc:creator><dc:creator>Shuo Li</dc:creator><dc:creator>Lingling Li</dc:creator><dc:creator>Puhua Chen</dc:creator><dc:creator>Xu Liu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660143</prism:doi><description>Foundational vision-language models (VLMs) like CLIP are redefining the vision domain with their exceptional generalization capabilities. Prompt-based learning methods adapt pre-trained VLMs to video action recognition tasks using task-specific learnable text tokens. However, these tokens often struggle to generalize to unseen categories, as they tend to forget general textual knowledge. To address this, we construct knowledge prompts composed of handcrafted and descriptive prompts and introduce a novel knowledge-guided context mapping to enhance the generalization of learnable prompts to unseen categories. This approach mitigates the forgetting of fundamental knowledge by reducing the discrepancy between learnable prompts and knowledge prompts while simultaneously allowing the prompts to extract rich contextual knowledge from LLM data. Then, incorporating the knowledge-guided context mapping into the contrastive loss enables zero-shot transfer of prompts to new categories and data, providing discriminative prompts for both seen and unseen tasks. In addition, we propose an advanced temporal aggregation method that refines uniform mean pooling by incorporating frame-level textual relevance scoring. Extensive evaluations on multiple benchmarks demonstrate that learning to prompt with refining text knowledge is an effective quick-tuning method, achieving superior sample generalization performance without increasing training parameters.
Published: 2026-02-03T20:55:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wang; Fang Liu; Licheng Jiao; Jiahao Wang; Shuo Li; Lingling Li; Puhua Chen; Xu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660143"&gt;10.1109/tmm.2026.3660143&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Foundational vision-language models (VLMs) like CLIP are redefining the vision domain with their exceptional generalization capabilities. Prompt-based learning methods adapt pre-trained VLMs to video action recognition tasks using task-specific learnable text tokens. However, these tokens often struggle to generalize to unseen categories, as they tend to forget general textual knowledge. To address this, we construct knowledge prompts composed of handcrafted and descriptive prompts and introduce a novel knowledge-guided context mapping to enhance the generalization of learnable prompts to unseen categories. This approach mitigates the forgetting of fundamental knowledge by reducing the discrepancy between learnable prompts and knowledge prompts while simultaneously allowing the prompts to extract rich contextual knowledge from LLM data. Then, incorporating the knowledge-guided context mapping into the contrastive loss enables zero-shot transfer of prompts to new categories and data, providing discriminative prompts for both seen and unseen tasks. In addition, we propose an advanced temporal aggregation method that refines uniform mean pooling by incorporating frame-level textual relevance scoring. Extensive evaluations on multiple benchmarks demonstrate that learning to prompt with refining text knowledge is an effective quick-tuning method, achieving superior sample generalization performance without increasing training parameters.&lt;/p&gt;</content:encoded></item><item><title>ReasonEdit: Editing Vision-Language Models using Human Reasoning</title><link>https://arxiv.org/abs/2602.02408v2</link><guid>http://arxiv.org/abs/2602.02408v2</guid><pubDate>Mon, 02 Feb 2026 18:06:14 +0000</pubDate><dc:creator>Jiaxing Qiu</dc:creator><dc:creator>Kaihua Hou</dc:creator><dc:creator>Roxana Daneshjou</dc:creator><dc:creator>Ahmed Alaa</dc:creator><dc:creator>Thomas Hartvigsen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.
Published: 2026-02-02T18:06:14+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxing Qiu; Kaihua Hou; Roxana Daneshjou; Ahmed Alaa; Thomas Hartvigsen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.&lt;/p&gt;</content:encoded></item><item><title>Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles</title><link>https://arxiv.org/abs/2602.01452v1</link><guid>http://arxiv.org/abs/2602.01452v1</guid><pubDate>Sun, 01 Feb 2026 21:43:02 +0000</pubDate><dc:creator>Penghao Deng</dc:creator><dc:creator>Jidong J. Yang</dc:creator><dc:creator>Jiachen Bian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.
Published: 2026-02-01T21:43:02+00:00
Venue: arXiv
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Penghao Deng; Jidong J. Yang; Jiachen Bian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle&amp;#x27;s front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a &amp;quot;part-versus-whole&amp;quot; semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.&lt;/p&gt;</content:encoded></item><item><title>Region-Level Vision-Language Model for Detecting Distraction Behavior and Mobility Attributes of Vulnerable Road Users</title><link>https://doi.org/10.1109/tits.2026.3657271</link><guid>10.1109/tits.2026.3657271</guid><pubDate>Tue, 03 Feb 2026 20:56:28 +0000</pubDate><dc:creator>Dai Quoc Tran</dc:creator><dc:creator>Mohamed Abdel-Aty</dc:creator><dc:creator>Younggun Kim</dc:creator><dc:creator>Ahmed S. Abdelrahman</dc:creator><dc:creator>Zubayer Islam</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3657271</prism:doi><description>Vulnerable road users (VRUs) such as pedestrians and cyclists frequently engage in distracted behaviors (e.g., phone or headphone use) that elevate crash risk. General-purpose vision–language models (VLMs) struggle to capture these subtle, small, and context-dependent cues, and most camera pipelines still rely on frame-based analyses that ignore temporal context. We present IntersectionPAR, a lightweight vision–language framework that couples an Interactive Attribute Encoder with region-level captioning to recover safety-relevant semantic attributes (gender, phone/headphone use, mobility mode and aids) together with kinematic cues in a bird’s-eye view. We further introduce a temporal risk module that computes a continuous Situational Awareness Score (from phone, pose, and gaze proxies) and a Dynamic Risk Score that fuses awareness with spatial context (safe vs. unsafe zones), aligning system output with the specific moments practitioners need to act. Our dataset contains 12,273 annotated images drawn from~200 intersection-crossing scenarios under varied lighting and weather. Under a standardized zero-shot protocol against a broad suite of modern VLMs, IntersectionPAR attains keyword-based accuracy of 0.6123 with balanced precision/recall (0.7936/0.7864) and an F1-score of 0.7719 while requiring only ~9 GB of VRAM(&gt;40 GB for several baselines). Qualitative analyses show strong behavior on safety-critical cues (e.g., cane and mobility-aid detection) and illustrate how the temporal risk module elevates risk precisely when low-awareness VRUs enter the roadway. Statistical analyses of 200 crossing events further reveal behavioral impacts of phone use (e.g., longer waiting time and increased path deviation). By pairing accurate, attribute-aware perception with low computational cost and an actionable temporal risk signal, IntersectionPAR supports real-time intersection safety monitoring and proactive interventions to mitigate VRU risk.
Published: 2026-02-03T20:56:28+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dai Quoc Tran; Mohamed Abdel-Aty; Younggun Kim; Ahmed S. Abdelrahman; Zubayer Islam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3657271"&gt;10.1109/tits.2026.3657271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Vulnerable road users (VRUs) such as pedestrians and cyclists frequently engage in distracted behaviors (e.g., phone or headphone use) that elevate crash risk. General-purpose vision–language models (VLMs) struggle to capture these subtle, small, and context-dependent cues, and most camera pipelines still rely on frame-based analyses that ignore temporal context. We present IntersectionPAR, a lightweight vision–language framework that couples an Interactive Attribute Encoder with region-level captioning to recover safety-relevant semantic attributes (gender, phone/headphone use, mobility mode and aids) together with kinematic cues in a bird’s-eye view. We further introduce a temporal risk module that computes a continuous Situational Awareness Score (from phone, pose, and gaze proxies) and a Dynamic Risk Score that fuses awareness with spatial context (safe vs. unsafe zones), aligning system output with the specific moments practitioners need to act. Our dataset contains 12,273 annotated images drawn from~200 intersection-crossing scenarios under varied lighting and weather. Under a standardized zero-shot protocol against a broad suite of modern VLMs, IntersectionPAR attains keyword-based accuracy of 0.6123 with balanced precision/recall (0.7936/0.7864) and an F1-score of 0.7719 while requiring only ~9 GB of VRAM(&amp;gt;40 GB for several baselines). Qualitative analyses show strong behavior on safety-critical cues (e.g., cane and mobility-aid detection) and illustrate how the temporal risk module elevates risk precisely when low-awareness VRUs enter the roadway. Statistical analyses of 200 crossing events further reveal behavioral impacts of phone use (e.g., longer waiting time and increased path deviation). By pairing accurate, attribute-aware perception with low computational cost and an actionable temporal risk signal, IntersectionPAR supports real-time intersection safety monitoring and proactive interventions to mitigate VRU risk.&lt;/p&gt;</content:encoded></item><item><title>ObjEmbed: Towards Universal Multimodal Object Embeddings</title><link>https://arxiv.org/abs/2602.01753v2</link><guid>http://arxiv.org/abs/2602.01753v2</guid><pubDate>Mon, 02 Feb 2026 07:38:45 +0000</pubDate><dc:creator>Shenghao Fu</dc:creator><dc:creator>Yukun Su</dc:creator><dc:creator>Fengyun Rao</dc:creator><dc:creator>Jing Lyu</dc:creator><dc:creator>Xiaohua Xie</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.
Published: 2026-02-02T07:38:45+00:00
Venue: arXiv
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shenghao Fu; Yukun Su; Fengyun Rao; Jing Lyu; Xiaohua Xie; Wei-Shi Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.&lt;/p&gt;</content:encoded></item><item><title>ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</title><link>https://arxiv.org/abs/2602.02873v1</link><guid>http://arxiv.org/abs/2602.02873v1</guid><pubDate>Mon, 02 Feb 2026 22:29:57 +0000</pubDate><dc:creator>Weihang You</dc:creator><dc:creator>Qingchan Zhu</dc:creator><dc:creator>David Liu</dc:creator><dc:creator>Yi Pan</dc:creator><dc:creator>Geng Yuan</dc:creator><dc:creator>Hanqi Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.
Published: 2026-02-02T22:29:57+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weihang You; Qingchan Zhu; David Liu; Yi Pan; Geng Yuan; Hanqi Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.&lt;/p&gt;</content:encoded></item><item><title>MG-TVMF: Multi-grained Text-Video Matching and Fusing for Weakly Supervised Video Anomaly Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113201</link><guid>10.1016/j.patcog.2026.113201</guid><pubDate>Tue, 03 Feb 2026 00:28:41 +0000</pubDate><dc:creator>Ping He</dc:creator><dc:creator>Xiaonan Gao</dc:creator><dc:creator>Huibin Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113201</prism:doi><description>Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.
Published: 2026-02-03T00:28:41+00:00
Venue: Pattern Recognition
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ping He; Xiaonan Gao; Huibin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113201"&gt;10.1016/j.patcog.2026.113201&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.&lt;/p&gt;</content:encoded></item><item><title>FastSAM-CD: Remote Sensing Image Change Detection Using Vision Foundation Models with Stronger Encoder and Decoder</title><link>https://doi.org/10.1109/tgrs.2026.3660946</link><guid>10.1109/tgrs.2026.3660946</guid><pubDate>Tue, 03 Feb 2026 20:54:52 +0000</pubDate><dc:creator>Shuxin Zhang</dc:creator><dc:creator>Tao Lei</dc:creator><dc:creator>Xingwu Wang</dc:creator><dc:creator>Tongfei Liu</dc:creator><dc:creator>Zhiyong Lv</dc:creator><dc:creator>Daqi Liu</dc:creator><dc:creator>Maoguo Gong</dc:creator><dc:creator>Asoke K. Nandi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3660946</prism:doi><description>Remote sensing image change detection (RSICD) is a crucial technique for Earth observation. However, the mainstream RSICD methods still face two main challenges. First, the encoding stage often fails to capture fine-grained structural representations, particularly in scenarios involving cross-scale targets and complex boundaries. Second, the decoding stage lacks effective modeling of spectral heterogeneity and direction-sensitive channel interactions, which severely limits the ability to accurately recognize strip-shaped objects. To address these issues, this paper proposes a network for RSICD named FastSAM-CD, which extends FastSAM with two dedicated modules. First, we design a hyperfusion multi-view adapter (HFM Adapter) for the encoder of FastSAM-CD. It significantly enhances the model’s ability to capture fine-grained boundaries across scales by constructing multi-view paths in both spatial-channel and local-global dimensions. Second, we propose a spectral-axial dynamic modulation module (SDM Module) for the decoder. It enhances the decoder’s extraction of spatial-frequency domain information through axial sensing and frequency-domain analysis, significantly improving the model’s detection accuracy for strip-shaped objects. The experiments on three RSICD datasets demonstrate that the proposed method outperforms the existing mainstream RSICD methods in terms of accuracy and efficiency.
Published: 2026-02-03T20:54:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuxin Zhang; Tao Lei; Xingwu Wang; Tongfei Liu; Zhiyong Lv; Daqi Liu; Maoguo Gong; Asoke K. Nandi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3660946"&gt;10.1109/tgrs.2026.3660946&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image change detection (RSICD) is a crucial technique for Earth observation. However, the mainstream RSICD methods still face two main challenges. First, the encoding stage often fails to capture fine-grained structural representations, particularly in scenarios involving cross-scale targets and complex boundaries. Second, the decoding stage lacks effective modeling of spectral heterogeneity and direction-sensitive channel interactions, which severely limits the ability to accurately recognize strip-shaped objects. To address these issues, this paper proposes a network for RSICD named FastSAM-CD, which extends FastSAM with two dedicated modules. First, we design a hyperfusion multi-view adapter (HFM Adapter) for the encoder of FastSAM-CD. It significantly enhances the model’s ability to capture fine-grained boundaries across scales by constructing multi-view paths in both spatial-channel and local-global dimensions. Second, we propose a spectral-axial dynamic modulation module (SDM Module) for the decoder. It enhances the decoder’s extraction of spatial-frequency domain information through axial sensing and frequency-domain analysis, significantly improving the model’s detection accuracy for strip-shaped objects. The experiments on three RSICD datasets demonstrate that the proposed method outperforms the existing mainstream RSICD methods in terms of accuracy and efficiency.&lt;/p&gt;</content:encoded></item><item><title>Toward Cognitive Supersensing in Multimodal Large Language Model</title><link>https://arxiv.org/abs/2602.01541v1</link><guid>http://arxiv.org/abs/2602.01541v1</guid><pubDate>Mon, 02 Feb 2026 02:19:50 +0000</pubDate><dc:creator>Boyi Li</dc:creator><dc:creator>Yifan Shen</dc:creator><dc:creator>Yuanzhe Liu</dc:creator><dc:creator>Yifan Xu</dc:creator><dc:creator>Jiateng Liu</dc:creator><dc:creator>Xinzhuo Li</dc:creator><dc:creator>Zhengyuan Li</dc:creator><dc:creator>Jingyuan Zhu</dc:creator><dc:creator>Yunhan Zhong</dc:creator><dc:creator>Fangzhou Lan</dc:creator><dc:creator>Jianguo Cao</dc:creator><dc:creator>James M. Rehg</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Ismini Lourentzou</dc:creator><dc:creator>Xu Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.
Published: 2026-02-02T02:19:50+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyi Li; Yifan Shen; Yuanzhe Liu; Yifan Xu; Jiateng Liu; Xinzhuo Li; Zhengyuan Li; Jingyuan Zhu; Yunhan Zhong; Fangzhou Lan; Jianguo Cao; James M. Rehg; Heng Ji; Ismini Lourentzou; Xu Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.&lt;/p&gt;</content:encoded></item></channel></rss>