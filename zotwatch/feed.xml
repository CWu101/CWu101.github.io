<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 31 Jan 2026 03:19:04 +0000</lastBuildDate><item><title>Set-CVGL: A new perspective on cross-view geo-localization with unordered ground-view image sets</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.037</link><guid>10.1016/j.isprsjprs.2026.01.037</guid><pubDate>Fri, 30 Jan 2026 10:57:03 +0000</pubDate><dc:creator>Qiong Wu</dc:creator><dc:creator>Panwang Xia</dc:creator><dc:creator>Lei Yu</dc:creator><dc:creator>Yi Liu</dc:creator><dc:creator>Mingtao Xiong</dc:creator><dc:creator>Liheng Zhong</dc:creator><dc:creator>Jingdong Chen</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:creator>Yi Wan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.037</prism:doi><description>Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;#xD7; " role="presentation"&gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .
Published: 2026-01-30T10:57:03+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiong Wu; Panwang Xia; Lei Yu; Yi Liu; Mingtao Xiong; Liheng Zhong; Jingdong Chen; Ming Yang; Yongjun Zhang; Yi Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037"&gt;10.1016/j.isprsjprs.2026.01.037&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;amp;#xD7; &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .&lt;/p&gt;</content:encoded></item><item><title>SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</title><link>https://arxiv.org/abs/2601.21498v1</link><guid>http://arxiv.org/abs/2601.21498v1</guid><pubDate>Thu, 29 Jan 2026 10:15:55 +0000</pubDate><dc:creator>Thanh-Nhan Vo</dc:creator><dc:creator>Trong-Thuan Nguyen</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.
Published: 2026-01-29T10:15:55+00:00
Venue: arXiv
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Alignment Supervision Matters in Vision-and-Language Navigation</title><link>https://doi.org/10.1109/tpami.2026.3658949</link><guid>10.1109/tpami.2026.3658949</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Keji He</dc:creator><dc:creator>Yan Huang</dc:creator><dc:creator>Ya Jing</dc:creator><dc:creator>Qi Wu</dc:creator><dc:creator>Liang Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658949</prism:doi><description>The Vision-and-Language Navigation (VLN) task involves an agent navigating within 3D indoor environments based on provided instructions. Achieving cross-modal alignment presents one of the most critical challenges in VLN, as the predicted trajectory needs to precisely align with the given instruction. This paper focuses on addressing cross-modal alignment in VLN from a fine-grained perspective. Firstly, to address the issue of weak cross-modal alignment supervision arising from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset called Landmark-RxR. This dataset aims to offer precise, fine-grained supervision for VLN. Secondly, in order to comprehensively demonstrate the potential and advantage of the fine-grained data from Landmark-RxR, we explore the core components of the training process that depend on the characteristics of the training data. These components include data augmentation, training paradigm, reward shaping, and navigation loss design. Leveraging our fine-grained data, we carefully design methods for handling them and introduce a novel evaluation mechanism. The experimental results demonstrate that the fine-grained data can effectively improve the agent's cross-modal alignment ability. Access to the Landmark-RxR dataset can be obtained from https://github.com/hekj/Landmark-RxR.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.555 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keji He; Yan Huang; Ya Jing; Qi Wu; Liang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658949"&gt;10.1109/tpami.2026.3658949&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.555 (consider)&lt;/p&gt;
&lt;p&gt;The Vision-and-Language Navigation (VLN) task involves an agent navigating within 3D indoor environments based on provided instructions. Achieving cross-modal alignment presents one of the most critical challenges in VLN, as the predicted trajectory needs to precisely align with the given instruction. This paper focuses on addressing cross-modal alignment in VLN from a fine-grained perspective. Firstly, to address the issue of weak cross-modal alignment supervision arising from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset called Landmark-RxR. This dataset aims to offer precise, fine-grained supervision for VLN. Secondly, in order to comprehensively demonstrate the potential and advantage of the fine-grained data from Landmark-RxR, we explore the core components of the training process that depend on the characteristics of the training data. These components include data augmentation, training paradigm, reward shaping, and navigation loss design. Leveraging our fine-grained data, we carefully design methods for handling them and introduce a novel evaluation mechanism. The experimental results demonstrate that the fine-grained data can effectively improve the agent&amp;#x27;s cross-modal alignment ability. Access to the Landmark-RxR dataset can be obtained from https://github.com/hekj/Landmark-RxR.&lt;/p&gt;</content:encoded></item><item><title>Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</title><link>https://doi.org/10.1016/j.inffus.2026.104193</link><guid>10.1016/j.inffus.2026.104193</guid><pubDate>Thu, 29 Jan 2026 07:44:09 +0000</pubDate><dc:creator>Shunlei Li</dc:creator><dc:creator>Longsen Gao</dc:creator><dc:creator>Jin Wang</dc:creator><dc:creator>Chang Che</dc:creator><dc:creator>Xi Xiao</dc:creator><dc:creator>Jiuwen Cao</dc:creator><dc:creator>Yingbai Hu</dc:creator><dc:creator>Hamid Reza Karimi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104193</prism:doi><description>Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB(-D) human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95% graph accuracy and 93% subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94% grasp success, 89% placement accuracy, and 90% overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.
Published: 2026-01-29T07:44:09+00:00
Venue: Information Fusion
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shunlei Li; Longsen Gao; Jin Wang; Chang Che; Xi Xiao; Jiuwen Cao; Yingbai Hu; Hamid Reza Karimi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104193"&gt;10.1016/j.inffus.2026.104193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB(-D) human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95% graph accuracy and 93% subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94% grasp success, 89% placement accuracy, and 90% overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.&lt;/p&gt;</content:encoded></item><item><title>SCAL: A Semantic-Consistent Adaptive Alignment Learning Framework for UAV Remote Sensing Cross-Modal Retrieval</title><link>https://doi.org/10.1109/jstars.2026.3659670</link><guid>10.1109/jstars.2026.3659670</guid><pubDate>Thu, 29 Jan 2026 21:24:12 +0000</pubDate><dc:creator>Yaxiong Chen</dc:creator><dc:creator>Yutong Yang</dc:creator><dc:creator>Xiongbo Lu</dc:creator><dc:creator>Zhong Sai</dc:creator><dc:creator>Shili Xiong</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659670</prism:doi><description>The remote sensing cross-modal retrieval task aims to bridge the modality gap between visual data and textual descriptions. Among various remote sensing platforms, unmanned aerial vehicles (UAVs) have emerged as a prominent paradigm due to their flexible deployment and fine-grained observation capability, making UAV image-text retrieval a particularly important yet challenging setting. However, existing methods still suffer from significant limitations in UAV scenarios: on the one hand, most approaches rely on coarse-grained cross-modal alignment, and on the other hand, they are easily affected by background-dominated visual features. These issues hinder the establishment of reliable semantic associations between UAV images and text descriptions. To overcome these limitations, we propose a Semantic-Consistent Adaptive Alignment Learning (SCAL) framework that integrates two complementary modules: Confidence-Scaled Text-Conditioned Local Alignment (CSTLA) module and Contextual Semantic Alignment with Background Suppression (CSABG) module. Specifically, the CSTLA module refines fine-grained cross-modal interactions by performing textconditioned attention weighting and confidence-adaptive scaling, while the CSABG module maintains cross-modal consistency by suppressing background-dominated samples and transferring semantic relational structures from an EMA-based teacher model. Extensive experiments on two UAV image-text benchmark datasets demonstrate that the proposed SCAL framework achieves state-of-the-art retrieval performance, significantly enhancing fine-grained cross-modal alignment and overall semantic consistency.
Published: 2026-01-29T21:24:12+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaxiong Chen; Yutong Yang; Xiongbo Lu; Zhong Sai; Shili Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659670"&gt;10.1109/jstars.2026.3659670&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;The remote sensing cross-modal retrieval task aims to bridge the modality gap between visual data and textual descriptions. Among various remote sensing platforms, unmanned aerial vehicles (UAVs) have emerged as a prominent paradigm due to their flexible deployment and fine-grained observation capability, making UAV image-text retrieval a particularly important yet challenging setting. However, existing methods still suffer from significant limitations in UAV scenarios: on the one hand, most approaches rely on coarse-grained cross-modal alignment, and on the other hand, they are easily affected by background-dominated visual features. These issues hinder the establishment of reliable semantic associations between UAV images and text descriptions. To overcome these limitations, we propose a Semantic-Consistent Adaptive Alignment Learning (SCAL) framework that integrates two complementary modules: Confidence-Scaled Text-Conditioned Local Alignment (CSTLA) module and Contextual Semantic Alignment with Background Suppression (CSABG) module. Specifically, the CSTLA module refines fine-grained cross-modal interactions by performing textconditioned attention weighting and confidence-adaptive scaling, while the CSABG module maintains cross-modal consistency by suppressing background-dominated samples and transferring semantic relational structures from an EMA-based teacher model. Extensive experiments on two UAV image-text benchmark datasets demonstrate that the proposed SCAL framework achieves state-of-the-art retrieval performance, significantly enhancing fine-grained cross-modal alignment and overall semantic consistency.&lt;/p&gt;</content:encoded></item><item><title>DeepSeek-OCR 2: Visual Causal Flow</title><link>https://arxiv.org/abs/2601.20552v1</link><guid>http://arxiv.org/abs/2601.20552v1</guid><pubDate>Wed, 28 Jan 2026 12:46:07 +0000</pubDate><dc:creator>Haoran Wei</dc:creator><dc:creator>Yaofeng Sun</dc:creator><dc:creator>Yukun Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.
Published: 2026-01-28T12:46:07+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Wei; Yaofeng Sun; Yukun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.&lt;/p&gt;</content:encoded></item><item><title>Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.21159v1</link><guid>http://arxiv.org/abs/2601.21159v1</guid><pubDate>Thu, 29 Jan 2026 01:46:03 +0000</pubDate><dc:creator>Jianzheng Wang</dc:creator><dc:creator>Huan Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.
Published: 2026-01-29T01:46:03+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianzheng Wang; Huan Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using &amp;quot;one-way injection&amp;quot; and &amp;quot;shallow post-processing&amp;quot; strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.&lt;/p&gt;</content:encoded></item><item><title>FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</title><link>https://arxiv.org/abs/2601.21187v1</link><guid>http://arxiv.org/abs/2601.21187v1</guid><pubDate>Thu, 29 Jan 2026 02:36:19 +0000</pubDate><dc:creator>Chenyu Huang</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Xudong Tan</dc:creator><dc:creator>Jinhan Mu</dc:creator><dc:creator>Shenghe Zheng</dc:creator><dc:creator>Li Shen</dc:creator><dc:creator>Tao Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.
Published: 2026-01-29T02:36:19+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyu Huang; Peng Ye; Xudong Tan; Jinhan Mu; Shenghe Zheng; Li Shen; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&amp;#x27;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.&lt;/p&gt;</content:encoded></item><item><title>m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</title><link>https://arxiv.org/abs/2601.19099v1</link><guid>http://arxiv.org/abs/2601.19099v1</guid><pubDate>Tue, 27 Jan 2026 02:01:56 +0000</pubDate><dc:creator>Yosub Shin</dc:creator><dc:creator>Michael Buriek</dc:creator><dc:creator>Igor Molybog</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.
Published: 2026-01-27T02:01:56+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yosub Shin; Michael Buriek; Igor Molybog&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Diffusion Models and Representation Learning: A Survey</title><link>https://doi.org/10.1109/tpami.2026.3658965</link><guid>10.1109/tpami.2026.3658965</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Michael Fuest</dc:creator><dc:creator>Pingchuan Ma</dc:creator><dc:creator>Ming Gui</dc:creator><dc:creator>Johannes Schusterbauer</dc:creator><dc:creator>Vincent Tao Hu</dc:creator><dc:creator>Björn Ommer</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658965</prism:doi><description>Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Michael Fuest; Pingchuan Ma; Ming Gui; Johannes Schusterbauer; Vincent Tao Hu; Björn Ommer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658965"&gt;10.1109/tpami.2026.3658965&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models&amp;#x27; essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Model with Siamese Bilateral Difference Network and Text-Guided Image Feature Enhancement for Acute Ischemic Stroke Outcome Prediction on CT Angiography</title><link>https://doi.org/10.1016/j.inffus.2026.104195</link><guid>10.1016/j.inffus.2026.104195</guid><pubDate>Fri, 30 Jan 2026 00:38:27 +0000</pubDate><dc:creator>Hulin Kuang</dc:creator><dc:creator>Bin Hu</dc:creator><dc:creator>Shuai Yang</dc:creator><dc:creator>Dongcui Wang</dc:creator><dc:creator>Guanghua Luo</dc:creator><dc:creator>Weihua Liao</dc:creator><dc:creator>Wu Qiu</dc:creator><dc:creator>Shulin Liu</dc:creator><dc:creator>Jianxin Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104195</prism:doi><description>Acute ischemic stroke (AIS) outcome prediction is crucial for treatment decisions. However, AIS outcome prediction is challenging due to the combined influence of lesion characteristics, vascular status, and other health conditions. In this study, we introduce a vision-language model with a Siamese bilateral difference network and a text-guided image feature enhancement module for predicting AIS outcome (e.g., modified Rankin Scale, mRS) on CT angiography. In the Siamese bilateral difference network, based on fine-tuning the foundation model LVM-Med, we design an interactive Transformer fine-tuning encoder and a vision question answering guided bilateral difference awareness module, which generates bilateral difference text via image-text pair question answering as a prompt to enhance the extracted brain vascular difference features. Additionally, in the text-guided image feature enhancement module, we propose a text feature extraction module to extract patient phrase-level and inter-phrase embeddings from clinical notes, and employ a multi-scale image-text interaction module to obtain fine-grained phrase-enhanced image attention feature and coarse-grained phrase context-aware image attention feature. We validate our model on the public ISLES2024 dataset, a private dataset A, and an external AIS dataset. It achieves accuracies of 81.11%, 83.05%, and 80.00% and AUCs of 80.06%, 85.48% and 82.62% for 90-day mRS prediction on the 3 datasets, respectively, outperforming several state-of-the-art methods and demonstrating its generalization ability. Moreover, the proposed method can be effectively extended to glaucoma visual field progression prediction, which is also related to vascular differences and clinical notes.
Published: 2026-01-30T00:38:27+00:00
Venue: Information Fusion
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hulin Kuang; Bin Hu; Shuai Yang; Dongcui Wang; Guanghua Luo; Weihua Liao; Wu Qiu; Shulin Liu; Jianxin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104195"&gt;10.1016/j.inffus.2026.104195&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Acute ischemic stroke (AIS) outcome prediction is crucial for treatment decisions. However, AIS outcome prediction is challenging due to the combined influence of lesion characteristics, vascular status, and other health conditions. In this study, we introduce a vision-language model with a Siamese bilateral difference network and a text-guided image feature enhancement module for predicting AIS outcome (e.g., modified Rankin Scale, mRS) on CT angiography. In the Siamese bilateral difference network, based on fine-tuning the foundation model LVM-Med, we design an interactive Transformer fine-tuning encoder and a vision question answering guided bilateral difference awareness module, which generates bilateral difference text via image-text pair question answering as a prompt to enhance the extracted brain vascular difference features. Additionally, in the text-guided image feature enhancement module, we propose a text feature extraction module to extract patient phrase-level and inter-phrase embeddings from clinical notes, and employ a multi-scale image-text interaction module to obtain fine-grained phrase-enhanced image attention feature and coarse-grained phrase context-aware image attention feature. We validate our model on the public ISLES2024 dataset, a private dataset A, and an external AIS dataset. It achieves accuracies of 81.11%, 83.05%, and 80.00% and AUCs of 80.06%, 85.48% and 82.62% for 90-day mRS prediction on the 3 datasets, respectively, outperforming several state-of-the-art methods and demonstrating its generalization ability. Moreover, the proposed method can be effectively extended to glaucoma visual field progression prediction, which is also related to vascular differences and clinical notes.&lt;/p&gt;</content:encoded></item><item><title>Rewarding Fine-grained Image Captioning with Keyword Group Contrastive</title><link>https://doi.org/10.1016/j.eswa.2026.131405</link><guid>10.1016/j.eswa.2026.131405</guid><pubDate>Thu, 29 Jan 2026 19:01:20 +0000</pubDate><dc:creator>Kailiang Ye</dc:creator><dc:creator>Zheng Lu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Tianxiang Cui</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131405</prism:doi><description>Fine-grained image captioning aims to automatically generate a description with detailed information from a given image. The task poses significant challenges, as it requires image captioning models to accurately capture fine-grained details, effectively differentiate between visually similar yet distinct elements within an image, and generate detailed captions that comprehensively describe the image content. In this paper, we propose a novel framework for fine-grained image captioning that combines reinforcement learning and contrastive learning with specifically designed loss and rewards. Specifically, three image captioning objectives are devised: 1) a novel Keyword Group Contrastive loss for token representation learning by leveraging different groups of keywords matched by visual information; 2) a CLIP Contrastive reward encouraging the generated caption to be more similar to its input image and dissimilar to the other images; 3) a Fine-grained Grammar reward using the grammar ELECTRA discriminator for high-quality caption generation with good grammar. We evaluate the performance of our framework on the FineCapEval benchmark dataset and show that it significantly outperforms the existing state-of-the-art methods in terms of describing fine-grained information from its input images.
Published: 2026-01-29T19:01:20+00:00
Venue: Expert Systems with Applications
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kailiang Ye; Zheng Lu; Linlin Shen; Tianxiang Cui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131405"&gt;10.1016/j.eswa.2026.131405&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained image captioning aims to automatically generate a description with detailed information from a given image. The task poses significant challenges, as it requires image captioning models to accurately capture fine-grained details, effectively differentiate between visually similar yet distinct elements within an image, and generate detailed captions that comprehensively describe the image content. In this paper, we propose a novel framework for fine-grained image captioning that combines reinforcement learning and contrastive learning with specifically designed loss and rewards. Specifically, three image captioning objectives are devised: 1) a novel Keyword Group Contrastive loss for token representation learning by leveraging different groups of keywords matched by visual information; 2) a CLIP Contrastive reward encouraging the generated caption to be more similar to its input image and dissimilar to the other images; 3) a Fine-grained Grammar reward using the grammar ELECTRA discriminator for high-quality caption generation with good grammar. We evaluate the performance of our framework on the FineCapEval benchmark dataset and show that it significantly outperforms the existing state-of-the-art methods in terms of describing fine-grained information from its input images.&lt;/p&gt;</content:encoded></item><item><title>HACG: Leveraging Hierarchical Alignment and Caption Generation for Text-Video Retrieval</title><link>https://doi.org/10.1007/s11263-025-02645-7</link><guid>10.1007/s11263-025-02645-7</guid><pubDate>Thu, 29 Jan 2026 14:57:00 +0000</pubDate><dc:creator>Donglin Zhang</dc:creator><dc:creator>Zhiwen Wang</dc:creator><dc:creator>Xiao-Jun Wu</dc:creator><dc:creator>Josef Kittler</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02645-7</prism:doi><description>With the development of the Internet and multimedia technologies, text-video retrieval tasks have attracted increasing attention. Recently, some text-video retrieval methods have been proposed and demonstrated good performance. Typically, these methods leverage the given text and videos to train the model. However, the video modality may contain subtitles or other relevant textual information in real applications. Therefore, some effective information in video may not be well explored. Besides, existing text-video retrieval approaches may suffer from insufficient interaction between text and video, reducing semantic closeness and performance. To address these issues, we propose a novel retrieval framework, named HACG. To be specific, we utilize the video to generate assisted captions to further explore video information. The hierarchical video-caption interaction scheme is given in this work, which integrates caption features with both the frame and patch features of the video to enhance semantic richness and generalizability. Moreover, we introduce an attention-masking mechanism to selectively mask word tokens and propose a conditional reconstruction method to minimize the domain gap between auxiliary caption features and original text features. Experimental results show the developed method can achieve good performance on three mainstream datasets (e.g., MSRVTT, MSVD, and DiDeMo). The source code will be publicly available at: https://github.com/junmaZ/HACG
Published: 2026-01-29T14:57:00+00:00
Venue: International Journal of Computer Vision
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Donglin Zhang; Zhiwen Wang; Xiao-Jun Wu; Josef Kittler&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02645-7"&gt;10.1007/s11263-025-02645-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;With the development of the Internet and multimedia technologies, text-video retrieval tasks have attracted increasing attention. Recently, some text-video retrieval methods have been proposed and demonstrated good performance. Typically, these methods leverage the given text and videos to train the model. However, the video modality may contain subtitles or other relevant textual information in real applications. Therefore, some effective information in video may not be well explored. Besides, existing text-video retrieval approaches may suffer from insufficient interaction between text and video, reducing semantic closeness and performance. To address these issues, we propose a novel retrieval framework, named HACG. To be specific, we utilize the video to generate assisted captions to further explore video information. The hierarchical video-caption interaction scheme is given in this work, which integrates caption features with both the frame and patch features of the video to enhance semantic richness and generalizability. Moreover, we introduce an attention-masking mechanism to selectively mask word tokens and propose a conditional reconstruction method to minimize the domain gap between auxiliary caption features and original text features. Experimental results show the developed method can achieve good performance on three mainstream datasets (e.g., MSRVTT, MSVD, and DiDeMo). The source code will be publicly available at: https://github.com/junmaZ/HACG&lt;/p&gt;</content:encoded></item><item><title>Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</title><link>https://arxiv.org/abs/2601.21193v1</link><guid>http://arxiv.org/abs/2601.21193v1</guid><pubDate>Thu, 29 Jan 2026 02:49:33 +0000</pubDate><dc:creator>Zecheng Zhao</dc:creator><dc:creator>Zhi Chen</dc:creator><dc:creator>Zi Huang</dc:creator><dc:creator>Shazia Sadiq</dc:creator><dc:creator>Tong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.
Published: 2026-01-29T02:49:33+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zecheng Zhao; Zhi Chen; Zi Huang; Shazia Sadiq; Tong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.&lt;/p&gt;</content:encoded></item><item><title>LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge</title><link>https://arxiv.org/abs/2601.19155v1</link><guid>http://arxiv.org/abs/2601.19155v1</guid><pubDate>Tue, 27 Jan 2026 03:40:03 +0000</pubDate><dc:creator>Qiujun Li</dc:creator><dc:creator>Zijin Xiao</dc:creator><dc:creator>Xulin Wang</dc:creator><dc:creator>Zhidan Ma</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Haifeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.
Published: 2026-01-27T03:40:03+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiujun Li; Zijin Xiao; Xulin Wang; Zhidan Ma; Cheng Yang; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.&lt;/p&gt;</content:encoded></item><item><title>DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.20064v1</link><guid>http://arxiv.org/abs/2601.20064v1</guid><pubDate>Tue, 27 Jan 2026 21:15:10 +0000</pubDate><dc:creator>Zhen Yao</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Taotao Jing</dc:creator><dc:creator>Shuai Zhang</dc:creator><dc:creator>Mooi Choo Chuah</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.
Published: 2026-01-27T21:15:10+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Yao; Xin Li; Taotao Jing; Shuai Zhang; Mooi Choo Chuah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</title><link>https://arxiv.org/abs/2601.19433v1</link><guid>http://arxiv.org/abs/2601.19433v1</guid><pubDate>Tue, 27 Jan 2026 10:10:55 +0000</pubDate><dc:creator>Jisheng Chu</dc:creator><dc:creator>Wenrui Li</dc:creator><dc:creator>Rui Zhao</dc:creator><dc:creator>Wangmeng Zuo</dc:creator><dc:creator>Shifeng Chen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.
Published: 2026-01-27T10:10:55+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Chu; Wenrui Li; Rui Zhao; Wangmeng Zuo; Shifeng Chen; Xiaopeng Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.&lt;/p&gt;</content:encoded></item><item><title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.22060v1</link><guid>http://arxiv.org/abs/2601.22060v1</guid><pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate><dc:creator>Wenxuan Huang</dc:creator><dc:creator>Yu Zeng</dc:creator><dc:creator>Qiuchen Wang</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Shaosheng Cao</dc:creator><dc:creator>Zheng Chu</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Zhenfei Yin</dc:creator><dc:creator>Lin Chen</dc:creator><dc:creator>Zehui Chen</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Philip Torr</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Wanli Ouyang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.
Published: 2026-01-29T17:58:40+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxuan Huang; Yu Zeng; Qiuchen Wang; Zhen Fang; Shaosheng Cao; Zheng Chu; Qingyu Yin; Shuang Chen; Zhenfei Yin; Lin Chen; Zehui Chen; Yao Hu; Philip Torr; Feng Zhao; Wanli Ouyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&amp;#x27;&amp;#x27; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.&lt;/p&gt;</content:encoded></item><item><title>VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</title><link>https://arxiv.org/abs/2601.19887v1</link><guid>http://arxiv.org/abs/2601.19887v1</guid><pubDate>Tue, 27 Jan 2026 18:54:29 +0000</pubDate><dc:creator>Dominic Maggio</dc:creator><dc:creator>Luca Carlone</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.
Published: 2026-01-27T18:54:29+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dominic Maggio; Luca Carlone&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.&lt;/p&gt;</content:encoded></item><item><title>Thinker: A vision-language foundation model for embodied intelligence</title><link>https://arxiv.org/abs/2601.21199v1</link><guid>http://arxiv.org/abs/2601.21199v1</guid><pubDate>Thu, 29 Jan 2026 02:52:08 +0000</pubDate><dc:creator>Baiyu Pan</dc:creator><dc:creator>Daqin Luo</dc:creator><dc:creator>Junpeng Yang</dc:creator><dc:creator>Jiyuan Wang</dc:creator><dc:creator>Yixuan Zhang</dc:creator><dc:creator>Hailin Shi</dc:creator><dc:creator>Jichao Jiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.
Published: 2026-01-29T02:52:08+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baiyu Pan; Daqin Luo; Junpeng Yang; Jiyuan Wang; Yixuan Zhang; Hailin Shi; Jichao Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model&amp;#x27;s capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.&lt;/p&gt;</content:encoded></item><item><title>Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing</title><link>https://arxiv.org/abs/2601.20107v1</link><guid>http://arxiv.org/abs/2601.20107v1</guid><pubDate>Tue, 27 Jan 2026 22:50:11 +0000</pubDate><dc:creator>Zhuchenyang Liu</dc:creator><dc:creator>Ziyu Hu</dc:creator><dc:creator>Yao Zhang</dc:creator><dc:creator>Yu Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.
Published: 2026-01-27T22:50:11+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuchenyang Liu; Ziyu Hu; Yao Zhang; Yu Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&amp;gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.&lt;/p&gt;</content:encoded></item><item><title>PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization</title><link>https://arxiv.org/abs/2601.21617v1</link><guid>http://arxiv.org/abs/2601.21617v1</guid><pubDate>Thu, 29 Jan 2026 12:21:16 +0000</pubDate><dc:creator>Songhan Jiang</dc:creator><dc:creator>Fengchun Liu</dc:creator><dc:creator>Ziyue Wang</dc:creator><dc:creator>Linghan Cai</dc:creator><dc:creator>Yongbing Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.
Published: 2026-01-29T12:21:16+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Jiang; Fengchun Liu; Ziyue Wang; Linghan Cai; Yongbing Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.&lt;/p&gt;</content:encoded></item><item><title>CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization</title><link>https://arxiv.org/abs/2601.20355v1</link><guid>http://arxiv.org/abs/2601.20355v1</guid><pubDate>Wed, 28 Jan 2026 08:15:56 +0000</pubDate><dc:creator>Yue Liang</dc:creator><dc:creator>Jiatong Du</dc:creator><dc:creator>Ziyi Yang</dc:creator><dc:creator>Yanjun Huang</dc:creator><dc:creator>Hong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.
Published: 2026-01-28T08:15:56+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Liang; Jiatong Du; Ziyi Yang; Yanjun Huang; Hong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.&lt;/p&gt;</content:encoded></item><item><title>ReSaP: Reasoning-Enhanced and Scale-Aware Prompting for Referring Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/jstars.2026.3659080</link><guid>10.1109/jstars.2026.3659080</guid><pubDate>Thu, 29 Jan 2026 21:24:12 +0000</pubDate><dc:creator>Ning Lv</dc:creator><dc:creator>Jisheng Dang</dc:creator><dc:creator>Teng Wang</dc:creator><dc:creator>Bimei Wang</dc:creator><dc:creator>Yichu Liu</dc:creator><dc:creator>Hong Peng</dc:creator><dc:creator>Haowen Yan</dc:creator><dc:creator>Bin Hu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659080</prism:doi><description>Recent research has actively explored diverse mechanisms to unlock pixel-level segmentation capabilities in Multimodal Large Language Models (MLLMs), aiming to bridge the gap between high-level semantic reasoning and fine-grained visual perception. However, directly transferring these general-domain frameworks to Referring Remote Sensing Image Segmentation (RRSIS) faces significant hurdles. These challenges primarily stem from the weak pixel-level discrimination capability of MLLMs in complex geospatial scenes and the severe granularity mismatch caused by drastic scale variations in remote sensing targets. To overcome these limitations, this paper proposes ReSaP, a Reasoning-enhanced and Scale-aware Prompting framework. ReSaP incorporates two core components to effectively adapt MLLMs for pixel- wise tasks. First, we introduce a Pixel-Aware GRPO training scheme. By utilizing a reinforcement learning framework with a hybrid reward mechanism that integrates bipartite matching for localization and classification accuracy for verification, this scheme explicitly enhances the MLLM's fine-grained pixel discrimination and localization precision. Second, we propose the Scale-Aware Prompting strategy for inference. This mechanism employs a density-adaptive grid sampling approach to dynamically adjust the prompt configuration based on target dimensions, effectively harmonizing prompt granularity with object scale. Extensive experiments on the RRSIS-D and RIS-LAD benchmarks demonstrate that ReSaP significantly outperforms existing state-of-the-art methods, validating its superior performance and robustness across both satellite and unmanned aerial vehicle (UAV) observation perspectives. The source code is available at https://github.com/gray114514/ReSaP.
Published: 2026-01-29T21:24:12+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ning Lv; Jisheng Dang; Teng Wang; Bimei Wang; Yichu Liu; Hong Peng; Haowen Yan; Bin Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659080"&gt;10.1109/jstars.2026.3659080&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Recent research has actively explored diverse mechanisms to unlock pixel-level segmentation capabilities in Multimodal Large Language Models (MLLMs), aiming to bridge the gap between high-level semantic reasoning and fine-grained visual perception. However, directly transferring these general-domain frameworks to Referring Remote Sensing Image Segmentation (RRSIS) faces significant hurdles. These challenges primarily stem from the weak pixel-level discrimination capability of MLLMs in complex geospatial scenes and the severe granularity mismatch caused by drastic scale variations in remote sensing targets. To overcome these limitations, this paper proposes ReSaP, a Reasoning-enhanced and Scale-aware Prompting framework. ReSaP incorporates two core components to effectively adapt MLLMs for pixel- wise tasks. First, we introduce a Pixel-Aware GRPO training scheme. By utilizing a reinforcement learning framework with a hybrid reward mechanism that integrates bipartite matching for localization and classification accuracy for verification, this scheme explicitly enhances the MLLM&amp;#x27;s fine-grained pixel discrimination and localization precision. Second, we propose the Scale-Aware Prompting strategy for inference. This mechanism employs a density-adaptive grid sampling approach to dynamically adjust the prompt configuration based on target dimensions, effectively harmonizing prompt granularity with object scale. Extensive experiments on the RRSIS-D and RIS-LAD benchmarks demonstrate that ReSaP significantly outperforms existing state-of-the-art methods, validating its superior performance and robustness across both satellite and unmanned aerial vehicle (UAV) observation perspectives. The source code is available at https://github.com/gray114514/ReSaP.&lt;/p&gt;</content:encoded></item><item><title>Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework</title><link>https://arxiv.org/abs/2601.19640v1</link><guid>http://arxiv.org/abs/2601.19640v1</guid><pubDate>Tue, 27 Jan 2026 14:17:04 +0000</pubDate><dc:creator>Hao Chang</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Lingxiang Wu</dc:creator><dc:creator>Peijin Wang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Jinqiao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.
Published: 2026-01-27T14:17:04+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Chang; Zhihui Wang; Lingxiang Wu; Peijin Wang; Wenhui Diao; Jinqiao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.&lt;/p&gt;</content:encoded></item><item><title>Towards Pixel-Level VLM Perception via Simple Points Prediction</title><link>https://arxiv.org/abs/2601.19228v1</link><guid>http://arxiv.org/abs/2601.19228v1</guid><pubDate>Tue, 27 Jan 2026 05:50:40 +0000</pubDate><dc:creator>Tianhui Song</dc:creator><dc:creator>Haoyu Lu</dc:creator><dc:creator>Hao Yang</dc:creator><dc:creator>Lin Sui</dc:creator><dc:creator>Haoning Wu</dc:creator><dc:creator>Zaida Zhou</dc:creator><dc:creator>Zhiqi Huang</dc:creator><dc:creator>Yiping Bao</dc:creator><dc:creator>Y. Charles</dc:creator><dc:creator>Xinyu Zhou</dc:creator><dc:creator>Limin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/
Published: 2026-01-27T05:50:40+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianhui Song; Haoyu Lu; Hao Yang; Lin Sui; Haoning Wu; Zaida Zhou; Zhiqi Huang; Yiping Bao; Y. Charles; Xinyu Zhou; Limin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/&lt;/p&gt;</content:encoded></item><item><title>IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113189</link><guid>10.1016/j.patcog.2026.113189</guid><pubDate>Thu, 29 Jan 2026 07:37:21 +0000</pubDate><dc:creator>Jifeng Shen</dc:creator><dc:creator>Haibo Zhan</dc:creator><dc:creator>Xin Zuo</dc:creator><dc:creator>Heng Fan</dc:creator><dc:creator>Xiaohui Yuan</dc:creator><dc:creator>Jun Li</dc:creator><dc:creator>Wankou Yang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113189</prism:doi><description>Current multispectral object detection methods often retain extraneous background or noise during feature fusion, limiting perceptual performance. To address this, we propose a feature fusion framework based on cross-modal feature contrastive and screening strategy, diverging from conventional approaches. The proposed method adaptively enhances salient structures by fusing object-aware complementary cross-modal features while suppressing shared background interference. Our solution centers on two novel, specially designed modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and inter-modal feature representations by modeling their relationships, thereby improving cross-modal alignment and discriminative power. Inspired by feedback differential amplifiers, the DFFM dynamically computes inter-modal differential features as guidance signals and feeds them back to the MFRM, enabling adaptive fusion of complementary information while suppressing common-mode noise across modalities. To enable robust feature learning, the MFRM and DFFM are integrated into a unified framework, which is formally formulated as an Iterative Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion. IRDFusion enables high-quality cross-modal fusion by progressively amplifying salient relational signals through iterative feedback, while suppressing feature noise, leading to significant performance gains. In extensive experiments on FLIR, LLVIP and M 3 FD datasets, IRDFusion achieves state-of-the-art performance and consistently outperforms existing methods across diverse challenging scenarios, demonstrating its robustness and effectiveness. Code will be available at https://github.com/61s61min/IRDFusion.git .
Published: 2026-01-29T07:37:21+00:00
Venue: Pattern Recognition
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jifeng Shen; Haibo Zhan; Xin Zuo; Heng Fan; Xiaohui Yuan; Jun Li; Wankou Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113189"&gt;10.1016/j.patcog.2026.113189&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Current multispectral object detection methods often retain extraneous background or noise during feature fusion, limiting perceptual performance. To address this, we propose a feature fusion framework based on cross-modal feature contrastive and screening strategy, diverging from conventional approaches. The proposed method adaptively enhances salient structures by fusing object-aware complementary cross-modal features while suppressing shared background interference. Our solution centers on two novel, specially designed modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and inter-modal feature representations by modeling their relationships, thereby improving cross-modal alignment and discriminative power. Inspired by feedback differential amplifiers, the DFFM dynamically computes inter-modal differential features as guidance signals and feeds them back to the MFRM, enabling adaptive fusion of complementary information while suppressing common-mode noise across modalities. To enable robust feature learning, the MFRM and DFFM are integrated into a unified framework, which is formally formulated as an Iterative Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion. IRDFusion enables high-quality cross-modal fusion by progressively amplifying salient relational signals through iterative feedback, while suppressing feature noise, leading to significant performance gains. In extensive experiments on FLIR, LLVIP and M 3 FD datasets, IRDFusion achieves state-of-the-art performance and consistently outperforms existing methods across diverse challenging scenarios, demonstrating its robustness and effectiveness. Code will be available at https://github.com/61s61min/IRDFusion.git .&lt;/p&gt;</content:encoded></item><item><title>ZUMA: Training-free Zero-shot Unified Multimodal Anomaly Detection</title><link>https://doi.org/10.1109/tpami.2026.3658856</link><guid>10.1109/tpami.2026.3658856</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Yunfeng Ma</dc:creator><dc:creator>Min Liu</dc:creator><dc:creator>Shuai Jiang</dc:creator><dc:creator>Jingyu Zhou</dc:creator><dc:creator>Yuan Bian</dc:creator><dc:creator>Xueping Wang</dc:creator><dc:creator>Yaonan Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658856</prism:doi><description>Multimodal anomaly detection (MAD) aims to exploit both texture and spatial attributes to identify deviations from normal patterns in complex scenarios. However, zero-shot (ZS) settings arising from privacy concerns or confidentiality constraints present significant challenges to existing MAD methods. To address this issue, we introduce ZUMA, a training-free, Zero-shot Unified Multimodal Anomaly detection framework that unleashes CLIP's cross-modal potential to perform ZS MAD. To mitigate the domain gap between CLIP's pretraining space and point clouds, we propose cross-domain calibration (CDC), which efficiently bridges the manifold misalignment through source-domain semantic transfer and establishes a hybrid semantic space, enabling a joint embedding of 2D and 3D representations. Subsequently, ZUMA performs dynamic semantic interaction (DSI) to enable structural decoupling of anomaly regions in the high-dimensional embedding space constructed by CDC, where natural languages serve as semantic anchors to help DSI establish discriminative hyperplanes within hybrid modality representations. Within this framework, ZUMA enables plug-and-play detection of 2D, 3D or multimodal anomalies, without training or fine-tuning even for cross-dataset or incomplete-modality scenarios. Additionally, to further investigate the potential of the training-free ZUMA within the training-based paradigm, we develop ZUMA-FT, a fine-tuned variant that achieves notable improvements with minimal parameter trade-off. Extensive experiments are conducted on two MAD benchmarks, MVTec 3D-AD and Eyecandies. Notably, the training-free ZUMA achieves state-of-the-art (SOTA) performance on both datasets, outperforming existing ZS MAD methods, including training-based approaches. Moreover, ZUMA-FT further extends the performance boundary of ZUMA with only 6.75 M learnable parameters. Code is available at: https://github.com/yif-ma/ZUMA
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfeng Ma; Min Liu; Shuai Jiang; Jingyu Zhou; Yuan Bian; Xueping Wang; Yaonan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658856"&gt;10.1109/tpami.2026.3658856&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal anomaly detection (MAD) aims to exploit both texture and spatial attributes to identify deviations from normal patterns in complex scenarios. However, zero-shot (ZS) settings arising from privacy concerns or confidentiality constraints present significant challenges to existing MAD methods. To address this issue, we introduce ZUMA, a training-free, Zero-shot Unified Multimodal Anomaly detection framework that unleashes CLIP&amp;#x27;s cross-modal potential to perform ZS MAD. To mitigate the domain gap between CLIP&amp;#x27;s pretraining space and point clouds, we propose cross-domain calibration (CDC), which efficiently bridges the manifold misalignment through source-domain semantic transfer and establishes a hybrid semantic space, enabling a joint embedding of 2D and 3D representations. Subsequently, ZUMA performs dynamic semantic interaction (DSI) to enable structural decoupling of anomaly regions in the high-dimensional embedding space constructed by CDC, where natural languages serve as semantic anchors to help DSI establish discriminative hyperplanes within hybrid modality representations. Within this framework, ZUMA enables plug-and-play detection of 2D, 3D or multimodal anomalies, without training or fine-tuning even for cross-dataset or incomplete-modality scenarios. Additionally, to further investigate the potential of the training-free ZUMA within the training-based paradigm, we develop ZUMA-FT, a fine-tuned variant that achieves notable improvements with minimal parameter trade-off. Extensive experiments are conducted on two MAD benchmarks, MVTec 3D-AD and Eyecandies. Notably, the training-free ZUMA achieves state-of-the-art (SOTA) performance on both datasets, outperforming existing ZS MAD methods, including training-based approaches. Moreover, ZUMA-FT further extends the performance boundary of ZUMA with only 6.75 M learnable parameters. Code is available at: https://github.com/yif-ma/ZUMA&lt;/p&gt;</content:encoded></item><item><title>Video-KTR: Reinforcing Video Reasoning via Key Token Attribution</title><link>https://arxiv.org/abs/2601.19686v1</link><guid>http://arxiv.org/abs/2601.19686v1</guid><pubDate>Tue, 27 Jan 2026 15:02:23 +0000</pubDate><dc:creator>Ziyue Wang</dc:creator><dc:creator>Sheng Jin</dc:creator><dc:creator>Zhongrong Zuo</dc:creator><dc:creator>Jiawei Wu</dc:creator><dc:creator>Han Qiu</dc:creator><dc:creator>Qi She</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Xudong Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.
Published: 2026-01-27T15:02:23+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyue Wang; Sheng Jin; Zhongrong Zuo; Jiawei Wu; Han Qiu; Qi She; Hao Zhang; Xudong Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.&lt;/p&gt;</content:encoded></item><item><title>ArgusNet: Understanding 3D scenes more like humans</title><link>https://doi.org/10.1016/j.neucom.2026.132895</link><guid>10.1016/j.neucom.2026.132895</guid><pubDate>Thu, 29 Jan 2026 19:00:28 +0000</pubDate><dc:creator>Keyu Guo</dc:creator><dc:creator>Hongkai Wei</dc:creator><dc:creator>Yongle Huang</dc:creator><dc:creator>Xiangyu Song</dc:creator><dc:creator>Shijie Sun</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Huansheng Song</dc:creator><dc:creator>Jianxin Li</dc:creator><dc:creator>Yongjun Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132895</prism:doi><description>In areas like autonomous driving, human-computer interaction, and augmented reality, it is essential for machines to comprehend natural language commands and identify targets within 3D scenes. To this end, this paper introduces and investigates the task of M onocular M ultiple targets 3D V isual G rounding ( MM-3DVG ), which aims to detect multiple 3D targets in a monocular RGB image using the descriptions provided in natural language. To address the absence of suitable datasets for this task, we build two comprehensive datasets: MM3DRefer and MT3DRefer. Furthermore, we propose the ArgusNet network architecture, which simulates visual reasoning processes of humans. The network involves identifying potential targets with a monocular 3D detector, followed by linking language descriptions to these targets using the proposed Selective Matching Module (SMM). The SMM consists of the Selective Fusion Module (SFM) for multimodal information fusion and the Selective Interaction Module (SIM) for deep feature interaction, where the SIM incorporates our specifically designed GateMamba module. Experimental results demonstrate that ArgusNet significantly outperforms other existing methods on multiple datasets, achieving state-of-the-art performance in the domain of language-guided multi-target 3D detection from monocular RGB images, a lightweight yet widely available 3D scene representation in practice. The code and datasets are available at: https://github.com/klaygky/ArgusNet .
Published: 2026-01-29T19:00:28+00:00
Venue: Neurocomputing
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keyu Guo; Hongkai Wei; Yongle Huang; Xiangyu Song; Shijie Sun; Mingtao Feng; Huansheng Song; Jianxin Li; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132895"&gt;10.1016/j.neucom.2026.132895&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;In areas like autonomous driving, human-computer interaction, and augmented reality, it is essential for machines to comprehend natural language commands and identify targets within 3D scenes. To this end, this paper introduces and investigates the task of M onocular M ultiple targets 3D V isual G rounding ( MM-3DVG ), which aims to detect multiple 3D targets in a monocular RGB image using the descriptions provided in natural language. To address the absence of suitable datasets for this task, we build two comprehensive datasets: MM3DRefer and MT3DRefer. Furthermore, we propose the ArgusNet network architecture, which simulates visual reasoning processes of humans. The network involves identifying potential targets with a monocular 3D detector, followed by linking language descriptions to these targets using the proposed Selective Matching Module (SMM). The SMM consists of the Selective Fusion Module (SFM) for multimodal information fusion and the Selective Interaction Module (SIM) for deep feature interaction, where the SIM incorporates our specifically designed GateMamba module. Experimental results demonstrate that ArgusNet significantly outperforms other existing methods on multiple datasets, achieving state-of-the-art performance in the domain of language-guided multi-target 3D detection from monocular RGB images, a lightweight yet widely available 3D scene representation in practice. The code and datasets are available at: https://github.com/klaygky/ArgusNet .&lt;/p&gt;</content:encoded></item></channel></rss>