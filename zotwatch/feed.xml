<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 03 Jan 2026 02:35:12 +0000</lastBuildDate><item><title>GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3650165</link><guid>10.1109/tpami.2025.3650165</guid><pubDate>Fri, 02 Jan 2026 18:16:22 +0000</pubDate><dc:creator>Zihui Zhang</dc:creator><dc:creator>Weisheng Dai</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Bo Li</dc:creator><dc:creator>Bo Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650165</prism:doi><description>We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.
Published: 2026-01-02T18:16:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhang; Weisheng Dai; Bing Wang; Bo Li; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650165"&gt;10.1109/tpami.2025.3650165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.&lt;/p&gt;</content:encoded></item><item><title>Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104110</link><guid>10.1016/j.inffus.2025.104110</guid><pubDate>Fri, 02 Jan 2026 07:42:39 +0000</pubDate><dc:creator>Zhenhao Wang</dc:creator><dc:creator>Tian Tian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104110</prism:doi><description>Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .
Published: 2026-01-02T07:42:39+00:00
Venue: Information Fusion
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenhao Wang; Tian Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104110"&gt;10.1016/j.inffus.2025.104110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .&lt;/p&gt;</content:encoded></item><item><title>Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM</title><link>https://doi.org/10.1109/tgrs.2025.3650151</link><guid>10.1109/tgrs.2025.3650151</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Junxiao Xue</dc:creator><dc:creator>Quan Deng</dc:creator><dc:creator>Xuecheng Wu</dc:creator><dc:creator>Kelu Yao</dc:creator><dc:creator>Xinyi Yin</dc:creator><dc:creator>Fei Yu</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Dingkang Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650151</prism:doi><description>Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junxiao Xue; Quan Deng; Xuecheng Wu; Kelu Yao; Xinyi Yin; Fei Yu; Wei Zhou; Yanfei Zhong; Yang Liu; Dingkang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650151"&gt;10.1109/tgrs.2025.3650151&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.&lt;/p&gt;</content:encoded></item><item><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.24331v1</link><guid>http://arxiv.org/abs/2512.24331v1</guid><pubDate>Tue, 30 Dec 2025 16:35:00 +0000</pubDate><dc:creator>Weijie Wei</dc:creator><dc:creator>Zhipeng Luo</dc:creator><dc:creator>Ling Feng</dc:creator><dc:creator>Venice Erin Liong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.
Published: 2025-12-30T16:35:00+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weijie Wei; Zhipeng Luo; Ling Feng; Venice Erin Liong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</content:encoded></item><item><title>RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios</title><link>https://arxiv.org/abs/2512.24561v1</link><guid>http://arxiv.org/abs/2512.24561v1</guid><pubDate>Wed, 31 Dec 2025 02:01:02 +0000</pubDate><dc:creator>Tianyi Zhao</dc:creator><dc:creator>Jiawen Xi</dc:creator><dc:creator>Linhui Xiao</dc:creator><dc:creator>Junnan Li</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.
Published: 2025-12-31T02:01:02+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Zhao; Jiawen Xi; Linhui Xiao; Junnan Li; Xue Yang; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.&lt;/p&gt;</content:encoded></item><item><title>Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</title><link>https://arxiv.org/abs/2512.24323v1</link><guid>http://arxiv.org/abs/2512.24323v1</guid><pubDate>Tue, 30 Dec 2025 16:22:14 +0000</pubDate><dc:creator>Haijing Liu</dc:creator><dc:creator>Zhiyuan Song</dc:creator><dc:creator>Hefeng Wu</dc:creator><dc:creator>Tao Pu</dc:creator><dc:creator>Keze Wang</dc:creator><dc:creator>Liang Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.
Published: 2025-12-30T16:22:14+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haijing Liu; Zhiyuan Song; Hefeng Wu; Tao Pu; Keze Wang; Liang Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.&lt;/p&gt;</content:encoded></item><item><title>Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</title><link>https://arxiv.org/abs/2512.24702v1</link><guid>http://arxiv.org/abs/2512.24702v1</guid><pubDate>Wed, 31 Dec 2025 08:10:03 +0000</pubDate><dc:creator>Kai Ye</dc:creator><dc:creator>Xiaotong You</dc:creator><dc:creator>Jianghang Lin</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Pingyang Dai</dc:creator><dc:creator>Liujuan Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass "generate-then-segment" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a "Generate-Evaluate-Evolve" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.
Published: 2025-12-31T08:10:03+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Ye; Xiaotong You; Jianghang Lin; Jiayi Ji; Pingyang Dai; Liujuan Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &amp;quot;generate-then-segment&amp;quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &amp;quot;Generate-Evaluate-Evolve&amp;quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.&lt;/p&gt;</content:encoded></item><item><title>Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation</title><link>https://arxiv.org/abs/2512.23997v1</link><guid>http://arxiv.org/abs/2512.23997v1</guid><pubDate>Tue, 30 Dec 2025 05:34:28 +0000</pubDate><dc:creator>Haotang Li</dc:creator><dc:creator>Zhenyu Qi</dc:creator><dc:creator>Hao Qin</dc:creator><dc:creator>Huanrui Yang</dc:creator><dc:creator>Sen He</dc:creator><dc:creator>Kebin Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.
Published: 2025-12-30T05:34:28+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haotang Li; Zhenyu Qi; Hao Qin; Huanrui Yang; Sen He; Kebin Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.&lt;/p&gt;</content:encoded></item><item><title>SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</title><link>https://arxiv.org/abs/2512.24330v1</link><guid>http://arxiv.org/abs/2512.24330v1</guid><pubDate>Tue, 30 Dec 2025 16:31:45 +0000</pubDate><dc:creator>Yong Xien Chng</dc:creator><dc:creator>Tao Hu</dc:creator><dc:creator>Wenwen Tong</dc:creator><dc:creator>Xueheng Li</dc:creator><dc:creator>Jiandong Chen</dc:creator><dc:creator>Haojia Yu</dc:creator><dc:creator>Jiefan Lu</dc:creator><dc:creator>Hewei Guo</dc:creator><dc:creator>Hanming Deng</dc:creator><dc:creator>Chengjun Xie</dc:creator><dc:creator>Gao Huang</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Lewei Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.
Published: 2025-12-30T16:31:45+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yong Xien Chng; Tao Hu; Wenwen Tong; Xueheng Li; Jiandong Chen; Haojia Yu; Jiefan Lu; Hewei Guo; Hanming Deng; Chengjun Xie; Gao Huang; Dahua Lin; Lewei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&amp;#x27;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.&lt;/p&gt;</content:encoded></item><item><title>MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation</title><link>https://arxiv.org/abs/2512.24243v1</link><guid>http://arxiv.org/abs/2512.24243v1</guid><pubDate>Tue, 30 Dec 2025 14:09:17 +0000</pubDate><dc:creator>Fuqiang Gu</dc:creator><dc:creator>Yuanke Li</dc:creator><dc:creator>Xianlei Long</dc:creator><dc:creator>Kangping Ji</dc:creator><dc:creator>Chao Chen</dc:creator><dc:creator>Qingyi Gu</dc:creator><dc:creator>Zhenliang Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.
Published: 2025-12-30T14:09:17+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuqiang Gu; Yuanke Li; Xianlei Long; Kangping Ji; Chao Chen; Qingyi Gu; Zhenliang Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.&lt;/p&gt;</content:encoded></item><item><title>Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation</title><link>https://arxiv.org/abs/2512.23938v1</link><guid>http://arxiv.org/abs/2512.23938v1</guid><pubDate>Tue, 30 Dec 2025 01:51:52 +0000</pubDate><dc:creator>Hualin Ye</dc:creator><dc:creator>Bingxi Liu</dc:creator><dc:creator>Jixiang Du</dc:creator><dc:creator>Yu Qin</dc:creator><dc:creator>Ziyi Chen</dc:creator><dc:creator>Hong Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.
Published: 2025-12-30T01:51:52+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hualin Ye; Bingxi Liu; Jixiang Du; Yu Qin; Ziyi Chen; Hong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.&lt;/p&gt;</content:encoded></item><item><title>RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations</title><link>https://arxiv.org/abs/2512.24023v1</link><guid>http://arxiv.org/abs/2512.24023v1</guid><pubDate>Tue, 30 Dec 2025 06:50:11 +0000</pubDate><dc:creator>Xingqi He</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Shuyong Gao</dc:creator><dc:creator>Wenjie Li</dc:creator><dc:creator>Lingyi Hong</dc:creator><dc:creator>Mingxi Chen</dc:creator><dc:creator>Kaixun Jiang</dc:creator><dc:creator>Jiyuan Fu</dc:creator><dc:creator>Wenqiang Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.
Published: 2025-12-30T06:50:11+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingqi He; Yujie Zhang; Shuyong Gao; Wenjie Li; Lingyi Hong; Mingxi Chen; Kaixun Jiang; Jiyuan Fu; Wenqiang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning</title><link>https://arxiv.org/abs/2512.24591v1</link><guid>http://arxiv.org/abs/2512.24591v1</guid><pubDate>Wed, 31 Dec 2025 03:28:17 +0000</pubDate><dc:creator>Fuyu Dong</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Nan Luo</dc:creator><dc:creator>Yiming Zhang</dc:creator><dc:creator>Kaiyu Li</dc:creator><dc:creator>Jianfei Yang</dc:creator><dc:creator>Quan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.
Published: 2025-12-31T03:28:17+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuyu Dong; Ke Li; Di Wang; Nan Luo; Yiming Zhang; Kaiyu Li; Jianfei Yang; Quan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis</title><link>https://arxiv.org/abs/2512.24013v1</link><guid>http://arxiv.org/abs/2512.24013v1</guid><pubDate>Tue, 30 Dec 2025 06:18:23 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Hui Li</dc:creator><dc:creator>Yiyun Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.
Published: 2025-12-30T06:18:23+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Hui Li; Yiyun Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.&lt;/p&gt;</content:encoded></item><item><title>Text-to-video person re-identification benchmark: Dataset and dual-modal contextual alignment</title><link>https://doi.org/10.1016/j.neucom.2025.132596</link><guid>10.1016/j.neucom.2025.132596</guid><pubDate>Fri, 02 Jan 2026 16:07:05 +0000</pubDate><dc:creator>Jiajun Su</dc:creator><dc:creator>Simin Zhan</dc:creator><dc:creator>Pudu Liu</dc:creator><dc:creator>Jianqing Zhu</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132596</prism:doi><description>Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 " role="presentation"&gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.
Published: 2026-01-02T16:07:05+00:00
Venue: Neurocomputing
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiajun Su; Simin Zhan; Pudu Liu; Jianqing Zhu; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132596"&gt;10.1016/j.neucom.2025.132596&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3650193</link><guid>10.1109/jstars.2025.3650193</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Jianfen Wei</dc:creator><dc:creator>Ping Yang</dc:creator><dc:creator>Chang Wang</dc:creator><dc:creator>Chunxiang Shi</dc:creator><dc:creator>Renlong Hang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650193</prism:doi><description>Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianfen Wei; Ping Yang; Chang Wang; Chunxiang Shi; Renlong Hang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650193"&gt;10.1109/jstars.2025.3650193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>BRSMamba: Boundary-Aware Mamba for Forest and Shrub Segmentation From Diverse Satellite Imagery</title><link>https://doi.org/10.1109/jstars.2025.3650425</link><guid>10.1109/jstars.2025.3650425</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Zhijie He</dc:creator><dc:creator>Xiang Weng</dc:creator><dc:creator>Kai Fang</dc:creator><dc:creator>Yane Li</dc:creator><dc:creator>Yaoping Ruan</dc:creator><dc:creator>Hailin Feng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650425</prism:doi><description>Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth's surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijie He; Xiang Weng; Kai Fang; Yane Li; Yaoping Ruan; Hailin Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650425"&gt;10.1109/jstars.2025.3650425&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth&amp;#x27;s surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...&lt;/p&gt;</content:encoded></item><item><title>DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</title><link>https://arxiv.org/abs/2512.24165v1</link><guid>http://arxiv.org/abs/2512.24165v1</guid><pubDate>Tue, 30 Dec 2025 11:51:18 +0000</pubDate><dc:creator>Zefeng He</dc:creator><dc:creator>Xiaoye Qu</dc:creator><dc:creator>Yafu Li</dc:creator><dc:creator>Tong Zhu</dc:creator><dc:creator>Siyuan Huang</dc:creator><dc:creator>Yu Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.
Published: 2025-12-30T11:51:18+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zefeng He; Xiaoye Qu; Yafu Li; Tong Zhu; Siyuan Huang; Yu Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.&lt;/p&gt;</content:encoded></item><item><title>Robust and Generalizable Rumor Detection with Semantic Evolving Graph Masked Autoencoder</title><link>https://doi.org/10.1016/j.patcog.2025.112995</link><guid>10.1016/j.patcog.2025.112995</guid><pubDate>Fri, 02 Jan 2026 16:41:18 +0000</pubDate><dc:creator>Qiang Liu</dc:creator><dc:creator>Xiang Tao</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Shu Wu</dc:creator><dc:creator>Liang Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112995</prism:doi><description>Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.
Published: 2026-01-02T16:41:18+00:00
Venue: Pattern Recognition
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiang Liu; Xiang Tao; Liang Wang; Shu Wu; Liang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112995"&gt;10.1016/j.patcog.2025.112995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.&lt;/p&gt;</content:encoded></item><item><title>UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</title><link>https://doi.org/10.1109/tgrs.2025.3645320</link><guid>10.1109/tgrs.2025.3645320</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Siyuan Yao</dc:creator><dc:creator>Dongxiu Liu</dc:creator><dc:creator>Taotao Li</dc:creator><dc:creator>Shengjie Li</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Xiaochun Cao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645320</prism:doi><description>Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.508 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siyuan Yao; Dongxiu Liu; Taotao Li; Shengjie Li; Wenqi Ren; Xiaochun Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645320"&gt;10.1109/tgrs.2025.3645320&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.508 (consider)&lt;/p&gt;
&lt;p&gt;Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.&lt;/p&gt;</content:encoded></item><item><title>MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</title><link>https://arxiv.org/abs/2512.24605v1</link><guid>http://arxiv.org/abs/2512.24605v1</guid><pubDate>Wed, 31 Dec 2025 03:56:28 +0000</pubDate><dc:creator>Panquan Yang</dc:creator><dc:creator>Junfei Huang</dc:creator><dc:creator>Zongzhangbao Yin</dc:creator><dc:creator>Yingsong Hu</dc:creator><dc:creator>Anni Xu</dc:creator><dc:creator>Xinyi Luo</dc:creator><dc:creator>Xueqi Sun</dc:creator><dc:creator>Hai Wu</dc:creator><dc:creator>Sheng Ao</dc:creator><dc:creator>Zhaoxing Zhu</dc:creator><dc:creator>Chenglu Wen</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.
Published: 2025-12-31T03:56:28+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Panquan Yang; Junfei Huang; Zongzhangbao Yin; Yingsong Hu; Anni Xu; Xinyi Luo; Xueqi Sun; Hai Wu; Sheng Ao; Zhaoxing Zhu; Chenglu Wen; Cheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;</content:encoded></item><item><title>DGSSformer: Dynamically Global-aware Spatiotemporal Synchronous Transformer for Traffic Prediction</title><link>https://doi.org/10.1016/j.eswa.2025.131063</link><guid>10.1016/j.eswa.2025.131063</guid><pubDate>Fri, 02 Jan 2026 23:48:55 +0000</pubDate><dc:creator>Qingyong Zhang</dc:creator><dc:creator>Qian Shang</dc:creator><dc:creator>Quan Zhou</dc:creator><dc:creator>Mengpeng Yang</dc:creator><dc:creator>Bingrong Xu</dc:creator><dc:creator>Zhihui Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131063</prism:doi><description>Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.
Published: 2026-01-02T23:48:55+00:00
Venue: Expert Systems with Applications
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyong Zhang; Qian Shang; Quan Zhou; Mengpeng Yang; Bingrong Xu; Zhihui Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131063"&gt;10.1016/j.eswa.2025.131063&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks</title><link>https://arxiv.org/abs/2512.24156v1</link><guid>http://arxiv.org/abs/2512.24156v1</guid><pubDate>Tue, 30 Dec 2025 11:40:23 +0000</pubDate><dc:creator>Evgenii Rudakov</dc:creator><dc:creator>Jonathan Shock</dc:creator><dc:creator>Benjamin Ultan Cowley</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.
Published: 2025-12-30T11:40:23+00:00
Venue: arXiv
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Evgenii Rudakov; Jonathan Shock; Benjamin Ultan Cowley&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.&lt;/p&gt;</content:encoded></item><item><title>ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation</title><link>https://arxiv.org/abs/2512.24224v1</link><guid>http://arxiv.org/abs/2512.24224v1</guid><pubDate>Tue, 30 Dec 2025 13:38:30 +0000</pubDate><dc:creator>Ziquan Liu</dc:creator><dc:creator>Zhewei Zhu</dc:creator><dc:creator>Xuyang Shi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP's internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP's internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere" paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.
Published: 2025-12-30T13:38:30+00:00
Venue: arXiv
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziquan Liu; Zhewei Zhu; Xuyang Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP&amp;#x27;s internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP&amp;#x27;s internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere&amp;quot; paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training</title><link>https://doi.org/10.1109/jstars.2025.3650394</link><guid>10.1109/jstars.2025.3650394</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Shun Zhang</dc:creator><dc:creator>Xuebin Zhang</dc:creator><dc:creator>Yaohui Xu</dc:creator><dc:creator>Ke Wang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650394</prism:doi><description>Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.504 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shun Zhang; Xuebin Zhang; Yaohui Xu; Ke Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650394"&gt;10.1109/jstars.2025.3650394&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.504 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.&lt;/p&gt;</content:encoded></item><item><title>Task-Driven Underwater Image Enhancement via Hierarchical Semantic Refinement</title><link>https://doi.org/10.1109/tip.2025.3647323</link><guid>10.1109/tip.2025.3647323</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Meng Yu</dc:creator><dc:creator>Liquan Shen</dc:creator><dc:creator>Yihan Yu</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:creator>Rui Le</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647323</prism:doi><description>Underwater image enhancement (UIE) is crucial for robust marine exploration, yet existing methods prioritize perceptual quality while overlooking irreversible semantic corruption that impairs downstream tasks. Unlike terrestrial images, underwater semantics exhibit layer-specific degradations: shallow features suffer from color shifts and edge erosion, while deep features face semantic ambiguity. These distortions entangle with semantic content across feature hierarchies, where direct enhancement amplifies interference in downstream tasks. Even if distortions are removed, the damaged semantic structures cannot be fully recovered, making it imperative to further enhance corrupted content. To address these challenges, we propose a task-driven UIE framework that redefines enhancement as machine-interpretable semantic recovery rather than mere distortion removal. First, we introduce a multi-scale underwater distortion-aware generator to perceive distortions across feature levels and provide a prior for distortion removal. Second, leveraging this prior and the absence of clean underwater references, we propose a stable self-supervised disentanglement strategy to explicitly separate distortions from corrupted content through CLIP-based semantic constraints and identity consistency. Finally, to compensate for the irreversible semantic loss, we design a task-aware hierarchical enhancement module that refines shallow details via spatial-frequency fusion and strengthens deep semantics through multi-scale context aggregation, aligning results with machine vision requirements. Extensive experiments on segmentation, detection, and saliency tasks demonstrate the superiority of our method in restoring machine-friendly semantics from degraded underwater images. Our code is available at https://github.com/gemyumeng/HSRUIE.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.502 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Yu; Liquan Shen; Yihan Yu; Yu Zhang; Rui Le&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647323"&gt;10.1109/tip.2025.3647323&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.502 (consider)&lt;/p&gt;
&lt;p&gt;Underwater image enhancement (UIE) is crucial for robust marine exploration, yet existing methods prioritize perceptual quality while overlooking irreversible semantic corruption that impairs downstream tasks. Unlike terrestrial images, underwater semantics exhibit layer-specific degradations: shallow features suffer from color shifts and edge erosion, while deep features face semantic ambiguity. These distortions entangle with semantic content across feature hierarchies, where direct enhancement amplifies interference in downstream tasks. Even if distortions are removed, the damaged semantic structures cannot be fully recovered, making it imperative to further enhance corrupted content. To address these challenges, we propose a task-driven UIE framework that redefines enhancement as machine-interpretable semantic recovery rather than mere distortion removal. First, we introduce a multi-scale underwater distortion-aware generator to perceive distortions across feature levels and provide a prior for distortion removal. Second, leveraging this prior and the absence of clean underwater references, we propose a stable self-supervised disentanglement strategy to explicitly separate distortions from corrupted content through CLIP-based semantic constraints and identity consistency. Finally, to compensate for the irreversible semantic loss, we design a task-aware hierarchical enhancement module that refines shallow details via spatial-frequency fusion and strengthens deep semantics through multi-scale context aggregation, aligning results with machine vision requirements. Extensive experiments on segmentation, detection, and saliency tasks demonstrate the superiority of our method in restoring machine-friendly semantics from degraded underwater images. Our code is available at https://github.com/gemyumeng/HSRUIE.&lt;/p&gt;</content:encoded></item><item><title>Test-Time Adaptive Vision-Language Alignment for Zero-Shot Group Activity Recognition</title><link>https://doi.org/10.1016/j.patcog.2025.113033</link><guid>10.1016/j.patcog.2025.113033</guid><pubDate>Fri, 02 Jan 2026 16:03:03 +0000</pubDate><dc:creator>Runhao Zeng</dc:creator><dc:creator>Yirui Wang</dc:creator><dc:creator>Wenfu Peng</dc:creator><dc:creator>Xionglin Zhu</dc:creator><dc:creator>Ronghao Zhang</dc:creator><dc:creator>Zhihua Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113033</prism:doi><description>Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.
Published: 2026-01-02T16:03:03+00:00
Venue: Pattern Recognition
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runhao Zeng; Yirui Wang; Wenfu Peng; Xionglin Zhu; Ronghao Zhang; Zhihua Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113033"&gt;10.1016/j.patcog.2025.113033&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.&lt;/p&gt;</content:encoded></item><item><title>Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion</title><link>https://doi.org/10.1109/tip.2025.3647207</link><guid>10.1109/tip.2025.3647207</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Yang Xu</dc:creator><dc:creator>Jian Zhu</dc:creator><dc:creator>Danfeng Hong</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Zebin Wu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647207</prism:doi><description>Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Xu; Jian Zhu; Danfeng Hong; Zhihui Wei; Zebin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647207"&gt;10.1109/tip.2025.3647207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.&lt;/p&gt;</content:encoded></item><item><title>STORM: Exploiting Spatiotemporal Continuity for Trajectory Similarity Learning in Road Networks</title><link>https://doi.org/10.1109/tkde.2025.3650227</link><guid>10.1109/tkde.2025.3650227</guid><pubDate>Thu, 01 Jan 2026 18:38:35 +0000</pubDate><dc:creator>Jialiang Li</dc:creator><dc:creator>Hua Lu</dc:creator><dc:creator>Cyrus Shahabi</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2025.3650227</prism:doi><description>Trajectory similarity in road networks is pivotal for numerous applications in transportation, urban planning, and ridesharing. However, due to the varying lengths of trajectories, employing similarity metrics directly on raw trajectory data (e.g., DTW [1]) becomes impractical at scale. Therefore, current research primarily revolves around applying deep learning to embed trajectories into vector representations, i.e., embeddings, enabling the application of simpler (and indexable) similarity metrics such as Euclidean distance. Existing research either involves embedding trajectories independent of the downstream tasks, or tailors the embedding specifically for a designated similarity metric. While the former offers versatility and allows for easy fine-tuning to accommodate various metrics, the latter typically yields more effective results but necessitates reconfiguration for different, yet similar metrics. Moreover, both approaches neglect the intrinsic spatiotemporal continuity in trajectory data, resulting in suboptimal trajectory modeling. Our objective is to address the limitations in modeling and have the best of the two worlds. Initially, we generate an embedding through pre-training, decoupled from any particular similarity metric. Subsequently, through a meticulous yet less complex fine-tuning process, we enhance the embedding to encapsulate the nuances of a designated similarity metric. Moreover, a significant aspect of our approach lies in our trajectory modeling that captures spatiotemporal continuity, which mainly consists of a trajectory-oriented road segment embedding and a Transformer encoder enhanced by spatiotemporal semantics inherent in road network-constrained trajectories. Our experimental results demonstrate the superiority of our approach in approximating multiple trajectory similarity metrics over existing state-of-the-art models from both categories of approaches.
Published: 2026-01-01T18:38:35+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jialiang Li; Hua Lu; Cyrus Shahabi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2025.3650227"&gt;10.1109/tkde.2025.3650227&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Trajectory similarity in road networks is pivotal for numerous applications in transportation, urban planning, and ridesharing. However, due to the varying lengths of trajectories, employing similarity metrics directly on raw trajectory data (e.g., DTW [1]) becomes impractical at scale. Therefore, current research primarily revolves around applying deep learning to embed trajectories into vector representations, i.e., embeddings, enabling the application of simpler (and indexable) similarity metrics such as Euclidean distance. Existing research either involves embedding trajectories independent of the downstream tasks, or tailors the embedding specifically for a designated similarity metric. While the former offers versatility and allows for easy fine-tuning to accommodate various metrics, the latter typically yields more effective results but necessitates reconfiguration for different, yet similar metrics. Moreover, both approaches neglect the intrinsic spatiotemporal continuity in trajectory data, resulting in suboptimal trajectory modeling. Our objective is to address the limitations in modeling and have the best of the two worlds. Initially, we generate an embedding through pre-training, decoupled from any particular similarity metric. Subsequently, through a meticulous yet less complex fine-tuning process, we enhance the embedding to encapsulate the nuances of a designated similarity metric. Moreover, a significant aspect of our approach lies in our trajectory modeling that captures spatiotemporal continuity, which mainly consists of a trajectory-oriented road segment embedding and a Transformer encoder enhanced by spatiotemporal semantics inherent in road network-constrained trajectories. Our experimental results demonstrate the superiority of our approach in approximating multiple trajectory similarity metrics over existing state-of-the-art models from both categories of approaches.&lt;/p&gt;</content:encoded></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985v1</link><guid>http://arxiv.org/abs/2512.24985v1</guid><pubDate>Wed, 31 Dec 2025 17:31:29 +0000</pubDate><dc:creator>Yohan Park</dc:creator><dc:creator>Hyunwoo Ha</dc:creator><dc:creator>Wonjun Jo</dc:creator><dc:creator>Tae-Hyun Oh</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.
Published: 2025-12-31T17:31:29+00:00
Venue: arXiv
Score: 0.500 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yohan Park; Hyunwoo Ha; Wonjun Jo; Tae-Hyun Oh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.500 (consider)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&amp;#x27; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.&lt;/p&gt;</content:encoded></item></channel></rss>