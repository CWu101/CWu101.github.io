<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 07 Jan 2026 02:51:28 +0000</lastBuildDate><item><title>SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition</title><link>https://doi.org/10.1109/tcsvt.2026.3651681</link><guid>10.1109/tcsvt.2026.3651681</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Zhi Hu</dc:creator><dc:creator>Liang Liao</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651681</prism:doi><description>Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.595 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhi Hu; Liang Liao; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651681"&gt;10.1109/tcsvt.2026.3651681&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.595 (consider)&lt;/p&gt;
&lt;p&gt;Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.&lt;/p&gt;</content:encoded></item><item><title>Visual Context and Commonsense-Guided Causal Chain-of-Thoughts for Visual Commonsense Reasoning</title><link>https://doi.org/10.1109/tmm.2026.3651070</link><guid>10.1109/tmm.2026.3651070</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Xinyu Li</dc:creator><dc:creator>Jing Zhao</dc:creator><dc:creator>Tongquan Wei</dc:creator><dc:creator>Shiliang Sun</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651070</prism:doi><description>Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.593 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Li; Jing Zhao; Tongquan Wei; Shiliang Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651070"&gt;10.1109/tmm.2026.3651070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.593 (consider)&lt;/p&gt;
&lt;p&gt;Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.&lt;/p&gt;</content:encoded></item><item><title>GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis</title><link>https://doi.org/10.1016/j.jag.2025.105038</link><guid>10.1016/j.jag.2025.105038</guid><pubDate>Tue, 06 Jan 2026 05:31:06 +0000</pubDate><dc:creator>Kai Deng</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Yibing Xiong</dc:creator><dc:creator>Aokun Liang</dc:creator><dc:creator>Jiong Xu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105038</prism:doi><description>Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .
Published: 2026-01-06T05:31:06+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Deng; Xiangyun Hu; Yibing Xiong; Aokun Liang; Jiong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105038"&gt;10.1016/j.jag.2025.105038&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .&lt;/p&gt;</content:encoded></item><item><title>EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework</title><link>https://arxiv.org/abs/2601.02783v1</link><guid>http://arxiv.org/abs/2601.02783v1</guid><pubDate>Tue, 06 Jan 2026 07:41:44 +0000</pubDate><dc:creator>Junjue Wang</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Zihang Chen</dc:creator><dc:creator>Zhuo Zheng</dc:creator><dc:creator>Ailong Ma</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.
Published: 2026-01-06T07:41:44+00:00
Venue: arXiv
Score: 0.574 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junjue Wang; Yanfei Zhong; Zihang Chen; Zhuo Zheng; Ailong Ma; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.574 (consider)&lt;/p&gt;
&lt;p&gt;Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&amp;#x27; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &amp;#x27;&amp;#x27;image-mask-text&amp;#x27;&amp;#x27;, advancing geographical applications for Earth vision.&lt;/p&gt;</content:encoded></item><item><title>SPENav: Dynamic Object Filtering with Spatial Perception Enhancement for Vision-Language Navigation</title><link>https://doi.org/10.1109/tcsvt.2026.3651320</link><guid>10.1109/tcsvt.2026.3651320</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Shuai Yuan</dc:creator><dc:creator>Huaxiang Zhang</dc:creator><dc:creator>Li Liu</dc:creator><dc:creator>Lei Zhu</dc:creator><dc:creator>Xinfeng Dong</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651320</prism:doi><description>The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.572 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yuan; Huaxiang Zhang; Li Liu; Lei Zhu; Xinfeng Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651320"&gt;10.1109/tcsvt.2026.3651320&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.572 (consider)&lt;/p&gt;
&lt;p&gt;The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.&lt;/p&gt;</content:encoded></item><item><title>3D Semantic Gaussian via Geometric-Semantic Hypergraph Computation</title><link>https://doi.org/10.1109/tmm.2026.3651112</link><guid>10.1109/tmm.2026.3651112</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Xinran Wang</dc:creator><dc:creator>Zhiqiang Tian</dc:creator><dc:creator>Dejian Guo</dc:creator><dc:creator>Siqi Li</dc:creator><dc:creator>Shaoyi Du</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Yue Gao</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651112</prism:doi><description>Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.572 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinran Wang; Zhiqiang Tian; Dejian Guo; Siqi Li; Shaoyi Du; Xiangmin Han; Yue Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651112"&gt;10.1109/tmm.2026.3651112&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.572 (consider)&lt;/p&gt;
&lt;p&gt;Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.&lt;/p&gt;</content:encoded></item><item><title>Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding</title><link>https://arxiv.org/abs/2601.02029v1</link><guid>http://arxiv.org/abs/2601.02029v1</guid><pubDate>Mon, 05 Jan 2026 11:42:49 +0000</pubDate><dc:creator>Toshihiko Nishimura</dc:creator><dc:creator>Hirofumi Abe</dc:creator><dc:creator>Kazuhiko Murasaki</dc:creator><dc:creator>Taiga Yoshida</dc:creator><dc:creator>Ryuichi Tanida</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1587/transinf.2025DVL0006</prism:doi><description>This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.
Published: 2026-01-05T11:42:49+00:00
Venue: arXiv
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Toshihiko Nishimura; Hirofumi Abe; Kazuhiko Murasaki; Taiga Yoshida; Ryuichi Tanida&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1587/transinf.2025DVL0006"&gt;10.1587/transinf.2025DVL0006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.&lt;/p&gt;</content:encoded></item><item><title>Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing</title><link>https://doi.org/10.1109/tiv.2025.3650682</link><guid>10.1109/tiv.2025.3650682</guid><pubDate>Mon, 05 Jan 2026 18:40:39 +0000</pubDate><dc:creator>Sicen Guo</dc:creator><dc:creator>Tianyou Wen</dc:creator><dc:creator>Chuang-Wei Liu</dc:creator><dc:creator>Qijun Chen</dc:creator><dc:creator>Rui Fan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2025.3650682</prism:doi><description>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.
Published: 2026-01-05T18:40:39+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sicen Guo; Tianyou Wen; Chuang-Wei Liu; Qijun Chen; Rui Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2025.3650682"&gt;10.1109/tiv.2025.3650682&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.&lt;/p&gt;</content:encoded></item><item><title>A Size-Aware Graph Embedding Approach to Remote Sensing Image Captioning with Object Relative Size Information</title><link>https://doi.org/10.1109/tgrs.2026.3650788</link><guid>10.1109/tgrs.2026.3650788</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Zihao Ni</dc:creator><dc:creator>Yinghao Xu</dc:creator><dc:creator>Weibo Zhang</dc:creator><dc:creator>Zhaoyun Zong</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650788</prism:doi><description>Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Ni; Yinghao Xu; Weibo Zhang; Zhaoyun Zong; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650788"&gt;10.1109/tgrs.2026.3650788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image captioning is the task of automatically generating descriptive texts for remotely sensed scenes and objects. A common shortcoming of existing methods is the inadequate consideration of object size, which often leads to captions that either omit size information or provide imprecise size descriptions. To overcome this deficiency, we develop a novel framework composed of three modules: (a) an object confirmation and relative size estimation module, (b) a graph construction and graph convolution module, and (c) a caption generation module. Our framework comprehensively characterizes object size to generate more quantitatively informative captions. Furthermore, we introduce a new evaluation metric, SizeNum-Meteor, designed to explicitly evaluate the correctness of object count and relative size information in generated captions. This provides a more comprehensive assessment, as standard metrics typically neglect the evaluation of object size. In addition, we construct extended benchmarks by enriching existing datasets with explicit annotations of object count and relative size. Extensive experiments on three existing benchmark datasets (i.e., UCM, Sydney, and RSICD) and the benchmarks we construct (i.e., UCM-N-S, Sydney-N-S, and RSICD-N-S) demonstrate that our framework achieves superior performance on both standard metrics and the proposed SizeNum-Meteor.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction</title><link>https://doi.org/10.1109/tpami.2025.3650478</link><guid>10.1109/tpami.2025.3650478</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Bohan Li</dc:creator><dc:creator>Jiajun Deng</dc:creator><dc:creator>Yasheng Sun</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Xin Jin</dc:creator><dc:creator>Wenjun Zeng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650478</prism:doi><description>Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bohan Li; Jiajun Deng; Yasheng Sun; Xiaofeng Wang; Xin Jin; Wenjun Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650478"&gt;10.1109/tpami.2025.3650478&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp;amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.&lt;/p&gt;</content:encoded></item><item><title>SURFNet: A Surface-aware UAV-Satellite Geolocation Framework via Feature Aggregation and Dual Positional Encoding</title><link>https://doi.org/10.1109/tgrs.2026.3651449</link><guid>10.1109/tgrs.2026.3651449</guid><pubDate>Tue, 06 Jan 2026 18:35:38 +0000</pubDate><dc:creator>Kun Liu</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651449</prism:doi><description>Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.
Published: 2026-01-06T18:35:38+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Liu; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651449"&gt;10.1109/tgrs.2026.3651449&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.&lt;/p&gt;</content:encoded></item><item><title>Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation</title><link>https://arxiv.org/abs/2601.01984v1</link><guid>http://arxiv.org/abs/2601.01984v1</guid><pubDate>Mon, 05 Jan 2026 10:38:26 +0000</pubDate><dc:creator>Weijian Ma</dc:creator><dc:creator>Shizhao Sun</dc:creator><dc:creator>Tianyu Yu</dc:creator><dc:creator>Ruiyu Wang</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><dc:creator>Jiang Bian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.
Published: 2026-01-05T10:38:26+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weijian Ma; Shizhao Sun; Tianyu Yu; Ruiyu Wang; Tat-Seng Chua; Jiang Bian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.&lt;/p&gt;</content:encoded></item><item><title>MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.021</link><guid>10.1016/j.isprsjprs.2025.12.021</guid><pubDate>Mon, 05 Jan 2026 11:18:47 +0000</pubDate><dc:creator>Weipeng Jing</dc:creator><dc:creator>Peilun Kang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Lei Fan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.021</prism:doi><description>Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.
Published: 2026-01-05T11:18:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.554 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weipeng Jing; Peilun Kang; Donglin Di; Jian Wang; Yang Song; Chao Li; Lei Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021"&gt;10.1016/j.isprsjprs.2025.12.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.554 (consider)&lt;/p&gt;
&lt;p&gt;Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.&lt;/p&gt;</content:encoded></item><item><title>BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding</title><link>https://arxiv.org/abs/2601.01526v1</link><guid>http://arxiv.org/abs/2601.01526v1</guid><pubDate>Sun, 04 Jan 2026 13:30:06 +0000</pubDate><dc:creator>Hongbing Li</dc:creator><dc:creator>Linhui Xiao</dc:creator><dc:creator>Zihan Zhao</dc:creator><dc:creator>Qi Shen</dc:creator><dc:creator>Yixiang Huang</dc:creator><dc:creator>Bo Xiao</dc:creator><dc:creator>Zhanyu Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.
Published: 2026-01-04T13:30:06+00:00
Venue: arXiv
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbing Li; Linhui Xiao; Zihan Zhao; Qi Shen; Yixiang Huang; Bo Xiao; Zhanyu Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.&lt;/p&gt;</content:encoded></item><item><title>Cross-view and Multi-step Interaction for Change Captioning</title><link>https://doi.org/10.1109/tmm.2026.3651125</link><guid>10.1109/tmm.2026.3651125</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Tiantao Xian</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Delu Zeng</dc:creator><dc:creator>Bo Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651125</prism:doi><description>Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.553 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tiantao Xian; Zhiheng Zhou; Wenlve Zhou; Delu Zeng; Bo Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651125"&gt;10.1109/tmm.2026.3651125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.553 (consider)&lt;/p&gt;
&lt;p&gt;Change captioning is a task that describes changes in image pairs using natural language. This task is more complex than single-image captioning as it requires a comprehensive understanding of each image and the ability to recognize and describe the semantic changes in image pairs. The key challenge lies in making the network generate an accurate and stable change representation under the interference of viewpoint shift. In this paper, we propose a cross-view and multi-step interaction network to generate robust change representation to resist pseudo-change. Specifically, in the intra-image representation learning stage, a cross-view interaction encoder is designed to enhance internal relationships by cross-referencing in image pairs. In the change feature learning stage, a multi-step change perceptron is employed to capture the change semantics from coarse to fine progressively. Then, a fusion module dynamically combines them as a fine-grained change representation. Besides, we propose a backward representation reconstruction module that facilitates the capture of semantic changes, thus improving the quality of captions in a self-supervised manner. Extensive experiments have shown that the method effectively captures real semantic changes under the interference of viewpoint shift and achieves state-of-the-art performance on five public datasets. The code is available at https://github.com/TTXiann/CVMSI&lt;/p&gt;</content:encoded></item><item><title>AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval</title><link>https://arxiv.org/abs/2601.01416v1</link><guid>http://arxiv.org/abs/2601.01416v1</guid><pubDate>Sun, 04 Jan 2026 07:38:51 +0000</pubDate><dc:creator>Yue Zhou</dc:creator><dc:creator>Ran Ding</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Xue Jiang</dc:creator><dc:creator>Xingzhao Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot
Published: 2026-01-04T07:38:51+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhou; Ran Ding; Xue Yang; Xue Jiang; Xingzhao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot&lt;/p&gt;</content:encoded></item><item><title>FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.01513v1</link><guid>http://arxiv.org/abs/2601.01513v1</guid><pubDate>Sun, 04 Jan 2026 12:46:35 +0000</pubDate><dc:creator>Gen Li</dc:creator><dc:creator>Peiyu Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.
Published: 2026-01-04T12:46:35+00:00
Venue: arXiv
Score: 0.549 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gen Li; Peiyu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.549 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.&lt;/p&gt;</content:encoded></item><item><title>Causality-Inspired Graph Neural Networks for Cross-Modal Retrieval</title><link>https://doi.org/10.1109/tmm.2026.3651028</link><guid>10.1109/tmm.2026.3651028</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Bo Li</dc:creator><dc:creator>Zhixin Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651028</prism:doi><description>Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Li; Zhixin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651028"&gt;10.1109/tmm.2026.3651028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.&lt;/p&gt;</content:encoded></item><item><title>CAMformer: A Single-Stage CNN-Transformer Hybrid for Weakly Supervised Semantic Segmentation in Aerial Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3651514</link><guid>10.1109/tgrs.2026.3651514</guid><pubDate>Tue, 06 Jan 2026 18:35:38 +0000</pubDate><dc:creator>Ruixue Zhou</dc:creator><dc:creator>Jihao Li</dc:creator><dc:creator>Wenkai Zhang</dc:creator><dc:creator>Shuoke Li</dc:creator><dc:creator>Jialiang Chen</dc:creator><dc:creator>Chongyang Li</dc:creator><dc:creator>Boyuan Tong</dc:creator><dc:creator>Weihang Zhang</dc:creator><dc:creator>Xian Sun</dc:creator><dc:creator>Kun Fu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651514</prism:doi><description>Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.
Published: 2026-01-06T18:35:38+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruixue Zhou; Jihao Li; Wenkai Zhang; Shuoke Li; Jialiang Chen; Chongyang Li; Boyuan Tong; Weihang Zhang; Xian Sun; Kun Fu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651514"&gt;10.1109/tgrs.2026.3651514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.&lt;/p&gt;</content:encoded></item><item><title>Text-Injected Discriminative Model for Remote Sensing Visual Grounding</title><link>https://doi.org/10.3390/rs18010161</link><guid>10.3390/rs18010161</guid><pubDate>Mon, 05 Jan 2026 08:40:53 +0000</pubDate><dc:creator>Minhan Hu</dc:creator><dc:creator>Keke Yang</dc:creator><dc:creator>Jing Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010161</prism:doi><description>Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.
Published: 2026-01-05T08:40:53+00:00
Venue: Remote Sensing
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minhan Hu; Keke Yang; Jing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010161"&gt;10.3390/rs18010161&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Remote Sensing Visual Grounding (RSVG) requires fine-grained understanding of language descriptions to localize the specific image regions. Conventional methods typically employ a pipeline of separate visual and textual encoders and a fusion module. However, as visual and textual features are extracted independently, they tend to lack semantic focus on object features during extraction, leading to suboptimal object focus. While some recent attempts have incorporated textual cues into visual feature extraction, they often design complex fusion modules. To address this, we introduce a simple fusion strategy to integrate textual information into visual backbone networks with minimal architectural changes. Moreover, most of the current works use common object detection losses, which only focus on the features inside the bounding box and neglect the background features. In remote sensing images, the high visual similarity between objects can confuse models, making it difficult to locate the correct target accurately. To this end, we design a novel attention regularization strategy to enhance the model’s ability to distinguish similar features outside bounding box regions. Experiments on three benchmark datasets demonstrate the promising performance of our approach.&lt;/p&gt;</content:encoded></item><item><title>PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3650671</link><guid>10.1109/tcsvt.2025.3650671</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ke Wang</dc:creator><dc:creator>Weilin Gao</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Tianyi Shao</dc:creator><dc:creator>Liyang Li</dc:creator><dc:creator>Tianqiang Zhou</dc:creator><dc:creator>Jianbo Lu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3650671</prism:doi><description>To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Wang; Weilin Gao; Kai Chen; Tianyi Shao; Liyang Li; Tianqiang Zhou; Jianbo Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3650671"&gt;10.1109/tcsvt.2025.3650671&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.&lt;/p&gt;</content:encoded></item><item><title>Excluding the Interference for Open-Vocabulary Semantic Segmentation</title><link>https://doi.org/10.1109/tcsvt.2026.3650803</link><guid>10.1109/tcsvt.2026.3650803</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Shuai Shao</dc:creator><dc:creator>Shiyuan Zhao</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Baodi Liu</dc:creator><dc:creator>Weifeng Liu</dc:creator><dc:creator>Yicong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650803</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Shao; Shiyuan Zhao; Rui Xu; Yan Wang; Baodi Liu; Weifeng Liu; Yicong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650803"&gt;10.1109/tcsvt.2026.3650803&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.&lt;/p&gt;</content:encoded></item><item><title>AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs</title><link>https://arxiv.org/abs/2601.02771v1</link><guid>http://arxiv.org/abs/2601.02771v1</guid><pubDate>Tue, 06 Jan 2026 07:05:35 +0000</pubDate><dc:creator>Boyu Chang</dc:creator><dc:creator>Qi Wang</dc:creator><dc:creator>Xi Guo</dc:creator><dc:creator>Zhixiong Nan</dc:creator><dc:creator>Yazhou Yao</dc:creator><dc:creator>Tianfei Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.
Published: 2026-01-06T07:05:35+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyu Chang; Qi Wang; Xi Guo; Zhixiong Nan; Yazhou Yao; Tianfei Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER&amp;#x27;s output embeddings to &amp;quot;imagine&amp;quot; plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs&amp;#x27; contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.&lt;/p&gt;</content:encoded></item><item><title>Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation</title><link>https://doi.org/10.1109/tmm.2026.3651094</link><guid>10.1109/tmm.2026.3651094</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Hojun Song</dc:creator><dc:creator>Chae-yeong Song</dc:creator><dc:creator>Dong-hun Lee</dc:creator><dc:creator>Heejung Choi</dc:creator><dc:creator>Jinwoo Jeong</dc:creator><dc:creator>Sungjei Kim</dc:creator><dc:creator>Sang-hyo Park</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651094</prism:doi><description>3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hojun Song; Chae-yeong Song; Dong-hun Lee; Heejung Choi; Jinwoo Jeong; Sungjei Kim; Sang-hyo Park&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651094"&gt;10.1109/tmm.2026.3651094&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.&lt;/p&gt;</content:encoded></item><item><title>Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning</title><link>https://arxiv.org/abs/2601.02422v1</link><guid>http://arxiv.org/abs/2601.02422v1</guid><pubDate>Sun, 04 Jan 2026 02:50:55 +0000</pubDate><dc:creator>Wenting Lu</dc:creator><dc:creator>Didi Zhu</dc:creator><dc:creator>Tao Shen</dc:creator><dc:creator>Donglin Zhu</dc:creator><dc:creator>Ayong Ye</dc:creator><dc:creator>Chao Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.
Published: 2026-01-04T02:50:55+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenting Lu; Didi Zhu; Tao Shen; Donglin Zhu; Ayong Ye; Chao Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.&lt;/p&gt;</content:encoded></item><item><title>Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models</title><link>https://doi.org/10.1109/tpami.2026.3650761</link><guid>10.1109/tpami.2026.3650761</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>YiFan Zhang</dc:creator><dc:creator>Qingsong Wen</dc:creator><dc:creator>Chaoyou Fu</dc:creator><dc:creator>Kun Wang</dc:creator><dc:creator>Xue Wang</dc:creator><dc:creator>Zhang Zhang</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Rong Jin</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650761</prism:doi><description>Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YiFan Zhang; Qingsong Wen; Chaoyou Fu; Kun Wang; Xue Wang; Zhang Zhang; Liang Wang; Rong Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650761"&gt;10.1109/tpami.2026.3650761&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.&lt;/p&gt;</content:encoded></item><item><title>Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.02289v1</link><guid>http://arxiv.org/abs/2601.02289v1</guid><pubDate>Mon, 05 Jan 2026 17:24:50 +0000</pubDate><dc:creator>Tom Burgert</dc:creator><dc:creator>Leonard Hackel</dc:creator><dc:creator>Paolo Rota</dc:creator><dc:creator>Begüm Demir</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.
Published: 2026-01-05T17:24:50+00:00
Venue: arXiv
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tom Burgert; Leonard Hackel; Paolo Rota; Begüm Demir&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.&lt;/p&gt;</content:encoded></item><item><title>A radiometrically and spatially consistent super-resolution framework for Sentinel-2</title><link>https://doi.org/10.1016/j.rse.2025.115222</link><guid>10.1016/j.rse.2025.115222</guid><pubDate>Tue, 06 Jan 2026 09:45:48 +0000</pubDate><dc:creator>Cesar Aybar</dc:creator><dc:creator>Julio Contreras</dc:creator><dc:creator>Simon Donike</dc:creator><dc:creator>Enrique Portalés-Julià</dc:creator><dc:creator>Gonzalo Mateo-García</dc:creator><dc:creator>Luis Gómez-Chova</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115222</prism:doi><description>Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .
Published: 2026-01-06T09:45:48+00:00
Venue: Remote Sensing of Environment
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cesar Aybar; Julio Contreras; Simon Donike; Enrique Portalés-Julià; Gonzalo Mateo-García; Luis Gómez-Chova&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115222"&gt;10.1016/j.rse.2025.115222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .&lt;/p&gt;</content:encoded></item><item><title>EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</title><link>https://doi.org/10.1109/tcsvt.2026.3651537</link><guid>10.1109/tcsvt.2026.3651537</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Zhaoyang Wang</dc:creator><dc:creator>Wen Lu</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Lihuo He</dc:creator><dc:creator>Maoguo Gong</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651537</prism:doi><description>Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoyang Wang; Wen Lu; Jie Li; Lihuo He; Maoguo Gong; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651537"&gt;10.1109/tcsvt.2026.3651537&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.&lt;/p&gt;</content:encoded></item><item><title>UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving</title><link>https://doi.org/10.1109/tcsvt.2026.3651369</link><guid>10.1109/tcsvt.2026.3651369</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Hao Zhou</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Honggang Qi</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651369</prism:doi><description>Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhou; Yi Zhang; Honggang Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651369"&gt;10.1109/tcsvt.2026.3651369&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.&lt;/p&gt;</content:encoded></item></channel></rss>