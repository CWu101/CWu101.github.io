<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 02 Jan 2026 02:38:09 +0000</lastBuildDate><item><title>Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM</title><link>https://doi.org/10.1109/tgrs.2025.3650151</link><guid>10.1109/tgrs.2025.3650151</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Junxiao Xue</dc:creator><dc:creator>Quan Deng</dc:creator><dc:creator>Xuecheng Wu</dc:creator><dc:creator>Kelu Yao</dc:creator><dc:creator>Xinyi Yin</dc:creator><dc:creator>Fei Yu</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Dingkang Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650151</prism:doi><description>Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.560 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junxiao Xue; Quan Deng; Xuecheng Wu; Kelu Yao; Xinyi Yin; Fei Yu; Wei Zhou; Yanfei Zhong; Yang Liu; Dingkang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650151"&gt;10.1109/tgrs.2025.3650151&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.560 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.&lt;/p&gt;</content:encoded></item><item><title>TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding</title><link>https://arxiv.org/abs/2512.23483v1</link><guid>http://arxiv.org/abs/2512.23483v1</guid><pubDate>Mon, 29 Dec 2025 14:10:22 +0000</pubDate><dc:creator>Zongsheng Cao</dc:creator><dc:creator>Yangfan He</dc:creator><dc:creator>Anran Liu</dc:creator><dc:creator>Feng Chen</dc:creator><dc:creator>Zepeng Wang</dc:creator><dc:creator>Jun Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.
Published: 2025-12-29T14:10:22+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongsheng Cao; Yangfan He; Anran Liu; Feng Chen; Zepeng Wang; Jun Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.&lt;/p&gt;</content:encoded></item><item><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.24331v1</link><guid>http://arxiv.org/abs/2512.24331v1</guid><pubDate>Tue, 30 Dec 2025 16:35:00 +0000</pubDate><dc:creator>Weijie Wei</dc:creator><dc:creator>Zhipeng Luo</dc:creator><dc:creator>Ling Feng</dc:creator><dc:creator>Venice Erin Liong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.
Published: 2025-12-30T16:35:00+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weijie Wei; Zhipeng Luo; Ling Feng; Venice Erin Liong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</content:encoded></item><item><title>RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios</title><link>https://arxiv.org/abs/2512.24561v1</link><guid>http://arxiv.org/abs/2512.24561v1</guid><pubDate>Wed, 31 Dec 2025 02:01:02 +0000</pubDate><dc:creator>Tianyi Zhao</dc:creator><dc:creator>Jiawen Xi</dc:creator><dc:creator>Linhui Xiao</dc:creator><dc:creator>Junnan Li</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.
Published: 2025-12-31T02:01:02+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Zhao; Jiawen Xi; Linhui Xiao; Junnan Li; Xue Yang; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.&lt;/p&gt;</content:encoded></item><item><title>Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</title><link>https://arxiv.org/abs/2512.24323v1</link><guid>http://arxiv.org/abs/2512.24323v1</guid><pubDate>Tue, 30 Dec 2025 16:22:14 +0000</pubDate><dc:creator>Haijing Liu</dc:creator><dc:creator>Zhiyuan Song</dc:creator><dc:creator>Hefeng Wu</dc:creator><dc:creator>Tao Pu</dc:creator><dc:creator>Keze Wang</dc:creator><dc:creator>Liang Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.
Published: 2025-12-30T16:22:14+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haijing Liu; Zhiyuan Song; Hefeng Wu; Tao Pu; Keze Wang; Liang Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.&lt;/p&gt;</content:encoded></item><item><title>Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</title><link>https://arxiv.org/abs/2512.24702v1</link><guid>http://arxiv.org/abs/2512.24702v1</guid><pubDate>Wed, 31 Dec 2025 08:10:03 +0000</pubDate><dc:creator>Kai Ye</dc:creator><dc:creator>Xiaotong You</dc:creator><dc:creator>Jianghang Lin</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Pingyang Dai</dc:creator><dc:creator>Liujuan Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass "generate-then-segment" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a "Generate-Evaluate-Evolve" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.
Published: 2025-12-31T08:10:03+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Ye; Xiaotong You; Jianghang Lin; Jiayi Ji; Pingyang Dai; Liujuan Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &amp;quot;generate-then-segment&amp;quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &amp;quot;Generate-Evaluate-Evolve&amp;quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.&lt;/p&gt;</content:encoded></item><item><title>GIMMNet: Geometry-Aware Interactive Multi-Modal Network for Semantic Segmentation of High-Resolution Remote Sensing Imagery</title><link>https://doi.org/10.3390/rs18010124</link><guid>10.3390/rs18010124</guid><pubDate>Wed, 31 Dec 2025 13:30:52 +0000</pubDate><dc:creator>Qian Weng</dc:creator><dc:creator>Xiansheng Huang</dc:creator><dc:creator>Yifeng Lin</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:creator>Zhaocheng Li</dc:creator><dc:creator>Cairen Jian</dc:creator><dc:creator>Jiawen Lin</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010124</prism:doi><description>Remote sensing semantic segmentation holds significant application value in urban planning, environmental monitoring, and related fields. In recent years, multimodal approaches that fuse optical imagery with normalized Digital Surface Models (nDSM) have attracted widespread attention due to their superior performance. However, existing methods typically treat nDSM merely as an additional input channel, failing to effectively exploit its inherent 3D geometric priors, which limits segmentation accuracy in complex urban scenes. To address this issue, we propose a Geometry-aware Interactive Multi-Modal Network (GIMMNet), which explicitly models the geometric structure embedded in nDSM to guide the spatial distribution of semantic categories. Specifically, we first design a Geometric Position Prior Module (GPPM) to construct 3D coordinates for each pixel based on nDSM and extract intrinsic geometric priors. Next, a Geometry-Guided Disentangled Fusion Module (GDFM) dynamically adjusts fusion weights according to the differential responses of each modality to the geometric priors, enabling adaptive multimodal feature integration. Finally, during decoding, a Geometry-Attentive Context Module (GACM) explicitly captures the dependencies between land-cover categories and geometric structures, enhancing the model’s spatial awareness and semantic recovery capability. Experimental results on two public remote sensing datasets—Vaihingen and Potsdam—show that the proposed GIMMNet outperforms existing mainstream methods in segmentation performance, demonstrating that enhancing the model’s geometric perception capability effectively improves semantic segmentation accuracy. Notably, our method achieves an mIoU of 85.2% on the Potsdam dataset, surpassing the second-best multimodal approach, PACSCNet, by 2.3%.
Published: 2025-12-31T13:30:52+00:00
Venue: Remote Sensing
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Weng; Xiansheng Huang; Yifeng Lin; Yu Zhang; Zhaocheng Li; Cairen Jian; Jiawen Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010124"&gt;10.3390/rs18010124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing semantic segmentation holds significant application value in urban planning, environmental monitoring, and related fields. In recent years, multimodal approaches that fuse optical imagery with normalized Digital Surface Models (nDSM) have attracted widespread attention due to their superior performance. However, existing methods typically treat nDSM merely as an additional input channel, failing to effectively exploit its inherent 3D geometric priors, which limits segmentation accuracy in complex urban scenes. To address this issue, we propose a Geometry-aware Interactive Multi-Modal Network (GIMMNet), which explicitly models the geometric structure embedded in nDSM to guide the spatial distribution of semantic categories. Specifically, we first design a Geometric Position Prior Module (GPPM) to construct 3D coordinates for each pixel based on nDSM and extract intrinsic geometric priors. Next, a Geometry-Guided Disentangled Fusion Module (GDFM) dynamically adjusts fusion weights according to the differential responses of each modality to the geometric priors, enabling adaptive multimodal feature integration. Finally, during decoding, a Geometry-Attentive Context Module (GACM) explicitly captures the dependencies between land-cover categories and geometric structures, enhancing the model’s spatial awareness and semantic recovery capability. Experimental results on two public remote sensing datasets—Vaihingen and Potsdam—show that the proposed GIMMNet outperforms existing mainstream methods in segmentation performance, demonstrating that enhancing the model’s geometric perception capability effectively improves semantic segmentation accuracy. Notably, our method achieves an mIoU of 85.2% on the Potsdam dataset, surpassing the second-best multimodal approach, PACSCNet, by 2.3%.&lt;/p&gt;</content:encoded></item><item><title>Degradation-Aware Graph Neural Network for Blind Super-Resolution</title><link>https://doi.org/10.1016/j.patcog.2025.113007</link><guid>10.1016/j.patcog.2025.113007</guid><pubDate>Wed, 31 Dec 2025 00:08:29 +0000</pubDate><dc:creator>Zehui Xiao</dc:creator><dc:creator>Xianhong Wen</dc:creator><dc:creator>Xuyang Tan</dc:creator><dc:creator>Xiangyuan Zhu</dc:creator><dc:creator>Kehua Guo</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113007</prism:doi><description>Blind super-resolution (BSR) aims to restore high-resolution (HR) images from low-resolution (LR) inputs suffering from unknown and complex degradations, a challenging yet crucial task for real-world applications. Existing methods based on convolutional neural network or Transformer often struggle due to fixed receptive fields or rigid structural constraints, and they perform poorly in diverse blind settings. To address these limitations, we propose DAG-BSR, a novel Degradation-Aware Graph Neural Network for Blind Super-Resolution. DAG-BSR uniquely synergizes the flexible relational modeling capabilities of graph neural network with unsupervised degradation representation learning via graph-based contrastive learning. Specifically, we introduce Degradation-Aware Graph Processing Blocks (DAGPB), featuring an Adaptive Graph Construction Unit (AGCU) that dynamically builds content and degradation-aware graphs by incorporating a detail significance metric and learned degradation priors. Within DAGPB, Multi-scale Graph Aggregation Units (MGAU) further refine features by performing aggregation on these adaptive graphs, leveraging both local and global structures. This allows DAG-BSR to effectively model intrinsic image structures while adaptively responding to extrinsic unknown degradations. Extensive experiments on benchmark datasets demonstrate that DAG-BSR achieves state of-the-art performance. Notably, under noise-free degradations with isotropic blur and challenging anisotropic degradation with noise, DAG-BSR surpasses the strong baseline DSAT by up to 0.79 dB on the Urban100 dataset and 0.77 dB on the Set14 dataset respectively. Our method produces visually superior results with enhanced detail and fewer artifacts compared to existing approaches. Codes are available at https://github.com/huihuihuiZX/DAG-BSR .
Published: 2025-12-31T00:08:29+00:00
Venue: Pattern Recognition
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehui Xiao; Xianhong Wen; Xuyang Tan; Xiangyuan Zhu; Kehua Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113007"&gt;10.1016/j.patcog.2025.113007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Blind super-resolution (BSR) aims to restore high-resolution (HR) images from low-resolution (LR) inputs suffering from unknown and complex degradations, a challenging yet crucial task for real-world applications. Existing methods based on convolutional neural network or Transformer often struggle due to fixed receptive fields or rigid structural constraints, and they perform poorly in diverse blind settings. To address these limitations, we propose DAG-BSR, a novel Degradation-Aware Graph Neural Network for Blind Super-Resolution. DAG-BSR uniquely synergizes the flexible relational modeling capabilities of graph neural network with unsupervised degradation representation learning via graph-based contrastive learning. Specifically, we introduce Degradation-Aware Graph Processing Blocks (DAGPB), featuring an Adaptive Graph Construction Unit (AGCU) that dynamically builds content and degradation-aware graphs by incorporating a detail significance metric and learned degradation priors. Within DAGPB, Multi-scale Graph Aggregation Units (MGAU) further refine features by performing aggregation on these adaptive graphs, leveraging both local and global structures. This allows DAG-BSR to effectively model intrinsic image structures while adaptively responding to extrinsic unknown degradations. Extensive experiments on benchmark datasets demonstrate that DAG-BSR achieves state of-the-art performance. Notably, under noise-free degradations with isotropic blur and challenging anisotropic degradation with noise, DAG-BSR surpasses the strong baseline DSAT by up to 0.79 dB on the Urban100 dataset and 0.77 dB on the Set14 dataset respectively. Our method produces visually superior results with enhanced detail and fewer artifacts compared to existing approaches. Codes are available at https://github.com/huihuihuiZX/DAG-BSR .&lt;/p&gt;</content:encoded></item><item><title>Planes, Not A380: How Prompting, Context, and Granularity Shape VLM Performance in Aerial Imagery</title><link>https://doi.org/10.1109/jstars.2025.3649701</link><guid>10.1109/jstars.2025.3649701</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Naël Ouerghemi</dc:creator><dc:creator>Ciprian Tomoiagă</dc:creator><dc:creator>Marcin Detyniecki</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649701</prism:doi><description>Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naël Ouerghemi; Ciprian Tomoiagă; Marcin Detyniecki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649701"&gt;10.1109/jstars.2025.3649701&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.&lt;/p&gt;</content:encoded></item><item><title>Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation</title><link>https://doi.org/10.1016/j.neunet.2025.108533</link><guid>10.1016/j.neunet.2025.108533</guid><pubDate>Wed, 31 Dec 2025 00:15:06 +0000</pubDate><dc:creator>Minghao Cui</dc:creator><dc:creator>Jing Nie</dc:creator><dc:creator>Hanqing Sun</dc:creator><dc:creator>Jin Xie</dc:creator><dc:creator>Jiale Cao</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108533</prism:doi><description>Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.
Published: 2025-12-31T00:15:06+00:00
Venue: Neural Networks
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghao Cui; Jing Nie; Hanqing Sun; Jin Xie; Jiale Cao; Yanwei Pang; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108533"&gt;10.1016/j.neunet.2025.108533&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.&lt;/p&gt;</content:encoded></item><item><title>Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation</title><link>https://arxiv.org/abs/2512.23997v1</link><guid>http://arxiv.org/abs/2512.23997v1</guid><pubDate>Tue, 30 Dec 2025 05:34:28 +0000</pubDate><dc:creator>Haotang Li</dc:creator><dc:creator>Zhenyu Qi</dc:creator><dc:creator>Hao Qin</dc:creator><dc:creator>Huanrui Yang</dc:creator><dc:creator>Sen He</dc:creator><dc:creator>Kebin Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.
Published: 2025-12-30T05:34:28+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haotang Li; Zhenyu Qi; Hao Qin; Huanrui Yang; Sen He; Kebin Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.&lt;/p&gt;</content:encoded></item><item><title>SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</title><link>https://arxiv.org/abs/2512.24330v1</link><guid>http://arxiv.org/abs/2512.24330v1</guid><pubDate>Tue, 30 Dec 2025 16:31:45 +0000</pubDate><dc:creator>Yong Xien Chng</dc:creator><dc:creator>Tao Hu</dc:creator><dc:creator>Wenwen Tong</dc:creator><dc:creator>Xueheng Li</dc:creator><dc:creator>Jiandong Chen</dc:creator><dc:creator>Haojia Yu</dc:creator><dc:creator>Jiefan Lu</dc:creator><dc:creator>Hewei Guo</dc:creator><dc:creator>Hanming Deng</dc:creator><dc:creator>Chengjun Xie</dc:creator><dc:creator>Gao Huang</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Lewei Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.
Published: 2025-12-30T16:31:45+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yong Xien Chng; Tao Hu; Wenwen Tong; Xueheng Li; Jiandong Chen; Haojia Yu; Jiefan Lu; Hewei Guo; Hanming Deng; Chengjun Xie; Gao Huang; Dahua Lin; Lewei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&amp;#x27;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.&lt;/p&gt;</content:encoded></item><item><title>MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation</title><link>https://arxiv.org/abs/2512.24243v1</link><guid>http://arxiv.org/abs/2512.24243v1</guid><pubDate>Tue, 30 Dec 2025 14:09:17 +0000</pubDate><dc:creator>Fuqiang Gu</dc:creator><dc:creator>Yuanke Li</dc:creator><dc:creator>Xianlei Long</dc:creator><dc:creator>Kangping Ji</dc:creator><dc:creator>Chao Chen</dc:creator><dc:creator>Qingyi Gu</dc:creator><dc:creator>Zhenliang Ni</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.
Published: 2025-12-30T14:09:17+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuqiang Gu; Yuanke Li; Xianlei Long; Kangping Ji; Chao Chen; Qingyi Gu; Zhenliang Ni&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.&lt;/p&gt;</content:encoded></item><item><title>Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation</title><link>https://arxiv.org/abs/2512.23938v1</link><guid>http://arxiv.org/abs/2512.23938v1</guid><pubDate>Tue, 30 Dec 2025 01:51:52 +0000</pubDate><dc:creator>Hualin Ye</dc:creator><dc:creator>Bingxi Liu</dc:creator><dc:creator>Jixiang Du</dc:creator><dc:creator>Yu Qin</dc:creator><dc:creator>Ziyi Chen</dc:creator><dc:creator>Hong Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.
Published: 2025-12-30T01:51:52+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hualin Ye; Bingxi Liu; Jixiang Du; Yu Qin; Ziyi Chen; Hong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.&lt;/p&gt;</content:encoded></item><item><title>RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations</title><link>https://arxiv.org/abs/2512.24023v1</link><guid>http://arxiv.org/abs/2512.24023v1</guid><pubDate>Tue, 30 Dec 2025 06:50:11 +0000</pubDate><dc:creator>Xingqi He</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Shuyong Gao</dc:creator><dc:creator>Wenjie Li</dc:creator><dc:creator>Lingyi Hong</dc:creator><dc:creator>Mingxi Chen</dc:creator><dc:creator>Kaixun Jiang</dc:creator><dc:creator>Jiyuan Fu</dc:creator><dc:creator>Wenqiang Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.
Published: 2025-12-30T06:50:11+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingqi He; Yujie Zhang; Shuyong Gao; Wenjie Li; Lingyi Hong; Mingxi Chen; Kaixun Jiang; Jiyuan Fu; Wenqiang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.&lt;/p&gt;</content:encoded></item><item><title>CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23453v1</link><guid>http://arxiv.org/abs/2512.23453v1</guid><pubDate>Mon, 29 Dec 2025 13:23:20 +0000</pubDate><dc:creator>Zongsheng Cao</dc:creator><dc:creator>Yangfan He</dc:creator><dc:creator>Anran Liu</dc:creator><dc:creator>Jun Xie</dc:creator><dc:creator>Feng Chen</dc:creator><dc:creator>Zepeng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.
Published: 2025-12-29T13:23:20+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongsheng Cao; Yangfan He; Anran Liu; Jun Xie; Feng Chen; Zepeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.&lt;/p&gt;</content:encoded></item><item><title>Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning</title><link>https://arxiv.org/abs/2512.24591v1</link><guid>http://arxiv.org/abs/2512.24591v1</guid><pubDate>Wed, 31 Dec 2025 03:28:17 +0000</pubDate><dc:creator>Fuyu Dong</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Nan Luo</dc:creator><dc:creator>Yiming Zhang</dc:creator><dc:creator>Kaiyu Li</dc:creator><dc:creator>Jianfei Yang</dc:creator><dc:creator>Quan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.
Published: 2025-12-31T03:28:17+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuyu Dong; Ke Li; Di Wang; Nan Luo; Yiming Zhang; Kaiyu Li; Jianfei Yang; Quan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.&lt;/p&gt;</content:encoded></item><item><title>Multi-label Classification with Panoptic Context Aggregation Networks</title><link>https://arxiv.org/abs/2512.23486v1</link><guid>http://arxiv.org/abs/2512.23486v1</guid><pubDate>Mon, 29 Dec 2025 14:16:21 +0000</pubDate><dc:creator>Mingyuan Jiu</dc:creator><dc:creator>Hailong Zhu</dc:creator><dc:creator>Wenchuan Wei</dc:creator><dc:creator>Hichem Sahbi</dc:creator><dc:creator>Rongrong Ji</dc:creator><dc:creator>Mingliang Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.
Published: 2025-12-29T14:16:21+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyuan Jiu; Hailong Zhu; Wenchuan Wei; Hichem Sahbi; Rongrong Ji; Mingliang Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.&lt;/p&gt;</content:encoded></item><item><title>MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning</title><link>https://arxiv.org/abs/2512.23369v1</link><guid>http://arxiv.org/abs/2512.23369v1</guid><pubDate>Mon, 29 Dec 2025 10:58:40 +0000</pubDate><dc:creator>Shuyuan Lin</dc:creator><dc:creator>Mengtin Lo</dc:creator><dc:creator>Haosheng Chen</dc:creator><dc:creator>Yanjie Liang</dc:creator><dc:creator>Qiangqiang Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.24963/ijcai.2025/172</prism:doi><description>Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.
Published: 2025-12-29T10:58:40+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuyuan Lin; Mengtin Lo; Haosheng Chen; Yanjie Liang; Qiangqiang Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.24963/ijcai.2025/172"&gt;10.24963/ijcai.2025/172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis</title><link>https://arxiv.org/abs/2512.24013v1</link><guid>http://arxiv.org/abs/2512.24013v1</guid><pubDate>Tue, 30 Dec 2025 06:18:23 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Hui Li</dc:creator><dc:creator>Yiyun Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.
Published: 2025-12-30T06:18:23+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Hui Li; Yiyun Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.&lt;/p&gt;</content:encoded></item><item><title>A DINO‑Based Progressive Semantic Enhanced Infrared And Visible Image Fusion Network</title><link>https://doi.org/10.1016/j.neunet.2025.108527</link><guid>10.1016/j.neunet.2025.108527</guid><pubDate>Wed, 31 Dec 2025 08:01:50 +0000</pubDate><dc:creator>Shihan Yao</dc:creator><dc:creator>Zhonghui Pei</dc:creator><dc:creator>Huiqin Zhang</dc:creator><dc:creator>Haiyang Jiang</dc:creator><dc:creator>Huabing Zhou</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108527</prism:doi><description>Infrared and visible image fusion aims to integrate complementary information from two source images into a single fused image with rich detail. However, most existing fusion methods focus on visual appearance and pay little attention to the semantic requirements of downstream applications. Although some semantic driven approaches enhance the semantic content of fused images, they rely on labelled data that contain only limited semantic target information.To address this limitation, this paper proposes a DINO-based progressive semantic enhanced infrared and visible image fusion network (DPSEF). DINO is a self supervised model that learns representations from large volumes of unlabelled images and exhibits powerful spatial semantic clustering capabilities. We exploit DINO to extract fine grained spatial semantic features as prior knowledge, and then introduce a semantic enhanced fusion module (SEFM) that progressively injects these semantic priors into the fusion network. This mechanism guides the model to focus on target relevant regions and generates high quality fused images that combine rich semantic and detailed information, thereby meeting the needs of subsequent high level vision tasks.Extensive experiments demonstrate that DPSEF produces fused images whose visual quality significantly exceeds that of mainstream algorithms. Qualitative and quantitative analyses further confirm the strong potential of DPSEF in high level vision applications. Moreover, additional experiments on multi focus image fusion validate the generality and robustness of the proposed network.
Published: 2025-12-31T08:01:50+00:00
Venue: Neural Networks
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihan Yao; Zhonghui Pei; Huiqin Zhang; Haiyang Jiang; Huabing Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108527"&gt;10.1016/j.neunet.2025.108527&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion aims to integrate complementary information from two source images into a single fused image with rich detail. However, most existing fusion methods focus on visual appearance and pay little attention to the semantic requirements of downstream applications. Although some semantic driven approaches enhance the semantic content of fused images, they rely on labelled data that contain only limited semantic target information.To address this limitation, this paper proposes a DINO-based progressive semantic enhanced infrared and visible image fusion network (DPSEF). DINO is a self supervised model that learns representations from large volumes of unlabelled images and exhibits powerful spatial semantic clustering capabilities. We exploit DINO to extract fine grained spatial semantic features as prior knowledge, and then introduce a semantic enhanced fusion module (SEFM) that progressively injects these semantic priors into the fusion network. This mechanism guides the model to focus on target relevant regions and generates high quality fused images that combine rich semantic and detailed information, thereby meeting the needs of subsequent high level vision tasks.Extensive experiments demonstrate that DPSEF produces fused images whose visual quality significantly exceeds that of mainstream algorithms. Qualitative and quantitative analyses further confirm the strong potential of DPSEF in high level vision applications. Moreover, additional experiments on multi focus image fusion validate the generality and robustness of the proposed network.&lt;/p&gt;</content:encoded></item><item><title>Empowering Large Language Models to Set up Knowledge Retrieval Indexing Via Self-Learning</title><link>https://doi.org/10.1109/tkde.2025.3649907</link><guid>10.1109/tkde.2025.3649907</guid><pubDate>Wed, 31 Dec 2025 18:45:41 +0000</pubDate><dc:creator>Simin Niu</dc:creator><dc:creator>Mengwei Wang</dc:creator><dc:creator>Xun Liang</dc:creator><dc:creator>Zhiyu Li</dc:creator><dc:creator>Sensen Zhang</dc:creator><dc:creator>Shichao Song</dc:creator><dc:creator>Hanyu Wang</dc:creator><dc:creator>Jiawei Yang</dc:creator><dc:creator>Feiyu Xiong</dc:creator><dc:creator>Chenyang Xi</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2025.3649907</prism:doi><description>Retrieval-augmented generation (RAG) provides an efficient solution for expanding the knowledge boundaries of large language models (LLMs), where the indexing serves as a compass to guide LLMs in locating query-relevant external knowledge. Nevertheless, current indexing methods commonly encounter a critical challenge: native indexing is convenient to construct, but it usually disrupts contextual associations and constrains the expressive capacity of rich knowledge. Conversely, knowledge indexing can structure contextual knowledge, but it is often based on preset schemas that limit its generalizability. To address it, we propose a universal and flexible knowledge indexing called pseudo-graph (PG) indexing. During the indexing construction phase, we use the advanced LLMs to transform the knowledge of each raw text into a concise and structured mind map, organizing intra-document knowledge. Subsequently, independent mind maps are linked by associating highly relevant topics or consistent facts across documents, thereby establishing inter-document knowledge connections. Eventually, using the resulting knowledge network PG as the knowledge indexing can circumvent the challenges associated with schema design reliant on preset knowledge and relationship types. During the knowledge retrieval phase, we develop a PG knowledge retriever to mimic human note-reviewing, adaptively navigating and recalling query-relevant knowledge from PG. Experimental results demonstrate that retrieving relevant pseudo-subgraphs from the PG via PG indexing and retriever significantly improves performance in fact-based Q&amp;A, hallucination correction, and two multi-document Q&amp;A tasks, achieving F 1 Q E F1_{QE} improvements of 15.85%, 8.12%, 3.34%, and 5.73%, respectively, and outperforming the state-of-the-art baseline KGP-LLaMA. Our code is available at: https://github.com/IAAR-Shanghai/PGRAG.
Published: 2025-12-31T18:45:41+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Simin Niu; Mengwei Wang; Xun Liang; Zhiyu Li; Sensen Zhang; Shichao Song; Hanyu Wang; Jiawei Yang; Feiyu Xiong; Chenyang Xi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2025.3649907"&gt;10.1109/tkde.2025.3649907&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Retrieval-augmented generation (RAG) provides an efficient solution for expanding the knowledge boundaries of large language models (LLMs), where the indexing serves as a compass to guide LLMs in locating query-relevant external knowledge. Nevertheless, current indexing methods commonly encounter a critical challenge: native indexing is convenient to construct, but it usually disrupts contextual associations and constrains the expressive capacity of rich knowledge. Conversely, knowledge indexing can structure contextual knowledge, but it is often based on preset schemas that limit its generalizability. To address it, we propose a universal and flexible knowledge indexing called pseudo-graph (PG) indexing. During the indexing construction phase, we use the advanced LLMs to transform the knowledge of each raw text into a concise and structured mind map, organizing intra-document knowledge. Subsequently, independent mind maps are linked by associating highly relevant topics or consistent facts across documents, thereby establishing inter-document knowledge connections. Eventually, using the resulting knowledge network PG as the knowledge indexing can circumvent the challenges associated with schema design reliant on preset knowledge and relationship types. During the knowledge retrieval phase, we develop a PG knowledge retriever to mimic human note-reviewing, adaptively navigating and recalling query-relevant knowledge from PG. Experimental results demonstrate that retrieving relevant pseudo-subgraphs from the PG via PG indexing and retriever significantly improves performance in fact-based Q&amp;amp;A, hallucination correction, and two multi-document Q&amp;amp;A tasks, achieving F 1 Q E F1_{QE} improvements of 15.85%, 8.12%, 3.34%, and 5.73%, respectively, and outperforming the state-of-the-art baseline KGP-LLaMA. Our code is available at: https://github.com/IAAR-Shanghai/PGRAG.&lt;/p&gt;</content:encoded></item><item><title>Same or Not? Enhancing Visual Perception in Vision-Language Models</title><link>https://arxiv.org/abs/2512.23592v1</link><guid>http://arxiv.org/abs/2512.23592v1</guid><pubDate>Mon, 29 Dec 2025 16:43:47 +0000</pubDate><dc:creator>Damiano Marsili</dc:creator><dc:creator>Aditya Mehta</dc:creator><dc:creator>Ryan Y. Lin</dc:creator><dc:creator>Georgia Gkioxari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/
Published: 2025-12-29T16:43:47+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Damiano Marsili; Aditya Mehta; Ryan Y. Lin; Georgia Gkioxari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (&amp;quot;Is it a cat or a dog?&amp;quot;) over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/&lt;/p&gt;</content:encoded></item><item><title>Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3650193</link><guid>10.1109/jstars.2025.3650193</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Jianfen Wei</dc:creator><dc:creator>Ping Yang</dc:creator><dc:creator>Chang Wang</dc:creator><dc:creator>Chunxiang Shi</dc:creator><dc:creator>Renlong Hang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650193</prism:doi><description>Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianfen Wei; Ping Yang; Chang Wang; Chunxiang Shi; Renlong Hang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650193"&gt;10.1109/jstars.2025.3650193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion</title><link>https://doi.org/10.1109/jstars.2025.3647819</link><guid>10.1109/jstars.2025.3647819</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Junyu Huang</dc:creator><dc:creator>Jiawei Chen</dc:creator><dc:creator>Renbo Luo</dc:creator><dc:creator>Yongan Lu</dc:creator><dc:creator>Jinxin Yang</dc:creator><dc:creator>Zhifeng Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647819</prism:doi><description>Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyu Huang; Jiawei Chen; Renbo Luo; Yongan Lu; Jinxin Yang; Zhifeng Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647819"&gt;10.1109/jstars.2025.3647819&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.&lt;/p&gt;</content:encoded></item><item><title>Uncertainty-based Dendritic Model for Multimodal Remote Sensing Data Classification</title><link>https://doi.org/10.1109/tgrs.2025.3649778</link><guid>10.1109/tgrs.2025.3649778</guid><pubDate>Wed, 31 Dec 2025 18:44:00 +0000</pubDate><dc:creator>Xin He</dc:creator><dc:creator>Xiao Han</dc:creator><dc:creator>Yaqin Zhao</dc:creator><dc:creator>Yushi Chen</dc:creator><dc:creator>Limin Zou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649778</prism:doi><description>Multimodal remote sensing data is inevitably affected by noise due to atmospheric conditions, sensor limitations, and other factors. However, existing deep learning-based multimodal remote sensing classification (MRSC) methods overlook the impact of these data noise, which produces the uncertainty and decreases classification accuracy. To address this problem, this paper first explores uncertainty-based dendritic model (UDM) for MRSC, which reduces the uncertainty at single-modality feature extraction and multimodal feature fusion stages. At the single-modality feature extraction stage, dendrites, as a novel type of neurons, have been demonstrated with strong feature extraction abilities. Inspired by the dendritic structure, we first design a dendritic-based spatial-channel feature extraction (DSCE) module. Specifically, a dendritic neural layer (DNL) is designed in DSCE. The proposed DNL constructs a multi-branch fusion strategy to enhance the expressive capacity of multimodal remote sensing feature extraction, which achieves the localized subspace computation ability. Furthermore, based on the extracted features by DSCE, a dendritic uncertainty-based feature enhancement (DUFE) module is explored to reduce the uncertainty from the extracted features. DUFE exploits uncertainty estimation to adaptively refine the extracted representations, thereby improving feature robustness and discriminative power for MRSC. At the multimodal feature fusion stage, considering the feature redundancy of the different modalities, a dendritic-based uncertainty-aware fusion (DUAF) module is proposed. DUAF performs feature fusion by dynamically assigning weights based on the estimated uncertainty of each modality, thus enhancing classification performance. Experiments on benchmark datasets demonstrate that the proposed UDM outperforms current state-of-the-art methods based on Transformer, convolutional neural network, and Mamba for MRSC. The code is available at https://github.com/hx0558...
Published: 2025-12-31T18:44:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin He; Xiao Han; Yaqin Zhao; Yushi Chen; Limin Zou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649778"&gt;10.1109/tgrs.2025.3649778&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal remote sensing data is inevitably affected by noise due to atmospheric conditions, sensor limitations, and other factors. However, existing deep learning-based multimodal remote sensing classification (MRSC) methods overlook the impact of these data noise, which produces the uncertainty and decreases classification accuracy. To address this problem, this paper first explores uncertainty-based dendritic model (UDM) for MRSC, which reduces the uncertainty at single-modality feature extraction and multimodal feature fusion stages. At the single-modality feature extraction stage, dendrites, as a novel type of neurons, have been demonstrated with strong feature extraction abilities. Inspired by the dendritic structure, we first design a dendritic-based spatial-channel feature extraction (DSCE) module. Specifically, a dendritic neural layer (DNL) is designed in DSCE. The proposed DNL constructs a multi-branch fusion strategy to enhance the expressive capacity of multimodal remote sensing feature extraction, which achieves the localized subspace computation ability. Furthermore, based on the extracted features by DSCE, a dendritic uncertainty-based feature enhancement (DUFE) module is explored to reduce the uncertainty from the extracted features. DUFE exploits uncertainty estimation to adaptively refine the extracted representations, thereby improving feature robustness and discriminative power for MRSC. At the multimodal feature fusion stage, considering the feature redundancy of the different modalities, a dendritic-based uncertainty-aware fusion (DUAF) module is proposed. DUAF performs feature fusion by dynamically assigning weights based on the estimated uncertainty of each modality, thus enhancing classification performance. Experiments on benchmark datasets demonstrate that the proposed UDM outperforms current state-of-the-art methods based on Transformer, convolutional neural network, and Mamba for MRSC. The code is available at https://github.com/hx0558...&lt;/p&gt;</content:encoded></item><item><title>Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning</title><link>https://doi.org/10.1109/tip.2025.3646890</link><guid>10.1109/tip.2025.3646890</guid><pubDate>Wed, 31 Dec 2025 18:46:40 +0000</pubDate><dc:creator>Guoqing Wang</dc:creator><dc:creator>Chao Ma</dc:creator><dc:creator>Xiaokang Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646890</prism:doi><description>Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.
Published: 2025-12-31T18:46:40+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoqing Wang; Chao Ma; Xiaokang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646890"&gt;10.1109/tip.2025.3646890&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>BRSMamba: Boundary-Aware Mamba for Forest and Shrub Segmentation From Diverse Satellite Imagery</title><link>https://doi.org/10.1109/jstars.2025.3650425</link><guid>10.1109/jstars.2025.3650425</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Zhijie He</dc:creator><dc:creator>Xiang Weng</dc:creator><dc:creator>Kai Fang</dc:creator><dc:creator>Yane Li</dc:creator><dc:creator>Yaoping Ruan</dc:creator><dc:creator>Hailin Feng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650425</prism:doi><description>Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth's surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijie He; Xiang Weng; Kai Fang; Yane Li; Yaoping Ruan; Hailin Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650425"&gt;10.1109/jstars.2025.3650425&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth&amp;#x27;s surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...&lt;/p&gt;</content:encoded></item><item><title>DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</title><link>https://arxiv.org/abs/2512.24165v1</link><guid>http://arxiv.org/abs/2512.24165v1</guid><pubDate>Tue, 30 Dec 2025 11:51:18 +0000</pubDate><dc:creator>Zefeng He</dc:creator><dc:creator>Xiaoye Qu</dc:creator><dc:creator>Yafu Li</dc:creator><dc:creator>Tong Zhu</dc:creator><dc:creator>Siyuan Huang</dc:creator><dc:creator>Yu Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.
Published: 2025-12-30T11:51:18+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zefeng He; Xiaoye Qu; Yafu Li; Tong Zhu; Siyuan Huang; Yu Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.&lt;/p&gt;</content:encoded></item><item><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>https://arxiv.org/abs/2512.23176v1</link><guid>http://arxiv.org/abs/2512.23176v1</guid><pubDate>Mon, 29 Dec 2025 03:34:39 +0000</pubDate><dc:creator>Yi Zhang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Lei Yao</dc:creator><dc:creator>Lap-Pui Chau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).
Published: 2025-12-29T03:34:39+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhang; Yi Wang; Lei Yao; Lap-Pui Chau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</content:encoded></item></channel></rss>