<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 03 Feb 2026 03:28:23 +0000</lastBuildDate><item><title>Multi-Domain Incremental Learning for Semantic Segmentation via Visual Domain Prompt in Remote Sensing Data</title><link>https://doi.org/10.3390/rs18030464</link><guid>10.3390/rs18030464</guid><pubDate>Mon, 02 Feb 2026 14:12:46 +0000</pubDate><dc:creator>Junxi Li</dc:creator><dc:creator>Zhiyuan Yan</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Yidan Zhang</dc:creator><dc:creator>Zicong Zhu</dc:creator><dc:creator>Yichen Tian</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030464</prism:doi><description>Domain incremental learning for semantic segmentation has gained lots of attention due to its importance for many fields including urban planning and autonomous driving. The catastrophic forgetting problem caused by domain shift has been alleviated by structure expansion of the model or data rehearsal. However, these methods ignore similar contextual knowledge between the new and the old data domain and assume that new knowledge and old knowledge are completely mutually exclusive, which cause the model to be trained in a suboptimal direction. Motivated by the prompt learning, we proposed a new domain incremental learning framework named RS-VDP. The key innovation of RS-VDP is to utilize a visual domain prompt to change the optimization direction from input data space and feature space. First, we designed a domain prompt based on a dynamic location module, which applied a visual domain prompt according to a local entropy map to update the distribution of the input images. Second, in order to filter the feature vectors with high confidence, a representation feature alignment based on an entropy map module is proposed. This module ensures the accuracy and stability of the feature vectors involved in the regularization loss, alleviating the problem of semantic drift. Finally, we introduced a new evaluation metric to measure the overall performance of the incremental learning models, solving the problem that the traditional evaluation metric is affected by the single-task accuracy. Comprehensive experiments demonstrated the effectiveness of the proposed method by significantly reducing the degree of catastrophic forgetting.
Published: 2026-02-02T14:12:46+00:00
Venue: Remote Sensing
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junxi Li; Zhiyuan Yan; Wenhui Diao; Yidan Zhang; Zicong Zhu; Yichen Tian; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030464"&gt;10.3390/rs18030464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Domain incremental learning for semantic segmentation has gained lots of attention due to its importance for many fields including urban planning and autonomous driving. The catastrophic forgetting problem caused by domain shift has been alleviated by structure expansion of the model or data rehearsal. However, these methods ignore similar contextual knowledge between the new and the old data domain and assume that new knowledge and old knowledge are completely mutually exclusive, which cause the model to be trained in a suboptimal direction. Motivated by the prompt learning, we proposed a new domain incremental learning framework named RS-VDP. The key innovation of RS-VDP is to utilize a visual domain prompt to change the optimization direction from input data space and feature space. First, we designed a domain prompt based on a dynamic location module, which applied a visual domain prompt according to a local entropy map to update the distribution of the input images. Second, in order to filter the feature vectors with high confidence, a representation feature alignment based on an entropy map module is proposed. This module ensures the accuracy and stability of the feature vectors involved in the regularization loss, alleviating the problem of semantic drift. Finally, we introduced a new evaluation metric to measure the overall performance of the incremental learning models, solving the problem that the traditional evaluation metric is affected by the single-task accuracy. Comprehensive experiments demonstrated the effectiveness of the proposed method by significantly reducing the degree of catastrophic forgetting.&lt;/p&gt;</content:encoded></item><item><title>Toward Cognitive Supersensing in Multimodal Large Language Model</title><link>https://arxiv.org/abs/2602.01541v1</link><guid>http://arxiv.org/abs/2602.01541v1</guid><pubDate>Mon, 02 Feb 2026 02:19:50 +0000</pubDate><dc:creator>Boyi Li</dc:creator><dc:creator>Yifan Shen</dc:creator><dc:creator>Yuanzhe Liu</dc:creator><dc:creator>Yifan Xu</dc:creator><dc:creator>Jiateng Liu</dc:creator><dc:creator>Xinzhuo Li</dc:creator><dc:creator>Zhengyuan Li</dc:creator><dc:creator>Jingyuan Zhu</dc:creator><dc:creator>Yunhan Zhong</dc:creator><dc:creator>Fangzhou Lan</dc:creator><dc:creator>Jianguo Cao</dc:creator><dc:creator>James M. Rehg</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Ismini Lourentzou</dc:creator><dc:creator>Xu Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.
Published: 2026-02-02T02:19:50+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyi Li; Yifan Shen; Yuanzhe Liu; Yifan Xu; Jiateng Liu; Xinzhuo Li; Zhengyuan Li; Jingyuan Zhu; Yunhan Zhong; Fangzhou Lan; Jianguo Cao; James M. Rehg; Heng Ji; Ismini Lourentzou; Xu Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.&lt;/p&gt;</content:encoded></item><item><title>Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.01163v1</link><guid>http://arxiv.org/abs/2602.01163v1</guid><pubDate>Sun, 01 Feb 2026 11:30:03 +0000</pubDate><dc:creator>Chunliang Hua</dc:creator><dc:creator>Zeyuan Yang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Jiayang Sun</dc:creator><dc:creator>Fengwen Chen</dc:creator><dc:creator>Chunlan Zeng</dc:creator><dc:creator>Xiao Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.
Published: 2026-02-01T11:30:03+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunliang Hua; Zeyuan Yang; Lei Zhang; Jiayang Sun; Fengwen Chen; Chunlan Zeng; Xiao Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.&lt;/p&gt;</content:encoded></item><item><title>From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking</title><link>https://arxiv.org/abs/2602.00593v1</link><guid>http://arxiv.org/abs/2602.00593v1</guid><pubDate>Sat, 31 Jan 2026 08:18:34 +0000</pubDate><dc:creator>Yifan Jiang</dc:creator><dc:creator>Cong Zhang</dc:creator><dc:creator>Bofei Zhang</dc:creator><dc:creator>Yifan Yang</dc:creator><dc:creator>Bingzhang Wang</dc:creator><dc:creator>Yew-Soon Ong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.
Published: 2026-01-31T08:18:34+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Jiang; Cong Zhang; Bofei Zhang; Yifan Yang; Bingzhang Wang; Yew-Soon Ong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.&lt;/p&gt;</content:encoded></item><item><title>Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment</title><link>https://arxiv.org/abs/2602.01257v1</link><guid>http://arxiv.org/abs/2602.01257v1</guid><pubDate>Sun, 01 Feb 2026 14:35:46 +0000</pubDate><dc:creator>Yunchuan Ma</dc:creator><dc:creator>Laiyun Qing</dc:creator><dc:creator>Guorong Li</dc:creator><dc:creator>Yuqing Liu</dc:creator><dc:creator>Yuankai Qi</dc:creator><dc:creator>Qingming Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.
Published: 2026-02-01T14:35:46+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunchuan Ma; Laiyun Qing; Guorong Li; Yuqing Liu; Yuankai Qi; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.&lt;/p&gt;</content:encoded></item><item><title>PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization</title><link>https://arxiv.org/abs/2601.22492v1</link><guid>http://arxiv.org/abs/2601.22492v1</guid><pubDate>Fri, 30 Jan 2026 03:04:06 +0000</pubDate><dc:creator>Duncan McCain</dc:creator><dc:creator>Hossein Kashiani</dc:creator><dc:creator>Fatemeh Afghah</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.
Published: 2026-01-30T03:04:06+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Duncan McCain; Hossein Kashiani; Fatemeh Afghah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.&lt;/p&gt;</content:encoded></item><item><title>Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning</title><link>https://arxiv.org/abs/2601.23224v1</link><guid>http://arxiv.org/abs/2601.23224v1</guid><pubDate>Fri, 30 Jan 2026 17:47:30 +0000</pubDate><dc:creator>Xiangyu Zeng</dc:creator><dc:creator>Zhiqiu Zhang</dc:creator><dc:creator>Yuhan Zhu</dc:creator><dc:creator>Xinhao Li</dc:creator><dc:creator>Zikang Wang</dc:creator><dc:creator>Changlian Ma</dc:creator><dc:creator>Qingyu Zhang</dc:creator><dc:creator>Zizheng Huang</dc:creator><dc:creator>Kun Ouyang</dc:creator><dc:creator>Tianxiang Jiang</dc:creator><dc:creator>Ziang Yan</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Hongjie Zhang</dc:creator><dc:creator>Yali Wang</dc:creator><dc:creator>Limin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.
Published: 2026-01-30T17:47:30+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Zeng; Zhiqiu Zhang; Yuhan Zhu; Xinhao Li; Zikang Wang; Changlian Ma; Qingyu Zhang; Zizheng Huang; Kun Ouyang; Tianxiang Jiang; Ziang Yan; Yi Wang; Hongjie Zhang; Yali Wang; Limin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3&amp;#x27;s strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.&lt;/p&gt;</content:encoded></item><item><title>HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation</title><link>https://arxiv.org/abs/2601.23064v1</link><guid>http://arxiv.org/abs/2601.23064v1</guid><pubDate>Fri, 30 Jan 2026 15:16:07 +0000</pubDate><dc:creator>Hari Krishna Gadi</dc:creator><dc:creator>Daniel Matos</dc:creator><dc:creator>Hongyi Luo</dc:creator><dc:creator>Lu Liu</dc:creator><dc:creator>Yongliang Wang</dc:creator><dc:creator>Yanfeng Zhang</dc:creator><dc:creator>Liqiu Meng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.
Published: 2026-01-30T15:16:07+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hari Krishna Gadi; Daniel Matos; Hongyi Luo; Lu Liu; Yongliang Wang; Yanfeng Zhang; Liqiu Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.&lt;/p&gt;</content:encoded></item><item><title>Vision-language alignment with sigmoid loss and dual-token contrastive change localizer for precise change captioning</title><link>https://doi.org/10.1016/j.neucom.2026.132920</link><guid>10.1016/j.neucom.2026.132920</guid><pubDate>Mon, 02 Feb 2026 07:14:49 +0000</pubDate><dc:creator>Ziyang Yu</dc:creator><dc:creator>Xiaodong Gu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132920</prism:doi><description>The task of change captioning focuses on generating detailed descriptions of fine-grained differences between a pair of similar images. Unlike single-image captioning, this task demands that the model not only thoroughly analyzes the visual content but also accurately identifies the regions where changes occur within the image pair. A significant challenge in this process is detecting changes amidst noise and viewpoint variations. To tackle this challenge, we propose a Dual-Token Contrastive Change Localizer, which decouples the changed and unchanged features of the image pair. Specifically, we utilize two distinct tokens to learn common features and difference features, guided by our common constraints and difference constraints, respectively. These tokens are then used to generate representations of the changed and unchanged regions, which are subsequently transformed into descriptive sentences via a transformer decoder. Additionally, we introduce a sigmoid loss to replace the traditional InfoNCE loss, enhancing the alignment between visual and textual features. Extensive experiments demonstrate that our model achieves state-of-the-art performance across various change scenarios.
Published: 2026-02-02T07:14:49+00:00
Venue: Neurocomputing
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyang Yu; Xiaodong Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132920"&gt;10.1016/j.neucom.2026.132920&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;The task of change captioning focuses on generating detailed descriptions of fine-grained differences between a pair of similar images. Unlike single-image captioning, this task demands that the model not only thoroughly analyzes the visual content but also accurately identifies the regions where changes occur within the image pair. A significant challenge in this process is detecting changes amidst noise and viewpoint variations. To tackle this challenge, we propose a Dual-Token Contrastive Change Localizer, which decouples the changed and unchanged features of the image pair. Specifically, we utilize two distinct tokens to learn common features and difference features, guided by our common constraints and difference constraints, respectively. These tokens are then used to generate representations of the changed and unchanged regions, which are subsequently transformed into descriptive sentences via a transformer decoder. Additionally, we introduce a sigmoid loss to replace the traditional InfoNCE loss, enhancing the alignment between visual and textual features. Extensive experiments demonstrate that our model achieves state-of-the-art performance across various change scenarios.&lt;/p&gt;</content:encoded></item><item><title>FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images</title><link>https://arxiv.org/abs/2601.22809v1</link><guid>http://arxiv.org/abs/2601.22809v1</guid><pubDate>Fri, 30 Jan 2026 10:37:34 +0000</pubDate><dc:creator>Haiyang Wu</dc:creator><dc:creator>Weiliang Mu</dc:creator><dc:creator>Jipeng Zhang</dc:creator><dc:creator>Zhong Dandan</dc:creator><dc:creator>Zhuofei Du</dc:creator><dc:creator>Haifeng Li</dc:creator><dc:creator>Tao Chao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.
Published: 2026-01-30T10:37:34+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Wu; Weiliang Mu; Jipeng Zhang; Zhong Dandan; Zhuofei Du; Haifeng Li; Tao Chao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Static Weights: Language-Guided Adaptive Weight Adjustment for 3D Visual Grounding</title><link>https://doi.org/10.1016/j.knosys.2026.115467</link><guid>10.1016/j.knosys.2026.115467</guid><pubDate>Sun, 01 Feb 2026 22:48:12 +0000</pubDate><dc:creator>Zongshun Wang</dc:creator><dc:creator>Ce Li</dc:creator><dc:creator>Zhiqiang Feng</dc:creator><dc:creator>Limei Xiao</dc:creator><dc:creator>Pengcheng Wang</dc:creator><dc:creator>Mengmeng Ping</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115467</prism:doi><description>3D Visual Grounding (3DVG) aims to accurately localize target objects in complex 3D point cloud scenes using natural language descriptions. However, current methods typically utilize static visual encoders with fixed parameters to handle the infinite variety of linguistic queries. This static approach inevitably leads to low signal-to-noise ratios in the feature inputs during the subsequent visual-language fusion stage. To overcome this limitation, we propose a Language-guided Adaptive Weight Adjustment (LAWA) framework that equips the visual backbone with query-aware dynamic adaptability during the early visual encoding stage via a lightweight language-guided strategy. Specifically, we first construct visual features that integrate class prior information using Object Semantic Augmented Encoding. Then, by leveraging weight coefficients derived from multimodal embeddings, we employ a Low-Rank Adaptation-based Dynamic Weight Adjustment (DWA) module to update the linear projection layers and weight matrices within the visual encoder’s attention mechanism. This approach enables the model to focus more effectively on visual regions that are semantically aligned with the textual descriptions. Extensive experiments demonstrate that LAWA achieves an Acc@0.25 of 86.2% on the ScanRefer dataset, and overall accuracies of 69.5% and 58.4% on the Sr3D and Nr3D datasets, respectively, all while maintaining superior parameter efficiency.
Published: 2026-02-01T22:48:12+00:00
Venue: Knowledge-Based Systems
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongshun Wang; Ce Li; Zhiqiang Feng; Limei Xiao; Pengcheng Wang; Mengmeng Ping&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115467"&gt;10.1016/j.knosys.2026.115467&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) aims to accurately localize target objects in complex 3D point cloud scenes using natural language descriptions. However, current methods typically utilize static visual encoders with fixed parameters to handle the infinite variety of linguistic queries. This static approach inevitably leads to low signal-to-noise ratios in the feature inputs during the subsequent visual-language fusion stage. To overcome this limitation, we propose a Language-guided Adaptive Weight Adjustment (LAWA) framework that equips the visual backbone with query-aware dynamic adaptability during the early visual encoding stage via a lightweight language-guided strategy. Specifically, we first construct visual features that integrate class prior information using Object Semantic Augmented Encoding. Then, by leveraging weight coefficients derived from multimodal embeddings, we employ a Low-Rank Adaptation-based Dynamic Weight Adjustment (DWA) module to update the linear projection layers and weight matrices within the visual encoder’s attention mechanism. This approach enables the model to focus more effectively on visual regions that are semantically aligned with the textual descriptions. Extensive experiments demonstrate that LAWA achieves an Acc@0.25 of 86.2% on the ScanRefer dataset, and overall accuracies of 69.5% and 58.4% on the Sr3D and Nr3D datasets, respectively, all while maintaining superior parameter efficiency.&lt;/p&gt;</content:encoded></item><item><title>RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding</title><link>https://arxiv.org/abs/2602.00504v1</link><guid>http://arxiv.org/abs/2602.00504v1</guid><pubDate>Sat, 31 Jan 2026 04:13:57 +0000</pubDate><dc:creator>Jiahe Wu</dc:creator><dc:creator>Bing Cao</dc:creator><dc:creator>Qilong Wang</dc:creator><dc:creator>Qinghua Hu</dc:creator><dc:creator>Dongdong Li</dc:creator><dc:creator>Pengfei Zhu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.
Published: 2026-01-31T04:13:57+00:00
Venue: arXiv
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahe Wu; Bing Cao; Qilong Wang; Qinghua Hu; Dongdong Li; Pengfei Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM&amp;#x27;s perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs&amp;#x27; RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.&lt;/p&gt;</content:encoded></item><item><title>ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning</title><link>https://arxiv.org/abs/2602.01610v1</link><guid>http://arxiv.org/abs/2602.01610v1</guid><pubDate>Mon, 02 Feb 2026 03:56:05 +0000</pubDate><dc:creator>Zitao Guo</dc:creator><dc:creator>Changyang Jiang</dc:creator><dc:creator>Tianhong Zhao</dc:creator><dc:creator>Jinzhou Cao</dc:creator><dc:creator>Genan Dai</dc:creator><dc:creator>Bowen Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.
Published: 2026-02-02T03:56:05+00:00
Venue: arXiv
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zitao Guo; Changyang Jiang; Tianhong Zhao; Jinzhou Cao; Genan Dai; Bowen Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.&lt;/p&gt;</content:encoded></item><item><title>Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance</title><link>https://arxiv.org/abs/2602.01047v1</link><guid>http://arxiv.org/abs/2602.01047v1</guid><pubDate>Sun, 01 Feb 2026 06:12:05 +0000</pubDate><dc:creator>Xinrong Chen</dc:creator><dc:creator>Xu Chu</dc:creator><dc:creator>Yingmin Qiu</dc:creator><dc:creator>Hengyuan Zhang</dc:creator><dc:creator>Jing Xiong</dc:creator><dc:creator>Shiyu Tang</dc:creator><dc:creator>Shuai Liu</dc:creator><dc:creator>Shaokang Yang</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Hayden Kwok-Hay So</dc:creator><dc:creator>Ngai Wong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.
Published: 2026-02-01T06:12:05+00:00
Venue: arXiv
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinrong Chen; Xu Chu; Yingmin Qiu; Hengyuan Zhang; Jing Xiong; Shiyu Tang; Shuai Liu; Shaokang Yang; Cheng Yang; Hayden Kwok-Hay So; Ngai Wong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.&lt;/p&gt;</content:encoded></item><item><title>Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery</title><link>https://arxiv.org/abs/2602.01836v1</link><guid>http://arxiv.org/abs/2602.01836v1</guid><pubDate>Mon, 02 Feb 2026 09:09:07 +0000</pubDate><dc:creator>Yin Wu</dc:creator><dc:creator>Daniel Slieter</dc:creator><dc:creator>Carl Esselborn</dc:creator><dc:creator>Ahmed Abouelazm</dc:creator><dc:creator>Tsung Yuan Tseng</dc:creator><dc:creator>J. Marius Zöllner</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.
Published: 2026-02-02T09:09:07+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yin Wu; Daniel Slieter; Carl Esselborn; Ahmed Abouelazm; Tsung Yuan Tseng; J. Marius Zöllner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.&lt;/p&gt;</content:encoded></item><item><title>Multimodal large language models meet self-supervised diffusion for real-world aerial image super-resolution</title><link>https://doi.org/10.1016/j.jag.2026.105136</link><guid>10.1016/j.jag.2026.105136</guid><pubDate>Mon, 02 Feb 2026 13:09:38 +0000</pubDate><dc:creator>Lijing Lu</dc:creator><dc:creator>Zhou Huang</dc:creator><dc:creator>Yi Bao</dc:creator><dc:creator>Lin Wan</dc:creator><dc:creator>Zhihang Li</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105136</prism:doi><description>Real-world aerial image super-resolution (SR) remains particularly challenging because degradations in remote-sensing imagery involve random combinations of anisotropic blur, signal-dependent noise, and unknown downsampling kernels. Most existing SR methods either rely on simplified degradation assumptions or lack semantic perception of degradation, resulting in limited generalization to real-world conditions. To address these gaps, we propose a novel diffusion-based SR framework that integrates Multi-modal Large Language Models (MLLMs) and self-supervised contrastive learning for extracting degradation-insensitive representation. Specifically, we introduce a contrastive learning strategy into a ControlNet module, where the HR and LR counterparts of the same image are regarded as positive pairs, while representations from different images serve as negative pairs, enabling the network to learn degradation-insensitive structural features. To further enhance semantic awareness of degradation, an MLLM-generated change caption is incorporated into the diffusion process as textual guidance, allowing the model to explicitly perceive and reconstruct different degradation types. Moreover, a classifier-free guidance (CFG) distillation strategy compresses the original dual-branch diffusion model into a single lightweight network, substantially improving inference efficiency while maintaining high reconstruction fidelity. Extensive experiments conducted on various datasets have showcased the superior performance of our proposed model compared to existing state-of-the-art methods. Furthermore, our distillation algorithm achieves a twofold reduction in inference time compared to its non-distilled counterpart, making it more feasible for real-time and resource-limited applications.
Published: 2026-02-02T13:09:38+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijing Lu; Zhou Huang; Yi Bao; Lin Wan; Zhihang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105136"&gt;10.1016/j.jag.2026.105136&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Real-world aerial image super-resolution (SR) remains particularly challenging because degradations in remote-sensing imagery involve random combinations of anisotropic blur, signal-dependent noise, and unknown downsampling kernels. Most existing SR methods either rely on simplified degradation assumptions or lack semantic perception of degradation, resulting in limited generalization to real-world conditions. To address these gaps, we propose a novel diffusion-based SR framework that integrates Multi-modal Large Language Models (MLLMs) and self-supervised contrastive learning for extracting degradation-insensitive representation. Specifically, we introduce a contrastive learning strategy into a ControlNet module, where the HR and LR counterparts of the same image are regarded as positive pairs, while representations from different images serve as negative pairs, enabling the network to learn degradation-insensitive structural features. To further enhance semantic awareness of degradation, an MLLM-generated change caption is incorporated into the diffusion process as textual guidance, allowing the model to explicitly perceive and reconstruct different degradation types. Moreover, a classifier-free guidance (CFG) distillation strategy compresses the original dual-branch diffusion model into a single lightweight network, substantially improving inference efficiency while maintaining high reconstruction fidelity. Extensive experiments conducted on various datasets have showcased the superior performance of our proposed model compared to existing state-of-the-art methods. Furthermore, our distillation algorithm achieves a twofold reduction in inference time compared to its non-distilled counterpart, making it more feasible for real-time and resource-limited applications.&lt;/p&gt;</content:encoded></item><item><title>ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding</title><link>https://arxiv.org/abs/2601.22666v1</link><guid>http://arxiv.org/abs/2601.22666v1</guid><pubDate>Fri, 30 Jan 2026 07:38:04 +0000</pubDate><dc:creator>Junyi Hu</dc:creator><dc:creator>Tian Bai</dc:creator><dc:creator>Fengyi Wu</dc:creator><dc:creator>Wenyan Li</dc:creator><dc:creator>Zhenming Peng</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.
Published: 2026-01-30T07:38:04+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyi Hu; Tian Bai; Fengyi Wu; Wenyan Li; Zhenming Peng; Yi Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.&lt;/p&gt;</content:encoded></item><item><title>ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model</title><link>https://arxiv.org/abs/2601.22730v1</link><guid>http://arxiv.org/abs/2601.22730v1</guid><pubDate>Fri, 30 Jan 2026 09:06:45 +0000</pubDate><dc:creator>Xiaoshu Chen</dc:creator><dc:creator>Sihang Zhou</dc:creator><dc:creator>Ke Liang</dc:creator><dc:creator>Taichun Zhou</dc:creator><dc:creator>Xinwang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.
Published: 2026-01-30T09:06:45+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoshu Chen; Sihang Zhou; Ke Liang; Taichun Zhou; Xinwang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.&lt;/p&gt;</content:encoded></item><item><title>Learning Sparse Visual Representations via Spatial-Semantic Factorization</title><link>https://arxiv.org/abs/2602.01905v1</link><guid>http://arxiv.org/abs/2602.01905v1</guid><pubDate>Mon, 02 Feb 2026 10:12:17 +0000</pubDate><dc:creator>Theodore Zhengde Zhao</dc:creator><dc:creator>Sid Kiblawi</dc:creator><dc:creator>Jianwei Yang</dc:creator><dc:creator>Naoto Usuyama</dc:creator><dc:creator>Reuben Tan</dc:creator><dc:creator>Noel C Codella</dc:creator><dc:creator>Tristan Naumann</dc:creator><dc:creator>Hoifung Poon</dc:creator><dc:creator>Mu Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.
Published: 2026-02-02T10:12:17+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Theodore Zhengde Zhao; Sid Kiblawi; Jianwei Yang; Naoto Usuyama; Reuben Tan; Noel C Codella; Tristan Naumann; Hoifung Poon; Mu Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.&lt;/p&gt;</content:encoded></item><item><title>Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd</title><link>https://arxiv.org/abs/2602.01561v1</link><guid>http://arxiv.org/abs/2602.01561v1</guid><pubDate>Mon, 02 Feb 2026 02:54:34 +0000</pubDate><dc:creator>Yejin Son</dc:creator><dc:creator>Saejin Kim</dc:creator><dc:creator>Dongjun Min</dc:creator><dc:creator>Younjae Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.
Published: 2026-02-02T02:54:34+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yejin Son; Saejin Kim; Dongjun Min; Younjae Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models&amp;#x27; ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models&amp;#x27; robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.&lt;/p&gt;</content:encoded></item><item><title>VVLoc: Prior-free 3-DoF Vehicle Visual Localization</title><link>https://arxiv.org/abs/2602.00810v1</link><guid>http://arxiv.org/abs/2602.00810v1</guid><pubDate>Sat, 31 Jan 2026 16:37:30 +0000</pubDate><dc:creator>Ze Huang</dc:creator><dc:creator>Zhongyang Xiao</dc:creator><dc:creator>Mingliang Song</dc:creator><dc:creator>Longan Yang</dc:creator><dc:creator>Hongyuan Yuan</dc:creator><dc:creator>Li Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.
Published: 2026-01-31T16:37:30+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Huang; Zhongyang Xiao; Mingliang Song; Longan Yang; Hongyuan Yuan; Li Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.&lt;/p&gt;</content:encoded></item><item><title>How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing</title><link>https://arxiv.org/abs/2602.01851v1</link><guid>http://arxiv.org/abs/2602.01851v1</guid><pubDate>Mon, 02 Feb 2026 09:24:45 +0000</pubDate><dc:creator>Huanyu Zhang</dc:creator><dc:creator>Xuehai Bai</dc:creator><dc:creator>Chengzu Li</dc:creator><dc:creator>Chen Liang</dc:creator><dc:creator>Haochen Tian</dc:creator><dc:creator>Haodong Li</dc:creator><dc:creator>Ruichuan An</dc:creator><dc:creator>Yifan Zhang</dc:creator><dc:creator>Anna Korhonen</dc:creator><dc:creator>Zhang Zhang</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Tieniu Tan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.
Published: 2026-02-02T09:24:45+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huanyu Zhang; Xuehai Bai; Chengzu Li; Chen Liang; Haochen Tian; Haodong Li; Ruichuan An; Yifan Zhang; Anna Korhonen; Zhang Zhang; Liang Wang; Tieniu Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.&lt;/p&gt;</content:encoded></item><item><title>Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure</title><link>https://arxiv.org/abs/2602.00414v1</link><guid>http://arxiv.org/abs/2602.00414v1</guid><pubDate>Sat, 31 Jan 2026 00:08:41 +0000</pubDate><dc:creator>Trishna Chakraborty</dc:creator><dc:creator>Udita Ghosh</dc:creator><dc:creator>Aldair Ernesto Gongora</dc:creator><dc:creator>Ruben Glatt</dc:creator><dc:creator>Yue Dong</dc:creator><dc:creator>Jiachen Li</dc:creator><dc:creator>Amit K. Roy-Chowdhury</dc:creator><dc:creator>Chengyu Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.
Published: 2026-01-31T00:08:41+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Trishna Chakraborty; Udita Ghosh; Aldair Ernesto Gongora; Ruben Glatt; Yue Dong; Jiachen Li; Amit K. Roy-Chowdhury; Chengyu Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.&lt;/p&gt;</content:encoded></item><item><title>Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering</title><link>https://arxiv.org/abs/2602.00621v1</link><guid>http://arxiv.org/abs/2602.00621v1</guid><pubDate>Sat, 31 Jan 2026 09:21:04 +0000</pubDate><dc:creator>Guangtao Lyu</dc:creator><dc:creator>Xinyi Cheng</dc:creator><dc:creator>Qi Liu</dc:creator><dc:creator>Chenghao Xu</dc:creator><dc:creator>Jiexi Yan</dc:creator><dc:creator>Muli Yang</dc:creator><dc:creator>Fen Fang</dc:creator><dc:creator>Cheng Deng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.
Published: 2026-01-31T09:21:04+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangtao Lyu; Xinyi Cheng; Qi Liu; Chenghao Xu; Jiexi Yan; Muli Yang; Fen Fang; Cheng Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.&lt;/p&gt;</content:encoded></item><item><title>SHED Light on Segmentation for Dense Prediction</title><link>https://arxiv.org/abs/2601.22529v1</link><guid>http://arxiv.org/abs/2601.22529v1</guid><pubDate>Fri, 30 Jan 2026 04:02:29 +0000</pubDate><dc:creator>Seung Hyun Lee</dc:creator><dc:creator>Sangwoo Mo</dc:creator><dc:creator>Stella X. Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.
Published: 2026-01-30T04:02:29+00:00
Venue: arXiv
Score: 0.513 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seung Hyun Lee; Sangwoo Mo; Stella X. Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.513 (consider)&lt;/p&gt;
&lt;p&gt;Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition</title><link>https://arxiv.org/abs/2602.00360v1</link><guid>http://arxiv.org/abs/2602.00360v1</guid><pubDate>Fri, 30 Jan 2026 22:17:29 +0000</pubDate><dc:creator>Sumana Biswas</dc:creator><dc:creator>Karen Young</dc:creator><dc:creator>Josephine Griffith</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.
Published: 2026-01-30T22:17:29+00:00
Venue: arXiv
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sumana Biswas; Karen Young; Josephine Griffith&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis&amp;#x27; (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.&lt;/p&gt;</content:encoded></item><item><title>What can Computer Vision learn from Ranganathan?</title><link>https://arxiv.org/abs/2601.22634v1</link><guid>http://arxiv.org/abs/2601.22634v1</guid><pubDate>Fri, 30 Jan 2026 06:51:13 +0000</pubDate><dc:creator>Mayukh Bagchi</dc:creator><dc:creator>Fausto Giunchiglia</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.
Published: 2026-01-30T06:51:13+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mayukh Bagchi; Fausto Giunchiglia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.&lt;/p&gt;</content:encoded></item><item><title>DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification</title><link>https://arxiv.org/abs/2602.01059v1</link><guid>http://arxiv.org/abs/2602.01059v1</guid><pubDate>Sun, 01 Feb 2026 06:59:53 +0000</pubDate><dc:creator>Ying Shu</dc:creator><dc:creator>Pujian Zhan</dc:creator><dc:creator>Huiqi Yang</dc:creator><dc:creator>Hehe Fan</dc:creator><dc:creator>Youfang Lin</dc:creator><dc:creator>Kai Lv</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.
Published: 2026-02-01T06:59:53+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ying Shu; Pujian Zhan; Huiqi Yang; Hehe Fan; Youfang Lin; Kai Lv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification</title><link>https://arxiv.org/abs/2602.00292v1</link><guid>http://arxiv.org/abs/2602.00292v1</guid><pubDate>Fri, 30 Jan 2026 20:28:01 +0000</pubDate><dc:creator>Rory Driscoll</dc:creator><dc:creator>Alexandros Christoforos</dc:creator><dc:creator>Chadbourne Davis</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.
Published: 2026-01-30T20:28:01+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rory Driscoll; Alexandros Christoforos; Chadbourne Davis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.&lt;/p&gt;</content:encoded></item><item><title>Modeling Image-Caption Rating from Comparative Judgments</title><link>https://arxiv.org/abs/2602.00381v1</link><guid>http://arxiv.org/abs/2602.00381v1</guid><pubDate>Fri, 30 Jan 2026 23:00:07 +0000</pubDate><dc:creator>Kezia Minni</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Monoshiz Mahbub Khan</dc:creator><dc:creator>Zhe Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.
Published: 2026-01-30T23:00:07+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kezia Minni; Qiang Zhang; Monoshiz Mahbub Khan; Zhe Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson&amp;#x27;s $ρ$: 0.7609 and Spearman&amp;#x27;s $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.&lt;/p&gt;</content:encoded></item></channel></rss>