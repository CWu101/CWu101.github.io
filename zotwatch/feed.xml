<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 08 Jan 2026 03:23:53 +0000</lastBuildDate><item><title>SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition</title><link>https://doi.org/10.1109/tcsvt.2026.3651681</link><guid>10.1109/tcsvt.2026.3651681</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Zhi Hu</dc:creator><dc:creator>Liang Liao</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651681</prism:doi><description>Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.595 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhi Hu; Liang Liao; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651681"&gt;10.1109/tcsvt.2026.3651681&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.595 (consider)&lt;/p&gt;
&lt;p&gt;Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.&lt;/p&gt;</content:encoded></item><item><title>Visual Context and Commonsense-Guided Causal Chain-of-Thoughts for Visual Commonsense Reasoning</title><link>https://doi.org/10.1109/tmm.2026.3651070</link><guid>10.1109/tmm.2026.3651070</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Xinyu Li</dc:creator><dc:creator>Jing Zhao</dc:creator><dc:creator>Tongquan Wei</dc:creator><dc:creator>Shiliang Sun</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651070</prism:doi><description>Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.593 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Li; Jing Zhao; Tongquan Wei; Shiliang Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651070"&gt;10.1109/tmm.2026.3651070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.593 (consider)&lt;/p&gt;
&lt;p&gt;Humans are capable of inferring dynamic context from a still image and, with the provision of additional commonsense knowledge, can accurately complete visual commonsense reasoning tasks. Nevertheless, this remains a highly challenging cognitive-level task for current vision-language models. Previous work has primarily focused on utilizing models fine-tuned for specific downstream tasks and introduces external world knowledge to tackle these challenging tasks, while neglecting the importance of accurate context and the key role of commonsense knowledge in reasoning. In this paper, we propose a novel framework to enhance visual commonsense reasoning by incorporating context and commonsense knowledge. We decompose the visual commonsense reasoning problem into four distinct but interrelated sub-problems and combine visual language models with a large language model to enable zero-shot reasoning. The uniqueness of this work lies in the proposed commonsense knowledge filtering module, which filters out relevant commonsense knowledge through the causal strength of visual context. This process constructs Visual Context and Commonsense-guided Causal Chain-of-Thought ( V C 3 \mathrm{VC^{3}} -CoT) reasoning paths, thereby providing double robustness to visual commonsense reasoning by incorporating weighted majority voting strategy. Extensive experiments on several downstream tasks demonstrate that the proposed method significantly improves performance compared to baseline models and the state-of-the-art method, and confirm the effectiveness of the proposed components.&lt;/p&gt;</content:encoded></item><item><title>GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis</title><link>https://doi.org/10.1016/j.jag.2025.105038</link><guid>10.1016/j.jag.2025.105038</guid><pubDate>Tue, 06 Jan 2026 05:31:06 +0000</pubDate><dc:creator>Kai Deng</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Yibing Xiong</dc:creator><dc:creator>Aokun Liang</dc:creator><dc:creator>Jiong Xu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105038</prism:doi><description>Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .
Published: 2026-01-06T05:31:06+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Deng; Xiangyun Hu; Yibing Xiong; Aokun Liang; Jiong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105038"&gt;10.1016/j.jag.2025.105038&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Semantic image synthesis (SIS) is essential for remote sensing, particularly in generating high-quality training data for scarce annotated datasets. While existing SIS methods have advanced pixel-wise mappings between semantic maps and images, they often overlook spatial priors, such as relationships between geographic objects (e.g., road-building adjacency), leading to structural inconsistencies in synthesized images. To address this, we propose the graph-prior diffusion transformer (GDiT) for semantically controllable remote sensing image synthesis. We first convert semantic maps into semantic graphs, encoding geographic objects as nodes with structured spatial interactions. To capture spatial and semantic relationships, we propose the Geometric-Semantic Aware Module (GSAM), which integrates CLIP-extracted semantics and geometric attributes for a more context-aware representation. Furthermore, we design the Graph Diffusion Transformer (GDiT) Block, which employs graph-to-image cross-attention to refine spatial structures, ensuring topological coherence and semantic fidelity in synthesized images. Experiments on the landcover and landuse dataset show that GDiT achieves competitive performance by incorporating text prompts to enable multilevel control across global, object and pixel dimensions, generating high-fidelity images while using only 38.9% of the parameters compared to GeoSynth, significantly improving efficiency and accuracy. The code and dataset will be released at https://github.com/whudk/GDiT .&lt;/p&gt;</content:encoded></item><item><title>SPENav: Dynamic Object Filtering with Spatial Perception Enhancement for Vision-Language Navigation</title><link>https://doi.org/10.1109/tcsvt.2026.3651320</link><guid>10.1109/tcsvt.2026.3651320</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Shuai Yuan</dc:creator><dc:creator>Huaxiang Zhang</dc:creator><dc:creator>Li Liu</dc:creator><dc:creator>Lei Zhu</dc:creator><dc:creator>Xinfeng Dong</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651320</prism:doi><description>The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.572 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yuan; Huaxiang Zhang; Li Liu; Lei Zhu; Xinfeng Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651320"&gt;10.1109/tcsvt.2026.3651320&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.572 (consider)&lt;/p&gt;
&lt;p&gt;The Vision-language navigation task requires agents to efficiently interpret visual cues in the environment and accurately follow long-range instructions, posing significant challenges to their scene memory and spatial reasoning capabilities. Existing methods typically construct memory systems directly from raw visual observations. However, task-irrelevant cues commonly present in the environment can continuously introduce localization errors during navigation, severely limiting the agent’s performance in complex scenes. Meanwhile, due to the lack of transferable general knowledge priors, existing agents exhibit notable limitations in spatial perception, which undermines the reliability of their decision-making in unseen environments. To address these issues, this paper proposes the dynamic object filtering with Spatial Perception Enhancement for Vision-Language Navigation (SPENav), which aggregates open-vocabulary perception with multi-level information modeling. At the local level, the Hierarchical Semantic Prior Extractor and Room-Information-Guided Filtering construct task-oriented semantic priors to capture critical objects and suppress irrelevant features. At the global level, the Spatial-Instructional Guided Dual Attention module leverages spatial information and instruction guidance to enable the agent to develop selective memory that is goal- and task-oriented. On the unseen test split of R2R, SPENav achieves a 76% Success Rate (SR) and a 65% Success weighted by Path Length (SPL). These results demonstrate the effectiveness of task-oriented feature selection and multi-level semantic modeling in enhancing cross-modal understanding and adaptive navigation performance.&lt;/p&gt;</content:encoded></item><item><title>3D Semantic Gaussian via Geometric-Semantic Hypergraph Computation</title><link>https://doi.org/10.1109/tmm.2026.3651112</link><guid>10.1109/tmm.2026.3651112</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Xinran Wang</dc:creator><dc:creator>Zhiqiang Tian</dc:creator><dc:creator>Dejian Guo</dc:creator><dc:creator>Siqi Li</dc:creator><dc:creator>Shaoyi Du</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Yue Gao</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651112</prism:doi><description>Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.572 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinran Wang; Zhiqiang Tian; Dejian Guo; Siqi Li; Shaoyi Du; Xiangmin Han; Yue Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651112"&gt;10.1109/tmm.2026.3651112&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.572 (consider)&lt;/p&gt;
&lt;p&gt;Semantic labels are inherently tied to geometry and luminance reconstruction, as entities with similar shapes and appearances often share categories. Traditional methods use synthesis-analysis, NeRF, or 3D Gaussian representations to encode semantics and geometry separately. However, 2D methods lack view consistency, NeRF extensions are slow, and faster 3D Gaussian methods risk spatial and channel inconsistencies between semantic and RGB. Moreover, these methods require costly manual dense semantic labels. To alleviate resource demands and achieve effective semantic reconstruction with sparse inputs while enhancing RGB rendering quality, we build upon 3D Gaussian by integrating semantic features from pre-trained models-requiring no additional ground truth input-into Gaussian features, and construct a hypergraph neural network to capture higher-order correlations across RGB and semantic information as well as between different frames. Hypergraphs use hyperedges to link multiple vertices, capturing complex relationships essential for cross-modal tasks. This higher-order structure addresses the limitations of NeRF and Gaussian methods, which lack the capacity for such advanced associations. This framework enables precise novel view synthesis and 2D semantic reconstruction without manual annotations, achieving state-of-the-art results for RGB and semantic tasks on room-scale scenes in the ScanNet and Replica datasets, while supporting real-time rendering speeds of 34 FPS.&lt;/p&gt;</content:encoded></item><item><title>SURFNet: A Surface-aware UAV-Satellite Geolocation Framework via Feature Aggregation and Dual Positional Encoding</title><link>https://doi.org/10.1109/tgrs.2026.3651449</link><guid>10.1109/tgrs.2026.3651449</guid><pubDate>Tue, 06 Jan 2026 18:35:38 +0000</pubDate><dc:creator>Kun Liu</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651449</prism:doi><description>Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.
Published: 2026-01-06T18:35:38+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Liu; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651449"&gt;10.1109/tgrs.2026.3651449&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization (CVGL) between UAV and satellite imagery is crucial for GNSS-denied navigation but brittle under domain shift due to background shortcuts and layout misalignment. We present SURFNet, a surface-aware two-stage framework that boosts accuracy and cross-dataset generalization via explicit ground-structure modeling. A Satellite Semantic Augmentation Algorithm (SSAA) preserves salient surface elements and synthesizes structure-preserving pseudo-backgrounds to diversify satellite training data. A Dual Positional Encoding (DPE) combines learnable 2D embeddings with rotation-aware encodings, while a Ground-Aware Aggregation Block (GAB) performs dynamic multi-branch integration of local–global cues.On University-1652 and SUES-200, SURFNet delivers competitive in-dataset results and achieves state-of-the-art zero-shot generalization from University-1652→SUES-200 across altitudes. Additional transfers underline deployability: University-1652→DenseUAV exposes a large style/coverage gap that is substantially closed by lightweight fine-tuning (freezing the backbone, adapting small heads on a 50-class subset), while SUES-200→University-1652 shows expected asymmetry tied to the smaller source diversity of SUES-200. Subset studies on SUES-200 (dense/sparse buildings, water scenes, natural landscapes) further indicate that SURFNet exploits general surface saliency—not only the four SSAA categories—to generalize across scene types. Overall, surface-aware augmentation plus structure-guided aggregation yields features that are both accurate and adaptable for real-world CVGL.&lt;/p&gt;</content:encoded></item><item><title>Causality-Inspired Graph Neural Networks for Cross-Modal Retrieval</title><link>https://doi.org/10.1109/tmm.2026.3651028</link><guid>10.1109/tmm.2026.3651028</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Bo Li</dc:creator><dc:creator>Zhixin Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651028</prism:doi><description>Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Li; Zhixin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651028"&gt;10.1109/tmm.2026.3651028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Multimedia data have rich semantic knowledge, and cross-modal retrieval (CMR) methods are able to explore their correlations. Graph neural networks (GNN) can represent complex connection information, so some CMR methods apply GNNs as semantic comprehender to improve matching accuracy. However, fine-grained classifiers can accurately obtain object-centric semantics, but these semantics may be conflicting, potentially leading to inexplicability responses that are difficult to ground, for example. Meanwhile, it may be concerned that the credibility of GNN, mainly includes sensitivity to out-of-distribution changes and lack of interpretability. Therefore, we attempt to integrate causal learning into GNNs and capture potential causal relationships rather than surface object-centric classification. Firstly, we analyze semantic causality and build cross-modal structure causal model, then achieve cross-modal interventional-causal learning by causality-inspired graph neural network (CIGNN). Secondly, we propose modality contrastive learning to characterize the intra-modal and inter-modal correlations, and project into the common representation space. Thirdly, a new soft rank loss method is designed beyond binary similarity to achieve fine-grained similarity sorting. Comprehensive experiments on three widely used benchmark datasets prove the superiority of our proposed method, while ablation experiments demonstrated the effectiveness of each component.&lt;/p&gt;</content:encoded></item><item><title>CAMformer: A Single-Stage CNN-Transformer Hybrid for Weakly Supervised Semantic Segmentation in Aerial Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3651514</link><guid>10.1109/tgrs.2026.3651514</guid><pubDate>Tue, 06 Jan 2026 18:35:38 +0000</pubDate><dc:creator>Ruixue Zhou</dc:creator><dc:creator>Jihao Li</dc:creator><dc:creator>Wenkai Zhang</dc:creator><dc:creator>Shuoke Li</dc:creator><dc:creator>Jialiang Chen</dc:creator><dc:creator>Chongyang Li</dc:creator><dc:creator>Boyuan Tong</dc:creator><dc:creator>Weihang Zhang</dc:creator><dc:creator>Xian Sun</dc:creator><dc:creator>Kun Fu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651514</prism:doi><description>Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.
Published: 2026-01-06T18:35:38+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruixue Zhou; Jihao Li; Wenkai Zhang; Shuoke Li; Jialiang Chen; Chongyang Li; Boyuan Tong; Weihang Zhang; Xian Sun; Kun Fu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651514"&gt;10.1109/tgrs.2026.3651514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised semantic segmentation (WSSS) with image-level labels has emerged as a cost-effective solution for large-scale remote sensing (RS) imagery, yet CNN-based approaches often suffer from limited receptive fields, incomplete activations, and background confusion in complex multi-class scenes. Transformer-based methods alleviate these issues by modeling long-range dependencies, but many existing designs are multi-stage and inefficient, or fail to adequately address noise and contextual bias in RS data. This work proposes CAMformer, a single-stage hybrid CNN–Transformer framework that jointly captures local details and global semantics for more reliable pseudo-label generation. To further improve robustness, we introduce an inter-class decoupling strategy (ICD) to resolve category conflicts, an uncertainty-weighted optimization (UWO) scheme to suppress noisy activations, and a context intervention sample enhancement (CISE) module to mitigate biased contextual dependencies. Extensive experiments on the challenging iSAID and ISPRS Vaihingen datasets demonstrate that CAMformer achieves state-of-the-art performance with 39.5% and 46.4% mIoU respectively, surpassing recent one-stage methods, while cross-domain validation on PASCAL VOC2012 confirms its generalization to a certain extent. These results highlight the effectiveness of combining CNN–Transformer hybrids with context-aware learning for WSSS in RS applications.&lt;/p&gt;</content:encoded></item><item><title>Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation</title><link>https://doi.org/10.1109/tmm.2026.3651094</link><guid>10.1109/tmm.2026.3651094</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Hojun Song</dc:creator><dc:creator>Chae-yeong Song</dc:creator><dc:creator>Dong-hun Lee</dc:creator><dc:creator>Heejung Choi</dc:creator><dc:creator>Jinwoo Jeong</dc:creator><dc:creator>Sungjei Kim</dc:creator><dc:creator>Sang-hyo Park</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651094</prism:doi><description>3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hojun Song; Chae-yeong Song; Dong-hun Lee; Heejung Choi; Jinwoo Jeong; Sungjei Kim; Sang-hyo Park&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651094"&gt;10.1109/tmm.2026.3651094&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;3D scene graph generation (3DSGG), which involves classifying objects and predicates, is an emerging topic in 3D scene understanding. Recent studies leveraging graph neural networks (GNNs) have introduced sophisticated architectures that enhance classification performance. However, since GNNs serve as the core and constitute the majority of parameters in 3DSGG models, their computational demands substantially increase overall complexity, which makes it difficult to determine the optimal model capacity. In this paper, we propose the first compression framework for lightweight 3DSGG models, based on pruning-as-search and knowledge distillation. This framework integrates multiple strategies and modules. In phase 1, the framework identifies the optimal compression ratio through pruning-as-search. In phase 2, to mitigate the accuracy loss incurred during compression, we employ structured pruning and a novel knowledge distillation strategy that effectively transfers precise information from the teacher to the compressed model. Experimental results show that our approach reduces model size by more than half while improving classification accuracy. Code is available at https://github.com/hojunking/3DSGG-compression.&lt;/p&gt;</content:encoded></item><item><title>A radiometrically and spatially consistent super-resolution framework for Sentinel-2</title><link>https://doi.org/10.1016/j.rse.2025.115222</link><guid>10.1016/j.rse.2025.115222</guid><pubDate>Tue, 06 Jan 2026 09:45:48 +0000</pubDate><dc:creator>Cesar Aybar</dc:creator><dc:creator>Julio Contreras</dc:creator><dc:creator>Simon Donike</dc:creator><dc:creator>Enrique Portalés-Julià</dc:creator><dc:creator>Gonzalo Mateo-García</dc:creator><dc:creator>Luis Gómez-Chova</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115222</prism:doi><description>Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .
Published: 2026-01-06T09:45:48+00:00
Venue: Remote Sensing of Environment
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cesar Aybar; Julio Contreras; Simon Donike; Enrique Portalés-Julià; Gonzalo Mateo-García; Luis Gómez-Chova&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115222"&gt;10.1016/j.rse.2025.115222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .&lt;/p&gt;</content:encoded></item><item><title>EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</title><link>https://doi.org/10.1109/tcsvt.2026.3651537</link><guid>10.1109/tcsvt.2026.3651537</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Zhaoyang Wang</dc:creator><dc:creator>Wen Lu</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Lihuo He</dc:creator><dc:creator>Maoguo Gong</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651537</prism:doi><description>Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoyang Wang; Wen Lu; Jie Li; Lihuo He; Maoguo Gong; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651537"&gt;10.1109/tcsvt.2026.3651537&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Modeling visual perception in a manner consistent with human subjective evaluation has become a central direction in both video quality assessment (VQA) and broader visual understanding tasks. While free-energy-guided self-repair mechanisms—reflecting human observational experience—have proven effective in image quality assessment, extending them to VQA remains non-trivial. In addition, biologically inspired paradigms such as holistic perception, local analysis, and gaze-driven scanning have achieved notable success in high-level vision tasks, yet their potential within the VQA context remains largely underexplored. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs—resized full-frame images and patch-based fragments—to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design. Our code will be publicly available at https://github.com/handsomewzy/EyeSim-VQA.&lt;/p&gt;</content:encoded></item><item><title>UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving</title><link>https://doi.org/10.1109/tcsvt.2026.3651369</link><guid>10.1109/tcsvt.2026.3651369</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Hao Zhou</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Honggang Qi</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651369</prism:doi><description>Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhou; Yi Zhang; Honggang Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651369"&gt;10.1109/tcsvt.2026.3651369&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>Mamba-driven Diffusion Model for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tcsvt.2026.3651594</link><guid>10.1109/tcsvt.2026.3651594</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Jinsheng Yang</dc:creator><dc:creator>Bineng Zhong</dc:creator><dc:creator>Qihua Liang</dc:creator><dc:creator>Yufei Tan</dc:creator><dc:creator>Haiying Xia</dc:creator><dc:creator>Shuxiang Song</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651594</prism:doi><description>Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinsheng Yang; Bineng Zhong; Qihua Liang; Yufei Tan; Haiying Xia; Shuxiang Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651594"&gt;10.1109/tcsvt.2026.3651594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.&lt;/p&gt;</content:encoded></item><item><title>Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation</title><link>https://doi.org/10.1109/tmm.2026.3651068</link><guid>10.1109/tmm.2026.3651068</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Huadong Tang</dc:creator><dc:creator>Youpeng Zhao</dc:creator><dc:creator>Min Xu</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Qiang Wu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651068</prism:doi><description>Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes. Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases). However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images. At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the model's effectiveness in identifying and segmenting minority class regions. In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information. Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling. Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network. Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huadong Tang; Youpeng Zhao; Min Xu; Jun Wang; Qiang Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651068"&gt;10.1109/tmm.2026.3651068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes. Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases). However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images. At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the model&amp;#x27;s effectiveness in identifying and segmenting minority class regions. In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information. Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling. Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network. Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context.&lt;/p&gt;</content:encoded></item><item><title>Beyond synthetic scenarios: Weakly-supervised super-resolution for spatiotemporally misaligned remote sensing images</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.019</link><guid>10.1016/j.isprsjprs.2025.12.019</guid><pubDate>Tue, 06 Jan 2026 09:49:35 +0000</pubDate><dc:creator>Quanyi Guo</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Yangtian Fang</dc:creator><dc:creator>Yi Gao</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Xin Tian</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.019</prism:doi><description>Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.
Published: 2026-01-06T09:49:35+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Quanyi Guo; Rui Liu; Yangtian Fang; Yi Gao; Jun Chen; Xin Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019"&gt;10.1016/j.isprsjprs.2025.12.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Funny-Valen-Tine: Planning Solution Distribution Enhances Machine Abstract Reasoning Ability</title><link>https://doi.org/10.1109/tnnls.2025.3647282</link><guid>10.1109/tnnls.2025.3647282</guid><pubDate>Tue, 06 Jan 2026 18:36:24 +0000</pubDate><dc:creator>Ruizhuo Song</dc:creator><dc:creator>Beiming Yuan</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3647282</prism:doi><description>The importance of visual abstract reasoning problems in the field of image processing cannot be overstated. Both Bongard-Logo problems and Raven’s progressive matrices (RPM) belong to the domain of visual abstract reasoning tasks, with Bongard-Logo categorized as image clustering reasoning and RPM involving image progression pattern reasoning. This article introduces a novel baseline model, visual abstraction learning network (Valen), which falls under the umbrella of probability-highlighting models. Valen demonstrates remarkable performance in solving both RPM and Bongard-Logo problems, offering a versatile solution for these reasoning tasks. Our investigation extends beyond the application of Valen, delving into the underlying mechanisms of probability-highlighting solvers. In revisiting how these solvers handle RPM and Bongard-Logo tasks, we realize that they approximate the solution to each reasoning problem as a distribution in which primary samples are compliant while auxiliary samples are not. This prompts us to propose that the learning objective of probability-highlighting solvers is not the distribution of correct solutions but rather one jointly delineated by primary and auxiliary samples. To bridge the discrepancies, we introduced the Tine method, an adversarial learning-based approach that helps Valen estimate a distribution close to that of the correct solutions. However, adversarial training in Tine suffers from instability. Motivated by this limitation, we model the sample distribution of reasoning problems as a mixture of Gaussian distributions, enabling Valen to capture the correct solution distribution more efficiently. This nonadversarial methodology leads to the development of the framework utilizing neural networks for yielding (Funny) method. Building on a similar Gaussian-mixture paradigm, we further propose the supervised representation distribution planning method (SBR) method to plan the distribution of progressive pattern representations....
Published: 2026-01-06T18:36:24+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruizhuo Song; Beiming Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3647282"&gt;10.1109/tnnls.2025.3647282&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;The importance of visual abstract reasoning problems in the field of image processing cannot be overstated. Both Bongard-Logo problems and Raven’s progressive matrices (RPM) belong to the domain of visual abstract reasoning tasks, with Bongard-Logo categorized as image clustering reasoning and RPM involving image progression pattern reasoning. This article introduces a novel baseline model, visual abstraction learning network (Valen), which falls under the umbrella of probability-highlighting models. Valen demonstrates remarkable performance in solving both RPM and Bongard-Logo problems, offering a versatile solution for these reasoning tasks. Our investigation extends beyond the application of Valen, delving into the underlying mechanisms of probability-highlighting solvers. In revisiting how these solvers handle RPM and Bongard-Logo tasks, we realize that they approximate the solution to each reasoning problem as a distribution in which primary samples are compliant while auxiliary samples are not. This prompts us to propose that the learning objective of probability-highlighting solvers is not the distribution of correct solutions but rather one jointly delineated by primary and auxiliary samples. To bridge the discrepancies, we introduced the Tine method, an adversarial learning-based approach that helps Valen estimate a distribution close to that of the correct solutions. However, adversarial training in Tine suffers from instability. Motivated by this limitation, we model the sample distribution of reasoning problems as a mixture of Gaussian distributions, enabling Valen to capture the correct solution distribution more efficiently. This nonadversarial methodology leads to the development of the framework utilizing neural networks for yielding (Funny) method. Building on a similar Gaussian-mixture paradigm, we further propose the supervised representation distribution planning method (SBR) method to plan the distribution of progressive pattern representations....&lt;/p&gt;</content:encoded></item><item><title>Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering</title><link>https://doi.org/10.1109/tmm.2026.3651122</link><guid>10.1109/tmm.2026.3651122</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Zhifei Li</dc:creator><dc:creator>Feng Qiu</dc:creator><dc:creator>Yiran Wang</dc:creator><dc:creator>Yujing Xia</dc:creator><dc:creator>Kui Xiao</dc:creator><dc:creator>Miao Zhang</dc:creator><dc:creator>Yan Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651122</prism:doi><description>Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhifei Li; Feng Qiu; Yiran Wang; Yujing Xia; Kui Xiao; Miao Zhang; Yan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651122"&gt;10.1109/tmm.2026.3651122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.&lt;/p&gt;</content:encoded></item><item><title>Mitigating Hallucinations in Large Vision-Language Models via Visual-Enhanced Contrastive Decoding</title><link>https://doi.org/10.1109/tmm.2026.3651099</link><guid>10.1109/tmm.2026.3651099</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Pengpeng Qiang</dc:creator><dc:creator>Hongye Tan</dc:creator><dc:creator>Hu Zhang</dc:creator><dc:creator>Xiaoli Li</dc:creator><dc:creator>Ru Li</dc:creator><dc:creator>Jiye Liang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651099</prism:doi><description>Despite significant advancements in large visual-language models (LVLMs), hallucinations remain a major bottleneck in their practical applications. One key factor contributing to hallucinations is the over-reliance on language priors during the autoregressive text generation process. Visual Contrastive Decoding (VCD), a popular technique for mitigating hallucinations, perturbs the visual input and compares the perturbed output with the original. However, it often overlooks the gradual attenuation of visual information within the decoder, limiting the model's ability to generate text based on actual visual content. We propose a novel, training-free method—Visual-Enhanced Contrastive Decoding (VECD)—which addresses this issue by amplifying visual information within the decoder, thereby reducing hallucinations caused by excessive reliance on language priors. VECD dynamically selects later layers for visual injection, while retaining only essential visual tokens in early layers. This approach enhances the generation process by adaptively balancing visual and language priors. By comparing outputs with and without visual amplification, we derive a refined probability distribution for the next token. Moreover, we improve the beam search algorithm by introducing a visually guided token selection strategy, enabling the generation of text that aligns more closely with the image content. Our extensive experiments show that VECD significantly reduces hallucinations and improves the quality of generated text, demonstrating its effectiveness as a practical solution.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengpeng Qiang; Hongye Tan; Hu Zhang; Xiaoli Li; Ru Li; Jiye Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651099"&gt;10.1109/tmm.2026.3651099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Despite significant advancements in large visual-language models (LVLMs), hallucinations remain a major bottleneck in their practical applications. One key factor contributing to hallucinations is the over-reliance on language priors during the autoregressive text generation process. Visual Contrastive Decoding (VCD), a popular technique for mitigating hallucinations, perturbs the visual input and compares the perturbed output with the original. However, it often overlooks the gradual attenuation of visual information within the decoder, limiting the model&amp;#x27;s ability to generate text based on actual visual content. We propose a novel, training-free method—Visual-Enhanced Contrastive Decoding (VECD)—which addresses this issue by amplifying visual information within the decoder, thereby reducing hallucinations caused by excessive reliance on language priors. VECD dynamically selects later layers for visual injection, while retaining only essential visual tokens in early layers. This approach enhances the generation process by adaptively balancing visual and language priors. By comparing outputs with and without visual amplification, we derive a refined probability distribution for the next token. Moreover, we improve the beam search algorithm by introducing a visually guided token selection strategy, enabling the generation of text that aligns more closely with the image content. Our extensive experiments show that VECD significantly reduces hallucinations and improves the quality of generated text, demonstrating its effectiveness as a practical solution.&lt;/p&gt;</content:encoded></item><item><title>LSFMamba: Local-enhanced Spiral Fusion Mamba for Multi-modal Land Cover Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3651397</link><guid>10.1109/tcsvt.2026.3651397</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Honghao Chang</dc:creator><dc:creator>Haixia Bi</dc:creator><dc:creator>Chen Xu</dc:creator><dc:creator>Fan Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651397</prism:doi><description>Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Honghao Chang; Haixia Bi; Chen Xu; Fan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651397"&gt;10.1109/tcsvt.2026.3651397&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...&lt;/p&gt;</content:encoded></item><item><title>ColView: Consistent Text-Guided Grayscale Scene Colorization From Multi-View Images</title><link>https://doi.org/10.1109/tmm.2026.3651101</link><guid>10.1109/tmm.2026.3651101</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Chaochao Niu</dc:creator><dc:creator>Ming Tao</dc:creator><dc:creator>Bing-Kun Bao</dc:creator><dc:creator>Changsheng Xu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651101</prism:doi><description>The colorization of scenes from multi-view grayscale images plays a crucial role in applications such as augmented reality and virtual exhibitions. Existing methods combine NeRF with an automatic colorization model, averaging multiple colorized patches to reduce inconsistency. However, they still face three key limitations: (1) Current methods cannot produce diverse colorization results due to the lack of multimodal conditional inputs, (2) They struggle to maintain multi-view consistency caused by unreliable geometric correspondence and ineffective propagation mechanisms, and (3) Computational inefficiency from NeRF's dense ray sampling and numerical integration. In this paper, we propose ColView, a unified framework for text-guided grayscale scene colorization that achieves both automatic and controllable colorization of grayscale scenes from multi-view grayscale images. First, for flexible color control, we leverage text description as the input to guide the colorization process, which allows users to specify desired colors through natural language descriptions. Second, to ensure multi-view consistency, we introduce a multi-view consistent colorization module that explicitly models dependencies between different views. This module follows three key steps: cross-view attention mechanism for collaborative key-view colorization, feature matching for inter-view correspondence establishment, and correspondence-guided feature propagation. Third, to improve computational efficiency, we adopt 3D Gaussian Splatting as our underlying representation. This explicit point-based representation renders significantly faster than NeRF. Extensive experimental results demonstrate that our method achieves superior visual quality and computational efficiency. Our code and models are publicly available at https://github.com/ChchNiu/ColView.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaochao Niu; Ming Tao; Bing-Kun Bao; Changsheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651101"&gt;10.1109/tmm.2026.3651101&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;The colorization of scenes from multi-view grayscale images plays a crucial role in applications such as augmented reality and virtual exhibitions. Existing methods combine NeRF with an automatic colorization model, averaging multiple colorized patches to reduce inconsistency. However, they still face three key limitations: (1) Current methods cannot produce diverse colorization results due to the lack of multimodal conditional inputs, (2) They struggle to maintain multi-view consistency caused by unreliable geometric correspondence and ineffective propagation mechanisms, and (3) Computational inefficiency from NeRF&amp;#x27;s dense ray sampling and numerical integration. In this paper, we propose ColView, a unified framework for text-guided grayscale scene colorization that achieves both automatic and controllable colorization of grayscale scenes from multi-view grayscale images. First, for flexible color control, we leverage text description as the input to guide the colorization process, which allows users to specify desired colors through natural language descriptions. Second, to ensure multi-view consistency, we introduce a multi-view consistent colorization module that explicitly models dependencies between different views. This module follows three key steps: cross-view attention mechanism for collaborative key-view colorization, feature matching for inter-view correspondence establishment, and correspondence-guided feature propagation. Third, to improve computational efficiency, we adopt 3D Gaussian Splatting as our underlying representation. This explicit point-based representation renders significantly faster than NeRF. Extensive experimental results demonstrate that our method achieves superior visual quality and computational efficiency. Our code and models are publicly available at https://github.com/ChchNiu/ColView.&lt;/p&gt;</content:encoded></item><item><title>MT-RoadNet: A heterogeneous network with local–global joint enhancement for road surface and centerline extraction</title><link>https://doi.org/10.1016/j.jag.2025.105080</link><guid>10.1016/j.jag.2025.105080</guid><pubDate>Tue, 06 Jan 2026 05:31:06 +0000</pubDate><dc:creator>Zejiao Wang</dc:creator><dc:creator>Longgang Xiang</dc:creator><dc:creator>Meng Wang</dc:creator><dc:creator>Xingjuan Wang</dc:creator><dc:creator>Fengwei Jiao</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105080</prism:doi><description>Road network information has extensive applications, such as urban planning and navigation. However, current road extraction methods mostly rely on single model architectures and single-source remote sensing images, neglecting the potential benefits of collaborative extraction with heterogeneous networks and multi-source data fusion. Moreover, existing methods often suffer from road fragmentation and poorly connected road graphs due to occlusions. To address these challenges, we propose MT-RoadNet, a local–global joint enhancement framework for extracting road surfaces and centerlines, which incorporates multi-network collaborative optimization. Specifically, MT-RoadNet adopts a dual-branch structure, which not only facilitates the collaborative extraction of local details and global semantics but also achieves the coupling of geometry and topology through a cross-task dynamic interaction mechanism. Besides, we propose the Local–Global Feature Fusion module (LGFF), which dynamically integrates local details and global semantics through multi-level feature interaction. Furthermore, to reduce interference features with high inter-class separability and low intra-class variation, we innovatively design the Visual State Space Module (VSSM) and the Spatial-Channel Mutual Attention (SCMA). The VSSM weighs features dynamically using multi-directional cross-scanning and global receptive fields, emphasizing prominent area information while improving computational efficiency. SCMA effectively guides the model to focus on semantically relevant regions. Finally, MT-RoadNet adopts a dual-path decoder to produce road surfaces and centerlines. Extensive experiments on three road datasets demonstrate that MT-RoadNet significantly outperforms existing state-of-the-art methods in terms of road completeness and recognition accuracy of topological structure. The code is available at https://github.com/508hz1207/MTNet .
Published: 2026-01-06T05:31:06+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zejiao Wang; Longgang Xiang; Meng Wang; Xingjuan Wang; Fengwei Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105080"&gt;10.1016/j.jag.2025.105080&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Road network information has extensive applications, such as urban planning and navigation. However, current road extraction methods mostly rely on single model architectures and single-source remote sensing images, neglecting the potential benefits of collaborative extraction with heterogeneous networks and multi-source data fusion. Moreover, existing methods often suffer from road fragmentation and poorly connected road graphs due to occlusions. To address these challenges, we propose MT-RoadNet, a local–global joint enhancement framework for extracting road surfaces and centerlines, which incorporates multi-network collaborative optimization. Specifically, MT-RoadNet adopts a dual-branch structure, which not only facilitates the collaborative extraction of local details and global semantics but also achieves the coupling of geometry and topology through a cross-task dynamic interaction mechanism. Besides, we propose the Local–Global Feature Fusion module (LGFF), which dynamically integrates local details and global semantics through multi-level feature interaction. Furthermore, to reduce interference features with high inter-class separability and low intra-class variation, we innovatively design the Visual State Space Module (VSSM) and the Spatial-Channel Mutual Attention (SCMA). The VSSM weighs features dynamically using multi-directional cross-scanning and global receptive fields, emphasizing prominent area information while improving computational efficiency. SCMA effectively guides the model to focus on semantically relevant regions. Finally, MT-RoadNet adopts a dual-path decoder to produce road surfaces and centerlines. Extensive experiments on three road datasets demonstrate that MT-RoadNet significantly outperforms existing state-of-the-art methods in terms of road completeness and recognition accuracy of topological structure. The code is available at https://github.com/508hz1207/MTNet .&lt;/p&gt;</content:encoded></item><item><title>UniBVR: Balancing visual and reasoning abilities in unified 3D scene understanding</title><link>https://doi.org/10.1016/j.neucom.2025.132599</link><guid>10.1016/j.neucom.2025.132599</guid><pubDate>Tue, 06 Jan 2026 17:04:30 +0000</pubDate><dc:creator>Panqi Yang</dc:creator><dc:creator>Haodong Jing</dc:creator><dc:creator>Nanning Zheng</dc:creator><dc:creator>Yongqiang Ma</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132599</prism:doi><description>Recent advances in Large Language Models (LLMs) enable remarkable general-purpose task-solving in computer vision, robotics, and beyond. Although LLMs perform well in 2D tasks, their adaptation to 3D scene understanding faces critical challenges: (1) the inherent complexity of 3D spatial relationships and multimodal alignment, (2) the performance imbalance between vision-centric tasks and reasoning-centric tasks. Existing approaches either develop specialized models for individual tasks or rely on LLM fine-tuning with limited visual grounding capabilities, failing to achieve unified 3D scene understanding. To bridge this gap, we propose UniBVR , a U nified framework that B alances V isual and R easoning abilities through two innovative components: (i) task-agnostic Align-Former module that establishes fine-grained 3D vision-language correspondence through cross-modal attention, and (ii) task-specific lightweight decoders that dynamically generate diverse outputs (texts, boxes or masks) via efficient routing. To mitigate task imbalance, we design a multi-task balancing strategy that automatically adjusts loss weights based on task difficulty. Experiments on seven benchmarks (ScanRefer, Nr3D, ScanQA, etc.) achieve state-of-the-art results, with gains of 5.8% (3D-VG), 4.3% (3D-DC), and 6.1% (3D-QA) over prior methods.
Published: 2026-01-06T17:04:30+00:00
Venue: Neurocomputing
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Panqi Yang; Haodong Jing; Nanning Zheng; Yongqiang Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132599"&gt;10.1016/j.neucom.2025.132599&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Large Language Models (LLMs) enable remarkable general-purpose task-solving in computer vision, robotics, and beyond. Although LLMs perform well in 2D tasks, their adaptation to 3D scene understanding faces critical challenges: (1) the inherent complexity of 3D spatial relationships and multimodal alignment, (2) the performance imbalance between vision-centric tasks and reasoning-centric tasks. Existing approaches either develop specialized models for individual tasks or rely on LLM fine-tuning with limited visual grounding capabilities, failing to achieve unified 3D scene understanding. To bridge this gap, we propose UniBVR , a U nified framework that B alances V isual and R easoning abilities through two innovative components: (i) task-agnostic Align-Former module that establishes fine-grained 3D vision-language correspondence through cross-modal attention, and (ii) task-specific lightweight decoders that dynamically generate diverse outputs (texts, boxes or masks) via efficient routing. To mitigate task imbalance, we design a multi-task balancing strategy that automatically adjusts loss weights based on task difficulty. Experiments on seven benchmarks (ScanRefer, Nr3D, ScanQA, etc.) achieve state-of-the-art results, with gains of 5.8% (3D-VG), 4.3% (3D-DC), and 6.1% (3D-QA) over prior methods.&lt;/p&gt;</content:encoded></item><item><title>BuildingMultiView:powering multi-scale building characterization with large language models and Multi-perspective imagery</title><link>https://doi.org/10.1016/j.jag.2025.105034</link><guid>10.1016/j.jag.2025.105034</guid><pubDate>Tue, 06 Jan 2026 22:27:46 +0000</pubDate><dc:creator>Zongrong Li</dc:creator><dc:creator>Yunlei Su</dc:creator><dc:creator>Filip Biljecki</dc:creator><dc:creator>Wufan Zhao</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105034</prism:doi><description>Buildings play a crucial role in shaping urban environments, influencing their physical, functional, and aesthetic characteristics. However, urban analytics is frequently limited by datasets lacking essential semantic details as well as fragmentation across diverse and incompatible data sources. To address these challenges, we conducted a comprehensive meta -analysis of 6,285 publications (2019–2024). From this review, we identified 11 key visually discernible building characteristics grouped into three branches: satellite house, satellite neighborhood, and street-view. Based on this structured characteristic system, we introduce BuildingMultiView, an innovative framework leveraging fine-tuned Large Language Models (LLMs) to systematically extract semantically detailed building characteristics from integrated satellite and street-view imagery. Using structured image–prompt–label triplets, the model efficiently annotates characteristics at multiple spatial scales. These characteristics include swimming pools, roof types, building density, wall–window ratio, and property types. Together, they provide a comprehensive and multi-perspective building database. Experiments conducted across five cities in the USA with diverse architecture and urban form, San Francisco, San Diego, Salt Lake City, Austin, and New York City, demonstrate significant performance improvements, with an F1 score of 79.77% compared to the untuned base version of ChatGPT’s 45.66%. These results reveal diverse urban building patterns and correlations between architectural and environmental characteristics, showcasing the framework’s capability to analyze both macro-scale and micro-scale urban building data. By integrating multi-perspective data sources with cutting-edge LLMs, BuildingMultiView enhances building data extraction, offering a scalable tool for urban planners to address sustainability, infrastructure, and human-centered design, enabling smarter, resilient cities.
Published: 2026-01-06T22:27:46+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongrong Li; Yunlei Su; Filip Biljecki; Wufan Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105034"&gt;10.1016/j.jag.2025.105034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Buildings play a crucial role in shaping urban environments, influencing their physical, functional, and aesthetic characteristics. However, urban analytics is frequently limited by datasets lacking essential semantic details as well as fragmentation across diverse and incompatible data sources. To address these challenges, we conducted a comprehensive meta -analysis of 6,285 publications (2019–2024). From this review, we identified 11 key visually discernible building characteristics grouped into three branches: satellite house, satellite neighborhood, and street-view. Based on this structured characteristic system, we introduce BuildingMultiView, an innovative framework leveraging fine-tuned Large Language Models (LLMs) to systematically extract semantically detailed building characteristics from integrated satellite and street-view imagery. Using structured image–prompt–label triplets, the model efficiently annotates characteristics at multiple spatial scales. These characteristics include swimming pools, roof types, building density, wall–window ratio, and property types. Together, they provide a comprehensive and multi-perspective building database. Experiments conducted across five cities in the USA with diverse architecture and urban form, San Francisco, San Diego, Salt Lake City, Austin, and New York City, demonstrate significant performance improvements, with an F1 score of 79.77% compared to the untuned base version of ChatGPT’s 45.66%. These results reveal diverse urban building patterns and correlations between architectural and environmental characteristics, showcasing the framework’s capability to analyze both macro-scale and micro-scale urban building data. By integrating multi-perspective data sources with cutting-edge LLMs, BuildingMultiView enhances building data extraction, offering a scalable tool for urban planners to address sustainability, infrastructure, and human-centered design, enabling smarter, resilient cities.&lt;/p&gt;</content:encoded></item><item><title>HybridCount: Multi-Scale Transformer with Knowledge Distillation for Object Counting</title><link>https://doi.org/10.1016/j.patcog.2026.113043</link><guid>10.1016/j.patcog.2026.113043</guid><pubDate>Tue, 06 Jan 2026 00:13:29 +0000</pubDate><dc:creator>Jayanthan K S</dc:creator><dc:creator>Domnic S</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113043</prism:doi><description>This work introduces a novel architecture that integrates a multi-scale Visual Transformer (ViT) encoder with a graph attention network decoder to model contextual relationships in visual scenes. Our approach achieves real-time, parameter-efficient object counting through an innovative Knowledge Distillation framework that integrates density estimation maps with regression-based counting mechanisms. The distillation process optimizes performance through a three-component loss function: encoder loss, decoder loss, and our proposed Dual-Domain Density-Regression Loss (DD-R Loss). This novel loss formulation simultaneously supervises both spatial density distribution and direct count regression, providing complementary learning signals for robust object quantification. A key contribution is our scale-aware token embedding technique and cross-attention fusion across varying receptive fields within the ViT architecture, enabling precise counting in cluttered visual environments. Experiments are conducted on four crowd-counting datasets, two vehicle counting datasets. Our detailed experimental evaluation shows that the proposed method delivers outcomes comparable to SOTA methods in terms of counting accuracy and density estimate precision. The detailed comparisons presented in our results and discussion sections highlight the significant strengths and advantages of our methodology within the challenging domain of visual object counting. Our framework bridges the gap between the representational power of transformer-based models and graph network architectures. The efficiency of our approach enables real-time performance comparable to other CNN based approaches. This combination delivers a comprehensive solution for object counting tasks that performs effectively even in resource-constrained environments.
Published: 2026-01-06T00:13:29+00:00
Venue: Pattern Recognition
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jayanthan K S; Domnic S&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113043"&gt;10.1016/j.patcog.2026.113043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;This work introduces a novel architecture that integrates a multi-scale Visual Transformer (ViT) encoder with a graph attention network decoder to model contextual relationships in visual scenes. Our approach achieves real-time, parameter-efficient object counting through an innovative Knowledge Distillation framework that integrates density estimation maps with regression-based counting mechanisms. The distillation process optimizes performance through a three-component loss function: encoder loss, decoder loss, and our proposed Dual-Domain Density-Regression Loss (DD-R Loss). This novel loss formulation simultaneously supervises both spatial density distribution and direct count regression, providing complementary learning signals for robust object quantification. A key contribution is our scale-aware token embedding technique and cross-attention fusion across varying receptive fields within the ViT architecture, enabling precise counting in cluttered visual environments. Experiments are conducted on four crowd-counting datasets, two vehicle counting datasets. Our detailed experimental evaluation shows that the proposed method delivers outcomes comparable to SOTA methods in terms of counting accuracy and density estimate precision. The detailed comparisons presented in our results and discussion sections highlight the significant strengths and advantages of our methodology within the challenging domain of visual object counting. Our framework bridges the gap between the representational power of transformer-based models and graph network architectures. The efficiency of our approach enables real-time performance comparable to other CNN based approaches. This combination delivers a comprehensive solution for object counting tasks that performs effectively even in resource-constrained environments.&lt;/p&gt;</content:encoded></item><item><title>CLIP-Enhanced Segmentation for Neural Radiance Fields</title><link>https://doi.org/10.1016/j.eswa.2026.131121</link><guid>10.1016/j.eswa.2026.131121</guid><pubDate>Tue, 06 Jan 2026 17:06:08 +0000</pubDate><dc:creator>Chong Zhao</dc:creator><dc:creator>Pengcheng Hou</dc:creator><dc:creator>Xing Wei</dc:creator><dc:creator>Chengjun Yang</dc:creator><dc:creator>Jiansheng Peng</dc:creator><dc:creator>Xiang Bi</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131121</prism:doi><description>Recently, the Neural Radiance Fields (NeRF) has been effective in 3D scene reconstruction, while the Segment Anything Model (SAM) has demonstrated excellent zero-shot segmentation capabilities. Although initial attempts to combine these two methods for 3D segmentation exist, it is important to note that they still face the problems of poor segmentation quality and poor multi-view consistency in complex scenes. To address this issue, we introduce the Enhancing Segmentation in NeRF with CLIP (ES-NeRF), which aims to enhance the segmentation quality by leveraging CLIP’s powerful semantic comprehension for feature fusion. Specifically, we propose a CLIP2SAM module, which utilizes the image-text features extracted by CLIP for cross-modal multiscale interactions to obtain the semantic features of CLIP on rough segmentation. These features will then be aligned with those extracted by SAM to achieve feature fusion and complete segmentation. Finally, NeRF is employed to aggregate masks from disparate viewpoints, thereby attaining high-quality 3D segmentation. The efficacy of our method is substantiated by a multitude of experimental results, demonstrating its superiority over existing state-of-the-art methods.
Published: 2026-01-06T17:06:08+00:00
Venue: Expert Systems with Applications
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chong Zhao; Pengcheng Hou; Xing Wei; Chengjun Yang; Jiansheng Peng; Xiang Bi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131121"&gt;10.1016/j.eswa.2026.131121&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Recently, the Neural Radiance Fields (NeRF) has been effective in 3D scene reconstruction, while the Segment Anything Model (SAM) has demonstrated excellent zero-shot segmentation capabilities. Although initial attempts to combine these two methods for 3D segmentation exist, it is important to note that they still face the problems of poor segmentation quality and poor multi-view consistency in complex scenes. To address this issue, we introduce the Enhancing Segmentation in NeRF with CLIP (ES-NeRF), which aims to enhance the segmentation quality by leveraging CLIP’s powerful semantic comprehension for feature fusion. Specifically, we propose a CLIP2SAM module, which utilizes the image-text features extracted by CLIP for cross-modal multiscale interactions to obtain the semantic features of CLIP on rough segmentation. These features will then be aligned with those extracted by SAM to achieve feature fusion and complete segmentation. Finally, NeRF is employed to aggregate masks from disparate viewpoints, thereby attaining high-quality 3D segmentation. The efficacy of our method is substantiated by a multitude of experimental results, demonstrating its superiority over existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>CapHDR2IR: Caption-Driven Transfer from Visible Light to Infrared Domain</title><link>https://doi.org/10.1109/tmm.2026.3651090</link><guid>10.1109/tmm.2026.3651090</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Jingchao Peng</dc:creator><dc:creator>Thomas Bashford-Rogers</dc:creator><dc:creator>Zhuang Shao</dc:creator><dc:creator>Haitao Zhao</dc:creator><dc:creator>Aru Ranjan Singh</dc:creator><dc:creator>Abhishek Goswami</dc:creator><dc:creator>Kurt Debattista</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651090</prism:doi><description>Infrared (IR) imaging offers advantages in several fields due to its unique ability of capturing content in extreme light conditions. However, the demanding hardware requirements of high-resolution IR sensors limit its widespread application. As an alternative, visible light can be used to synthesize IR images but this causes a loss of fidelity in image details and introduces inconsistencies due to lack of contextual awareness of the scene. This stems from a combination of using visible light with a standard dynamic range, especially under extreme lighting, and a lack of contextual awareness can result in pseudo-thermal-crossover artifacts. This occurs when multiple objects with similar temperatures appear indistinguishable in the training data, further exacerbating the loss of fidelity. To solve this challenge, this paper proposes CapHDR2IR, a novel framework incorporating vision-language models using high dynamic range (HDR) images as inputs to generate IR images. HDR images capture a wider range of luminance variations, ensuring reliable IR image generation in different light conditions. Additionally, a dense caption branch integrates semantic understanding, resulting in more meaningful and discernible IR outputs. Extensive experiments on the HDRT dataset show that the proposed CapHDR2IR achieves state-of-the-art performance compared with existing general domain transfer methods and those tailored for visible-to-infrared image translation. The source code is available at https://github.com/PengJingchao/CapHDR2IR.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingchao Peng; Thomas Bashford-Rogers; Zhuang Shao; Haitao Zhao; Aru Ranjan Singh; Abhishek Goswami; Kurt Debattista&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651090"&gt;10.1109/tmm.2026.3651090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Infrared (IR) imaging offers advantages in several fields due to its unique ability of capturing content in extreme light conditions. However, the demanding hardware requirements of high-resolution IR sensors limit its widespread application. As an alternative, visible light can be used to synthesize IR images but this causes a loss of fidelity in image details and introduces inconsistencies due to lack of contextual awareness of the scene. This stems from a combination of using visible light with a standard dynamic range, especially under extreme lighting, and a lack of contextual awareness can result in pseudo-thermal-crossover artifacts. This occurs when multiple objects with similar temperatures appear indistinguishable in the training data, further exacerbating the loss of fidelity. To solve this challenge, this paper proposes CapHDR2IR, a novel framework incorporating vision-language models using high dynamic range (HDR) images as inputs to generate IR images. HDR images capture a wider range of luminance variations, ensuring reliable IR image generation in different light conditions. Additionally, a dense caption branch integrates semantic understanding, resulting in more meaningful and discernible IR outputs. Extensive experiments on the HDRT dataset show that the proposed CapHDR2IR achieves state-of-the-art performance compared with existing general domain transfer methods and those tailored for visible-to-infrared image translation. The source code is available at https://github.com/PengJingchao/CapHDR2IR.&lt;/p&gt;</content:encoded></item><item><title>Weakly Semi-supervised Temporal Sentence Grounding in Videos with Point Annotations</title><link>https://doi.org/10.1109/tmm.2026.3651062</link><guid>10.1109/tmm.2026.3651062</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Jianxiang Dong</dc:creator><dc:creator>Zhaozheng Yin</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651062</prism:doi><description>Temporal Sentence Grounding (TSG) in videos aims to localize a temporal interval from an untrimmed video that is semantically relevant to a given query sentence. To achieve a balance between tremendous annotation burden and grounding performance, we propose a new Weakly Semi-supervised Temporal Sentence Grounding with Points (WSS-TSG-P) task, where the dataset comprises limited fully-annotated video-sentence pairs by start and end timestamps (full label) and a large amount of weakly-annotated pairs by a single point timestamp (point label). Based on this setting, we first introduce a point-tomoment1 regressor which converts point annotations to pseudo moment labels. To train a good regressor for reliable pseudo moment labels, we propose a point-guided feature aggregation module to aggregate cross-modal representations based on the prototype feature at the given point position. In addition, we propose to perform regressor self-training and design pseudo label generation strategies to exploit both full annotations and point annotations. All heterogeneous labels (full, pseudo moment, and point labels) are used to train a TSG backbone. In addition, we propose a novel point-guided group contrastive learning method by constructing reliable positive and negative sets and re-weighting pseudo moment labels to further improve the model performance. Extensive experiments on benchmark datasets verify that our proposed method outperforms other semi-supervised learning methods and bridges the performance gap between weakly-supervised and fully-supervised learning methods in TSG.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianxiang Dong; Zhaozheng Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651062"&gt;10.1109/tmm.2026.3651062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Temporal Sentence Grounding (TSG) in videos aims to localize a temporal interval from an untrimmed video that is semantically relevant to a given query sentence. To achieve a balance between tremendous annotation burden and grounding performance, we propose a new Weakly Semi-supervised Temporal Sentence Grounding with Points (WSS-TSG-P) task, where the dataset comprises limited fully-annotated video-sentence pairs by start and end timestamps (full label) and a large amount of weakly-annotated pairs by a single point timestamp (point label). Based on this setting, we first introduce a point-tomoment1 regressor which converts point annotations to pseudo moment labels. To train a good regressor for reliable pseudo moment labels, we propose a point-guided feature aggregation module to aggregate cross-modal representations based on the prototype feature at the given point position. In addition, we propose to perform regressor self-training and design pseudo label generation strategies to exploit both full annotations and point annotations. All heterogeneous labels (full, pseudo moment, and point labels) are used to train a TSG backbone. In addition, we propose a novel point-guided group contrastive learning method by constructing reliable positive and negative sets and re-weighting pseudo moment labels to further improve the model performance. Extensive experiments on benchmark datasets verify that our proposed method outperforms other semi-supervised learning methods and bridges the performance gap between weakly-supervised and fully-supervised learning methods in TSG.&lt;/p&gt;</content:encoded></item><item><title>TextRSR: Enhanced Arbitrary-Shaped Scene Text Representation Via Robust Subspace Recovery</title><link>https://doi.org/10.1109/tmm.2026.3651034</link><guid>10.1109/tmm.2026.3651034</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Zhiwen Shao</dc:creator><dc:creator>Shengtian Jiang</dc:creator><dc:creator>Hancheng Zhu</dc:creator><dc:creator>Xuehuai Shi</dc:creator><dc:creator>Canlin Li</dc:creator><dc:creator>Lizhuang Ma</dc:creator><dc:creator>Dit-Yan Yeung</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651034</prism:doi><description>In recent years, scene text detection research has increasingly focused on arbitrary-shaped texts, where text representation is a fundamental problem. However, most existing methods still struggle to separate adjacent or overlapping texts due to ambiguous spatial positions of points or segmentation masks. Besides, the time efficiency of the entire pipeline is often neglected, resulting in sub-optimal inference speed. To tackle these problems, we first propose a novel text representation method based on robust subspace recovery, which robustly represents complex text shapes by combining orthogonal basis vectors learned from labeled text contours. These basis vectors capture basis contour patterns with distinct information, enabling clearer boundaries even in densely populated text scenarios. Moreover, we propose a dynamic sparse assignment scheme for positive samples that adaptively adjusts their weights during training, which not only accelerates inference speed by eliminating redundant predictions but also enhances feature learning by providing sufficient supervision signals. Building on these innovations, we present TextRSR, an accurate and efficient scene text detection network. Extensive experiments on challenging benchmarks demonstrate the superior accuracy and efficiency of TextRSR compared to state-of-the-art methods. Particularly, TextRSR achieves an F-measure of 88.5% at 37.8 frames per second (FPS) for CTW1500 dataset and an F-measure of 89.1% at 23.1 FPS for Total-Text dataset.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiwen Shao; Shengtian Jiang; Hancheng Zhu; Xuehuai Shi; Canlin Li; Lizhuang Ma; Dit-Yan Yeung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651034"&gt;10.1109/tmm.2026.3651034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;In recent years, scene text detection research has increasingly focused on arbitrary-shaped texts, where text representation is a fundamental problem. However, most existing methods still struggle to separate adjacent or overlapping texts due to ambiguous spatial positions of points or segmentation masks. Besides, the time efficiency of the entire pipeline is often neglected, resulting in sub-optimal inference speed. To tackle these problems, we first propose a novel text representation method based on robust subspace recovery, which robustly represents complex text shapes by combining orthogonal basis vectors learned from labeled text contours. These basis vectors capture basis contour patterns with distinct information, enabling clearer boundaries even in densely populated text scenarios. Moreover, we propose a dynamic sparse assignment scheme for positive samples that adaptively adjusts their weights during training, which not only accelerates inference speed by eliminating redundant predictions but also enhances feature learning by providing sufficient supervision signals. Building on these innovations, we present TextRSR, an accurate and efficient scene text detection network. Extensive experiments on challenging benchmarks demonstrate the superior accuracy and efficiency of TextRSR compared to state-of-the-art methods. Particularly, TextRSR achieves an F-measure of 88.5% at 37.8 frames per second (FPS) for CTW1500 dataset and an F-measure of 89.1% at 23.1 FPS for Total-Text dataset.&lt;/p&gt;</content:encoded></item><item><title>Asymmetric Frequency-Adaptive State-Space Model for Roadside Cooperative Perception</title><link>https://doi.org/10.1109/tcsvt.2026.3651666</link><guid>10.1109/tcsvt.2026.3651666</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Jiaqian Wang</dc:creator><dc:creator>Yiling Wu</dc:creator><dc:creator>Mingkai Qiu</dc:creator><dc:creator>Xiying Li</dc:creator><dc:creator>Yaowei Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651666</prism:doi><description>Accurate and efficient roadside cooperative perception is crucial for reducing blind spots and extending sensing ranges. However, it faces challenges in modeling long-short range cooperative dependencies and representing the heterogeneous-density distribution of cross-infrastructure data. While CNNs, Transformers, and State-Space Models have demonstrated superior performance, they inherently struggle to balance the flexibility of long-short range receptive fields with computational costs. Additionally, frequency-domain decomposition remains underutilized for heterogeneous-density data representation. In this work, we propose an innovative Asymmetric Multi-Frequency Scale-Adaptive Mamba (AsymMamba) framework, performing lightweight heterogeneous-density data decomposition to support scalable long-short range cooperative representation. First, an Asymmetric Multi-Frequency Decomposition (AsymFreq) module is designed with wavelet transforms, which unifies the spatial distribution representation of heterogeneous-density data in the frequency domain while mitigating information loss through asymmetric scale partitioning. Subsequently, AsymMamba designs a Scale-Adaptive State-Space Model (AdaSSM) module with a spatial compression and channel expansion mechanism. It not only effectively captures local short-range semantic information but also efficiently models global long-range cooperative dependencies with linear complexity. Experiments on real-world DAIR-V2X and RCooper datasets demonstrate that AsymMamba outperforms state-of-the-art methods, including the Transformer-based CoBEVT and recent Mamba-based variants. Specifically, it achieves 3.4%, 4.3%, and 0.6% 3D object detection improvements at AP@0.5 in vehicle-to-infrastructure cooperation, complex intersection, and long-range corridor roadside cooperative perception scenarios, respectively. Moreover, AsymMamba also achieves superior real-time efficiency with 4x faster inference latency than CoBEVT in a 100m sensing r...
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqian Wang; Yiling Wu; Mingkai Qiu; Xiying Li; Yaowei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651666"&gt;10.1109/tcsvt.2026.3651666&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Accurate and efficient roadside cooperative perception is crucial for reducing blind spots and extending sensing ranges. However, it faces challenges in modeling long-short range cooperative dependencies and representing the heterogeneous-density distribution of cross-infrastructure data. While CNNs, Transformers, and State-Space Models have demonstrated superior performance, they inherently struggle to balance the flexibility of long-short range receptive fields with computational costs. Additionally, frequency-domain decomposition remains underutilized for heterogeneous-density data representation. In this work, we propose an innovative Asymmetric Multi-Frequency Scale-Adaptive Mamba (AsymMamba) framework, performing lightweight heterogeneous-density data decomposition to support scalable long-short range cooperative representation. First, an Asymmetric Multi-Frequency Decomposition (AsymFreq) module is designed with wavelet transforms, which unifies the spatial distribution representation of heterogeneous-density data in the frequency domain while mitigating information loss through asymmetric scale partitioning. Subsequently, AsymMamba designs a Scale-Adaptive State-Space Model (AdaSSM) module with a spatial compression and channel expansion mechanism. It not only effectively captures local short-range semantic information but also efficiently models global long-range cooperative dependencies with linear complexity. Experiments on real-world DAIR-V2X and RCooper datasets demonstrate that AsymMamba outperforms state-of-the-art methods, including the Transformer-based CoBEVT and recent Mamba-based variants. Specifically, it achieves 3.4%, 4.3%, and 0.6% 3D object detection improvements at AP@0.5 in vehicle-to-infrastructure cooperation, complex intersection, and long-range corridor roadside cooperative perception scenarios, respectively. Moreover, AsymMamba also achieves superior real-time efficiency with 4x faster inference latency than CoBEVT in a 100m sensing r...&lt;/p&gt;</content:encoded></item><item><title>ShadowNeRF: Learning Neural Radiance Field with Sight Degradation and Recovery</title><link>https://doi.org/10.1109/tmm.2026.3651081</link><guid>10.1109/tmm.2026.3651081</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Yu Zheng</dc:creator><dc:creator>Hongru Yan</dc:creator><dc:creator>Yueqi Duan</dc:creator><dc:creator>Jiwen Lu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651081</prism:doi><description>Inherently equipped with arbitrary resolution and multi-view consistency, the Neural Radiance Field (NeRF) as an implicit scene representation has drawn extensive attention. While traditional NeRFs excel at novel view synthesis (NVS) under ideal conditions, they overlook the potential of learning consistent geometric representations across varying sight qualities. Current methods mainly focus on optimizing synthesis under clear visibility, which limits their effectiveness in downstream scene understanding tasks where robust geometry comprehension is crucial. In this paper, we propose a NVS pre-training technique named ShadowNeRF which firstly synthesizes degraded views with shadowed regions to challenge the model in inferring complete scene geometries. We then design a self-supervised sight recovery process with a two-stage unshadowing framework, which progressively recovers neighboring areas and reveals geometric properties of invisible regions. This pre-training strategy of degradation synthesis and recovery, when combined with taskspecific fine-tuning, enhances the understanding of underlying scene structure for the model and strengthens its ability to process scenes under varying sight conditions. Through extensive experiments, we demonstrate that our pre-training and finetuning pipeline significantly improves the model performances in semantic segmentation and 3D object detection, as well as the reconstruction quality of complex scenes.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Zheng; Hongru Yan; Yueqi Duan; Jiwen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651081"&gt;10.1109/tmm.2026.3651081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Inherently equipped with arbitrary resolution and multi-view consistency, the Neural Radiance Field (NeRF) as an implicit scene representation has drawn extensive attention. While traditional NeRFs excel at novel view synthesis (NVS) under ideal conditions, they overlook the potential of learning consistent geometric representations across varying sight qualities. Current methods mainly focus on optimizing synthesis under clear visibility, which limits their effectiveness in downstream scene understanding tasks where robust geometry comprehension is crucial. In this paper, we propose a NVS pre-training technique named ShadowNeRF which firstly synthesizes degraded views with shadowed regions to challenge the model in inferring complete scene geometries. We then design a self-supervised sight recovery process with a two-stage unshadowing framework, which progressively recovers neighboring areas and reveals geometric properties of invisible regions. This pre-training strategy of degradation synthesis and recovery, when combined with taskspecific fine-tuning, enhances the understanding of underlying scene structure for the model and strengthens its ability to process scenes under varying sight conditions. Through extensive experiments, we demonstrate that our pre-training and finetuning pipeline significantly improves the model performances in semantic segmentation and 3D object detection, as well as the reconstruction quality of complex scenes.&lt;/p&gt;</content:encoded></item></channel></rss>