<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 31 Dec 2025 02:56:09 +0000</lastBuildDate><item><title>CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3649001</link><guid>10.1109/tpami.2025.3649001</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Ziyang Gong</dc:creator><dc:creator>Zhixiang Wei</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Xiaoxing Hu</dc:creator><dc:creator>Xianzheng Ma</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Yuru Jia</dc:creator><dc:creator>Yupeng Deng</dc:creator><dc:creator>Zhenming Ji</dc:creator><dc:creator>Xiangwei Zhu</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Naoto Yokoya</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:creator>Liangpei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649001</prism:doi><description>Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.604 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyang Gong; Zhixiang Wei; Di Wang; Xiaoxing Hu; Xianzheng Ma; Hongruixuan Chen; Yuru Jia; Yupeng Deng; Zhenming Ji; Xiangwei Zhu; Xue Yang; Naoto Yokoya; Jing Zhang; Bo Du; Junchi Yan; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649001"&gt;10.1109/tpami.2025.3649001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.604 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Topology-aware visual localization: a graph-based framework for content-driven geolocation</title><link>https://doi.org/10.1080/17538947.2025.2607168</link><guid>10.1080/17538947.2025.2607168</guid><pubDate>Tue, 30 Dec 2025 16:28:00 +0000</pubDate><dc:creator>Weiyi Chen</dc:creator><dc:creator>Jinchao Gui</dc:creator><dc:creator>Hao Jin</dc:creator><dc:creator>Lisha Zhou</dc:creator><dc:creator>Yuhao Wang</dc:creator><dc:creator>Kai Qin</dc:creator><dc:creator>Yuchen Li</dc:creator><prism:publicationName>International Journal of Digital Earth</prism:publicationName><prism:doi>10.1080/17538947.2025.2607168</prism:doi><description>Visual positioning is a crucial technology for localization in GNSS-denied environments, forming the basis for scene understanding and accurate interaction. This paper proposes a graph-driven visual positioning framework that combines knowledge graphs with deep neural networks to determine the position of targets in images. The framework consists of three main steps. First, we construct a multimodal knowledge graph by integrating images, text, and structured data. Second, visual features are extracted using a pre-trained convolutional neural network. Third, a graph neural network is used to model the entities and relationships in the knowledge graph, enabling reasoning from visual data to spatial information. The proposed method leverages a multimodal knowledge graph with diverse entities and relationships, employs deep learning models to extract image features for semantic representation, and utilizes graph neural networks to process structured information for image localization. Experimental results show that the framework achieves higher positioning accuracy compared to traditional methods, with reduced errors across different distance ranges. This study provides a new approach to visual positioning and demonstrates the potential of integrating knowledge graphs into visual applications.
Published: 2025-12-30T16:28:00+00:00
Venue: International Journal of Digital Earth
Score: 0.599 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiyi Chen; Jinchao Gui; Hao Jin; Lisha Zhou; Yuhao Wang; Kai Qin; Yuchen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Digital Earth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/17538947.2025.2607168"&gt;10.1080/17538947.2025.2607168&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.599 (consider)&lt;/p&gt;
&lt;p&gt;Visual positioning is a crucial technology for localization in GNSS-denied environments, forming the basis for scene understanding and accurate interaction. This paper proposes a graph-driven visual positioning framework that combines knowledge graphs with deep neural networks to determine the position of targets in images. The framework consists of three main steps. First, we construct a multimodal knowledge graph by integrating images, text, and structured data. Second, visual features are extracted using a pre-trained convolutional neural network. Third, a graph neural network is used to model the entities and relationships in the knowledge graph, enabling reasoning from visual data to spatial information. The proposed method leverages a multimodal knowledge graph with diverse entities and relationships, employs deep learning models to extract image features for semantic representation, and utilizes graph neural networks to process structured information for image localization. Experimental results show that the framework achieves higher positioning accuracy compared to traditional methods, with reduced errors across different distance ranges. This study provides a new approach to visual positioning and demonstrates the potential of integrating knowledge graphs into visual applications.&lt;/p&gt;</content:encoded></item><item><title>STVRM: Spatio-Temporal Relational Modeling with Vision Transformer for Dynamic Scene Graph Generation</title><link>https://doi.org/10.1016/j.eswa.2025.131018</link><guid>10.1016/j.eswa.2025.131018</guid><pubDate>Mon, 29 Dec 2025 23:47:44 +0000</pubDate><dc:creator>Linnan Lu</dc:creator><dc:creator>Guannan Si</dc:creator><dc:creator>Xinyu Liang</dc:creator><dc:creator>Mingshen Li</dc:creator><dc:creator>Fengyu Zhou</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131018</prism:doi><description>Dynamic scene graph generation aims to achieve semantic understanding of scenes by analyzing video content, identifying entities and relationships within the scene. However, on one hand, it struggles to effectively handle contextual noise in the scene; on the other hand, existing dynamic scene graph generation methods still exhibit insufficient capability in capturing the temporal dependencies of visual relationships between entities, particularly when recognizing subtle changes in relationships over time. This hinders the accurate detection of fine-grained dynamic features. To overcome these limitations, this paper presents the Spatial-Temporal Vision Transformer Relation Module (STVRM) , a novel framework aimed at enhancing dynamic scene graph generation and object relation classification. By incorporating the vision transformer architecture in place of the traditional transformer design, STVRM is better equipped to capture both local and global characteristics in video data, particularly excelling at modeling dependencies and transitions across time and space. Additionally, the paper introduces the video object-relation classification module and temporal difference aggregator module, both of which improve the precision of object relation classification and bolster the model’s awareness of changes over time. Comprehensive experiments on the Action Genome dataset demonstrate that the STVRM model significantly outperforms existing methods on both the SGCLS and SGDET tasks. Specifically, under the No Constraint setting, the model achieves improvements of 6% and 4.7% in R@50 and mR@50 for SGCLS, and enhancements of 8% and 4.1% in R@10 and mR@10 for SGDET. These results fully demonstrate the superiority and effectiveness of the STVRM approach.
Published: 2025-12-29T23:47:44+00:00
Venue: Expert Systems with Applications
Score: 0.582 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linnan Lu; Guannan Si; Xinyu Liang; Mingshen Li; Fengyu Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131018"&gt;10.1016/j.eswa.2025.131018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.582 (consider)&lt;/p&gt;
&lt;p&gt;Dynamic scene graph generation aims to achieve semantic understanding of scenes by analyzing video content, identifying entities and relationships within the scene. However, on one hand, it struggles to effectively handle contextual noise in the scene; on the other hand, existing dynamic scene graph generation methods still exhibit insufficient capability in capturing the temporal dependencies of visual relationships between entities, particularly when recognizing subtle changes in relationships over time. This hinders the accurate detection of fine-grained dynamic features. To overcome these limitations, this paper presents the Spatial-Temporal Vision Transformer Relation Module (STVRM) , a novel framework aimed at enhancing dynamic scene graph generation and object relation classification. By incorporating the vision transformer architecture in place of the traditional transformer design, STVRM is better equipped to capture both local and global characteristics in video data, particularly excelling at modeling dependencies and transitions across time and space. Additionally, the paper introduces the video object-relation classification module and temporal difference aggregator module, both of which improve the precision of object relation classification and bolster the model’s awareness of changes over time. Comprehensive experiments on the Action Genome dataset demonstrate that the STVRM model significantly outperforms existing methods on both the SGCLS and SGDET tasks. Specifically, under the No Constraint setting, the model achieves improvements of 6% and 4.7% in R@50 and mR@50 for SGCLS, and enhancements of 8% and 4.1% in R@10 and mR@10 for SGDET. These results fully demonstrate the superiority and effectiveness of the STVRM approach.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism</title><link>https://arxiv.org/abs/2512.23243v1</link><guid>http://arxiv.org/abs/2512.23243v1</guid><pubDate>Mon, 29 Dec 2025 06:51:20 +0000</pubDate><dc:creator>Siyu Zhang</dc:creator><dc:creator>Ying Chen</dc:creator><dc:creator>Lianlei Shan</dc:creator><dc:creator>Runhe Qiu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.
Published: 2025-12-29T06:51:20+00:00
Venue: arXiv
Score: 0.579 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siyu Zhang; Ying Chen; Lianlei Shan; Runhe Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.579 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.&lt;/p&gt;</content:encoded></item><item><title>With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs</title><link>https://arxiv.org/abs/2512.23024v1</link><guid>http://arxiv.org/abs/2512.23024v1</guid><pubDate>Sun, 28 Dec 2025 17:53:55 +0000</pubDate><dc:creator>Ciprian Constantinescu</dc:creator><dc:creator>Marius Leordeanu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.
Published: 2025-12-28T17:53:55+00:00
Venue: arXiv
Score: 0.578 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ciprian Constantinescu; Marius Leordeanu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.578 (consider)&lt;/p&gt;
&lt;p&gt;Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model&amp;#x27;s reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing</title><link>https://arxiv.org/abs/2512.23244v1</link><guid>http://arxiv.org/abs/2512.23244v1</guid><pubDate>Mon, 29 Dec 2025 06:58:46 +0000</pubDate><dc:creator>Xingwei Ma</dc:creator><dc:creator>Shiyang Feng</dc:creator><dc:creator>Bo Zhang</dc:creator><dc:creator>Bin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.
Published: 2025-12-29T06:58:46+00:00
Venue: arXiv
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingwei Ma; Shiyang Feng; Bo Zhang; Bin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains</title><link>https://arxiv.org/abs/2512.22545v1</link><guid>http://arxiv.org/abs/2512.22545v1</guid><pubDate>Sat, 27 Dec 2025 10:14:14 +0000</pubDate><dc:creator>Jesen Zhang</dc:creator><dc:creator>Ningyuan Liu</dc:creator><dc:creator>Kaitong Cai</dc:creator><dc:creator>Sidi Liu</dc:creator><dc:creator>Jing Yang</dc:creator><dc:creator>Ziliang Chen</dc:creator><dc:creator>Xiaofei Sun</dc:creator><dc:creator>Keze Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.
Published: 2025-12-27T10:14:14+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jesen Zhang; Ningyuan Liu; Kaitong Cai; Sidi Liu; Jing Yang; Ziliang Chen; Xiaofei Sun; Keze Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.&lt;/p&gt;</content:encoded></item><item><title>TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding</title><link>https://arxiv.org/abs/2512.23483v1</link><guid>http://arxiv.org/abs/2512.23483v1</guid><pubDate>Mon, 29 Dec 2025 14:10:22 +0000</pubDate><dc:creator>Zongsheng Cao</dc:creator><dc:creator>Yangfan He</dc:creator><dc:creator>Anran Liu</dc:creator><dc:creator>Feng Chen</dc:creator><dc:creator>Zepeng Wang</dc:creator><dc:creator>Jun Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.
Published: 2025-12-29T14:10:22+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongsheng Cao; Yangfan He; Anran Liu; Feng Chen; Zepeng Wang; Jun Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.&lt;/p&gt;</content:encoded></item><item><title>LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency</title><link>https://doi.org/10.1109/tip.2025.3646893</link><guid>10.1109/tip.2025.3646893</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Daosong Hu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Mingyue Cui</dc:creator><dc:creator>Kai Huang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646893</prism:doi><description>In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daosong Hu; Xi Li; Mingyue Cui; Kai Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646893"&gt;10.1109/tip.2025.3646893&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.&lt;/p&gt;</content:encoded></item><item><title>Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</title><link>https://arxiv.org/abs/2512.23035v1</link><guid>http://arxiv.org/abs/2512.23035v1</guid><pubDate>Sun, 28 Dec 2025 18:24:19 +0000</pubDate><dc:creator>Yi Zhou</dc:creator><dc:creator>Xuechao Zou</dc:creator><dc:creator>Shun Zhang</dc:creator><dc:creator>Kai Li</dc:creator><dc:creator>Shiying Wang</dc:creator><dc:creator>Jingming Chen</dc:creator><dc:creator>Congyan Lang</dc:creator><dc:creator>Tengfei Cao</dc:creator><dc:creator>Pin Tao</dc:creator><dc:creator>Yuanchun Shi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.
Published: 2025-12-28T18:24:19+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhou; Xuechao Zou; Shun Zhang; Kai Li; Shiying Wang; Jingming Chen; Congyan Lang; Tengfei Cao; Pin Tao; Yuanchun Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.&lt;/p&gt;</content:encoded></item><item><title>Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations</title><link>https://doi.org/10.1016/j.inffus.2025.104107</link><guid>10.1016/j.inffus.2025.104107</guid><pubDate>Tue, 30 Dec 2025 17:02:53 +0000</pubDate><dc:creator>Yibo Cui</dc:creator><dc:creator>Liang Xie</dc:creator><dc:creator>Yu Zhao</dc:creator><dc:creator>Jiawei Sun</dc:creator><dc:creator>Erwei Yin</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104107</prism:doi><description>Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.
Published: 2025-12-30T17:02:53+00:00
Venue: Information Fusion
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yibo Cui; Liang Xie; Yu Zhao; Jiawei Sun; Erwei Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104107"&gt;10.1016/j.inffus.2025.104107&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.&lt;/p&gt;</content:encoded></item><item><title>CLIP-driven feature disambiguation and cross-modal synergy for few-shot semantic segmentation</title><link>https://doi.org/10.1016/j.eswa.2025.130892</link><guid>10.1016/j.eswa.2025.130892</guid><pubDate>Mon, 29 Dec 2025 13:09:57 +0000</pubDate><dc:creator>Shangjing Chen</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Xin Lyu</dc:creator><dc:creator>Dafa Wang</dc:creator><dc:creator>Xin Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130892</prism:doi><description>Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .
Published: 2025-12-29T13:09:57+00:00
Venue: Expert Systems with Applications
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangjing Chen; Feng Xu; Xin Lyu; Dafa Wang; Xin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130892"&gt;10.1016/j.eswa.2025.130892&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .&lt;/p&gt;</content:encoded></item><item><title>MDPNet: Multimodal Diffusion Prior Guided Self-Text Attention Network for Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/jstars.2025.3648961</link><guid>10.1109/jstars.2025.3648961</guid><pubDate>Mon, 29 Dec 2025 18:38:55 +0000</pubDate><dc:creator>Fulin He</dc:creator><dc:creator>Zhen Wang</dc:creator><dc:creator>Nan Xu</dc:creator><dc:creator>Zhuhong You</dc:creator><dc:creator>Deshuang Huang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3648961</prism:doi><description>Accurate semantic segmentation of remote sensing images (RSIs) is crucial for urban planning, land monitoring, and related applications, yet remains challenging due to the heterogeneity of multimodal data and limited target attribute representation. In this work, we propose a novel Multimodal Diffusion Prior guided Self-Text Attention Network (MDPNet) that introduces three key innovations: 1) a denoising diffusion probabilistic model (DDPM) to extract robust structural priors from digital surface model (DSM) data, effectively suppressing noise and enhancing fine-grained boundary features; 2) a multimodal prior feature guidance (MPFG) module that employs a cross-modal selective state-space mechanism and a dislocated stacking strategy, enabling explicit patch-level fusion and capturing long-range dependencies between modalities; and 3) a self-text attention mechanism (STAM) that automatically generates and aligns category-related textual cues from segmentation masks, eliminating the need for external textual input and providing fine-grained semantic guidance. Extensive experiments on the ISPRS Potsdam and Vaihingen public datasets demonstrate that MDPNet sets a new state-of-the-art, particularly excelling in building edge delineation, robustness under complex scenarios, and balanced segmentation across multiple categories. Our approach outperforms existing mainstream methods in both overall accuracy and resilience, and provides a text-free, end-to-end solution for multimodal remote sensing semantic segmentation.
Published: 2025-12-29T18:38:55+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fulin He; Zhen Wang; Nan Xu; Zhuhong You; Deshuang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3648961"&gt;10.1109/jstars.2025.3648961&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Accurate semantic segmentation of remote sensing images (RSIs) is crucial for urban planning, land monitoring, and related applications, yet remains challenging due to the heterogeneity of multimodal data and limited target attribute representation. In this work, we propose a novel Multimodal Diffusion Prior guided Self-Text Attention Network (MDPNet) that introduces three key innovations: 1) a denoising diffusion probabilistic model (DDPM) to extract robust structural priors from digital surface model (DSM) data, effectively suppressing noise and enhancing fine-grained boundary features; 2) a multimodal prior feature guidance (MPFG) module that employs a cross-modal selective state-space mechanism and a dislocated stacking strategy, enabling explicit patch-level fusion and capturing long-range dependencies between modalities; and 3) a self-text attention mechanism (STAM) that automatically generates and aligns category-related textual cues from segmentation masks, eliminating the need for external textual input and providing fine-grained semantic guidance. Extensive experiments on the ISPRS Potsdam and Vaihingen public datasets demonstrate that MDPNet sets a new state-of-the-art, particularly excelling in building edge delineation, robustness under complex scenarios, and balanced segmentation across multiple categories. Our approach outperforms existing mainstream methods in both overall accuracy and resilience, and provides a text-free, end-to-end solution for multimodal remote sensing semantic segmentation.&lt;/p&gt;</content:encoded></item><item><title>Semantic Consistency Interaction with Calibration Loss for Remote Sensing Image-Text Retrieval</title><link>https://doi.org/10.1109/tgrs.2025.3649046</link><guid>10.1109/tgrs.2025.3649046</guid><pubDate>Mon, 29 Dec 2025 18:38:33 +0000</pubDate><dc:creator>Jinlong Xu</dc:creator><dc:creator>Yun Ge</dc:creator><dc:creator>Yan Zeng</dc:creator><dc:creator>Huyang Liu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649046</prism:doi><description>Remote Sensing Image-Text Retrieval (RSITR) plays a crucial role in the remote sensing community but faces difficulties due to the heterogeneity between the image and text modalities. While existing methods primarily focus on enhancing cross-modal alignment, they suffer from two key issues. One is that fine-grained local alignment approaches extract local features lacking contextual integration, which leads to semantic inconsistency. The other is that conventional losses fail to adapt to the varying model states of different subtasks, which may lead to the model calibration problems. This implies a mismatch between the confidence scores of the network and the true probabilities. To address these challenges, this study proposes two complementary components. One is a Local Semantic Consistency Interaction (LSCI) module, which uses cross-attention and self-attention along with an image-text alignment head to optimize multimodal representations and mitigate context-induced semantic inconsistency. The other is Task-specific Calibration loss (TC), which dynamically adjusts the gamma parameter in Focal or inv-Focal loss, allowing the model to prioritize both hard and easy sample pairs according to the calibration error specific to each subtask. The gamma parameter is adjusted by leveraging the previous states of the model and the knowledge of calibration performance on the validation set. By integrating the LSCI module and TC loss, the semantic inconsistency between image and text can be improved, and model calibration can be enhanced. Comparative assessments conducted on three public datasets show that our methods can achieve competitive performance on the RSITR task compared to many existing methods. The source code is available at https://github.com/StrongerPeople/LSCI-TC.
Published: 2025-12-29T18:38:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinlong Xu; Yun Ge; Yan Zeng; Huyang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649046"&gt;10.1109/tgrs.2025.3649046&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Remote Sensing Image-Text Retrieval (RSITR) plays a crucial role in the remote sensing community but faces difficulties due to the heterogeneity between the image and text modalities. While existing methods primarily focus on enhancing cross-modal alignment, they suffer from two key issues. One is that fine-grained local alignment approaches extract local features lacking contextual integration, which leads to semantic inconsistency. The other is that conventional losses fail to adapt to the varying model states of different subtasks, which may lead to the model calibration problems. This implies a mismatch between the confidence scores of the network and the true probabilities. To address these challenges, this study proposes two complementary components. One is a Local Semantic Consistency Interaction (LSCI) module, which uses cross-attention and self-attention along with an image-text alignment head to optimize multimodal representations and mitigate context-induced semantic inconsistency. The other is Task-specific Calibration loss (TC), which dynamically adjusts the gamma parameter in Focal or inv-Focal loss, allowing the model to prioritize both hard and easy sample pairs according to the calibration error specific to each subtask. The gamma parameter is adjusted by leveraging the previous states of the model and the knowledge of calibration performance on the validation set. By integrating the LSCI module and TC loss, the semantic inconsistency between image and text can be improved, and model calibration can be enhanced. Comparative assessments conducted on three public datasets show that our methods can achieve competitive performance on the RSITR task compared to many existing methods. The source code is available at https://github.com/StrongerPeople/LSCI-TC.&lt;/p&gt;</content:encoded></item><item><title>FFCA-UNet: Feature Fusion and Cross-attention Mechanism for Remote Sensing Image Semantic Segmentation</title><link>https://doi.org/10.1109/jstars.2025.3649532</link><guid>10.1109/jstars.2025.3649532</guid><pubDate>Tue, 30 Dec 2025 18:38:21 +0000</pubDate><dc:creator>Libin Chen</dc:creator><dc:creator>Zihan Li</dc:creator><dc:creator>Xiongwu Xiao</dc:creator><dc:creator>Mohamed Mosaad Ali Mahmoud Elisy</dc:creator><dc:creator>Weiwei Wu</dc:creator><dc:creator>Deren Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649532</prism:doi><description>High-resolution remote sensing (RS) image segmentation remains challenging due to large scale variations, the presence of small and sparsely distributed objects, and the high visual similarity between land-cover categories. In this article, we propose FFCA-UNet, a hybrid CNN–Transformer architecture that integrates two lightweight yet complementary modules. The Efficient Multi-Scale Feature Fusion Module (EMFM) aligns features of different resolutions via bilinear interpolation (BI) before fusion, ensuring spatial consistency with low cost. The CNN-Guided Cross Attention Module (CGCAM) leverages convolutional features as queries and semantically enriched multi-scale features as keys and values, enabling globally consistent reasoning guided by spatial priors. Extensive experiments on two ISPRS benchmarks validate the effectiveness of FFCA-UNet. On the Vaihingen dataset, our model achieves 80.43% mIoU, 91.13% mAccuracy, and 91.73% mAP; while on the Potsdam dataset, it obtains 64.00% mIoU, 81.89% mAccuracy, and 82.61% mAP, outperforming recent CNN-based and hybrid baselines. Ablation studies further demonstrate that EMFM and CGCAM each improve performance individually, while their combination yields the best results, highlighting their complementary strengths. Overall, FFCA-UNet provides a simple, effective, and computationally efficient solution for accurate RS semantic segmentation in complex urban scenes.
Published: 2025-12-30T18:38:21+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Libin Chen; Zihan Li; Xiongwu Xiao; Mohamed Mosaad Ali Mahmoud Elisy; Weiwei Wu; Deren Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649532"&gt;10.1109/jstars.2025.3649532&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing (RS) image segmentation remains challenging due to large scale variations, the presence of small and sparsely distributed objects, and the high visual similarity between land-cover categories. In this article, we propose FFCA-UNet, a hybrid CNN–Transformer architecture that integrates two lightweight yet complementary modules. The Efficient Multi-Scale Feature Fusion Module (EMFM) aligns features of different resolutions via bilinear interpolation (BI) before fusion, ensuring spatial consistency with low cost. The CNN-Guided Cross Attention Module (CGCAM) leverages convolutional features as queries and semantically enriched multi-scale features as keys and values, enabling globally consistent reasoning guided by spatial priors. Extensive experiments on two ISPRS benchmarks validate the effectiveness of FFCA-UNet. On the Vaihingen dataset, our model achieves 80.43% mIoU, 91.13% mAccuracy, and 91.73% mAP; while on the Potsdam dataset, it obtains 64.00% mIoU, 81.89% mAccuracy, and 82.61% mAP, outperforming recent CNN-based and hybrid baselines. Ablation studies further demonstrate that EMFM and CGCAM each improve performance individually, while their combination yields the best results, highlighting their complementary strengths. Overall, FFCA-UNet provides a simple, effective, and computationally efficient solution for accurate RS semantic segmentation in complex urban scenes.&lt;/p&gt;</content:encoded></item><item><title>FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts</title><link>https://doi.org/10.1109/tip.2025.3646861</link><guid>10.1109/tip.2025.3646861</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Xicheng Ding</dc:creator><dc:creator>Xiaofan Li</dc:creator><dc:creator>Mingang Chen</dc:creator><dc:creator>Jingyu Gong</dc:creator><dc:creator>Yuan Xie</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646861</prism:doi><description>Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xicheng Ding; Xiaofan Li; Mingang Chen; Jingyu Gong; Yuan Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646861"&gt;10.1109/tip.2025.3646861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.&lt;/p&gt;</content:encoded></item><item><title>Geometrically-Guided Transformer with Volume-Pose Positional Encoding for Multi-View Stereo</title><link>https://doi.org/10.1016/j.knosys.2025.115240</link><guid>10.1016/j.knosys.2025.115240</guid><pubDate>Tue, 30 Dec 2025 15:51:15 +0000</pubDate><dc:creator>Tianyu Han</dc:creator><dc:creator>Jiangming Kan</dc:creator><dc:creator>Ruifang Dong</dc:creator><dc:creator>Xixuan Zhao</dc:creator><dc:creator>Shun Yao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115240</prism:doi><description>This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.
Published: 2025-12-30T15:51:15+00:00
Venue: Knowledge-Based Systems
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyu Han; Jiangming Kan; Ruifang Dong; Xixuan Zhao; Shun Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115240"&gt;10.1016/j.knosys.2025.115240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.&lt;/p&gt;</content:encoded></item><item><title>UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation</title><link>https://doi.org/10.1016/j.knosys.2025.115207</link><guid>10.1016/j.knosys.2025.115207</guid><pubDate>Mon, 29 Dec 2025 17:00:32 +0000</pubDate><dc:creator>Jiaqi Yang</dc:creator><dc:creator>Xiangjian He</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Yaning Zhang</dc:creator><dc:creator>Jingxi Hu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Guoping Qiu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115207</prism:doi><description>Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.
Published: 2025-12-29T17:00:32+00:00
Venue: Knowledge-Based Systems
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Yang; Xiangjian He; Xin Chen; Yaning Zhang; Jingxi Hu; Linlin Shen; Guoping Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115207"&gt;10.1016/j.knosys.2025.115207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.&lt;/p&gt;</content:encoded></item><item><title>Hexagonal Grid-based Representation and Generative Prediction Method for Citywide Traffic Accident Situations in Urban Area</title><link>https://doi.org/10.1016/j.eswa.2025.130927</link><guid>10.1016/j.eswa.2025.130927</guid><pubDate>Mon, 29 Dec 2025 16:23:31 +0000</pubDate><dc:creator>Xueshen Li</dc:creator><dc:creator>Guangxu Mei</dc:creator><dc:creator>Shijun Liu</dc:creator><dc:creator>Sheharyar Khan</dc:creator><dc:creator>Li Pan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130927</prism:doi><description>Citywide traffic accident situation prediction refers to forecasting future accident situation across urban areas at a citywide level. Most existing approaches still exhibit limitations in effectively representing and visually predicting citywide accident situations. To address this issue, we propose a He xagonal G rid-based R epresentation and Generative Prediction Method for Citywide Traffic A ccidents Situation (HeGRA). Specifically, we introduce hexagonal grids to overlapping urban areas. Then, we perform clustering analysis on traffic accident data to identify accident situation categories, and an image classification model is trained to categorize future traffic situation images. Additionally, we fine-tune the text-to-image generation model to produce future citywide traffic accident situation images. Experimental results on public traffic accident datasets from Chicago and New York demonstrate that the proposed method achieves Top-3 classification accuracies of 78% and 74%, respectively, indicating strong performance and practical potential.
Published: 2025-12-29T16:23:31+00:00
Venue: Expert Systems with Applications
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xueshen Li; Guangxu Mei; Shijun Liu; Sheharyar Khan; Li Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130927"&gt;10.1016/j.eswa.2025.130927&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Citywide traffic accident situation prediction refers to forecasting future accident situation across urban areas at a citywide level. Most existing approaches still exhibit limitations in effectively representing and visually predicting citywide accident situations. To address this issue, we propose a He xagonal G rid-based R epresentation and Generative Prediction Method for Citywide Traffic A ccidents Situation (HeGRA). Specifically, we introduce hexagonal grids to overlapping urban areas. Then, we perform clustering analysis on traffic accident data to identify accident situation categories, and an image classification model is trained to categorize future traffic situation images. Additionally, we fine-tune the text-to-image generation model to produce future citywide traffic accident situation images. Experimental results on public traffic accident datasets from Chicago and New York demonstrate that the proposed method achieves Top-3 classification accuracies of 78% and 74%, respectively, indicating strong performance and practical potential.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network</title><link>https://doi.org/10.1109/tip.2025.3646940</link><guid>10.1109/tip.2025.3646940</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Yangfan Li</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646940</prism:doi><description>Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Li; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646940"&gt;10.1109/tip.2025.3646940&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.&lt;/p&gt;</content:encoded></item><item><title>Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images</title><link>https://doi.org/10.1109/taes.2025.3649185</link><guid>10.1109/taes.2025.3649185</guid><pubDate>Mon, 29 Dec 2025 18:40:09 +0000</pubDate><dc:creator>Wanjie Lu</dc:creator><dc:creator>Chaoyang Niu</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Shiju Wang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3649185</prism:doi><description>When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.
Published: 2025-12-29T18:40:09+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanjie Lu; Chaoyang Niu; Wei Liu; Chaozhen Lan; Shiju Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3649185"&gt;10.1109/taes.2025.3649185&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.&lt;/p&gt;</content:encoded></item><item><title>HFPN: Hierarchical Fusion and Prediction Network with Multi-Level Cross-Modality Relation Learning for Audio-Visual Event Localization</title><link>https://doi.org/10.1016/j.inffus.2025.104111</link><guid>10.1016/j.inffus.2025.104111</guid><pubDate>Tue, 30 Dec 2025 17:02:50 +0000</pubDate><dc:creator>Pufen Zhang</dc:creator><dc:creator>Lei Jia</dc:creator><dc:creator>Jiaxiang Wang</dc:creator><dc:creator>Meng Wan</dc:creator><dc:creator>Sijie Chang</dc:creator><dc:creator>Tianle Zhang</dc:creator><dc:creator>Peng Shi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104111</prism:doi><description>Audio-visual event localization (AVEL) task needs to fuse audio-visual modalities via mining their cross-modality relation (CMR). However, existing AVEL works encounter several challenges in CMR learning: (a) The event-unrelated visual regions are not filtered when learning the region-level CMR; (b) The segment-level CMR is modeled in a one-to-one way, ignoring the cross-modality locality context correlation; (c) The holistic semantics of audio and visual tracks of event are consistent, but such a track-level CMR is not explored; (d) The low- and middle-level visual semantics are ignored in existing fusion and CMR learning strategies. To address these issues, a Hierarchical Fusion and Prediction Network (HFPN) with Multi-level Cross-modality Relation Learning Framework (MCRLF) is proposed. Specifically, for challenge (a), MCRLF proposes an audio-adaptive region filter to dynamically filter out event-irrelevant image regions according to event audio. To deal with challenge (b), MCRLF designs a bilateral locality context attention, which captures the cross-modality locality context correlation via convolution windows to guide segment-level CMR learning. For challenge (c), MCRLF introduces a novel dual-track alignment loss to achieve the whole semantic alignment on the audio and visual tracks of event. Finally, to tackle challenge (d), HFPN uses MCRLF as unified fusion framework to hierarchically fuse audio signals with the low-, middle- and high-level visual features, obtaining comprehensive semantics for event prediction. With modest model complexity, HFPN achieves the state-of-the-art results on AVE (84.8% and 80.2%) and VGGSound-AVEL100k (67.2% and 62.7%) benchmarks under both fully- and weakly-supervised settings, it offers a significant reference for practical application.
Published: 2025-12-30T17:02:50+00:00
Venue: Information Fusion
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pufen Zhang; Lei Jia; Jiaxiang Wang; Meng Wan; Sijie Chang; Tianle Zhang; Peng Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104111"&gt;10.1016/j.inffus.2025.104111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Audio-visual event localization (AVEL) task needs to fuse audio-visual modalities via mining their cross-modality relation (CMR). However, existing AVEL works encounter several challenges in CMR learning: (a) The event-unrelated visual regions are not filtered when learning the region-level CMR; (b) The segment-level CMR is modeled in a one-to-one way, ignoring the cross-modality locality context correlation; (c) The holistic semantics of audio and visual tracks of event are consistent, but such a track-level CMR is not explored; (d) The low- and middle-level visual semantics are ignored in existing fusion and CMR learning strategies. To address these issues, a Hierarchical Fusion and Prediction Network (HFPN) with Multi-level Cross-modality Relation Learning Framework (MCRLF) is proposed. Specifically, for challenge (a), MCRLF proposes an audio-adaptive region filter to dynamically filter out event-irrelevant image regions according to event audio. To deal with challenge (b), MCRLF designs a bilateral locality context attention, which captures the cross-modality locality context correlation via convolution windows to guide segment-level CMR learning. For challenge (c), MCRLF introduces a novel dual-track alignment loss to achieve the whole semantic alignment on the audio and visual tracks of event. Finally, to tackle challenge (d), HFPN uses MCRLF as unified fusion framework to hierarchically fuse audio signals with the low-, middle- and high-level visual features, obtaining comprehensive semantics for event prediction. With modest model complexity, HFPN achieves the state-of-the-art results on AVE (84.8% and 80.2%) and VGGSound-AVEL100k (67.2% and 62.7%) benchmarks under both fully- and weakly-supervised settings, it offers a significant reference for practical application.&lt;/p&gt;</content:encoded></item><item><title>A Multi-View Difference Features Perception Network for Change Detection in Bi-Temporal Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3638628</link><guid>10.1109/tgrs.2025.3638628</guid><pubDate>Mon, 29 Dec 2025 18:38:33 +0000</pubDate><dc:creator>Lanxue Dang</dc:creator><dc:creator>Shilong Li</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Shenshen Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638628</prism:doi><description>Perceiving difference features from bi-temporal remote sensing images is one of the critical steps in deep learning-based change detection models. Current methods usually fuse features of bi-temporal images from a single view at a fixed stage. However, they face two key challenges. (1) the bias of a single view leads to inadequate capture of change information; (2) the fusion strategy in the fixed stage ignores the potential correlation between the high-level semantic features and the underlying detailed features. To address these limitations, this paper proposes a multi-view difference features perception network. It captures the essential difference information from various perspectives via the multi-view perception module, which runs through the encoding and decoding stages. The multi-view perception module consists of three stages: interaction, extraction, and fusion. Specifically, the multi-view interaction convolution operator is designed to obtain a unified representation of the multi-view difference features during the interaction phase. Then, in the extraction stage, the unified representation is continuously decomposed into multi-view difference features through the feature decoupling module. Subsequently, the multi-view fusion module is constructed during the fusion stage, this module can enhance the ability of neural networks to recognize change information by aggregating the multi-view difference features of different receptive fields. Furthermore, an independent fusion module is designed at the end of the network to enhance the network’s perception of various scale change targets. Extensive experiments on three publicly available change detection datasets validate the effectiveness of the proposed method.
Published: 2025-12-29T18:38:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lanxue Dang; Shilong Li; Xiao Wang; Shenshen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638628"&gt;10.1109/tgrs.2025.3638628&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Perceiving difference features from bi-temporal remote sensing images is one of the critical steps in deep learning-based change detection models. Current methods usually fuse features of bi-temporal images from a single view at a fixed stage. However, they face two key challenges. (1) the bias of a single view leads to inadequate capture of change information; (2) the fusion strategy in the fixed stage ignores the potential correlation between the high-level semantic features and the underlying detailed features. To address these limitations, this paper proposes a multi-view difference features perception network. It captures the essential difference information from various perspectives via the multi-view perception module, which runs through the encoding and decoding stages. The multi-view perception module consists of three stages: interaction, extraction, and fusion. Specifically, the multi-view interaction convolution operator is designed to obtain a unified representation of the multi-view difference features during the interaction phase. Then, in the extraction stage, the unified representation is continuously decomposed into multi-view difference features through the feature decoupling module. Subsequently, the multi-view fusion module is constructed during the fusion stage, this module can enhance the ability of neural networks to recognize change information by aggregating the multi-view difference features of different receptive fields. Furthermore, an independent fusion module is designed at the end of the network to enhance the network’s perception of various scale change targets. Extensive experiments on three publicly available change detection datasets validate the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM</title><link>https://arxiv.org/abs/2512.22799v1</link><guid>http://arxiv.org/abs/2512.22799v1</guid><pubDate>Sun, 28 Dec 2025 06:12:28 +0000</pubDate><dc:creator>Jingchao Wang</dc:creator><dc:creator>Kaiwen Zhou</dc:creator><dc:creator>Zhijian Wu</dc:creator><dc:creator>Kunhua Ji</dc:creator><dc:creator>Dingjiang Huang</dc:creator><dc:creator>Yefeng Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.
Published: 2025-12-28T06:12:28+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingchao Wang; Kaiwen Zhou; Zhijian Wu; Kunhua Ji; Dingjiang Huang; Yefeng Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target&amp;#x27;s previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.&lt;/p&gt;</content:encoded></item><item><title>A Fuzzy-Embedded Edge Enhancement Network via Segment Anything Model for VHR Remote Sensing Images Change Detection</title><link>https://doi.org/10.1109/tgrs.2025.3649220</link><guid>10.1109/tgrs.2025.3649220</guid><pubDate>Mon, 29 Dec 2025 18:38:33 +0000</pubDate><dc:creator>Zhiyuan Yang</dc:creator><dc:creator>Jindong Xu</dc:creator><dc:creator>Mengying Ni</dc:creator><dc:creator>Menghui Su</dc:creator><dc:creator>Jiantao Peng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649220</prism:doi><description>Change detection (CD) is a fundamental yet challenging task for remote sensing (RS), with wide applications in land cover monitoring, urban planning, disaster assessment, and environmental management. In recent years, vision foundation models (VFMs) have made breakthroughs in the field of computer vision. With their powerful zero-shot generalization ability, VFMs, represented by the Segment Anything Model (SAM), have recently been applied to remote sensing change detection (RSCD). However, existing FastSAM-based detection methods still suffer from significant shortcomings in edge detection. To address the problems of edge blurring and missed detection of small-scale changes in very high resolution (VHR) remote sensing image (RSI) CD, this paper proposes an edge-enhanced detection framework, FEENet, which improves FastSAM for CD on RSIs. For accurate edge detail extraction, an Edge-Aware Module (EAM) is designed to fuse frequency-domain enhancement with semantic features. Furthermore, a Multi-scale Fuzzy Edge-Guided Fusion Module (MFEFM) is innovatively proposed, where Gaussian fuzzy logic is employed to model edge uncertainty, and dynamic feature injection is adopted to strengthen the association between edge semantics and changed regions. Experiments on five public datasets, LEVIR-CD, WHU-CD, SYSU-CD, CLCD and S2Looking, demonstrate that FEENet achieves better detection performance than existing methods, especially in detecting subtle changes in edge regions such as building contours and road boundaries, thus providing an effective solution for edge-aware CD tasks in VHR RSIs.
Published: 2025-12-29T18:38:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyuan Yang; Jindong Xu; Mengying Ni; Menghui Su; Jiantao Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649220"&gt;10.1109/tgrs.2025.3649220&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Change detection (CD) is a fundamental yet challenging task for remote sensing (RS), with wide applications in land cover monitoring, urban planning, disaster assessment, and environmental management. In recent years, vision foundation models (VFMs) have made breakthroughs in the field of computer vision. With their powerful zero-shot generalization ability, VFMs, represented by the Segment Anything Model (SAM), have recently been applied to remote sensing change detection (RSCD). However, existing FastSAM-based detection methods still suffer from significant shortcomings in edge detection. To address the problems of edge blurring and missed detection of small-scale changes in very high resolution (VHR) remote sensing image (RSI) CD, this paper proposes an edge-enhanced detection framework, FEENet, which improves FastSAM for CD on RSIs. For accurate edge detail extraction, an Edge-Aware Module (EAM) is designed to fuse frequency-domain enhancement with semantic features. Furthermore, a Multi-scale Fuzzy Edge-Guided Fusion Module (MFEFM) is innovatively proposed, where Gaussian fuzzy logic is employed to model edge uncertainty, and dynamic feature injection is adopted to strengthen the association between edge semantics and changed regions. Experiments on five public datasets, LEVIR-CD, WHU-CD, SYSU-CD, CLCD and S2Looking, demonstrate that FEENet achieves better detection performance than existing methods, especially in detecting subtle changes in edge regions such as building contours and road boundaries, thus providing an effective solution for edge-aware CD tasks in VHR RSIs.&lt;/p&gt;</content:encoded></item><item><title>TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts</title><link>https://arxiv.org/abs/2512.22748v1</link><guid>http://arxiv.org/abs/2512.22748v1</guid><pubDate>Sun, 28 Dec 2025 02:40:56 +0000</pubDate><dc:creator>Hao Zhang</dc:creator><dc:creator>Mengsi Lyu</dc:creator><dc:creator>Bo Huang</dc:creator><dc:creator>Yulong Ao</dc:creator><dc:creator>Yonghua Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.
Published: 2025-12-28T02:40:56+00:00
Venue: arXiv
Score: 0.520 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhang; Mengsi Lyu; Bo Huang; Yulong Ao; Yonghua Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.520 (consider)&lt;/p&gt;
&lt;p&gt;Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.&lt;/p&gt;</content:encoded></item><item><title>CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</title><link>https://arxiv.org/abs/2512.23453v1</link><guid>http://arxiv.org/abs/2512.23453v1</guid><pubDate>Mon, 29 Dec 2025 13:23:20 +0000</pubDate><dc:creator>Zongsheng Cao</dc:creator><dc:creator>Yangfan He</dc:creator><dc:creator>Anran Liu</dc:creator><dc:creator>Jun Xie</dc:creator><dc:creator>Feng Chen</dc:creator><dc:creator>Zepeng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.
Published: 2025-12-29T13:23:20+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongsheng Cao; Yangfan He; Anran Liu; Jun Xie; Feng Chen; Zepeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.&lt;/p&gt;</content:encoded></item><item><title>Multi-label Classification with Panoptic Context Aggregation Networks</title><link>https://arxiv.org/abs/2512.23486v1</link><guid>http://arxiv.org/abs/2512.23486v1</guid><pubDate>Mon, 29 Dec 2025 14:16:21 +0000</pubDate><dc:creator>Mingyuan Jiu</dc:creator><dc:creator>Hailong Zhu</dc:creator><dc:creator>Wenchuan Wei</dc:creator><dc:creator>Hichem Sahbi</dc:creator><dc:creator>Rongrong Ji</dc:creator><dc:creator>Mingliang Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.
Published: 2025-12-29T14:16:21+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyuan Jiu; Hailong Zhu; Wenchuan Wei; Hichem Sahbi; Rongrong Ji; Mingliang Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.&lt;/p&gt;</content:encoded></item><item><title>MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning</title><link>https://arxiv.org/abs/2512.23369v1</link><guid>http://arxiv.org/abs/2512.23369v1</guid><pubDate>Mon, 29 Dec 2025 10:58:40 +0000</pubDate><dc:creator>Shuyuan Lin</dc:creator><dc:creator>Mengtin Lo</dc:creator><dc:creator>Haosheng Chen</dc:creator><dc:creator>Yanjie Liang</dc:creator><dc:creator>Qiangqiang Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.24963/ijcai.2025/172</prism:doi><description>Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.
Published: 2025-12-29T10:58:40+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuyuan Lin; Mengtin Lo; Haosheng Chen; Yanjie Liang; Qiangqiang Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.24963/ijcai.2025/172"&gt;10.24963/ijcai.2025/172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.&lt;/p&gt;</content:encoded></item><item><title>Roof-aware indoor BIM reconstruction from LiDAR via graph-attention for residential buildings</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.024</link><guid>10.1016/j.isprsjprs.2025.12.024</guid><pubDate>Mon, 29 Dec 2025 14:45:50 +0000</pubDate><dc:creator>Biao Xiong</dc:creator><dc:creator>Bohan Wang</dc:creator><dc:creator>Yiyi Liu</dc:creator><dc:creator>Liangliang Wang</dc:creator><dc:creator>Yanchao Yang</dc:creator><dc:creator>Liang Zhou</dc:creator><dc:creator>Qiegen Liu</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.024</prism:doi><description>Building Information Models (BIMs) provide structured, parametric representations that are fundamental for simulation, facility management, and digital twin applications. However, reconstructing BIMs from terrestrial LiDAR scans remains challenging due to clutter, occlusions, and the geometric complexity of roof structures. This paper presents a roof-aware scan-to-BIM pipeline tailored for residential buildings, which processes indoor LiDAR data through four geometric abstractions, raw points, superpoints, triangle meshes, and volumetric polyhedra, each represented by task-specific graphs. The pipeline integrates three modules: LGNet for semantic segmentation, QTNet for floor plan reconstruction, and PPO for roof–floor fusion. It demonstrates strong cross-dataset generalization, being trained on Structured3D and fine-tuned on the real-world WHUTS dataset. The method produces watertight, Revit-compatible BIMs with an average surface deviation of 9 mm RMS on WHUTS scenes featuring slanted roofs. Compared with state-of-the-art scan-to-BIM and floor plan reconstruction methods, the proposed approach achieves higher geometric accuracy on scenes with slanted roofs, reducing surface reconstruction error by over 12–18% and improving layout reconstruction F1-scores by up to 6–8% . The proposed framework provides a robust, accurate, and fully automated solution for roof-aware BIM reconstruction of residential buildings from terrestrial LiDAR data, offering comprehensive support for slanted roof modeling. The source code and datasets are publicly available at https://github.com/Wangbohan-x/roof-aware-scan2bim.git .
Published: 2025-12-29T14:45:50+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Biao Xiong; Bohan Wang; Yiyi Liu; Liangliang Wang; Yanchao Yang; Liang Zhou; Qiegen Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.024"&gt;10.1016/j.isprsjprs.2025.12.024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Building Information Models (BIMs) provide structured, parametric representations that are fundamental for simulation, facility management, and digital twin applications. However, reconstructing BIMs from terrestrial LiDAR scans remains challenging due to clutter, occlusions, and the geometric complexity of roof structures. This paper presents a roof-aware scan-to-BIM pipeline tailored for residential buildings, which processes indoor LiDAR data through four geometric abstractions, raw points, superpoints, triangle meshes, and volumetric polyhedra, each represented by task-specific graphs. The pipeline integrates three modules: LGNet for semantic segmentation, QTNet for floor plan reconstruction, and PPO for roof–floor fusion. It demonstrates strong cross-dataset generalization, being trained on Structured3D and fine-tuned on the real-world WHUTS dataset. The method produces watertight, Revit-compatible BIMs with an average surface deviation of 9 mm RMS on WHUTS scenes featuring slanted roofs. Compared with state-of-the-art scan-to-BIM and floor plan reconstruction methods, the proposed approach achieves higher geometric accuracy on scenes with slanted roofs, reducing surface reconstruction error by over 12–18% and improving layout reconstruction F1-scores by up to 6–8% . The proposed framework provides a robust, accurate, and fully automated solution for roof-aware BIM reconstruction of residential buildings from terrestrial LiDAR data, offering comprehensive support for slanted roof modeling. The source code and datasets are publicly available at https://github.com/Wangbohan-x/roof-aware-scan2bim.git .&lt;/p&gt;</content:encoded></item></channel></rss>