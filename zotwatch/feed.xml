<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 25 Dec 2025 02:46:48 +0000</lastBuildDate><item><title>Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion</title><link>https://doi.org/10.1109/tip.2025.3635475</link><guid>10.1109/tip.2025.3635475</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Haihong Xiao</dc:creator><dc:creator>Wenxiong Kang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Ying He</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635475</prism:doi><description>Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.572 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haihong Xiao; Wenxiong Kang; Yulan Guo; Hao Liu; Ying He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635475"&gt;10.1109/tip.2025.3635475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.572 (consider)&lt;/p&gt;
&lt;p&gt;Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Models for Person Re-identification: A Survey and Outlook</title><link>https://doi.org/10.1016/j.inffus.2025.104095</link><guid>10.1016/j.inffus.2025.104095</guid><pubDate>Wed, 24 Dec 2025 16:39:57 +0000</pubDate><dc:creator>Guorong Lin</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Zuoyong Li</dc:creator><dc:creator>Yao Lu</dc:creator><dc:creator>Xiaowen Ma</dc:creator><dc:creator>Zhenhua Huang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104095</prism:doi><description>Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.
Published: 2025-12-24T16:39:57+00:00
Venue: Information Fusion
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guorong Lin; Wei-Shi Zheng; Zuoyong Li; Yao Lu; Xiaowen Ma; Zhenhua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104095"&gt;10.1016/j.inffus.2025.104095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.&lt;/p&gt;</content:encoded></item><item><title>UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</title><link>https://arxiv.org/abs/2512.21078v1</link><guid>http://arxiv.org/abs/2512.21078v1</guid><pubDate>Wed, 24 Dec 2025 09:55:16 +0000</pubDate><dc:creator>Tianchen Deng</dc:creator><dc:creator>Xun Chen</dc:creator><dc:creator>Ziming Li</dc:creator><dc:creator>Hongming Shen</dc:creator><dc:creator>Danwei Wang</dc:creator><dc:creator>Javier Civera</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.
Published: 2025-12-24T09:55:16+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianchen Deng; Xun Chen; Ziming Li; Hongming Shen; Danwei Wang; Javier Civera; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647855</link><guid>10.1109/tpami.2025.3647855</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Jingtao Sun</dc:creator><dc:creator>Yaonan Wang</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Chao Ding</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647855</prism:doi><description>Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.541 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingtao Sun; Yaonan Wang; Mingtao Feng; Chao Ding; Mike Zheng Shou; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647855"&gt;10.1109/tpami.2025.3647855&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.541 (consider)&lt;/p&gt;
&lt;p&gt;Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.&lt;/p&gt;</content:encoded></item><item><title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title><link>https://doi.org/10.1016/j.inffus.2025.104074</link><guid>10.1016/j.inffus.2025.104074</guid><pubDate>Tue, 23 Dec 2025 16:57:18 +0000</pubDate><dc:creator>Yihao Ding</dc:creator><dc:creator>Soyeon Caren Han</dc:creator><dc:creator>Zechuan Li</dc:creator><dc:creator>Hyunsuk Chung</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104074</prism:doi><description>Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
Published: 2025-12-23T16:57:18+00:00
Venue: Information Fusion
Score: 0.540 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihao Ding; Soyeon Caren Han; Zechuan Li; Hyunsuk Chung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104074"&gt;10.1016/j.inffus.2025.104074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.540 (consider)&lt;/p&gt;
&lt;p&gt;Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.&lt;/p&gt;</content:encoded></item><item><title>PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</title><link>https://arxiv.org/abs/2512.20907v1</link><guid>http://arxiv.org/abs/2512.20907v1</guid><pubDate>Wed, 24 Dec 2025 03:18:51 +0000</pubDate><dc:creator>Seongmin Jung</dc:creator><dc:creator>Seongho Choi</dc:creator><dc:creator>Gunwoo Jeon</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jongwoo Lim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.
Published: 2025-12-24T03:18:51+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seongmin Jung; Seongho Choi; Gunwoo Jeon; Minsu Cho; Jongwoo Lim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.&lt;/p&gt;</content:encoded></item><item><title>Dual Consistency Matching for Semi-Supervised Semantic Correspondence</title><link>https://doi.org/10.1007/s11263-025-02652-8</link><guid>10.1007/s11263-025-02652-8</guid><pubDate>Wed, 24 Dec 2025 17:57:36 +0000</pubDate><dc:creator>Hailong Jin</dc:creator><dc:creator>Huiying Li</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02652-8</prism:doi><description>Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.
Published: 2025-12-24T17:57:36+00:00
Venue: International Journal of Computer Vision
Score: 0.535 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hailong Jin; Huiying Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02652-8"&gt;10.1007/s11263-025-02652-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.535 (consider)&lt;/p&gt;
&lt;p&gt;Establishing correspondences across images sharing the same category remains a challenging task, primarily due to the large intra-class variations and the presence of background clutter. Typically, addressing these challenges necessitates an extensive amount of manually labeled data. However, pixel-level labeling is both time-consuming and labor-intensive. In this paper, we propose a novel teacher-student framework for semi-supervised semantic correspondence, termed Dual Consistency Matching (DCM). We introduce neighborhood shift consistency and semantic consistency to generate reliable pseudo labels, ensuring geometric and semantic coherence, respectively. Unlike previous methods relying on nearest neighbor search at the pixel level, our neighborhood shift consistency enables subpixel accurate estimation through offsets. Semantic consistency, on the other hand, aims to filter out matches by discarding matches that lack semantic coherence despite geometric consistency. Additionally, we propose part-aware prototype learning to impose spatial constraints through the identification of key parts of the object. These filtering strategies enhance the quality of pseudo labels generated by the teacher model. Our framework leverages a pre-trained large-scale vision model as the backbone, which is fine-tuned to improve its representation capabilities. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method. It is worth noting that our proposed DCM achieves 79.6% PCK@0.1 on the SPair-71k dataset using only 1% labeled data, with an average of 29 training samples per category. Moreover, if 5% labeled data is employed, our method obtains a significantly higher PCK@0.1 score of 86.9%, even surpassing the performance of a fully supervised model trained with all labeled data.&lt;/p&gt;</content:encoded></item><item><title>Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647829</link><guid>10.1109/tpami.2025.3647829</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Yifei Shi</dc:creator><dc:creator>Boyan Wan</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647829</prism:doi><description>Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Shi; Boyan Wan; Xin Xu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647829"&gt;10.1109/tpami.2025.3647829&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&amp;#x27;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&amp;#x27;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&amp;#x27;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...&lt;/p&gt;</content:encoded></item><item><title>Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</title><link>https://arxiv.org/abs/2512.21221v1</link><guid>http://arxiv.org/abs/2512.21221v1</guid><pubDate>Wed, 24 Dec 2025 15:02:33 +0000</pubDate><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Phu-Hoa Pham</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval
Published: 2025-12-24T15:02:33+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dao Sy Duy Minh; Huynh Trung Kiet; Nguyen Lam Phu Quy; Phu-Hoa Pham; Tran Chi Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval&lt;/p&gt;</content:encoded></item><item><title>The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</title><link>https://arxiv.org/abs/2512.19693v1</link><guid>http://arxiv.org/abs/2512.19693v1</guid><pubDate>Mon, 22 Dec 2025 18:59:57 +0000</pubDate><dc:creator>Weichen Fan</dc:creator><dc:creator>Haiwen Diao</dc:creator><dc:creator>Quan Wang</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.
Published: 2025-12-22T18:59:57+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weichen Fan; Haiwen Diao; Quan Wang; Dahua Lin; Ziwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder&amp;#x27;s feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning</title><link>https://doi.org/10.1109/tgrs.2025.3648057</link><guid>10.1109/tgrs.2025.3648057</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Lanxiao Wang</dc:creator><dc:creator>Heqian Qiu</dc:creator><dc:creator>Minjian Zhang</dc:creator><dc:creator>Fanman Meng</dc:creator><dc:creator>Qingbo Wu</dc:creator><dc:creator>Hongliang Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648057</prism:doi><description>Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lanxiao Wang; Heqian Qiu; Minjian Zhang; Fanman Meng; Qingbo Wu; Hongliang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648057"&gt;10.1109/tgrs.2025.3648057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Semantic representation of cross-modal events based on social multi-view graph attention network</title><link>https://doi.org/10.1016/j.ins.2025.123025</link><guid>10.1016/j.ins.2025.123025</guid><pubDate>Tue, 23 Dec 2025 07:38:00 +0000</pubDate><dc:creator>Wanqiu Cui</dc:creator><dc:creator>Dawei Wang</dc:creator><dc:creator>Wengang Feng</dc:creator><dc:creator>Jingjing Lu</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123025</prism:doi><description>Semantic representation learning is a crucial technology for analyzing and detecting public opinion events propagated through social networks. However, the randomness and sparsity of news present significant challenges to effective semantic learning. In addition to text and image, temporal information is another vital element for accurately reflecting events. Therefore, we propose a Multi-View Graph Attention Network guided by Time disruption information (T-MVGAN). This model enhances event semantics by employing neighbor aggregation and multi-view fusion within a heterogeneous cross-modal event graph. Specifically, we construct a cross-modal heterogeneous graph by incorporating hashtags to link isolated messages and provide a comprehensive description of events. Then, we learn view-specific representations of events via graph convolutional networks, considering the perspectives of text, image, and temporal distribution, respectively. Finally, we design a time-based multi-view graph attention mechanism to capture the intrinsic interactions across different views and integrate their feature representations. This approach learns the deep semantics of social network events by transforming diverse observations into a unified semantic space. Extensive experiments on public Twitter datasets reveal that T-MVGAN performs favorably compared to many state-of-the-art semantic learning algorithms. These results also show that more meaningful signals, such as publication time and hashtags, can contribute to enhancing the performance of semantic learning.
Published: 2025-12-23T07:38:00+00:00
Venue: Information Sciences
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanqiu Cui; Dawei Wang; Wengang Feng; Jingjing Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123025"&gt;10.1016/j.ins.2025.123025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Semantic representation learning is a crucial technology for analyzing and detecting public opinion events propagated through social networks. However, the randomness and sparsity of news present significant challenges to effective semantic learning. In addition to text and image, temporal information is another vital element for accurately reflecting events. Therefore, we propose a Multi-View Graph Attention Network guided by Time disruption information (T-MVGAN). This model enhances event semantics by employing neighbor aggregation and multi-view fusion within a heterogeneous cross-modal event graph. Specifically, we construct a cross-modal heterogeneous graph by incorporating hashtags to link isolated messages and provide a comprehensive description of events. Then, we learn view-specific representations of events via graph convolutional networks, considering the perspectives of text, image, and temporal distribution, respectively. Finally, we design a time-based multi-view graph attention mechanism to capture the intrinsic interactions across different views and integrate their feature representations. This approach learns the deep semantics of social network events by transforming diverse observations into a unified semantic space. Extensive experiments on public Twitter datasets reveal that T-MVGAN performs favorably compared to many state-of-the-art semantic learning algorithms. These results also show that more meaningful signals, such as publication time and hashtags, can contribute to enhancing the performance of semantic learning.&lt;/p&gt;</content:encoded></item><item><title>PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</title><link>https://arxiv.org/abs/2512.18660v1</link><guid>http://arxiv.org/abs/2512.18660v1</guid><pubDate>Sun, 21 Dec 2025 09:16:11 +0000</pubDate><dc:creator>Pengxiang Ouyang</dc:creator><dc:creator>Qing Ma</dc:creator><dc:creator>Zheng Wang</dc:creator><dc:creator>Cong Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.
Published: 2025-12-21T09:16:11+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengxiang Ouyang; Qing Ma; Zheng Wang; Cong Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.&lt;/p&gt;</content:encoded></item><item><title>Data Augmentation with Attentional Feature Aggregation for Node Classification in GNNs</title><link>https://doi.org/10.1016/j.inffus.2025.104089</link><guid>10.1016/j.inffus.2025.104089</guid><pubDate>Wed, 24 Dec 2025 16:05:48 +0000</pubDate><dc:creator>Guangquan Lu</dc:creator><dc:creator>Shilong Lin</dc:creator><dc:creator>Yuxuan Hu</dc:creator><dc:creator>Debo Cheng</dc:creator><dc:creator>Chen Li</dc:creator><dc:creator>Shichao Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104089</prism:doi><description>Graph Neural Networks (GNNs) have demonstrated remarkable success in classification tasks on graphs, including multimedia applications such as image recognition, video analysis, and recommendation systems. However, most GNNs methods assume that the category of samples is balanced, which contradicts real-world class distribution. In practice, imbalanced category distribution often causes GNNs to neglect minority-class nodes during training, which in turn negatively impacts overall classification performance. Existing methods still face key challenges, including insufficient feature learning and inadequate generation of node homogeneity. To tackle these challenges, we propose GraphAFA, a novel Graph -based method that utilizes A ttentional F eature A ggregation to generate a small number of synthetic class nodes, thereby promoting sample equilibrium. GraphAFA consists of two key components: attention-based feature extraction and neighbor-aware node aggregation. Firstly, GraphAFA constructs a feature space and utilizes an attention mechanism to extract node features, enabling effective learning higher-order relationships among nodes. Secondly, during the node generation process, GraphAFA aggregates information from neighboring nodes to capture shared features, ensuring the newly generated nodes are more homogeneous and reducing the risk of generating heterogeneous samples. Finally, GraphAFA connects edges to the newly generated nodes, integrating them into the graph for downstream classification. Comprehensive experiments on three benchmark datasets show that GraphAFA consistently outperforms state-of-the-art methods in class-imbalanced node classification.
Published: 2025-12-24T16:05:48+00:00
Venue: Information Fusion
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangquan Lu; Shilong Lin; Yuxuan Hu; Debo Cheng; Chen Li; Shichao Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104089"&gt;10.1016/j.inffus.2025.104089&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Graph Neural Networks (GNNs) have demonstrated remarkable success in classification tasks on graphs, including multimedia applications such as image recognition, video analysis, and recommendation systems. However, most GNNs methods assume that the category of samples is balanced, which contradicts real-world class distribution. In practice, imbalanced category distribution often causes GNNs to neglect minority-class nodes during training, which in turn negatively impacts overall classification performance. Existing methods still face key challenges, including insufficient feature learning and inadequate generation of node homogeneity. To tackle these challenges, we propose GraphAFA, a novel Graph -based method that utilizes A ttentional F eature A ggregation to generate a small number of synthetic class nodes, thereby promoting sample equilibrium. GraphAFA consists of two key components: attention-based feature extraction and neighbor-aware node aggregation. Firstly, GraphAFA constructs a feature space and utilizes an attention mechanism to extract node features, enabling effective learning higher-order relationships among nodes. Secondly, during the node generation process, GraphAFA aggregates information from neighboring nodes to capture shared features, ensuring the newly generated nodes are more homogeneous and reducing the risk of generating heterogeneous samples. Finally, GraphAFA connects edges to the newly generated nodes, integrating them into the graph for downstream classification. Comprehensive experiments on three benchmark datasets show that GraphAFA consistently outperforms state-of-the-art methods in class-imbalanced node classification.&lt;/p&gt;</content:encoded></item><item><title>Vision Model Fine-tuning based on Two-level Prompts Fusion</title><link>https://doi.org/10.1016/j.knosys.2025.115185</link><guid>10.1016/j.knosys.2025.115185</guid><pubDate>Wed, 24 Dec 2025 00:12:37 +0000</pubDate><dc:creator>Keming Mao</dc:creator><dc:creator>Haoming Fang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115185</prism:doi><description>Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.
Published: 2025-12-24T00:12:37+00:00
Venue: Knowledge-Based Systems
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keming Mao; Haoming Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115185"&gt;10.1016/j.knosys.2025.115185&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Prompt learning is an efficient method for fine-tuning pre-trained Visual Language Models for specific downstream tasks. Recent studies have primarily focused on designing prompt structures. However, the complex relationship between image tokens and visual prompts, especially the integration of global and local information, remains underexplored. In this paper, we propose a two-level visual prompt fusion fine-tuning framework to address these challenges. Our method introduces two distinct visual prompts that capture the global and local spatial information of image tokens. For local prompt, we introduce the Deformable Spatial Alignment(DSA) module and Rigid Spatial Alignment(RSA) module to capture local spatial features with selective and fixed manners in shallow and deep layers, respectively. Additionally, we design the Internal Correlation(IC) module to capture the influence of local neighbors through interactions within the local prompts. For global prompt, we incorporate an Inheritance Mechanism(IM) to preserve and transfer the influence of shallow-layer features, preventing information loss as it propagates to deeper layers. Extensive experiments on three benchmarks demonstrate that TVPF consistently achieves superior performance across diverse visual downstream tasks, surpassing VPT-deep and SA 2 VP by an average of 3.7% and 0.6%.&lt;/p&gt;</content:encoded></item><item><title>Spatial–Frequency Feature Coupling Network for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3645809</link><guid>10.1109/jstars.2025.3645809</guid><pubDate>Tue, 23 Dec 2025 18:31:14 +0000</pubDate><dc:creator>Shipeng Tian</dc:creator><dc:creator>Zhongda Lu</dc:creator><dc:creator>Fengxia Xu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3645809</prism:doi><description>Semantic segmentation of high-resolution remote sensing images is a challenging task due to the diverse scale variations and significant intra-class variability of ground objects. Existing methods fail to establish an effective deep coupling mechanism between spatial- and frequency-domain features, resulting in limited capability for modeling fine-grained textures and periodic structures. To overcome these limitations, this paper proposes a novel spatial–frequency feature coupling network (SFFCNet), which dynamically decouples spatial and frequency features and establishes an cross-domain fusion mechanism to enhance both fine-grained texture and global context modeling. Specifically, the dual-domain feature coupling (DDFC) module adopts a dual-branch design, decoupling frequency- and spatial-domain features through spectral energy separation and gradient enhancement strategies, and then coupling them via a cross-domain feature fusion module to support fine-grained representation of ground objects. The cognitive state space (CSS) module captures sequential dependencies among targets through four designed scanning paths, effectively mitigating the impact of intra-class variability. The global–local feature integration (GLFI) module exploits the positional invariance and semantic similarity of extracted features to model relative spatial relationships between objects, thereby enhancing the completeness and discriminative power of semantic representations. Extensive experiments on four datasets demonstrate that the proposed SFFCNet exhibits superior performance compared to existing state-of-the-art methods.
Published: 2025-12-23T18:31:14+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shipeng Tian; Zhongda Lu; Fengxia Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3645809"&gt;10.1109/jstars.2025.3645809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of high-resolution remote sensing images is a challenging task due to the diverse scale variations and significant intra-class variability of ground objects. Existing methods fail to establish an effective deep coupling mechanism between spatial- and frequency-domain features, resulting in limited capability for modeling fine-grained textures and periodic structures. To overcome these limitations, this paper proposes a novel spatial–frequency feature coupling network (SFFCNet), which dynamically decouples spatial and frequency features and establishes an cross-domain fusion mechanism to enhance both fine-grained texture and global context modeling. Specifically, the dual-domain feature coupling (DDFC) module adopts a dual-branch design, decoupling frequency- and spatial-domain features through spectral energy separation and gradient enhancement strategies, and then coupling them via a cross-domain feature fusion module to support fine-grained representation of ground objects. The cognitive state space (CSS) module captures sequential dependencies among targets through four designed scanning paths, effectively mitigating the impact of intra-class variability. The global–local feature integration (GLFI) module exploits the positional invariance and semantic similarity of extracted features to model relative spatial relationships between objects, thereby enhancing the completeness and discriminative power of semantic representations. Extensive experiments on four datasets demonstrate that the proposed SFFCNet exhibits superior performance compared to existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>PCIR: An Open-World Remote Sensing Image Representation Learning Method from a Causal Perspective</title><link>https://doi.org/10.1109/tgrs.2025.3647787</link><guid>10.1109/tgrs.2025.3647787</guid><pubDate>Tue, 23 Dec 2025 18:31:04 +0000</pubDate><dc:creator>Ling Zhao</dc:creator><dc:creator>Mengyao Li</dc:creator><dc:creator>Run Shao</dc:creator><dc:creator>Haifeng Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647787</prism:doi><description>In the open world, visual representations of remote sensing images vary significantly and diversely. A model can correctly recognize and process unknown data in changing and open-ended scenarios only by learning invariant visual representations from the data. Causal diagram models, which describe the data generation process, can employ interventions and counterfactual operations to obtain invariant visual representations in such open environments, i.e., generic causal representations. However, these generic causal diagram models often overlook the multi-feature, multi-confounding, and multi-change characteristics of remote sensing images, as well as the causal differences these factors introduce into the imaging process. This limitation makes it difficult for the learned representations to be widely transferable in open scenes. Specifically, we argue that different objects are influenced by different environments, and the styles they exhibit in remote sensing images consequently vary. Therefore, the causal diagrams corresponding to these objects also differ. Intervening at the object-level causal diagram is crucial for obtaining object-specific causal representations and mitigating the problem of feature confusion.Based on this assumption, we derive the general form of the object-level causal diagram from the generic causal diagram, taking into account the characteristics of remote sensing images with multiple features and multiple confounders. We propose a remote sensing representation learning method, PCIR, which is based on a causal partition intervention strategy. The method consists of two main components: the Causal Partition Intervention (CPI) module and the Causal Representation Separation (CRS) module. 1) CPI module: To simulate the style differences caused by environmental influences on different objects, we propose remote sensing offset data augmentation. We then use target detection labels to identify independent object regions in the image and apply fea...
Published: 2025-12-23T18:31:04+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ling Zhao; Mengyao Li; Run Shao; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647787"&gt;10.1109/tgrs.2025.3647787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;In the open world, visual representations of remote sensing images vary significantly and diversely. A model can correctly recognize and process unknown data in changing and open-ended scenarios only by learning invariant visual representations from the data. Causal diagram models, which describe the data generation process, can employ interventions and counterfactual operations to obtain invariant visual representations in such open environments, i.e., generic causal representations. However, these generic causal diagram models often overlook the multi-feature, multi-confounding, and multi-change characteristics of remote sensing images, as well as the causal differences these factors introduce into the imaging process. This limitation makes it difficult for the learned representations to be widely transferable in open scenes. Specifically, we argue that different objects are influenced by different environments, and the styles they exhibit in remote sensing images consequently vary. Therefore, the causal diagrams corresponding to these objects also differ. Intervening at the object-level causal diagram is crucial for obtaining object-specific causal representations and mitigating the problem of feature confusion.Based on this assumption, we derive the general form of the object-level causal diagram from the generic causal diagram, taking into account the characteristics of remote sensing images with multiple features and multiple confounders. We propose a remote sensing representation learning method, PCIR, which is based on a causal partition intervention strategy. The method consists of two main components: the Causal Partition Intervention (CPI) module and the Causal Representation Separation (CRS) module. 1) CPI module: To simulate the style differences caused by environmental influences on different objects, we propose remote sensing offset data augmentation. We then use target detection labels to identify independent object regions in the image and apply fea...&lt;/p&gt;</content:encoded></item><item><title>Audio-Guided Video Scene Editing</title><link>https://doi.org/10.1007/s11263-025-02640-y</link><guid>10.1007/s11263-025-02640-y</guid><pubDate>Wed, 24 Dec 2025 12:00:27 +0000</pubDate><dc:creator>Kaixin Shen</dc:creator><dc:creator>Ruijie Quan</dc:creator><dc:creator>Linchao Zhu</dc:creator><dc:creator>Dong Zheng</dc:creator><dc:creator>Jun Xiao</dc:creator><dc:creator>Yi Yang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02640-y</prism:doi><description>This paper introduces audio-guided video scene editing, a new task that dynamically modifies the background of a video based on audio input while preserving the integrity of the foreground. To address this challenge, we propose AudioScenic, a specialized framework designed to achieve four core objectives: integrating audio semantics into video scenes, maintaining the foreground’s visual stability, ensuring alignment between audio condition and visual changes, and upholding temporal consistency throughout the video. At the heart of AudioScenic is a temporal-aware audio semantic injection mechanism, which effectively integrates audio conditions into the video background. To prevent undesired distortions in the foreground, we introduce SceneMasker, which ensures that foreground elements remain unaffected during editing. Furthermore, AudioScenic incorporates two key components: the Magnitude Modulator, which strengthens synchronization between auditory and visual elements over time, and the Frequency Fuser, which exploits the inherent frequency correlations between audio and video to enhance temporal coherence. With these innovations, AudioScenic generates visually diverse, seamlessly synchronized video scenes that adapt to the given audio while maintaining smooth frame transitions. Additionally, we introduce a new evaluation metric, the Semantic-Temporal Consistency Score (STC-Score), which offers a more comprehensive measurement of temporal consistency in video scene editing. Experimental results demonstrate that AudioScenic significantly outperforms existing methods, establishing it as a robust solution for synchronized video scene editing.
Published: 2025-12-24T12:00:27+00:00
Venue: International Journal of Computer Vision
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaixin Shen; Ruijie Quan; Linchao Zhu; Dong Zheng; Jun Xiao; Yi Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02640-y"&gt;10.1007/s11263-025-02640-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;This paper introduces audio-guided video scene editing, a new task that dynamically modifies the background of a video based on audio input while preserving the integrity of the foreground. To address this challenge, we propose AudioScenic, a specialized framework designed to achieve four core objectives: integrating audio semantics into video scenes, maintaining the foreground’s visual stability, ensuring alignment between audio condition and visual changes, and upholding temporal consistency throughout the video. At the heart of AudioScenic is a temporal-aware audio semantic injection mechanism, which effectively integrates audio conditions into the video background. To prevent undesired distortions in the foreground, we introduce SceneMasker, which ensures that foreground elements remain unaffected during editing. Furthermore, AudioScenic incorporates two key components: the Magnitude Modulator, which strengthens synchronization between auditory and visual elements over time, and the Frequency Fuser, which exploits the inherent frequency correlations between audio and video to enhance temporal coherence. With these innovations, AudioScenic generates visually diverse, seamlessly synchronized video scenes that adapt to the given audio while maintaining smooth frame transitions. Additionally, we introduce a new evaluation metric, the Semantic-Temporal Consistency Score (STC-Score), which offers a more comprehensive measurement of temporal consistency in video scene editing. Experimental results demonstrate that AudioScenic significantly outperforms existing methods, establishing it as a robust solution for synchronized video scene editing.&lt;/p&gt;</content:encoded></item><item><title>Hypergraph Foundation Model</title><link>https://doi.org/10.1109/tpami.2025.3647504</link><guid>10.1109/tpami.2025.3647504</guid><pubDate>Tue, 23 Dec 2025 18:31:03 +0000</pubDate><dc:creator>Yue Gao</dc:creator><dc:creator>Yifan Feng</dc:creator><dc:creator>Shiquan Liu</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Shaoyi Du</dc:creator><dc:creator>Zongze Wu</dc:creator><dc:creator>Han Hu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647504</prism:doi><description>Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
Published: 2025-12-23T18:31:03+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Gao; Yifan Feng; Shiquan Liu; Xiangmin Han; Shaoyi Du; Zongze Wu; Han Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647504"&gt;10.1109/tpami.2025.3647504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.&lt;/p&gt;</content:encoded></item><item><title>Vehicle-centric Perception via Multimodal Structured Pre-training</title><link>https://arxiv.org/abs/2512.19934v1</link><guid>http://arxiv.org/abs/2512.19934v1</guid><pubDate>Mon, 22 Dec 2025 23:42:45 +0000</pubDate><dc:creator>Wentao Wu</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.
Published: 2025-12-22T23:42:45+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Wu; Xiao Wang; Chenglong Li; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model&amp;#x27;s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.&lt;/p&gt;</content:encoded></item><item><title>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</title><link>https://arxiv.org/abs/2512.20174v1</link><guid>http://arxiv.org/abs/2512.20174v1</guid><pubDate>Tue, 23 Dec 2025 09:14:16 +0000</pubDate><dc:creator>Hao Guo</dc:creator><dc:creator>Xugong Qin</dc:creator><dc:creator>Jun Jie Ou Yang</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Gangyan Zeng</dc:creator><dc:creator>Yubo Li</dc:creator><dc:creator>Hailun Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.
Published: 2025-12-23T09:14:16+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Guo; Xugong Qin; Jun Jie Ou Yang; Peng Zhang; Gangyan Zeng; Yubo Li; Hailun Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.&lt;/p&gt;</content:encoded></item><item><title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title><link>https://arxiv.org/abs/2512.18954v1</link><guid>http://arxiv.org/abs/2512.18954v1</guid><pubDate>Mon, 22 Dec 2025 02:05:45 +0000</pubDate><dc:creator>Zaidao Han</dc:creator><dc:creator>Risa Higashita</dc:creator><dc:creator>Jiang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.
Published: 2025-12-22T02:05:45+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zaidao Han; Risa Higashita; Jiang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking</title><link>https://doi.org/10.1109/tpami.2025.3648020</link><guid>10.1109/tpami.2025.3648020</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hanzheng Wang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Xiang-Gen Xia</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648020</prism:doi><description>Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model's sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanzheng Wang; Wei Li; Xiang-Gen Xia; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648020"&gt;10.1109/tpami.2025.3648020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&amp;#x27;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.&lt;/p&gt;</content:encoded></item><item><title>Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</title><link>https://arxiv.org/abs/2512.20042v1</link><guid>http://arxiv.org/abs/2512.20042v1</guid><pubDate>Tue, 23 Dec 2025 04:21:15 +0000</pubDate><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Pham Phu Hoa</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Nguyen Hoang Minh Ngoc</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding
Published: 2025-12-23T04:21:15+00:00
Venue: arXiv
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nguyen Lam Phu Quy; Pham Phu Hoa; Tran Chi Nguyen; Dao Sy Duy Minh; Nguyen Hoang Minh Ngoc; Huynh Trung Kiet&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding&lt;/p&gt;</content:encoded></item><item><title>ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</title><link>https://arxiv.org/abs/2512.19354v1</link><guid>http://arxiv.org/abs/2512.19354v1</guid><pubDate>Mon, 22 Dec 2025 12:54:26 +0000</pubDate><dc:creator>Zhenyang Huang</dc:creator><dc:creator>Xiao Yu</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Decheng Wang</dc:creator><dc:creator>Hang Ruan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.
Published: 2025-12-22T12:54:26+00:00
Venue: arXiv
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyang Huang; Xiao Yu; Yi Zhang; Decheng Wang; Hang Ruan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users&amp;#x27; CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users&amp;#x27; implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users&amp;#x27; implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.&lt;/p&gt;</content:encoded></item><item><title>MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</title><link>https://arxiv.org/abs/2512.20026v1</link><guid>http://arxiv.org/abs/2512.20026v1</guid><pubDate>Tue, 23 Dec 2025 03:38:57 +0000</pubDate><dc:creator>Ziwei Qin</dc:creator><dc:creator>Xuhui Song</dc:creator><dc:creator>Deqing Huang</dc:creator><dc:creator>Na Qin</dc:creator><dc:creator>Jun Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.
Published: 2025-12-23T03:38:57+00:00
Venue: arXiv
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziwei Qin; Xuhui Song; Deqing Huang; Na Qin; Jun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647662</link><guid>10.1109/tgrs.2025.3647662</guid><pubDate>Tue, 23 Dec 2025 18:31:04 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Zhongyuan Zhou</dc:creator><dc:creator>Dong Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647662</prism:doi><description>Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.
Published: 2025-12-23T18:31:04+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.511 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Zhongyuan Zhou; Dong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647662"&gt;10.1109/tgrs.2025.3647662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.511 (consider)&lt;/p&gt;
&lt;p&gt;Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.&lt;/p&gt;</content:encoded></item><item><title>Class-Domain Incremental Segmentation for Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648015</link><guid>10.1109/tgrs.2025.3648015</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Xingxing Weng</dc:creator><dc:creator>Chao Pang</dc:creator><dc:creator>Jiayu Li</dc:creator><dc:creator>Xiaoqian Sun</dc:creator><dc:creator>Gui-Song Xia</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648015</prism:doi><description>Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingxing Weng; Chao Pang; Jiayu Li; Xiaoqian Sun; Gui-Song Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648015"&gt;10.1109/tgrs.2025.3648015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Significant progress has been made in class-incremental (learning new classes without forgetting old ones) and domain-incremental (adapting to data from different distributions) semantic segmentation for remote sensing images. However, in real-world deployment, class-space changes and distribution shifts may co-occur between old and new data. Existing incremental learning methods typically address only one type of shift, struggling to handle joint class and domain incremental learning. To achieve class-domain incremental segmentation, we propose CDISeg, a novel framework that enables cross-domain knowledge accumulation through feature synthesis. CDISeg employs a temporary style encoder while repurposing the segmentation model’s backbone as the content encoder. By enforcing orthogonality between their outputs, the model disentangles image content from style, thereby preserving domainspecific style features throughout incremental learning steps. The framework synthesizes old-domain features by projecting old-domain styles onto new-domain content, which supports the preservation of old knowledge, extends new classes to past domains, and facilitates the learning of old classes over new-domain images. Additionally, we introduce class-aware style randomization to enhance feature disentanglement and improve synthesis quality. Extensive experiments on ISPRS and Open-EarthMap datasets demonstrate the remarkable superiority of CDISeg in enabling models to progressively acquire new classes from new domains while recognizing all learned classes across all encountered domains.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Sequential Context Modelling for High-Fidelity Image Inpainting</title><link>https://doi.org/10.1109/tcsvt.2025.3647673</link><guid>10.1109/tcsvt.2025.3647673</guid><pubDate>Tue, 23 Dec 2025 18:32:30 +0000</pubDate><dc:creator>Zexuan Sun</dc:creator><dc:creator>Jinjia Peng</dc:creator><dc:creator>Mengkai Li</dc:creator><dc:creator>Huibing Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3647673</prism:doi><description>Image inpainting aims to restore missing regions by leveraging surrounding spatial context, where nearby pixels provide crucial structural cues and distant regions offer complementary semantic guidance. To jointly model these complementary dependencies, this paper proposes Hierarchical Sequential Context Modeling (HSCM), a novel inpainting framework that employs state-space models for multi-scale autoregressive sequence modeling. Unlike existing single-scale SSM-based approaches, HSCM explicitly separates pixel-level and semanticlevel modeling into two complementary branches. The Local Perception Unit preserves fine-grained textures, and the Global Compensation Unit propagates high-level semantics across patches to enhance overall coherence. The asynchronous hierarchical design first reconstructs local textures and then performs semantic compensation, achieving notable performance gains with minimal computational overhead. Leveraging its four-directional architecture, HSCM maintains linear computational growth with spatial resolution and effectively establishes a comprehensive global receptive field. Furthermore, a Cross-Gated Feedforward Network is proposed to alleviate patch boundary artifacts and enhance inter-channel feature consistency. Built upon a multi-scale encoder–decoder architecture, HSCM delivers state-of-the-art inpainting quality and robust generalization across diverse benchmarks, including CelebA-HQ, FFHQ, Paris Street View, and Places2.
Published: 2025-12-23T18:32:30+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zexuan Sun; Jinjia Peng; Mengkai Li; Huibing Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3647673"&gt;10.1109/tcsvt.2025.3647673&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Image inpainting aims to restore missing regions by leveraging surrounding spatial context, where nearby pixels provide crucial structural cues and distant regions offer complementary semantic guidance. To jointly model these complementary dependencies, this paper proposes Hierarchical Sequential Context Modeling (HSCM), a novel inpainting framework that employs state-space models for multi-scale autoregressive sequence modeling. Unlike existing single-scale SSM-based approaches, HSCM explicitly separates pixel-level and semanticlevel modeling into two complementary branches. The Local Perception Unit preserves fine-grained textures, and the Global Compensation Unit propagates high-level semantics across patches to enhance overall coherence. The asynchronous hierarchical design first reconstructs local textures and then performs semantic compensation, achieving notable performance gains with minimal computational overhead. Leveraging its four-directional architecture, HSCM maintains linear computational growth with spatial resolution and effectively establishes a comprehensive global receptive field. Furthermore, a Cross-Gated Feedforward Network is proposed to alleviate patch boundary artifacts and enhance inter-channel feature consistency. Built upon a multi-scale encoder–decoder architecture, HSCM delivers state-of-the-art inpainting quality and robust generalization across diverse benchmarks, including CelebA-HQ, FFHQ, Paris Street View, and Places2.&lt;/p&gt;</content:encoded></item><item><title>Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</title><link>https://arxiv.org/abs/2512.18784v1</link><guid>http://arxiv.org/abs/2512.18784v1</guid><pubDate>Sun, 21 Dec 2025 15:57:13 +0000</pubDate><dc:creator>Fanis Mathioulakis</dc:creator><dc:creator>Gorjan Radevski</dc:creator><dc:creator>Tinne Tuytelaars</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.
Published: 2025-12-21T15:57:13+00:00
Venue: arXiv
Score: 0.509 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fanis Mathioulakis; Gorjan Radevski; Tinne Tuytelaars&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.509 (consider)&lt;/p&gt;
&lt;p&gt;We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object&amp;#x27;s rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.&lt;/p&gt;</content:encoded></item></channel></rss>